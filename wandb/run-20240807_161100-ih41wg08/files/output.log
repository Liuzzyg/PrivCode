/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/transformers/trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 1.1254, 'learning_rate': 5e-06, 'epoch': 0.01}
{'eval_loss': 1.0077983140945435, 'eval_runtime': 608.8769, 'eval_samples_per_second': 0.751, 'eval_steps_per_second': 0.751, 'epoch': 0.01}
{'loss': 0.8815, 'learning_rate': 4.998741355957963e-06, 'epoch': 0.02}
{'eval_loss': 0.7898814678192139, 'eval_runtime': 611.009, 'eval_samples_per_second': 0.748, 'eval_steps_per_second': 0.748, 'epoch': 0.02}
{'loss': 0.783, 'learning_rate': 4.994966691179712e-06, 'epoch': 0.03}
{'eval_loss': 0.7561112642288208, 'eval_runtime': 612.0942, 'eval_samples_per_second': 0.747, 'eval_steps_per_second': 0.747, 'epoch': 0.03}
{'loss': 0.7585, 'learning_rate': 4.988679806432712e-06, 'epoch': 0.04}
{'eval_loss': 0.7411835789680481, 'eval_runtime': 626.5246, 'eval_samples_per_second': 0.729, 'eval_steps_per_second': 0.729, 'epoch': 0.04}
{'loss': 0.7454, 'learning_rate': 4.9798870320769884e-06, 'epoch': 0.05}
{'eval_loss': 0.7326120734214783, 'eval_runtime': 624.2861, 'eval_samples_per_second': 0.732, 'eval_steps_per_second': 0.732, 'epoch': 0.05}
{'loss': 0.7363, 'learning_rate': 4.968597221690986e-06, 'epoch': 0.06}
{'eval_loss': 0.7270278334617615, 'eval_runtime': 613.5501, 'eval_samples_per_second': 0.745, 'eval_steps_per_second': 0.745, 'epoch': 0.06}
{'loss': 0.7301, 'learning_rate': 4.9548217431567665e-06, 'epoch': 0.07}
{'eval_loss': 0.723131000995636, 'eval_runtime': 623.4779, 'eval_samples_per_second': 0.733, 'eval_steps_per_second': 0.733, 'epoch': 0.07}
{'loss': 0.7243, 'learning_rate': 4.938574467213519e-06, 'epoch': 0.08}
{'eval_loss': 0.7202082872390747, 'eval_runtime': 610.7336, 'eval_samples_per_second': 0.748, 'eval_steps_per_second': 0.748, 'epoch': 0.08}
{'loss': 0.7193, 'learning_rate': 4.919871753490892e-06, 'epoch': 0.09}
{'eval_loss': 0.7179949879646301, 'eval_runtime': 605.3369, 'eval_samples_per_second': 0.755, 'eval_steps_per_second': 0.755, 'epoch': 0.09}
{'loss': 0.7158, 'learning_rate': 4.8987324340362445e-06, 'epoch': 0.1}
{'eval_loss': 0.716385543346405, 'eval_runtime': 606.518, 'eval_samples_per_second': 0.753, 'eval_steps_per_second': 0.753, 'epoch': 0.1}
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/transformers/trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/fzv6en/anaconda3/envs/fastdp/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 0.712, 'learning_rate': 4.875177794352364e-06, 'epoch': 0.11}
{'eval_loss': 0.7150253653526306, 'eval_runtime': 608.2182, 'eval_samples_per_second': 0.751, 'eval_steps_per_second': 0.751, 'epoch': 0.11}
{'loss': 0.7081, 'learning_rate': 4.849231551964771e-06, 'epoch': 0.12}
{'eval_loss': 0.7139631509780884, 'eval_runtime': 602.2491, 'eval_samples_per_second': 0.759, 'eval_steps_per_second': 0.759, 'epoch': 0.12}
{'loss': 0.7054, 'learning_rate': 4.8209198325401815e-06, 'epoch': 0.13}
