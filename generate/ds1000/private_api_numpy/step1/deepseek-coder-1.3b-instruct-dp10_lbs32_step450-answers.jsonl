{"id": 0, "code": "result = df.iloc[List]", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.iloc[List]\nprint(result)\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Count the number of occurrences for each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 2)\n\n# Create a dictionary to map the values to the new values\nmap_dict = {}\nfor i in range(len(counts.columns)):\n    map_dict[counts.columns[i]] = counts.columns[i]\n\n# Map the values to the new values\nresult = df.replace(map_dict)\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values to replace\nreplace_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {k: v for k, v in df.values}\n    # create a dictionary with the values of Qu3\n    dict_values_other = {k: v for k, v in dict_values.items() if k not in ['apple', 'egg']}\n    # create a dictionary with the values of Qu1 and Qu2 where the value of Qu1 is not in dict_values_other\n    dict_values_other_other = {k: v for k, v in dict_values.items() if k not in dict_values_other.keys()}\n    # create a dictionary with the values of Qu1 where the value of Qu1 is not in dict_values_other\n    dict_values_other_other_other = {k: v for k, v in dict_values.items() if k not in dict_values_other.keys() and k not in dict_values_other_other.keys()}\n    # create a dictionary with the values of Qu1 where the value of Qu1 is not in dict_values_other and not in dict_values_other_other\n    dict_values_other_other_other_other = {k: v for k, v in dict_values.items() if k not in dict_values_other.keys() and k not in dict_values_other_other.keys() and k not in dict_values_other_other_other.keys()}\n    # create a dictionary with the values of Qu1 where the value of Qu1 is not in dict_values_other and not in dict_values_other_other and not in dict_values_other_other_other\n    dict_values_other_other_other_other_other = {k: v for k, v in dict_values.items() if k not in dict_values_other.keys() and k not in dict_values_other_other.keys() and k not in dict_values_other_other_other.keys() and k not in dict_values_other_other_other_other.keys()}\n    # create a dictionary with the values of Qu1 where the value of Qu1 is not in dict_values_other and not in dict_values_other_other and not in dict_values_other_other_other and not in dict_values_other_other_other_other\n    dict_values_other_other_other_other_other_other = {k: v for k, v in dict_values.items() if k not in dict_values_other.keys() and k not in dict_values_other_other.keys() and k not in dict_values_other_other_other.keys() and k not in dict_values_other_other_other_other.keys() and k not in dict_values_other_other_other_other_other.keys()}\n    # create a dictionary with the values of Qu1 where the value of Qu1 is not in dict_values_other and not in dict_values_other_other and not in dict_values_other_other_other and not in dict_values_other_other_other_other and not in dict_values_other_other_other_other_other\n    dict_values_other_other_other_other_other_other_other = {k: v for k, v in dict_values.items() if k not in dict_values_other.keys() and k not in dict_values_other_other.keys() and k not in dict_values_other_other_other.keys() and k not in dict_values_other_other_other_other.keys() and k not in dict_values_other_other_other_other_other.keys() and k not in dict_values_other_other_other_other_other_other.keys()}\n    # create a dictionary with the values of Qu1 where the value of Qu1 is not in dict_values_other and not in dict_values_other_other and not in dict_values_other_other_other and not in dict_values_other_other_other_other and not in dict_values_other_other_other_other_other and not in dict_values_other_other_other_other_other_other\n    dict_values_other_other_other_other_other_other_other_other = {k: v for k, v in dict_values.items() if k not in dict_values_other.keys() and k not in dict_values_other", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Count the number of occurrences for each value in Qu1\ncounts = df['Qu1'].value_counts()\n\n# Create a new column with the values from Qu1 that have at least 3 occurrences\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if counts[x] < 3 else x)\n\n# Count the number of occurrences for each value in Qu2\ncounts = df['Qu2'].value_counts()\n\n# Create a new column with the values from Qu2 that have at least 2 occurrences\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if counts[x] < 2 else x)\n\n# Count the number of occurrences for each value in Qu3\ncounts = df['Qu3'].value_counts()\n\n# Create a new column with the values from Qu3 that have at least 2 occurrences\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if counts[x] < 2 else x)\n\nresult = df", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Count the number of times each value appears in each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a dictionary to map the values to the replacement values\nmapping = {\n    'apple': 'other',\n    'potato': 'other',\n    'sausage': 'other',\n    'cheese': 'other',\n    'banana': 'other',\n    'egg': 'egg'\n}\n\n# Replace the values in each column with the replacement values\nfor column in df.columns:\n    df[column] = df[column].map(mapping)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "result = df.drop_duplicates(subset='url', keep='last')\nresult", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndef convert_to_nested_dict(df):\n    result = {}\n    for index, row in df.iterrows():\n        if row['name'] not in result:\n            result[row['name']] = {}\n        result[row['name']][row['v1']] = {}\n        result[row['name']][row['v1']][row['v2']] = row['v3']\n    return result\n\nresult = convert_to_nested_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str.replace() function to remove the UTC offset\n    ### END SOLUTION ###\n    return df\n\ndef f(df=example_df):\n   ", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# Convert the message to a dictionary\nmessage_dict = {}\nfor i in range(len(df)):\n    message_dict[i] = eval(df['message'][i])\n\n# Create a new dataframe from the dictionary\nnew_df = pd.DataFrame(list(message_dict.items()), columns=['index', 'message'])\n\n# Extract the key value pairs\nresult = new_df['message'].apply(lambda x: x.split(': '))\n\n# Convert the result to a dataframe\nresult = result.apply(lambda x: pd.Series(x))\n\n# Expand the dataframe\nresult = result.apply(lambda x: pd.Series(x.tolist()))\n\n# Convert the result to a dataframe\nresult = result.apply(lambda x: pd.Series(x.tolist()))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "for product in products:\n    df.loc[df['product'] == product, 'score'] *= 10\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10\ndf['score'] = df['score'].where(df['product'].isin(products), 0)\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "for product in products:\n    df.loc[df['product'].between(product[0], product[1]), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "for product in products:\n    df.loc[df['product'] == product, 'score'] = (df.loc[df['product'] == product, 'score'] - df.loc[df['product'] == product, 'score'].min()) / (df.loc[df['product'] == product, 'score'].max() - df.loc[df['product'] == product, 'score'].min())", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = ''\nfor i in range(len(df)):\n    if df.iloc[i, 0] == 1:\n        df.iloc[i, 0] = 0\n    if df.iloc[i, 1] == 1:\n        df.iloc[i, 1] = 0\n    if df.iloc[i, 2] == 1:\n        df.iloc[i, 2] = 0\n    if df.iloc[i, 3] == 1:\n        df.iloc[i, 3] = 0\n    if df.iloc[i, 4] == 1:\n        df.iloc[i, 4] = 0\n    if df.iloc[i, 5] == 1:\n        df.iloc[i, 5] = 0\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'A'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 1:\n        df.iloc[i, 6] = 'B'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 1 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'C'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 1 and df.iloc[i, 4] == 1 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'D'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 1 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 1 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'E'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 1 and df.iloc[i, 3] == 1 and df.iloc[i, 4] == 1 and df.iloc[i, 5] == 1:\n        df.iloc[i, 6] = 'F'\n    if df.iloc[i, 0] == 1 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'G'\n    if df.iloc[i, 0] == 1 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 1:\n        df.iloc[i, 6] = 'H'\n    if df.iloc[i, 0] == 1 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = 'A'\ndf.loc[df['A'] == 1, 'category'] = 'B'\ndf.loc[df['B'] == 1, 'category'] = 'C'\ndf.loc[df['C'] == 1, 'category'] = 'D'\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndef convert_to_category(df):\n    categories = []\n    for i in range(len(df.columns)):\n        if df.iloc[:, i].sum() == 0:\n            categories.append(df.columns[i])\n        else:\n            categories.append(list(df.columns[i]))\n    return categories\n\ncategories = convert_to_category(df)\n\ndf['category'] = categories\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime(\"%b-%Y\")\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime(\"%d-%b-%Y\")\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime(\"%d-%b-%Y\")\ndf['Date'] = df['Date'].apply(lambda x: x[:4] + \"-\" + x[4:7] + \"-\" + x[7:])\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'] = df['Date'].apply(lambda x: x.strftime(\"%d-%b-%Y\"))\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x.strftime('%Y-%m-%d'))\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[0]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[0]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[0]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[0]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[0]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[0]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[0]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[0]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-05'] = shifted_df.loc['1980-01-05'].values[-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[-1]\nshifted_df.loc['1980-01-0", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x.strftime('%Y-%m-%d'))\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-01-01'] = shifted_df.loc['1980-01-01'].values[::-1]\nshifted_df.loc['1980-0", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x + ' ' + str(int(x.split('-')[1]) + 1))\nshifted_df.loc[0] = df.loc[0]\nshifted_df.loc[-1] = df.loc[-1]\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-1, '#2']\nshifted_df.loc[-1, '#2'] = shifted_df.loc[-1, '#2'] + shifted_df.loc[-1, '#1']\nshifted_df.loc[-1, '#1'] = shifted_df.loc[-1, '#1'] + shifted_df.loc[-", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\nshifted_df = df.shift(1)\n\n# shift the last row of the first column up 1 row\nshifted_df.loc[0] = df.loc[0]\n\n# calculate the R^2 values of the first and second columns\nr2_values = shifted_df['#1'].values[:-1]**2 + shifted_df['#2'].values[:-1]**2\n\n# create a new dataframe with the minimum R^2 values\nmin_r2_df = pd.DataFrame({'#1': shifted_df['#1'].values[:-1], '#2': shifted_df['#2'].values[:-1]}, index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# print the minimum R^2 values dataframe\nprint(min_r2_df)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "new_col_names = [f'HeaderAX' if i==0 else f'HeaderBX' if i==1 else f'HeaderCX' for i in range(len(df.columns))]\ndf.columns = new_col_names\nprint(df)\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "for col in df.columns:\n    df[col] = df[col] + 'X'\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "# Create a dictionary with the new column names\nnew_cols = {'HeaderA': 'HeaderAX',\n            'HeaderB': 'HeaderBX',\n            'HeaderC': 'HeaderCX',\n            'HeaderX': 'HeaderX'}\n\n# Use the dictionary to rename the columns\ndf = df.rename(columns=new_cols)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "result = df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' not in col})\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' not in col})\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndef get_mean_and_sum(group):\n    mean_val1 = df.loc[group, 'val1'].mean()\n    mean_val2 = df.loc[group, 'val2'].mean()\n    mean_val42 = df.loc[group, 'val42'].mean()\n    return mean_val1, mean_val2, mean_val42\n\nresult = df.groupby('group').apply(get_mean_and_sum)\nresult.columns = ['mean_val1', 'mean_val2', 'mean_val42']\nresult", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "def meanAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].mean(axis=axis)\n\nresult = meanAdvance(row_list, column_list, axis=0)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "def sumAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list, axis=0)\nprint(result)\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "def sumAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list, axis=0)\nresult.drop_duplicates(keep='first', inplace=True)\nresult", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.apply(lambda x: x.value_counts())\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "result = df.value_counts()\nresult", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0,1]]", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0,1]]", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "result = df.where(df.notnull(), 1).where(df.isnull(), 2)\nresult", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "result = df.apply(lambda x: x.where(x.isnull(), x.mean()), axis=1)\nresult", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "result = df.where(df.isnull(), np.nan)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "# Find rows where value is less than the threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# Sum the values of these rows\nresult = rows_less_than_thresh.sum()\n# Replace the rows with the sum with the original values\nresult = result.replace(rows_less_than_thresh.index, df.loc[rows_less_than_thresh.index, 'value'])\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "# find rows where value is less than threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# find rows where value is greater than threshold\nrows_greater_than_thresh = df[df['value'] >= thresh]\n# calculate average of values in rows_greater_than_thresh\navg_value = rows_greater_than_thresh['value'].mean()\n# replace rows in rows_greater_than_thresh with average value\nresult = rows_greater_than_thresh.assign(value=avg_value)\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "# find the rows in the section\nrows_in_section = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\n# calculate the average of the rows in the section\nresult = rows_in_section.mean()\n# replace the rows in the original dataframe with the average row\ndf.loc[section_left:section_right] = result\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "result = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndef exp_column(df, column):\n    return df[column]**(1/len(df[column]))\n\nfor column in df.columns:\n    df[f'exp_{column}'] = exp_column(df, column)\n\nresult = df\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "inv_A = df['A'].apply(lambda x: 1/x if x != 0 else 0)\ninv_B = df['B'].apply(lambda x: 1/x if x != 0 else 0)\n\nresult = pd.DataFrame({\"A\": df['A'], \"B\": df['B'], \"inv_A\": inv_A, \"inv_B\": inv_B})\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = sigmoid(df[col])\n\nresult = df", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = df.apply(lambda x: x[x == x.max()].index[-1], axis=1)\n\n# Create a mask to ignore the last occurrence of the column-wise maximum\nmask = np.arange(len(last_max_index)) != last_max_index\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the minimum value in each column\nmin_index = min_index[mask]\n\n# Create a new index with the last occurrence of the column-wise maximum\nnew_index = pd.Index(np.concatenate([last_max_index, min_index]))\n\n# Create a new DataFrame with the new index\nresult = pd.DataFrame(a, index=new_index, columns=list('abc'))\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Create a mask to ignore the rows before the minimum occurrence\nmask = np.arange(min_index.shape[0]) >= np.arange(min_index.shape[0])\n\n# Get the index of the first occurrence of the column-wise maximum\nmax_index = df.apply(lambda x: x[mask].argmax(), axis=1)\n\n# Create a new DataFrame with the maximum index\nresult = pd.DataFrame(max_index, index=min_index, columns=['max'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 0 for the val column\nresult = new_df.merge(df, on='dt', how='left').fillna(0)\nresult", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 0 for the val column\nresult = new_df.merge(df, on='dt', how='left').fillna(0)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = p", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Join the new data frame with the original data frame on the 'user' column\nresult = pd.merge(new_df, df, on='user')\n\n# Fill in the maximum val of the user for the val column\nresult['val'] = result.groupby('user')['val'].transform(max)\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "min_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# create a date range from the minimum to the maximum date\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# create a new data frame with the date range and the maximum value for each user\nresult = pd.DataFrame({'dt': date_range, 'user': df['user'], 'val': df['val'].max()})\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().reset_index()\nresult = result.set_index('user').sort_index().reset_index()\n\n# fill the date range with the maximum value for each user\nresult = result.set_index('dt').sort_index().", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "# Create a unique ID for each name\nunique_id = df['name'].apply(lambda x: x + 1)\n\n# Create a new column with unique ID\ndf['unique_id'] = unique_id\n\n# Replace the name with unique ID\nresult = df.replace({'name': {'Aaron': 1, 'Brave': 2, 'David': 3}})\n\nprint(result)\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "# Create a unique ID for each a value\nunique_id = df['a'].drop_duplicates().reset_index(drop=True)\n\n# Create a new column 'a' with unique ID\ndf['a'] = unique_id\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', '", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "# Create a unique ID for each name\nid_map = df['name'].drop_duplicates().to_dict()\n\n# Create a new column 'ID' with unique ID for each name\ndf['ID'] = df['name'].map(id_map)\n\n# Replace 'a' with unique ID\ndf['a'] = df['ID']\n\n# Replace 'b' with unique ID\ndf['b'] = df['ID']\n\n# Replace 'c' with unique ID\ndf['c'] = df['ID']\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = pd.pivot_table(df, values='value', index='user', columns='date', aggfunc='first')\n", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value')\ndf = df.rename(columns=lambda x: x.replace('/', ''))\ndf = df.reset_index()\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.pivot_table(index='user', columns='date', values='value', \n                    aggfunc=lambda x: x.values, \n                    margins=True)\n\n# replace None with NaN\ndf = df.replace(to_replace=None, value=np.nan)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df.c > 0.5][columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df['c'] > 0.45][columns].values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": " ###\n    # use the loc function to select the rows where the value in column 'c' is greater than 0.5\n    # then select the columns you want\n    # return the result\n    ### END SOLUTION ###\n\n# create a dataframe\ndf = pd.DataFrame({'a': [0.945686, 0.919359, 0.976163, 0.459039],\n                   'b': [0.000710, 0.667057, 0.621712, 0.788318],\n                   'c': [0.909158, 0.462478, 0.208423, 0.309892],\n                   'd': [0.892892, 0.008204, 0.980471, 0.100539],\n                   'e': [0.326670, 0.473096, 0.048334, 0.753992]})\n\n# call the function\nresult = f(df, ['b', 'e'])\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": " ###\n    # select the rows where column 'c' is greater than 0.5\n    # then select the columns 'b' and 'e'\n    # then compute the sum of these columns\n    # return the result\n    ### END SOLUTION ###\n    return df[df.c > 0.5][columns].sum()\n\n# create a dataframe\ndf = pd.DataFrame({'a': [0.945686, 0.919359, 0.976163, 0.459039],\n                   'b': [0.000710, 0.667057, 0.621712, 0.788318],\n                   'c': [0.909158, 0.462478, 0.208423, 0.309892],\n                   'd': [0.892892, 0.008204, 0.980471, 0.100539],\n                   'e': [0.326670, 0.473096, 0.048334, 0.753992]})\n\n# call the function\nprint f(df, ['b', 'e'])", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df[df[columns[0]] > 0.5][columns]\n    ### END SOLUTION\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint f(df, ['b', 'e'])\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndef find_overlap(row, X):\n    date_range = pd.date_range(start=row['date'], periods=X)\n    return date_range.isin(row['date'])\n\nresult = df[~df.apply(find_overlap, X=X, axis=1)]\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef find_overlap(row, X):\n    date = row['date']\n    date_format = pd.to_datetime(date)\n    dates = [date_format + pd.DateOffset(months=i) for i in range(X)]\n    return dates\n\ndef find_overlap_row(row, X):\n    dates = find_overlap(row, X)\n    return dates\n\ndef find_overlap_dates(df, X):\n    return df.apply(find_overlap_row, X=X, axis=1)\n\nresult = find_overlap_dates(df, X)\nresult", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef find_overlap(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v2(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v3(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v4(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v5(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v6(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v7(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v8(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v9(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v10(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v11(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v12(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v13(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\n# calculate the number of rows in the dataframe\nnum_rows = len(df)\n\n# calculate the number of bins\nnum_bins = num_rows // 3\n\n# create a new dataframe with the bins\nresult = pd.DataFrame({'col1':[num_rows]})\n\n# calculate the bin number for the last bin\nlast_bin = num_rows % 3\n\n# add the last bin to the result\nresult = result.append(pd.DataFrame({'col1':[last_bin]}))\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\n# Create a new column with the number of rows in each group\ndf['group'] = (df.index // 3).astype(int)\n\n# Group by the new column and count the number of rows in each group\nresult = df.groupby('group').size()\n\nprint(result)\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n# Create a new column with the bin number\ndf['bin'] = (df['col1'] // 4).astype(int)\n\n# Create a new column with the bin number\ndf['bin'] = (df['col1'] // 4).astype(int)\n\n# Group by bin and sum the values in each bin\nresult = df.groupby('bin').sum()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\n# Create a new column with the number of rows in each group\ndf['group'] = (df.index // 3).astype(int)\n\n# Group by the group column and calculate the mean of the column\nresult = df.groupby('group')['col1'].mean()\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[3:, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[2:, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[3:, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[2:, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[3:, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[2:, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[3:, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[2:, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[3:, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[2:, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[3:, 'col1'].sum()\n\n# Create a new column with the average of the first 2", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Calculate the sum of the last 3 rows\nlast_3_sum = last_3_rows['col1'].sum()\n\n# Calculate the average of the last 2 rows\nlast_2_avg = last_3_rows['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new = df.tail(3)\n\n# Calculate the sum of the last 3 rows of the original dataframe\nlast_3_sum_orig = df.tail(3)['col1'].sum()\n\n# Calculate the average of the last 2 rows of the original dataframe\nlast_2_avg_orig = df.tail(3)['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new = df.tail(3)\n\n# Calculate the sum of the last 3 rows of the original dataframe\nlast_3_sum_orig = df.tail(3)['col1'].sum()\n\n# Calculate the average of the last 2 rows of the original dataframe\nlast_2_avg_orig = df.tail(3)['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new = df.tail(3)\n\n# Calculate the sum of the last 3 rows of the original dataframe\nlast_3_sum_orig = df.tail(3)['col1'].sum()\n\n# Calculate the average of the last 2 rows of the original dataframe\nlast_2_avg_orig = df.tail(3)['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new = df.tail(3)\n\n# Calculate the sum of the last 3 rows of the original dataframe\nlast_3_sum_orig = df.tail(3)['col1'].sum()\n\n# Calculate the average of the last 2 rows of the original dataframe\nlast_2_avg_orig = df.tail(3)['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new = df.tail(3)\n\n# Calculate the sum of the last 3 rows of the original dataframe\nlast_3_sum_orig = df.tail(3)['col1'].sum()\n\n# Calculate the average of the last 2 rows of the original dataframe\nlast_2_avg_orig = df.tail(3)['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new = df.tail(3)\n\n# Calculate the sum of the last 3 rows of the original dataframe\nlast_3_sum_orig = df.tail(3)['col1'].sum()\n\n# Calculate the average of the last 2 rows of the original dataframe\nlast_2_avg_orig = df.tail(3)['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new = df.tail(3)\n\n# Calculate the sum of the last 3 rows of the original dataframe\nlast_3_sum_orig = df.tail(3)['col1'].sum()\n\n# Calculate the average of the last 2 rows of the original dataframe\nlast_2_avg_orig = df.tail(3)['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new = df.tail(3)\n\n# Calculate the sum of the last 3 rows of the original dataframe\nlast_3_sum_orig = df.tail(3)['col1'].sum()\n\n# Calculate the average of the last 2 rows of the original dataframe\nlast_2_avg_orig = df.tail(3)['col1'].mean()\n\n# Create a new dataframe with the last 3 rows of the original dataframe\nlast_3_rows_new =", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(method='ffill')\ndf = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\n# Find the index of the first zero after the first non-zero value\nfirst_zero_index = df[df['A'] == 0].index[1]\n\n# Find the index of the last zero before the last non-zero value\nlast_zero_index = df[df['A'] == 0].index[-1]\n\n# Find the maximum value between the last zero and the first zero\nmax_value = max(df['A'].loc[first_zero_index:last_zero_index])\n\n# Fill the zeros with the maximum value\ndf['A'] = df['A'].fillna(max_value)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['duration'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['time_days'] = df['time'].apply(lambda x: x.replace('year', 365).replace('day', 1).replace('week', 7).replace('month', 30))\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    df['duration'] = df['duration'].str.split(' ', 1).str[0]\n    df['number'] = df['duration'].str.split(' ', 1).str[1]\n    df['time'] = df['duration'].str.split(' ', 1).str[0]\n    df['time_days'] = df['time'].apply(lambda x: x.replace('year', 365).replace('day', 1).replace('week', 7).replace('month', 30))\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1].astype(int)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number']\nprint(df)\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.where(np.all(np.array([df1[column] == df2[column] for column in columns_check_list]), axis=1))", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = np.all(np.where(np.all([df1[column] == df2[column] for column in columns_check_list], axis=1), True, False), axis=1)", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: pd.to", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    # parse date index using pd.to_datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # swap the two levels\n    df = df.set_index(['id', 'date'])\n    ### END SOLUTION ###\n    return df\n\n# test the function\ndf = pd.DataFrame({'x': ['abc'], 'y': [100], 'date': ['3/1/1994']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot_table(index=['Country', 'year'], columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'Country': 'country', 'year': 'year', 'value': 'var1'})\ndf = df.drop(columns=['var2'])\ndf = df.sort_values(['country', 'year'])\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='var')\ndf = df.sort_values(['Country', 'year', 'Variable'], ascending=[True, True, True])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\n# Create a list of column names\ncolumn_names = [f'Value_{i}' for i in range(1, 11)]\n\n# Create a new data frame with only the columns that have a value less than 1\nnew_df = df[column_names]\n\n# Filter the new data frame to only include rows where all values are less than 1\nnew_df = new_df[new_df.max(axis=1) < 1]\n\n# Print the new data frame\nprint(new_df)\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\n# Create a list of column names\ncolumn_names = [f'Value_{i}' for i in range(1, 11)]\n\n# Create a new data frame with only the columns that have a value greater than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are greater than 1\nnew_df = new", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a list of column names that start with 'Value'\nvalue_columns = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df = df.replace({'&LT;': '<'}, regex=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    # replace &AMP; with '&'\n    df = df.replace({'&AMP;': '&'})\n    return df\n\nf(example_df)", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x.split(' ')[1] if ' ' in x else None)\n\nprint(df)\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['name'] = df['name'].apply(validate_single_space_name)\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge(df2, df1, on='Timestamp', how='left')\nresult['data'] = result['data'].fillna(result['stuff'])\nresult = result.drop(['stuff'], axis=1)\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge(df1, df2, on='Timestamp', how='left')\nresult['stuff'] = result['stuff'].astype(str)\nresult['stuff'] = result['stuff'].apply(lambda x: x.replace('.0', ''))\nresult['stuff'] = result['stuff'].astype(int)\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= 50, df['col1'], np.max(df[['col1', 'col2', 'col3']], axis=1))\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = np.where(df['col2'] > 50, df['col1'], np.sum(df[['col1', 'col2', 'col3']], axis=1))\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\nerrors = []\nfor index, row in df.iterrows():\n    if not pd.isnumeric(row['Field1']):\n        errors.append(row['Field1'])\n\nprint(errors)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndef get_integer_values(row):\n    values = row['Field1'].split(',')\n    return [int(val) for val in values if not isinstance(val, str)]\n\ndf['Field1'] = df.apply(get_integer_values, axis=1)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    # create a new column with the error values\n    df['Error'] = df['Field1'].apply(lambda x: x if pd.isnumeric(x) else 'and')\n    # return the new column\n    return df['Error']\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\n# calculate the total for each category\ntotal = df['val1'] + df['val2'] + df['val3'] + df['val4']\n\n# calculate the percentage for each value\npercentage = (df['val1'] / total) * 100\n\n# create a new dataframe with the percentage\nresult = pd.DataFrame({'cat': df['cat'], 'val1': percentage})\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "# calculate the pairwise distances between cars\ndistances = pd.DataFrame(np.sqrt((df['x'].values[:, None] - df['x'].values[None, :])**2 + (df['y'].values[:, None] - df['y'].values[None, :])**2))\n\n# find the nearest neighbour for each car\nnearest_neighbour = np.argmin(distances, axis=1)\n\n# calculate the average of the distances for each frame\navg_distances = distances.groupby('time').mean()\n\n# create a new dataframe with the nearest neighbour and the average distance\ndf2 = pd.DataFrame({'car': df['car'], 'nearest_neighbour': nearest_neighbour, 'euclidean_distance': avg_distances})\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "# Create a new column 'farmost_neighbour' that contains the farthest car for each time point\ndf['farmost_neighbour'] = df.groupby('time')['car'].transform('max')\n\n# Create a new column 'euclidean_distance' that contains the euclidean distance between each car and its farthest neighbour\ndf['euclidean_distance'] = df.groupby('time')['x'].transform('max') - df['x']\n\n# Calculate the average of the distances for each time point\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf['keywords_all'] = df[cols].apply(lambda x: ', '.join(x.dropna().values.tolist()), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n\ndef concat_cols(cols):\n    return \"-\".join(cols)\n\ndf['keywords_all'] = df[cols].apply(concat_cols, axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n\nfor col in cols:\n    df[col] = df[col].apply(lambda x: \"-\".join(x.dropna().tolist()))\n\nprint(df)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n\nfor col in cols:\n    df[col] = df[col].apply(lambda x: \"-\".join(x.dropna().values))\n\ndf['keywords_all'] = df[cols].apply(lambda x: \"-\".join(x.values), axis=1)\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Set Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep indexes of the altered rows\nsample_df.index = range(len(sample_df))\n\n# Print the result\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change value of ProductId to zero\nsample_df['ProductId'] = 0\n\n# Keep indexes of the altered rows\nsample_df.index = range(len(sample_df))\n\n# Print the result\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "# calculate the number of rows to select\nn = len(df) * 0.20\n\n# select the rows\nselected_rows = df.sample(n, random_state=0)\n\n# set Quantity to zero\nselected_rows['Quantity'] = 0\n\n# keep the indexes of the selected rows\nselected_rows.index = range(len(selected_rows))\n\n# print the selected rows\nprint(selected_rows)", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    # find the duplicates\n    duplicates = df.duplicated(subset=['col1','col2'], keep='first')\n    # find the index of the first duplicate\n    index_original = duplicates.index[duplicates]\n    # add a new column with the index of the first duplicate\n    df['index_original'] = index_original\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "min_count = df['count'].min()\nresult = df[df['count'] == min_count]", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "result = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                  ('A', 'B', 'F'),\n                                  ('A', 'C', 'G'),\n                                  ('A', 'C', 'H'),\n                                  ('A', 'D', 'I'),\n                                  ('A', 'D', 'J')])\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "result = pd.melt(df, value_vars=[list('BBCCDD'), list('EFGHIJ')], var_names=['variable_0', 'variable_1', 'variable_2'], value_name='value')\nresult", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] < 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), 0)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), 0)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l')['v'].apply(lambda x: x.sum() if x.isnull().any() else x.sum())\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-to-many'\n    elif column1.min() == column2.max():\n        return 'many-to-one'\n    elif column1.min() == column2.min():\n        return 'one-to-many'\n    elif column1.max() == column2.max():\n        return 'many-to-many'\n    else:\n        return 'none'\n\nresult = []\nfor i in range(df.shape[1]):\n    for j in range(i + 1, df.shape[1]):\n        result.append(f'{df.columns[i]} {df.columns[j]} {relationship(df[df.columns[i]], df[df.columns[j]])}')\n\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-2-many'\n    elif column1.min() == column2.max():\n        return 'one-2-one'\n    else:\n        return 'many-2-many'\n\nresult = []\nfor i in range(df.shape[1]):\n    for j in range(i + 1, df.shape[1]):\n        result.append(f'{df.columns[i]} {df.columns[j]} {relationship(df[df.columns[i]], df[df.columns[j]])}')\n\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def relationship(column1, column2, column3, column4, column5):\n    if column1.equals(column2) and column1.equals(column3) and column1.equals(column4) and column1.equals(column5):\n        return 'one-to-one'\n    elif column2.equals(column3) and column2.equals(column4) and column2.equals(column5):\n        return 'one-to-many'\n    elif column3.equals(column4) and column3.equals(column5):\n        return 'many-to-one'\n    elif column4.equals(column5):\n        return 'many-to-many'\n    else:\n        return 'none'\n\nresult = pd.DataFrame({\n    'Column1': ['one-to-one'],\n    'Column2': ['one-to-many'],\n    'Column3': ['many-to-one'],\n    'Column4': ['many-to-many'],\n    'Column5': ['none']\n})\n\nfor i in range(5):\n    for j in range(i + 1, 5):\n        result.loc[i, j] = relationship(df.iloc[:, i], df.iloc[:, j], df.iloc[:, i], df.iloc[:, j], df.iloc[:, i])\n\nprint(result)\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship(x, y):\n    if x.shape[0] == y.shape[0]:\n        return 'one-2-many'\n    elif x.shape[0] == y.shape[1]:\n        return 'many-2-one'\n    elif x.shape[1] == y.shape[0]:\n        return 'many-2-many'\n    else:\n        return 'one-2-one'\n\nresult = pd.DataFrame(columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5'])\nfor i in range(df.shape[1]):\n    for j in range(i + 1, df.shape[1]):\n        result.loc[i, j] = relationship(df[df.columns[i]], df[df.columns[j]])\n\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\nresult = dfiban_uniq\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install numpy\n!pip", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\n# Create a new column 'Family'\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\n\n# Group by 'Family' and calculate the mean of 'Survived'\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\n# Create a new column 'Family'\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\n\n# Group by 'Family' and calculate the mean of 'SibSp'\nresult = df.groupby('Family')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "group_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\n", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "def std_mean(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('a').apply(std_mean)\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "def mean_std(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('b').apply(mean_std)\nresult", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x, axis=0)\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = softmax(df['b'])\ndf['min-max'] = min_max(df['b'])\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "result = df.loc[:, (df.sum(axis=0) != 0)]\nresult = result.loc[:, (result.sum(axis=1) != 0)]\nresult", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "result = df[df.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "max_value = df.max().max()\nresult = df[df[max_value] == 2]", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "result = df.where(df == 2, 0)\nresult", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "s = s.sort_values(ascending=True)\nresult = s.sort_index()\nresult", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "sorted_s = s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].apply(lambda x: isinstance(x, str))]", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "min_count = df['count'].min()\nresult = df[df['count'] == min_count]", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": " ###\n    # map the dict to the date column\n    df['Date'] = df['Member'].map(dict)\n    # fill NaN with the date from the dict\n    df['Date'] = df['Date'].fillna(df['Member'].map(dict))\n    ### END SOLUTION ###\n    return df\n\nresult = f(example_dict, example_df)\nprint(result)\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17-Aug-1926')\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Year']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date','Year']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Year']).Date.transform('count')\nprint(df)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date','Val']).Date.transform('count')\nprint(df)", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_w'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date','Val']).Date.transform('count')\nprint(df)\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date')['B'].apply(lambda x: x.sum())\nresult2 = df.groupby('Date')['C'].apply(lambda x: x.sum())\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "result1 = df[df['B'] % 2 == 0].groupby('Date').count()\nresult2 = df[df['B'] % 2 != 0].groupby('Date').count()\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = np.sum(result['D'])\nresult['E'] = np.mean(result['E'])\nresult", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['mean'] = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = np.sum(result['D'])\nresult['E'] = np.mean(result['E'])\nresult", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\ndef split_string_to_rows(s):\n    return s.split(',')\n\ndf['var1'] = df['var1'].apply(lambda x: x.split(',')[0])\ndf['var2'] = df['var2'].apply(lambda x: x.split(',')[1])\n\nresult = df\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\n# Split the column into multiple rows\nresult = df.assign(var2=lambda x: x['var2'].str.split(','))\n\n# Flatten the column\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(i) for i in x]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\n# Convert the dataframe to a dask dataframe\ndask_df = dask.dataframe.from_pandas(df, npartitions=1)\n\n# Split the column into multiple rows\nresult = dask_df.assign(var2=lambda x: x['var2'].str.split('-'))\n\n# Convert the result to a pandas dataframe\nresult = result.to_pandas()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df['fips'] = df['row'].str[:-1]\ndf['row'] = df['row'].str[-1]\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df['fips'] = df['row'].str[:4]\ndf['medi'] = df['row'].str[4:]\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\n# calculate the average for each row\nfor col in df.columns:\n    if col != 'Name':\n        df[col] = df[col].apply(lambda x: x / (x.sum() if x.sum() != 0 else 1))\n\n# calculate the cumulative average for each row\nfor col in df.columns:\n    if col != 'Name':\n        df[col] = df[col].cumsum() / (df[col].sum() if df[col].sum() != 0 else 1)\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndef calculate_cumulative_average(df):\n    for i in range(1, 7):\n        df[f'2006-{i}'] = df[f'2006'] / i\n        df[f'2006-{i}'] = df[f'2006-{i}'] * (i - 1) / i\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    # calculate the cumulative average for each row using pandas\n    # ignore zero values\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x:", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        if col != 'Name':\n            df[col] = df[col].rolling(window=len(df[col])).mean()\n            df[col] = df[col].where(df[col] != 0, df[col])\n            df[col] = df[col].rolling(window=len(df[col])).mean()\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)) > 0\ndf['label'] = df['label'].apply(lambda x: 1 if x else 0)\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\n# Create a new column 'label'\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n\n# Set the label for the first row to 1\ndf.loc[0, 'label'] = 1\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: x[0] if x[0] > 0 else x[1] if x[1] > 0 else x[2])\ndf['DateTime'] = df['DateTime'].dt.strftime(\"%d-%b-%Y\")\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_datetime(df.departure_time) - pd.to_datetime(df.arrival_time)\ndf['Duration'] = df.Duration.apply(lambda x: str(x).split(' ')[0])\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = (df['departure_time'] - df['arrival_time']).apply(lambda x: x.seconds)\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "\n# Convert the arrival and departure times to datetime\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n\n# Calculate the duration in seconds\ndf['Duration'] = (df['departure_time'] - df['arrival_time']).apply(lambda x: x.total_seconds())\n\n# Convert the duration to seconds\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x:", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df[df['key2'].endswith('e')].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.index[df.value.argmax()]\nmin_result = df.index[df.value.argmin()]\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df['value'].mode()\nmedian_result = df['value'].median()\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(99 <= df['closing_price']) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df[~(99 <= df['closing_price'] <= 101)]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "min_diff = df.groupby(\"item\")[\"diff\"].min()\nmin_otherstuff = df.groupby(\"item\")[\"otherstuff\"].min()\n\nresult = df[df[\"diff\"] == min_diff]\nresult = result[result[\"otherstuff\"] == min_otherstuff]\n", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "\ndef get_last_underscore(s):\n    return s.split('_')[-1] if '_' in s else s\n\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(get_last_underscore)\nprint(df)\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "\ndef get_last_underscore(s):\n    return s.split('_')[-1] if '_' in s else s\n\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(get_last_underscore)\nprint(df)\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df\n\nf(example_df)\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "# calculate the number of NaN values\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# calculate the number of NaN values to fill\nnan_to_fill = nan_count // 2\n\n# fill the NaN values with the mode of the column\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0])\n\n# fill the first nan_to_fill values with 0 and the last nan_to_fill values with 1\ndf['Column_x'] = np.where(np.isnan(df['Column_x']), 0, 1)\n\n# print the dataframe\nprint(df)\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "# calculate the number of NaN values\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# calculate the number of NaN values to fill\nnan_to_fill = nan_count // 3\n\n# fill the NaN values with the first 30% of the first value\ndf['Column_x'][nan_to_fill:] = 0\n\n# fill the NaN values with the first 30% of the second value\ndf['Column_x'][nan_to_fill:nan_to_fill*2] = 0.5\n\n# fill the NaN values with the last 30% of the first value\ndf['Column_x'][nan_to_fill*2:] = 1\n\n# fill the NaN values with the last 30% of the second value\ndf['Column_x'][nan_to_fill*2:nan_to_fill*3] = 0.5\n\n# print the original data\nprint(df)\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "a_b = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0])], columns=['one', 'two'])\nresult = a_b", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "a_b = a.assign(one=lambda x: x.values.tolist())\nb_c = b.assign(one=lambda x: x.values.tolist())\nc_a = c.assign(one=lambda x: x.values.tolist())\n\nresult = a_b.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x.values.tolist())\nresult = result.assign(one=lambda x: x.values.tolist())\n\nresult = result.assign(two=lambda x: x", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "# Create a new dataframe with the same shape as a and b\na_b = pd.DataFrame(np.nan, index=a.index, columns=a.columns)\n\n# Iterate over the rows of a and b\nfor i in range(a.shape[0]):\n    # Iterate over the columns of a and b\n    for j in range(a.shape[1]):\n        # Set the value at the current row and column to a and b\n        a_b.loc[i, j] = (a.loc[i, j], b.loc[i, j])\n\n# Print the result\nprint(a_b)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins).value_counts()).fillna(0)\nresult = groups.reset_index()\nresult.columns = ['username', 'views']\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins).value_counts()).reset_index()\nresult = groups.set_index('username')\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins).value_counts()).fillna(0)\nresult = groups.reset_index()\nresult.columns = ['username', 'views']\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', ')))\n", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).values.tolist()\nresult = pd.DataFrame({'text': result})\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', ')))\nresult = result.values.reshape(-1, 1)\nresult = pd.DataFrame(result, columns=['text'])\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].to_series().str.split(', ', expand=True).apply(lambda x: ', '.join(x))\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame('text')\nresult", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.concat([df1, df2], axis=0)\nresult['city'] = result['city'].apply(lambda x: df2[df2['id'] == x]['city'].values[0])\nresult['district'] = result['district'].apply(lambda x: df2[df2['id'] == x]['district'].values[0])\nresult['date'] = result['date'].apply(lambda x: df2[df2['id'] == x]['date'].values[0])\nresult['value'] = result['value'].apply(lambda x: df2[df2['id'] == x]['value'].values[0])\nresult = result.sort_values('id')\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n# merge df1 and df2 on id\nmerged = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# convert date to datetime\nmerged['date'] = pd.to_datetime(merged['date'])\n\n# sort by date and id\nmerged = merged.sort_values(['date', 'id'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# convert date to string\nmerged['date'] = merged['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# print result\nprint(merged)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\n# merge df1 and df2 on id\nmerged = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by id and date\nmerged = merged.sort_values(['id', 'date'])\n\n# group by id and date, and fill NaN with NaN\nmerged = merged.groupby(['id', 'date']).apply(lambda x: x.fillna(np.nan))\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by id and date\nmerged = merged.sort_values(['id', 'date'])\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# print result\nprint(merged)\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x']\nresult['B_y'] = D['B']\nresult = result.sort_values('A')\nresult['B'] = result['B_x']\nresult.drop(['B_x'], axis=1, inplace=True)\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x']\nresult['B_y'] = D['B']\nresult = result.sort_values('A')\nresult['B_y'] = result['B_y'].replace(np.nan, result['B_x'])\nresult = result.drop(['B_x'], axis=1)\nresult", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['dulplicated'] = result['A'].apply(lambda x: x in D['A'])\nresult", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist())\nprint(result)\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).tolist())\nresult", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist())\nresult", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "df_concatenated = pd.concat([series.values.reshape(-1, 1) for series in series.values], axis=1)\ndf_concatenated.columns = series.index\nprint(df_concatenated)", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\n# Convert the series to a dataframe\ndf = series.to_frame()\n\n# Rename the columns\ndf.columns = ['name']\n\n# Add a new column with the index\ndf['index'] = df.index\n\n# Concatenate the dataframe\ndf_concatenated = df.join(series.to_frame(), how='left')\n\n# Print the dataframe\nprint(df_concatenated)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.columns[~df.columns.str.contains(s)].tolist()[0]\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "# Find the column names that contain the string 'spike'\nspike_cols = [col for col in df.columns if s in col]\n\n# Find the column names that contain the string 'spike-'\nspike_cols_with_dash = [col for col in spike_cols if col.startswith('spike-')]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike_cols_with_dash]\n\n# Find the column names that contain the string 'spike'\nspike_cols_without_dash = [col for col in spike_cols if col not in spike", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# Create a new dataframe with the first row of codes as a column\nnew_df = pd.DataFrame(df['codes'].values[0].tolist(), columns=[f'code_{i}' for i in range(len(df['codes'].values[0]))])\n\n# Create a new dataframe with the rest of the rows of codes\nnew_df = new_df.append(pd.DataFrame(df['codes'].values[1:].tolist(), columns=[f'code_{i}' for i in range(len(df['codes'].values[1]))]))\n\n# Fill the NaN values with NaN\nnew_df = new_df.fillna(np.nan)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original dataframe\nnew_df = pd.DataFrame(df.index)\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# Convert the list to a numpy array\narr = df['codes'].to_numpy()\n\n# Create a new dataframe with the same shape as the original dataframe\nnew_df = pd.DataFrame(np.zeros(arr.shape), columns=np.arange(1, arr.shape[1]+1))\n\n# Iterate over the rows of the original dataframe\nfor i in range(arr.shape[0]):\n    # Iterate over the columns of the original dataframe\n    for j in range(arr.shape[1]):\n        # Set the value of the new dataframe at the current row and column\n        new_df.loc[i, j] = arr[i, j]\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "result = df['col1'].apply(lambda x: x[0])\nresult = result.tolist()\nresult = [item for sublist in result for item in sublist]\nprint(result)\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x[::-1])))\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x)))\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "# Convert Time to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Create bins\nbins = pd.date_range(start=df['Time'].min(), end=df['Time'].max(), freq='2m')\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2 minutes\nbins = bins[1:]\n\n# Create bins for each 2", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "# Convert Time to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Create bins\nbins = pd.date_range(start=df['Time'].min(), end=df['Time'].max(), freq='3m')\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins.to_datetime()\n\n# Create bins for each 3-minute interval\nbins = bins", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[np.where(np.isnan(df).any(axis=1))]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.columns[np.all(np.isnan(df.iloc[0]) ^ np.isnan(df.iloc[8]))]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[np.isnan(df.iloc[0]).values].tolist() + df.columns[np.isnan(df.iloc[8]).values].tolist()\n", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(np.nan, np.nan) if np.isnan(x) and np.isnan(y) else (x, y) for x, y in zip(df.iloc[0], df.iloc[8])]\nprint(result)\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.stack().reset_index(level=1, drop=True).unstack().reset_index(level=1, drop=True)\ndf = df.rename(columns=lambda x: f'{x}_1')\ndf = df.rename(columns=lambda x: f'{x}_2')\ndf = df.rename(columns=lambda x: f'{x}_3')\ndf = df.rename(columns=lambda x: f'{x}_4')\ndf = df.rename(columns=lambda x: f'{x}_5')\ndf = df.rename(columns=lambda x: f'{x}_6')\ndf = df.rename(columns=lambda x: f'{x}_7')\ndf = df.rename(columns=lambda x: f'{x}_8')\ndf = df.rename(columns=lambda x: f'{x}_9')\ndf = df.rename(columns=lambda x: f'{x}_10')\ndf = df.rename(columns=lambda x: f'{x}_11')\ndf = df.rename(columns=lambda x: f'{x}_12')\ndf = df.rename(columns=lambda x: f'{x}_13')\ndf = df.rename(columns=lambda x: f'{x}_14')\ndf = df.rename(columns=lambda x: f'{x}_15')\ndf = df.rename(columns=lambda x: f'{x}_16')\ndf = df.rename(columns=lambda x: f'{x}_17')\ndf = df.rename(columns=lambda x: f'{x}_18')\ndf = df.rename(columns=lambda x: f'{x}_19')\ndf = df.rename(columns=lambda x: f'{x}_20')\ndf = df.rename(columns=lambda x: f'{x}_21')\ndf = df.rename(columns=lambda x: f'{x}_22')\ndf = df.rename(columns=lambda x: f'{x}_23')\ndf = df.rename(columns=lambda x: f'{x}_24')\ndf = df.rename(columns=lambda x: f'{x}_25')\ndf = df.rename(columns=lambda x: f'{x}_26')\ndf = df.rename(columns=lambda x: f'{x}_27')\ndf = df.rename(columns=lambda x: f'{x}_28')\ndf = df.rename(columns=lambda x: f'{x}_29')\ndf = df.rename(columns=lambda x: f'{x}_30')\ndf = df.rename(columns=lambda x: f'{x}_31')\ndf = df.rename(columns=lambda x: f'{x}_32')\ndf = df.rename(columns=lambda x: f'{x}_33')\ndf = df.rename(columns=lambda x: f'{x}_34')\ndf = df.rename(columns=lambda x: f'{x}_35')\ndf = df.rename(columns=lambda x: f'{x}_36')\ndf = df.rename(columns=lambda x: f'{x}_37')\ndf = df.rename(columns=lambda x: f'{x}_38')\ndf = df.rename(columns=lambda x: f'{x}_39')\ndf = df.rename(columns=lambda x: f'{x}_40')\ndf = df.rename(columns=lambda x: f'{x}_41')\ndf = df.rename(columns=lambda x: f'{x}_42')\ndf = df.rename(columns=lambda x: f'{x}_43')\ndf = df.rename(columns=lambda x: f'{x}_44')\ndf = df.rename(columns=lambda x: f'{x}_45')\ndf = df.rename(columns=lambda x: f'{x}_46')\ndf = df.rename(columns=lambda x: f'{x}_47')\ndf = df.rename(columns=lambda x:", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.set_index(['A','B','C','D','E']).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "def avg(columns):\n    return np.mean(df[columns].values, axis=1)\n\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\ndf['Avg'] = avg(list_of_my_columns)\nprint(df)", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "for column in list_of_my_columns:\n    df[column] = df[column].mean()\n\nprint(df)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by='time')", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(['treatment','dose','time'])", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "# Create a list of dates to delete\ndates_to_delete = ['2020-02-17', '2020-02-18']\n\n# Create a new column 'Date' with the date in the correct format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Delete the dates from the DataFrame\ndf = df[~df['Date'].isin(dates_to_delete)]\n\n# Set the index to the new date column\ndf.set_index('Date', inplace=True)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Create a new column 'Day' to store the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr[np.abs(corr) > 0.3].stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr[corr > 0.3].values", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "last_column = df.columns[-1]\nlast_column_renamed = last_column + '_renamed'\n\ndf.rename(columns={last_column: last_column_renamed}, inplace=True)\nresult = df", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "result = df.rename(columns={df.columns[0]: 'Test'})", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndef find_frequent(row):\n    return row.value_counts().index[0]\n\ndef find_freq_count(row):\n    return row.value_counts().values[0]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndef find_frequent(row):\n    return row.value_counts().index[0]\n\ndef find_freq_count(row):\n    return row.value_counts().values[0]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    return [row[i] for i in range(len(row)) if row[i] == row.max()]\n\ndef find_freq_count(row):\n    return len([row[i] for i in range(len(row)) if row[i] == row.max()])\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "res = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nres = res.reset_index()\nres", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "res = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nres['bar'] = res['bar'].apply(lambda x: x.replace('NULL', 0))\nres", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult.drop(['b_col'], axis=1, inplace=True)\nresult", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result.drop(['a_col'], axis=1)\nresult", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = np.nan_to_num(x, nan=np.nan)\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "x = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nx = x[~np.isnan(x)]", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((a.shape[0], 4))\nb[:, 0] = a == 0\nb[:, 1] = a == 1\nb[:, 2] = a == 2\nb[:, 3] = a == 3\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((a.shape[0], a.max() + 1))\nb[np.arange(a.shape[0]), a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((a.shape[0], a.max() + 1))\nb[np.arange(a.shape[0]), a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i, j] = a[i, j] + 1\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "def shift(a, shift):\n    return np.roll(a, shift, axis=1)\n\nresult = shift(a, shift)\nprint(result)", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "def shift(a, shift):\n    return np.roll(a, shift, axis=1)\n\nresult = shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a), a.shape, order='C')", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a), a.shape)\n    return max_index\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2, axis=-1)[:-2, -1], a.shape)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "z = np.any(np.isnan(a), axis=0)\na = np.delete(a, z, axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = np.delete(a, np.where(np.isnan(a)))", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.take(a, permutation, axis=1)", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.take(a, permutation, axis=0)", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravel_index(np.argmin(a), a.shape)\nresult = (min_index[0], min_index[1])\nprint(result)", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = (max_index[0], max_index[1])\nprint(result)", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "min_index = np.unravel_index(np.argmin(a, axis=None), a.shape)\nresult = np.array([[0, 1], [1, 0]])\nprint(min_index)\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.degrees(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.degrees(degree))", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "def is_degree(number):\n    return number >= 0 and number <= 90\n\ndef is_radian(number):\n    return number > 90 and number <= 180\n\nif is_degree(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * 180 / np.pi\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(a, 2))\nprint(f(a, 3))\nprint(f(a, 4))\nprint(f(a, 5))\nprint(f(a, 6))\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nprint(result)", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    # Use numpy's floor_divide function to divide the numerator and denominator by 100\n    # Then return the result as a tuple\n    return np.floor_divide(numerator, 100), np.floor_divide(denominator, 100)\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(98, 42)) # (7, 3)\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), result.astype(int))\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(a, np.maximum(b, c))", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.flatten())[::-1]\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a.flatten())[::-1]\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.concatenate((np.diag(a), np.diag(a.T)[::-1]))", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(a), np.diag(a[:, ::-1])))", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = np.array([x for x in X.flatten()])", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.sort(np.sort(X, axis=1), axis=0)", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = np.empty((0, 1), dtype=np.object)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result = np.append(result, X[i, j])\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty(X.shape[0], dtype=np.object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.fromstring(mystr, dtype=int, sep='')\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum set of linearly independent vectors\nresult = np.where(np.sum(a, axis=1) == max_independent_vectors)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "# calculate the t-statistic\nt_stat = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_stat, a.shape[0] + b.shape[0])\nprint(p_value)", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "# calculate the t-statistic\nt_stat = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_stat, a.shape[0] + b.shape[0])\nprint(p_value)", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the weighted mean and variance\nweight_a = 1 / (anobs * (anobs - 1))\nweight_b = 1 / (bnobs * (bnobs - 1))\n\n# calculate the weighted mean and variance\nmean_a = amean * weight_a\nmean_b = bmean * weight_b\nvar_a = avar * weight_a\nvar_b = bvar * weight_b\n\n# calculate the t-statistic\nt_stat = (mean_a - mean_b) / np.sqrt((var_a / anobs) + (var_b / bnobs))\n\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_stat, anobs + bnobs - 2)\n\nprint(p_value)", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.asarray([A[~np.in1d(A, B)]])", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, 0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "del_col = np.sort(del_col)[::-1]\nresult = np.delete(a, del_col, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a_l = a.tolist()\n    a_l.insert(pos, element)\n    a = np.asarray(a_l)\n    return a\n    ### END SOLUTION ###\n\nprint(f(a=example_a, pos=2, element=66))\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.all(a == a[0], axis=1))", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.all(a == a[0], axis=1))", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.all(a == a[0], axis=1))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    # calculate the 2D integral over (cosx)^4 + (siny)^2\n    # use Simpson's rule\n    # result = np.sum(np.sum(np.sin(x)**2 + np.cos(y)**4, axis=1))\n    ### END SOLUTION ###\n    return result\n\n# call the function\nf(x, y)", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "# compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow = np.min(grades[grades < threshold])\nhigh = np.max(grades[grades >= threshold])\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.randint(2, size=size)\nnums[nums == 0] = one_ratio\nnums[nums == 1] = 1 - one_ratio\n", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.tensor(a)", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = np.array(a)", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a) + 1\nresult = result.tolist()\nresult", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.sort(a)[::-1][:N]", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "patch_size = 2\npatch_count = a.shape[0] // patch_size\nresult = np.zeros((patch_count, patch_count, patch_size, patch_size))\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "patch_size = 2\npatch_count = a.shape[0] // patch_size\nresult = np.zeros((patch_count, patch_count, patch_size, patch_size))\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "patch_size = 2\npatch_count = a.shape[0] // patch_size\nresult = np.zeros((patch_count, patch_count, patch_size, patch_size))\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\npatch_shape = a.shape[:-1] + (a.shape[-1] // patch_size, patch_size)\n\nresult = np.array([a[i:i + patch_shape[0], j:j + patch_shape[1]] for i in range(0, a.shape[0], patch_size) for j in range(0, a.shape[1], patch_size)])\n\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\nresult", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\npatch_shape = a.shape[:-1] + (a.shape[-1] // patch_size, patch_size)\n\nresult = np.array([a[i:i + patch_shape[0], j:j + patch_shape[1]] for i in range(0, a.shape[0], patch_size) for j in range(0, a.shape[1], patch_size)])\n\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.fromstring(string)\nprint(a)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.log(np.random.uniform(min, max, n))", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.log(np.random.uniform(np.exp(min), np.exp(max), n))", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.log(np.random.uniform(min, max, n))\n    ### END SOLUTION ###\n\nprint(f(min=1, max=np.e, n=10000))\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "def sub2ind(dims, indices):\n    return np.ravel_index(indices, dims)\n\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "def sub2ind(dims, index):\n    return np.asscalar(np.prod(dims)) + np.asscalar(np.prod(index))\n\nresult = sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df.values.dtype)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.sum(a[accmap])\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0], axis=0)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.min(a[index])", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "def elementwise_function(element_1,element_2):\n    return (element_1 + element_2)\n\nz = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    z[i] = elementwise_function(x[i], y[i])\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, probabilit)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "a = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\n\n# Create a zero array of the same shape as a\nzero_array = np.zeros_like(a)\n\n# Create a slice of zero_array with the same shape as a\nslice_zero_array = zero_array[low_index:high_index, low_index:high_index]\n\n# Create a slice of a with the same shape as slice_zero_array\nslice_a = a[low_index:high_index, low_index:high_index]\n\n# Create a slice of zero_array with the same shape as slice_a\nslice_zero_array_a = zero_array[low_index:high_index, low_index:high_index]\n\n# Create a slice of a with the same shape as slice_zero_array_a\nslice_a_a = a[low_index:high_index, low_index:high_index]\n\n# Create a slice of zero_array with the same shape as slice_a_a\nslice_zero_array_a_a = zero_array[low_index:high_index, low_index:high_index]\n\n# Create a slice of a with the same shape as slice_zero_array_a_a\nslice_a_a_a = a[low_index:high_index, low_index:high_index]\n\n# Create a slice of zero_array with the same shape as slice_a_a_a\nslice_zero_array_a_a_a = zero_array[low_index:high_index, low_index:high_index]\n\n# Create a slice of a with the same shape as slice_zero_array_a_a_a\nslice_a_a_a_a = a[low_index:high_index, low_index:high_index]\n\n# Create a slice of zero_array with the same shape as slice_a_a_a_a\nslice_zero_array_a_a_a_a = zero_array[low_index:high_index, low_index:high_index]\n\n# Create a slice of a with the same shape as slice_zero_array_a_a_a_a\nslice_a_a_a_a_a = a[low_index:high_index, low_index:high_index]\n\n# Create a slice of zero_array with the same shape as slice_a_a_a_a_a\nslice_zero_array_a_a_a_a_a = zero_array[low_index:high_index, low_index:high_index]\n\n# Create a slice of a with the same shape as slice_zero_array_a_a_a_a_a\nslice_a_a_a_a_a_a = a[low_index:high_index, low_index:high_index]\n\n# Create a slice of zero_array with the same shape as slice_a_a_a_a_a_a\nslice_zero_array_a_a_a_a_a_a = zero_array[low_index:high_index, low_index:high_index]\n\n# Create a slice of a with the same shape as slice_zero_array_a_a_a_a_a_a\nslice_a_a_a_a_a_a_a = a[low_index:high_index, low_index:high_index]\n\n# Create a slice of zero_array with the same shape as slice_a_a_a_a_a_a_a\nslice_zero_array_a_a_a_a_a_a_a = zero_array[low_index:high_index, low_index:high_index]\n\n# Create a slice of a with the same shape as slice_zero_array_a_a_a_a_a_a_a\nslice_a_a_a_a_a_a_a_a = a[low_index:high_index, low_index:high_index]\n\n# Create a slice of zero_array with the same shape as slice_a_a_a_a_a_a_a_a\nslice_zero_array_a_a_a_a_a_a_a_a = zero_array[low_index:high_index, low_index:high_index]\n\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "x = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "x = np.array([-2+1j, -1.4, -1.1, 0, 1.2, 2.2+2j, 3.1, 4.4, 8.3, 9.9, 10+0j, 14, 16.2])\nx = x[x.real != 0]\nresult = x", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_max = np.max(bin_data, axis=1)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (x - x_min) * (x - x_max)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    x_min = np.clip(x_min, x, x_max)\n    x_max = np.clip(x_max, x, x_min)\n    x_min_min = np.clip(x_min, x, x_min)\n    x_max_max = np.clip(x_max, x, x_max)\n    x_min_min_min = np.clip(x_min_min, x, x_min_min)\n    x_max_max_max = np.clip(x_max_max, x, x_max_max)\n    x_min_min_min_min = np.clip(x_min_min_min, x, x_min_min_min)\n    x_max_max_max_max = np.clip(x_max_max_max, x, x_max_max_max)\n    x_min_min_min_min_min = np.clip(x_min_min_min_min, x, x_min_min_min_min)\n    x_max_max_max_max_max = np.clip(x_max_max_max_max, x, x_max_max_max_max)\n    x_min_min_min_min_min_min = np.clip(x_min_min_min_min_min, x, x_min_min_min_min_min)\n    x_max_max_max_max_max_max = np.clip(x_max_max_max_max_max, x, x_max_max_max_max_max)\n    x_min_min_min_min_min_min_min = np.clip(x_min_min_min_min_min_min, x, x_min_min_min_min_min_min)\n    x_max_max_max_max_max_max_max = np.clip(x_max_max_max_max_max_max, x, x_max_max_max_max_max_max)\n    x_min_min_min_min_min_min_min_min = np.clip(x_min_min_min_min_min_min_min, x, x_min_min_min_min_min_min)\n    x_max_max_max_max_max_max_max_max = np.clip(x_max_max_max_max_max_max_max, x, x_max_max_max_max_max_max)\n    x_min_min_min_min_min_min_min_min_min = np.clip(x_min_min_min_min_min_min_min_min, x, x_min_min_min_min_min_min)\n    x_max_max_max_max_max_max_max_max_max = np.clip(x_max_max_max_max_max_max_max_max, x, x_max_max_max_max_max_max)\n    x_min_min_min_min_min_min_min_min_min_min = np.clip(x_min_min_min_min_min_min_min_min_min, x, x_min_min_min_min_min_min)\n    x_max_max_max_max_max_max_max_max_max_max = np.clip(x_max_max_max_max_max_max_max_max_max, x, x_max_max_max_max_max_max)\n    x_min_min_min_min_min_min_min_min_min_min_min_min = np.clip(x_min_min_min_min_min_min_min_min_min_min_min, x, x_min_min_min_min_min_min)\n    x_max_max_max_max_max_max_max_max_max_max_max_max = np.clip(x_max_max_max_max_max_max_max_max_max_max_max, x, x_max_max_max_max_max_max)\n    x_min_min_min_min_min_min_min_", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "# Create a circular correlation matrix\ncorr_matrix = np.correlate(a, b, mode='full')\n# Remove the first element of the correlation matrix\ncorr_matrix = corr_matrix[1:]\n# Print the result\nprint(corr_matrix)", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(4)])\nresult = np.reshape(result, (4, 15, 5))\nprint(result)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(15)])\nprint(result)", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.unpackbits(np.uint8(a))\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:m]\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "def convert_to_binary(a, m):\n    binary_array = np.unpackbits(np.uint8(a))\n    binary_array = binary_array[:m]\n    return binary_array\n\nresult = convert_to_binary(a, m)\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc_3\u03c3 = \u03bc + 3*\u03c3\n\u03bc_3\u03c3_start = \u03bc - 3*\u03c3\n\u03bc_3\u03c3_end = \u03bc + 3*\u03c3\nresult = (\u03bc_3\u03c3_start, \u03bc_3\u03c3_end)\nprint(result)", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc_minus_2\u03c3 = \u03bc - 2*\u03c3\n\u03bc_plus_2\u03c3 = \u03bc + 2*\u03c3\nresult = (\u03bc_minus_2\u03c3, \u03bc_plus_2\u03c3)\nprint(result)", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std = np.std(a)\n    third_std_dev = np.mean(a) + 3*std\n    return third_std_dev\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd = np.std(a)\nlower_bound = mean - 2*std\nupper_bound = mean + 2*std\noutlier = a < lower_bound\nresult = outlier | a > upper_bound\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data, percentile)\nprint(prob)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[0, 1] = 0", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.all(a == np.max(a, axis=1, keepdims=True), axis=1)", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1), axis=1)", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "result = np.corrcoef(post, distance)[0, 1]", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.matmul(X, np.transpose(X, (1, 0)))", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((Y.shape[0], Y.shape[1]))\nfor i in range(Y.shape[1]):\n    X[:, i] = np.dot(Y[:, i], Y[:, i].T)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = np.any(a == number)", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\nC = A[~np.isin(A, B)]", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = np.array([x for x in A if x not in B])", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "# Create a boolean array that indicates whether the elements in A are in (1, 4) or (4, 8)\nmask = np.logical_or(np.logical_and(A == 1, B == 4), np.logical_and(A == 4, B == 8))\n# Use the mask to index A and B\nC = A[mask]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.max(rankdata(a)) - rankdata(a) + 1", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.max(rankdata(a)) - rankdata(a) + 1", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    return np.max(a) - np.min(a) + 1 - np.min(a)\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:, second, third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.sum(np.abs(X), axis=1)\nresult = X / l1.reshape(-1, 1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.array([LA.norm(v,ord=2) for v in X])\nresult = np.array([v/x for v in X])\nprint(result)", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = np.array([v/x for v in X])\nprint(result)", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = [a['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "result = np.sqrt(np.sum((a[:, None, :] - a[None, :, :])**2, axis=-1))", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "result = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if i != j:\n            result[i, j] = np.sqrt(np.sum((a[i]-a[j])**2))", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "import numpy as np\n\ndef calculate_distance(a):\n    n = a.shape[0]\n    result = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            result[i, j] = np.sqrt(np.sum((a[i] - a[j]) ** 2))\n    return result\n\nresult = calculate_distance(a)\nprint(result)\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "a = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\na = np.unique(a)\na = a[a!=0]\nresult = a", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "# Find the index of the first non-zero value\nindex = np.where(a != 0)[0][0]\n\n# Create a new array with the first non-zero value and all the subsequent values\nresult = a[index:].reshape(-1, 1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "lat_val = np.concatenate((lat, val), axis=1)\nlon_val = np.concatenate((lon, val), axis=1)\n\ndf = pd.DataFrame({'lat': lat_val[:, 0], 'lon': lon_val[:, 0], 'val': lat_val[:, 1]})\n\nprint(df)\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "lat_val = np.max(lat, axis=1)\nlon_val = np.max(lon, axis=1)\nval_val = np.max(val, axis=1)\n\ndf = pd.DataFrame({'lat': lat_val, 'lon': lon_val, 'val': val_val})\ndf['maximum'] = np.max(df, axis=1)\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, window_size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1]-window_size[0]+1, window_size[1])\n    strides = a.strides + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape, strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, window_size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1]-window_size[0]+1, window_size[1])\n    strides = a.strides + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape, strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:,-1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "print(c in CNTS) # prints True\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "print(c in CNTS) # True\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.diag(i)\nprint(i)", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a = np.where(np.triu(a, 1) == 0, 0, a)", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\n\n# calculate the number of days between the start and end dates\ndays = (end_date - start_date).days\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening = n\n\n# calculate the number of intervening elements\nn_intervening", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    result = -1\nelse:\n    result = result + 1\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))\nresult = indices[0]", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "# calculate the coefficients a, b and c\na, b, c = np.linalg.solve(np.vstack([x, np.ones(len(x))]), y)\nresult = np.array([a, b, c])\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "# calculate the coefficients\ncoeffs = np.polyfit(x, y, degree)\n# sort the coefficients in descending order\nresult = np.sort(coeffs)[::-1]\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = np.array([0, 1, 2, 3])\ntemp_df = pd.DataFrame(np.repeat([1, 2, 3, 4], 4).reshape(4, -1))\nresult = temp_df.apply(lambda x: x - temp_arr[x])\nprint(result)", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\nprint(result)", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "mask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr < -10\narr[mask] += 5\n\nmask = arr >= 15\narr[mask] += 5\n\nmask = arr < -10\narr[mask] = 0\n\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr = np.where(arr < n1, 0, np.where(arr >= n2, 30, arr + 5))", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(s1 != s2)[0].shape[0]", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "# compare the two arrays and find the number of different elements\nresult = np.nonzero(s1 != s2)[0].shape[0]\n# print the result\nprint(result)\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.all(np.array(a) == np.array(a[0]))", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all(axis=1) for arr in a)", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the maximum shape of the shape\n    max_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "# calculate the maximum shape\nmax_shape = np.max(shape)\n# calculate the padding\npadding = np.array([(max_shape - shape[0]) // 2, (max_shape - shape[1]) // 2])\n# zero pad the array\nresult = np.pad(a, padding, mode='constant')\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=2)", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=1)", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = np.where(df['a'] > 1, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "# Create a mask that is True for all rows except the first and last row\nmask = np.any(im[1:-1, :] != 0, axis=1)\n# Create a mask that is True for all columns except the first and last column\nmask_col = np.any(im[:, 1:-1] != 0, axis=0)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_col = np.any(im[:, 1:-1] != 0, axis=0)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col = np.any(im[:, 1:-1] != 0, axis=0)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col = np.any(im[:, 1:-1] != 0, axis=0)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row_row = np.any(im[:, 1:-1] != 0, axis=0)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row_row_row = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row_row_row_row = np.any(im[:, 1:-1] != 0, axis=0)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row_row_row_row_row = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row_row_row_row_row_row = np.any(im[:, 1:-1] != 0, axis=0)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row_row_row_row_row_row_row = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row_row_row_row_row_row_row_row = np.any(im[:, 1:-1] != 0, axis=0)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all_row_col_col_row_col_row_row_row_row_row_row_row_row_row = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "# Find the indices of the non-zero elements\nindices = np.where(A != 0)\n# Find the bounding box of the non-zero elements\nmin_row, max_row, min_col, max_col = np.min(indices[0]), np.max(indices[0]), np.min(indices[1]), np.max(indices[1])\n# Slice the array\nresult = A[min_row:max_row+1, min_col:max_col+1]", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "def remove_peripheral_zeros(im):\n    row_count = np.sum(im, axis=1)\n    col_count = np.sum(im, axis=0)\n    row_mask = row_count > 0\n    col_mask = col_count > 0\n    return im[row_mask, :][:, col_mask]\n\nresult = remove_peripheral_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "# Find the row and column indices of the first and last non-zero pixels in each row and column\nrow_indices = np.where(np.any(im, axis=1))\ncol_indices = np.where(np.any(im, axis=0))\n\n# Find the first and last non-zero pixels in each row and column\nfirst_row_indices = np.min(row_indices[0])\nlast_row_indices = np.max(row_indices[0])\nfirst_col_indices = np.min(col_indices[0])\nlast_col_indices = np.max(col_indices[0])\n\n# Create a new array with the rows and columns from the first and last non-zero pixels\nresult = im[first_row_indices:last_row_indices+1, first_col_indices:last_col_indices+1]\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 5))\nsns.scatterplot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "for i in range(10):\n    y = np.random.randn(10)\n    plt.plot(x, y, label=f'line {i}')\n\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "for i in range(10):\n    y = np.random.randn(10)\n    plt.plot(x, y, label=f'line {i}')\n\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, marker='d')\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 5))\nsns.lineplot(x, y, marker='d', linewidth=2, color='red')\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "highlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nplt.plot(x, highlight_range, highlight_color)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = x * 2 + 2\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    style=\"Gender\",\n    height=5,\n    aspect=1,\n)\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "sns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "legend_title = 'xyz'\nplt.figure(figsize=(10, 5))\nplt.plot(x, y, label=legend_title)\nplt.legend(title=legend_title, title_fontsize=20)\nplt.title('Legend Title')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"k\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.setp(l, color='r')\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "x_rotated = np.rot90(x, 1, 1)\ny_rotated = np.rot90(y, 1, 1)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "x_rotated = np.rot90(x, 1, 1)\ny_rotated = np.rot90(y, 1, 1)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.linspace(0, 2 * np.pi, 10)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='best')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "xlabel = \"X\"\nplt.xlabel(xlabel)\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticks(g.get_xticks() + 90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# fit a very long title myTitle into multiple lines\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = np.sort(y)[::-1]\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.linspace(0, 1.5, 11)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "y_ticks = np.linspace(-1, 1, 11)\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "x_y_z = np.stack([x, y, z], axis=1)\nx_y_z_sorted = np.sort(x_y_z, axis=1)\n\n# plot x, then y then z, but so that x covers y and y covers z\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "x = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.ticklabel_format(style='plain', axis='y')\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "# Create a new data frame with the x and y values\nnew_data = pd.DataFrame({'x': x, 'y': y})\n\n# Create a dashed line on the lineplot\nsns.lineplot(data=new_data, x='x', y='y', linestyle='-', color='red')\n\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y1, label='sin(x)')\naxs[1].plot(x, y2, label='cos(x)')\naxs[0].set_xlabel('x')\naxs[1].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[1].set_ylabel('y2')\naxs[0].legend()\naxs[1].legend()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[0].set_title('sin(x)')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y2')\naxs[1].set_title('cos(x)')\naxs[1].legend()\n\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.ticklabel_format(axis='x', style='plain')\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4, 1))\nplt.grid(True, axis='x')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.grid(True, which='both')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(min(y), max(y) + 1, 1))\nplt.xticks(np.arange(min(x), max(x) + 1, 1))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "# axes = axes.flatten()\n# for ax in axes:\n#     ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n#     ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n# plt.tight_layout()\n# plt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlabel(\"Column Labels\")\nax.set_ylabel(\"Row Labels\")\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Y')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y over X')\nplt.legend()\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.tick_params(axis='x', which='both', labelleft=False, labelright=False, left=False, right=False, labelright=False, labelleft=False)\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False, labelright=False, left=False, right=False)\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y, label='Y')\nplt.ylabel('Y')\nplt.xlabel('X')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "# Create a joint regression plot\nsns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', color='green')\n\n# Create a distribution plot\nsns.distplot(tips['total_bill'], color='blue')\n\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "# Create a new dataframe with the regression line\nregression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                   'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                   'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                   'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                   'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                   'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                   'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                   'tip': np.linspace(tips['tip'].", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "# Create a joint regression plot\nsns.jointplot(data=tips, x='total_bill', y='tip', kind='reg')\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 5))\nfor i, celltype in enumerate(df[\"celltype\"]):\n    plt.bar(i, df[\"s1\"][i], label=celltype)\n\nplt.xlabel(\"celltype\")\nplt.ylabel(\"s1\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 5))\nfor i, celltype in enumerate(df[\"celltype\"]):\n    plt.bar(i, df[\"s1\"][i], label=celltype)\n    plt.bar(i, df[\"s2\"][i], label=celltype, bottom=df[\"s1\"][i])\n    plt.xlabel(\"celltype\")\n    plt.xticks(range(len(df[\"celltype\"])), df[\"celltype\"], rotation=45)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Plot of Y over X')\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend(loc='upper left')\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\nx = [0.22058956, 0.33088437, 2.20589566]\ny = [0, 0, 0]\nplt.plot(x, y, '--')\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, aspect='auto')\nplt.xlabel(xlabels, rotation=90)\nplt.ylabel(ylabels)\nplt.tick_params(axis='x', which='both', labelbottom=False, labeltop=False, labelleft=False, labelright=False)\nplt.tick_params(axis='y', which='both', labelbottom=False, labeltop=False, labelleft=False, labelright=False)\nplt.show()  # END SOLUTION!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "# copy the code of the above plot and edit it to have legend for all three cruves in the two subplots\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\n\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "# make a data frame from a, b and c\ndf = pd.DataFrame({'a': a, 'b': b, 'c': c})\n\n# make scatter plot\nplt.scatter(df['a'], df['b'])\n\n# annotate each data point with correspond numbers in c\nfor i, (x, y) in enumerate(zip(df['a'], df['b'])):\n    plt.annotate(str(c[i]), (x, y), textcoords=\"data\")\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(labelcolor='black', title='Legend')\nplt.bold_legend()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, bins=10, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, ax1 = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n\n# Make the first subplot.\nax1.plot(x, y, 'r')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('First subplot')\n\n# Make the second subplot.\nax2 = ax1.twin()\nax2.plot(x, y, 'b')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nax2.set_title('Second subplot')\n\n# Make the third subplot.\nax3 = ax1.twin()\nax3.plot(x, y, 'g')\nax3.set_xlabel('x')\nax3.set_ylabel('y')\nax3.set_title('Third subplot')\n\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Set the labels of the x and y axis\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Set the title of the chart\nplt.title('Histogram of x and y')\n\n# Set the legend of the chart\nplt.legend()\n\n# Show the chart\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 5))\nplt.hist([x, y], bins=10, alpha=0.5, label=['x', 'y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "x = np.linspace(0, 5, 100)\ny = a * x + b\n\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2, 1)\n\n# create two colormaps\ncmap_x = plt.get_cmap('viridis')\ncmap_y = plt.get_cmap('viridis')\n\n# create two subplots\nax[0].imshow(x, cmap=cmap_x)\nax[1].imshow(y, cmap=cmap_y)\n\n# create a colorbar for the two subplots\ncbar_x = fig.colorbar(ax[0].imshow(x, cmap=cmap_x), ax=ax[0])\ncbar_y = fig.colorbar(ax[1].imshow(y, cmap=cmap_y), ax=ax[1])\n\n# set the colorbar's position\ncbar_x.set_position([0.9, 0.1])\ncbar_y.set_position([0.9, 0.9])\n\n# set the colorbar's label\ncbar_x.set_label('x')\ncbar_y.set_label('y')\n\n# set the title of the figure\nfig.set_title('Colorbar')\n\n# show the figure\nplt.show()\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(a, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].set_title('Y and Z')\naxs[0].legend()\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "x, y = zip(*points)\n\nplt.plot(x, y)\nplt.yscale('log')\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "x_log_y_log = np.log10(x) + np.log10(y)\n\nplt.loglog(x_log_y_log, x, x_log_y_log, y, 'o')\nplt.xlabel('log(x) + log(y)')\nplt.ylabel('x')\nplt.title('log-log plot of x and y')\n\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "# Calculate the total sum of data\ntotal_sum = np.sum(data)\n\n# Normalize the data to sum up to 1\nnormalized_data = [x / total_sum for x in data]\n\n# Format the y tick labels into percentage\ny_tick_labels = [str(int(x * 100)) + '%' for x in normalized_data]\n\n# Create the histogram\nplt.hist(normalized_data, bins=10, edgecolor='black')\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 110, 10))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 110, 10))\nplt.xlabel('Data')\n\n# Set the title\nplt.title('Histogram of Data')\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, alpha=0.5, linestyle='-')\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\nfig.legend(loc='upper right')\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 10))\n\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", ax=axs[0])\n\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"flipper_length_mm\", ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xlim(1, 10)\nax.set_xticks(range(1, 10))\nax.set_xlabel(\"second\")\nax.set_xlim(1, 10)\nax.set_xticks(range(1, 10))\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "x_extra = np.arange(2.1, 7.6, 0.1)\nplt.xticks(np.concatenate([x_extra, x]))\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(np.arange(2010, 2020), rotation=-60)\nplt.tight_layout()\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.yticks(np.arange(0, 10, 1))\nplt.xticks(np.arange(2010, 2020, 1))\nplt.plot(x, y)\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.tick_params(axis='x', which='both', labelsize=10, color='gray', alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(0.01)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(0.01)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\naxs.plot(x, y, label='y')\naxs.plot(x, y, label='y')\naxs.set_title('Figure')\naxs.legend()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df[\"Type A\"], label=\"Type A\")\nplt.plot(df[\"Type B\"], label=\"Type B\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='|')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none')\nplt.plot(x, y, '|', color='red', linewidth=1, hatch='-')\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*', hatch='v')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "xlim = [1, 5]\nylim = [1, 4]\n\nplt.figure(figsize=(10, 10))\nplt.imshow(data, extent=[xlim[0], xlim[1], ylim[0], ylim[1]], aspect='auto')\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "x = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y, label='sin(x)')\nplt.plot(x, np.zeros_like(x), label='cutoff')\nplt.legend()\n\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 5))\nplt.bar(labels, height, width=0.5, color='blue')\nplt.plot([0, 1], [0, 1], color='red')\nplt.plot([0, 1], [-1, -1], color='red')\nplt.plot([0, 1], [-1, 0], color='red')\nplt.plot([0, 1], [-1, 1], color='red')\nplt.plot([0, 1], [-1, 2], color='red')\nplt.plot([0, 1], [-1, 3], color='red')\nplt.plot([0, 1], [-1, 4], color='red')\nplt.plot([0, 1], [-1, 5], color='red')\nplt.plot([0, 1], [-1, 6], color='red')\nplt.plot([0, 1], [-1, 7], color='red')\nplt.plot([0, 1], [-1, 8], color='red')\nplt.plot([0, 1], [-1, 9], color='red')\nplt.plot([0, 1], [-1, 10], color='red')\nplt.plot([0, 1], [-1, 11], color='red')\nplt.plot([0, 1], [-1, 12], color='red')\nplt.plot([0, 1], [-1, 13], color='red')\nplt.plot([0, 1], [-1, 14], color='red')\nplt.plot([0, 1], [-1, 15], color='red')\nplt.plot([0, 1], [-1, 16], color='red')\nplt.plot([0, 1], [-1, 17], color='red')\nplt.plot([0, 1], [-1, 18], color='red')\nplt.plot([0, 1], [-1, 19], color='red')\nplt.plot([0, 1], [-1, 20], color='red')\nplt.plot([0, 1], [-1, 21], color='red')\nplt.plot([0, 1], [-1, 22], color='red')\nplt.plot([0, 1], [-1, 23], color='red')\nplt.plot([0, 1], [-1, 24], color='red')\nplt.plot([0, 1], [-1, 25], color='red')\nplt.plot([0, 1], [-1, 26], color='red')\nplt.plot([0, 1], [-1, 27], color='red')\nplt.plot([0, 1], [-1, 28], color='red')\nplt.plot([0, 1], [-1, 29], color='red')\nplt.plot([0, 1], [-1, 30], color='red')\nplt.plot([0, 1], [-1, 31], color='red')\nplt.plot([0, 1], [-1, 32], color='red')\nplt.plot([0, 1], [-1, 33], color='red')\nplt.plot([0, 1], [-1, 34], color='red')\nplt.plot([0, 1], [-1, 35], color='red')\nplt.plot([0, 1], [-1, 36], color='red')\nplt.plot([0, 1], [-1, 37], color='red')\nplt.plot([0, 1], [-1, 38], color='red')\nplt.plot([0, 1], [-1, 39], color='red')\nplt.plot([0, 1], [-1, 40], color='red')\nplt.plot([0, 1],", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedge_width=0.4)\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.grid(True)\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle='-', linewidth=0.5, color='gray')\nplt.grid(which='major', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title(\"Pie Chart\")\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title(\"Pie Chart\")\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, '--', alpha=0.5, edgecolor='none')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color='green')\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Specify the positions of the bars\nx = np.arange(len(blue_bar))\n# Specify the width of the bars\nwidth = np.ones(len(blue_bar))\n\n# Specify the colors of the bars\ncolor_blue = 'b'\ncolor_orange = 'orange'\n\n# Specify the labels of the bars\nlabel_blue = 'Blue Bar'\nlabel_orange = 'Orange Bar'\n\n# Specify the positions of the bars\nx_blue = x - width / 2\nx_orange = x + width / 2\n\n# Specify the heights of the bars\nheight_blue = blue_bar\nheight_orange = orange_bar\n\n# Specify the bars' colors\ncolor_blue_bar = color_blue\ncolor_orange_bar = color_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars' labels\nlabel_blue_bar = label_blue\nlabel_orange_bar = label_orange\n\n# Specify the bars' width\nwidth_blue_bar = width\nwidth_orange_bar = width\n\n# Specify the bars' heights\nheight_blue_bar = height_blue\nheight_orange_bar = height_orange\n\n# Specify the bars'", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "x = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.axis([0, 10, 0, 10])\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(data=df, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"species\", sharey=False, kind=\"bar\")\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(10, 10))\nplt.plot([0.5], [0.5], 'ro')\nplt.plot([0.5, 0.5], [0, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0], [0.5, 0], 'r')\nplt.plot([0.5, 0], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0], [0.5, 0], 'r')\nplt.plot([0.5, 0], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontsize=16, bold=True)\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Line')\nplt.legend(labelspacing=0.1)\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Line')\nplt.legend(handlelength=0.3)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.plot(x, y, marker=\"*\", label=\"Line\")\nplt.legend(loc='best')\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, cmap='hot')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(1)\nplt.plot(x, y)\nplt.title(\"Figure 1\", bold=True)\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolor='black')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax, x_val, y_val in zip(axs.flat, x, y):\n    ax.plot(x_val, y_val)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('y over x')\n\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('x')\nplt.ylabel('frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 5))\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.fill_between(x, y-error, y+error, alpha=0.2)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Error bars')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 0], 'w')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "for i in range(len(box_errors)):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], fmt='o', color=c[i])\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True)\nax1.plot(x, y, label='Y')\nax1.set_title('Y')\nax2.plot(a, z, label='Z')\nax2.set_title('Z')\nax2.set_position([[0, 0], [1, 1]])\nax2.set_ylabel('Z')\nax2.set_xlabel('A')\nax2.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor ax, x_val, y_val in zip(axs.flat, x, y):\n    ax.plot(x_val, y_val)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'x={x_val}, y={y_val}')\n    ax.set_xticks(x_val)\n    ax.set_yticks(y_val)\n    ax.set_xticks(x_val, minor=True)\n    ax.set_yticks(y_val, minor=True)\n    ax.grid(True, which='minor', axis='x', linestyle='-', linewidth=0.5)\n    ax.grid(True, which='minor', axis='y', linestyle='-', linewidth=0.5)\n\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1)\nplt.matshow(d, fignum=2)\nplt.matshow(d, fignum=3)\nplt.matshow(d, fignum=4)\nplt.matshow(d, fignum=5)\nplt.matshow(d, fignum=6)\nplt.matshow(d, fignum=7)\nplt.matshow(d, fignum=8)\nplt.matshow(d, fignum=9)\nplt.matshow(d, fignum=10)\nplt.matshow(d, fignum=11)\nplt.matshow(d, fignum=12)\nplt.matshow(d, fignum=13)\nplt.matshow(d, fignum=14)\nplt.matshow(d, fignum=15)\nplt.matshow(d, fignum=16)\nplt.matshow(d, fignum=17)\nplt.matshow(d, fignum=18)\nplt.matshow(d, fignum=19)\nplt.matshow(d, fignum=20)\nplt.matshow(d, fignum=21)\nplt.matshow(d, fignum=22)\nplt.matshow(d, fignum=23)\nplt.matshow(d, fignum=24)\nplt.matshow(d, fignum=25)\nplt.matshow(d, fignum=26)\nplt.matshow(d, fignum=27)\nplt.matshow(d, fignum=28)\nplt.matshow(d, fignum=29)\nplt.matshow(d, fignum=30)\nplt.matshow(d, fignum=31)\nplt.matshow(d, fignum=32)\nplt.matshow(d, fignum=33)\nplt.matshow(d, fignum=34)\nplt.matshow(d, fignum=35)\nplt.matshow(d, fignum=36)\nplt.matshow(d, fignum=37)\nplt.matshow(d, fignum=38)\nplt.matshow(d, fignum=39)\nplt.matshow(d, fignum=40)\nplt.matshow(d, fignum=41)\nplt.matshow(d, fignum=42)\nplt.matshow(d, fignum=43)\nplt.matshow(d, fignum=44)\nplt.matshow(d, fignum=45)\nplt.matshow(d, fignum=46)\nplt.matshow(d, fignum=47)\nplt.matshow(d, fignum=48)\nplt.matshow(d, fignum=49)\nplt.matshow(d, fignum=50)\nplt.matshow(d, fignum=51)\nplt.matshow(d, fignum=52)\nplt.matshow(d, fignum=53)\nplt.matshow(d, fignum=54)\nplt.matshow(d, fignum=55)\nplt.matshow(d, fignum=56)\nplt.matshow(d, fignum=57)\nplt.matshow(d, fignum=58)\nplt.matshow(d, fignum=59)\nplt.matshow(d, fignum=60)\nplt.matshow(d, fignum=61)\nplt.matshow(d, fignum=62)\nplt.matshow(d, fignum=63)\nplt.matshow(d, fignum=64)\nplt.matshow(d, f", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nplt.table(\n    df,\n    col_width=[1, 1, 1, 1],\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    cellcolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.grid(True)\nplt.xlim([0, 10])\nplt.ylim([0, 10])\nplt.xticks(np.arange(0, 11, 1))\nplt.yticks(np.arange(0, 11, 1))\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"diet\"] == \"No Fat\"]\n\n# Create a scatter plot with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nsns.scatterplot(data=new_df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\n\n# Set the title of the plot\nplt.title(\"Group: No Fat\")\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "# Create a scatter plot for each kind of exercise\nfor kind in df['kind'].unique():\n    # Get the data for the current kind of exercise\n    data = df[df['kind'] == kind]\n    # Create a scatter plot for the data\n    plt.scatter(data['time'], data['pulse'], c=data['diet'], hue=data['kind'], alpha=0.5, edgecolor='k')\n    # Set the x-axis label to \"time\"\n    plt.xlabel('time')\n    # Set the y-axis label to \"pulse\"\n    plt.ylabel('pulse')\n    # Set the title to the current kind of exercise\n    plt.title(kind)\n    # Show the subplot\n    plt.subplot(1, 1, 1)\n    # Show the plot\n    plt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Plot of y over x')\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='y')\nplt.legend(borderless=True)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 5))\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(data=df, x=\"sex\", y=\"bill_length_mm\", hue=\"species\")\nstripplot.set_axis_labels(\"sex\", \"bill_length_mm\")\nstripplot.legend(loc=\"upper left\")\nplt.show()\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_names=[\"a\"], col_names=[\"b\"], share_all=True)\n\n# Create a pointplot of \"c\" over \"a\"\nfacegrid.plot(\n    sns.pointplot,\n    x=\"a\",\n    y=\"c\",\n    hue=\"b\",\n    palette=\"Set3\",\n    err_style=\"bars\",\n    err_bars=\"mean\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2,\n    err_bars_color=\"black\",\n    err_bars_width=0.2", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_zlim([0, 1])\nax.set_view_angle(30, 30)\nax.set_view_angle(100, 50)\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tick_params(axis='x', which='both', labelleft=False, labelright=False, left=False, right=False, labelbottom=False, labeltop=False, bottom=False, top=False, labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_per_axis=0.1, height_per_axis=0.1)\n\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.imshow(x[i])\n    ax.axis('off')\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nx = tf.cast(x, tf.int32)\nprint(x.numpy())\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nx = tf.cast(x, tf.int32)\nprint(x.numpy())", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, 10)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, 10)\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, 10)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    # Create a tensor of zeros with the same shape as the labels\n    result = tf.zeros_like(labels)\n\n    # Iterate over the labels and set the value at the corresponding index to 1\n    for i in range(len(labels)):\n        result[i, labels[i]] = 1\n\n    return result\n\n# Call the function\nprint(f())\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, 10)\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.compat.v1.py_func(\n    func=lambda i: [i, i+1, i+2],\n    inp=[i],\n    Tout=[tf.int64]\n  )\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=my_map_func)\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return tf.concat([input, input + 1, input + 2], axis=1)\n\ndef map_func(input):\n    return tf.reshape(f(input), [-1])\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(map_func)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\nmask = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(mask == 1, 1, 0)\nresult = tf.where(result == 0, 1, 0)\nprint(result)\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "lengths = tf.constant([4, 3, 5, 2])\ntotal_length = 8\n\n# Create a mask of 1s and 0s with the length of the total length\nmask = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of 1s with the length of the lengths\nmask_one_length = tf.ones((total_length, 1))\n\n# Create a mask of 0s with the length of the lengths\nmask_zero_length = tf.zeros((total_length, 1))\n\n# Create a mask of ", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [0, 8-lengths.shape[0]]])\nmask = tf.cast(lengths_padded > 0, tf.float32)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.reshape(result, (4, 8))\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths),", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\nmask = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded > 0, mask, 0)\nresult = tf.reshape(result, (4, 8))\nprint(result)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([a, b], axis=1)\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.reshape(result, [2, -1])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1])\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    # Use tf.stack to combine the two tensors\n    # Then use tf.reshape to convert the result to a 2D array\n    # Finally use tf.transpose to swap the rows and columns\n    # result = tf.transpose(tf.reshape(tf.stack([a, b]), [2, -1]))\n    ### END SOLUTION ###\n    return result\n\nprint(f(a=example_a,b=example_b))\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "a = tf.reshape(a, (50, 100, 512))\nresult = a", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "a = tf.reshape(a, (50, 100, 1, 512))", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "a = tf.reshape(a, (1, 50, 100, 1, 512))\nresult = a", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.math.prod(A, axis=1)", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "# calculate the square of the difference between the two embeddings\ndiff = tf.square(a - b)\n\n# reduce the square difference by row\nresult = tf.reduce_sum(diff, axis=1)\n\nprint(result)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "# calculate the square difference\ndiff = tf.square(a - b)\n# reduce the square difference by column\nresult = tf.reduce_sum(diff, axis=0)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    # calculate the square of the difference between A and B\n    square_diff = tf.square(tf.subtract(A, B))\n    # reduce the square difference by row\n    result = tf.reduce_sum(square_diff, axis=1)\n    # return the result\n    return result\n\nprint(f(A=example_a,B=example_b))\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = x[y,z]\nresult = tf.gather(x, y, axis=1)\nresult = tf.gather(result, z, axis=1)\nresult = tf.gather(result, 0, axis=1)\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = x[row, col]\nresult = tf.gather(x, col, row)\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    return x[y,z]\n\nresult = f(example_x,example_y,example_z)\nprint(result) # prints: [2 6]\n<code> \n\nThis function f returns the value at the index y and z in the array x. The index y and z are specified as a 1D tensor. The function f is called with the example values of x, y, and z. The result is printed out.\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "C = tf.math.multiply(A, B)\nresult = tf.reduce_sum(C, axis=2)\nprint(result)", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "C = tf.math.multiply(A, B)\nresult = tf.reduce_sum(C, axis=2)\nprint(result)", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "x = tf.constant(x)\nx = tf.strings.unicode_transcode(x, 'UTF-8')\nresult = tf.strings.unicode_transcode(x, 'UTF-8')\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    # Convert the byte list to string list\n    result = tf.strings.unicode_transpose(x)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "x_nonzero = tf.where(x != 0, x, tf.zeros_like(x))\nx_nonzero_sum = tf.reduce_sum(x_nonzero, axis=-1, keepdims=True)\nresult = tf.where(x_nonzero_sum != 0, x_nonzero / x_nonzero_sum, tf.zeros_like(x))\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "x = tf.where(x != 0, x, tf.zeros_like(x))\nx = tf.reduce_mean(x, axis=-1)\nresult = tf.where(x != 0, x, tf.zeros_like(x))", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    # calculate the sum of the non-zero elements in the second to last dimension\n    sum_non_zero = tf.reduce_sum(x, axis=-2, keepdims=True)\n    # calculate the sum of the elements in the second to last dimension\n    sum_second_to_last_dim = tf.reduce_sum(x, axis=-2)\n    # calculate the average of the elements in the second to last dimension\n    average = tf.divide(sum_second_to_last_dim, sum_non_zero)\n    return average\n\nresult = f(example_x)\nprint(result)\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "import tensorflow as tf\n\n# Create two random 2D tensor with shape (100, 100)\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Calculate the sum of the matrix multiplication of A and B\nresult = tf.reduce_sum(tf.matmul(A, B))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "# Find the maximum value in each row\nmax_values = tf.argmax(a, axis=1)\n# Count the number of times the maximum value in each row is equal to the actual value\ncount = tf.math.count_nonzero(max_values == a, axis=1)\n# Calculate the accuracy\naccuracy = tf.math.divide(count, tf.math.count_nonzero(a, axis=1))\n# Print the accuracy\nprint(accuracy)", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "# Find the maximum value in each column\nmax_values = tf.argmax(a, axis=1)\n# Convert the maximum value to the index of the maximum value in each column\nresult = tf.where(a == max_values[:, None], max_values, tf.constant(0))\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    # find the maximum value in each row\n    max_values = tf.argmax(a, axis=1)\n    # count the number of times the maximum value in each row is equal to the actual value\n    count = tf.math.count_nonzero(a == max_values, axis=1)\n    # return the count\n    return count\n\nprint(f(example_a))\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1) + 1", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n\n# Save the model\nmodel.save('my_model')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "# generate 10 random integers from the uniform distribution\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "seed_x = 10\ntf.random.set_seed(seed_x)\n\nrandom_integers = tf.random.uniform(shape=(114,), minval=2, maxval=5, dtype=tf.int32)\nresult = tf.reshape(random_integers, (114, 1))\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": " ###\n    tf.random.set_seed(seed_x)\n    random_integers = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n    return random_integers\n\nprint(f())\n<code> \n\nThis function generates 10 random integers from a uniform random variable which takes values in {1, 2, 3, 4}. The seed is set to 10 with tf.random.set_seed(). The function returns the generated random integers.\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "import tensorflow as tf\n\n# get the version of tensorflow\nversion = tf.__version__\nprint(version)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "# Fit the data using a logarithmic function\nx_log = np.log(x)\ny_log = np.log(y)\n\n# Fit the data using a polynomial of order 1\nx_poly = np.polyfit(x_log, y_log, 1)\n\n# Convert the coefficients back to the original form\nA = np.exp(x_poly[0])\nB = x_poly[1]\n\nresult = np.array([A, B])\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the data using a polynomial of order 2\nresult = scipy.polyfit(x, y, 2)\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array([result[0], result[1], result[2]])\nprint(result)", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + (3*a - b - c)**2 + sin(b) + cos(b) + 4)\n\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.t.pdf(z_scores, 1)", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = scipy.stats.norm.cdf(z_scores, mu, sigma)", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = scipy.stats.zscore(p_values)", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(stddev, scale=np.exp(mu))\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    result = sA * sB\n    return result\n\nresult = f(example_sA, example_sB)\nprint(result)\n<code>\n\nThis function will return the result of the matrix multiplication of the two input sparse matrices. The result will be a sparse matrix.\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2], indexing='ij')\n\n# Create a LinearNDInterpolator object for each point\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\n\n# Get the interpolated value for the request point\nresult = interpolator(request)\nprint(result)", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100),\n                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), 100),\n                       np.linspace(np.min(points[:, 2]), np.max(points[:, 2]), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100),\n                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), 100),\n                       np.linspace(np.min(points[:, 2]), np.max(points[:, 2]), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100),\n                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), 100),\n                       np.linspace(np.min(points[:, 2]), np.max(points[:, 2]), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100),\n                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), 100),\n                       np.linspace(np.min(points[:, 2]), np.max(points[:, 2]), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100),\n                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), 100),\n                       np.linspace(np.min(points[:, 2]), np.max(points[:, 2]), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100),\n                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), 100),\n                       np.linspace(np.min(points[:, 2]), np.max(points[:, 2]), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100),\n                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), 100),\n                       np.linspace(np.min(points[:, 2]), np.max(points[:, 2]), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100),\n                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), 100),\n                       np.linspace(np.min(points[:, 2]), np.max(points[:, 2]), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, reshape=False)\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\nxrot, yrot = np.round(xrot), np.round(yrot)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = np.diagonal(M.toarray())", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # Use scipy's kstest function to perform the K-S test\n    # The result should be a tuple of two elements:\n    # 1. The K-S test statistic\n    # 2. The p-value\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that the observed counts are less than the expected counts\n    # The K-S test statistic is the difference between the observed and expected counts\n    # The p-value is the probability that", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "# calculate the distance between points\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# assign points2 to points1\nresult = points2[min_indices]\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "# calculate the distance between points\ndistances = scipy.spatial.distance.cdist(points1, points2, 'euclidean')\n# find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# assign points2 to points1\nresult = points2[min_indices]\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "region_count = np.sum(img > threshold)\nprint(region_count)\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "region_count = np.sum(img < threshold)", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use numpy's where function to find the values greater than the threshold\n    # Then use numpy's unique function to find the unique values and their counts\n    # Finally, return the unique values and their counts\n    ### END SOLUTION ###\n    return np.unique(np.where(img > threshold), return_counts=True)\n\nprint(f(example_img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the center of mass of such regions\ncenter_of_mass = np.mean(np.where(mask), axis=1)\n\n# Find the distance between the center of mass of such regions and the top left corner\ndistance = np.sqrt((center_of_mass[0]-0)**2 + (center_of_mass[1]-0)**2)\n\n# Output the distances as a list\nresult = distance.tolist()\nprint(result)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T\nM = M + M.T\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    sA = sA.tocsr()\n    ### END SOLUTION ###\n    return sA\n\n# call the function\nsA = f(example_sA)\nprint(sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "# erode the array by 1\neroded = scipy.ndimage.erosion(square, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# dilate the array by 1\ndilated = scipy.ndimage.dilation(eroded, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# remove the isolated single cells\nresult = np.where(dilated == 0, 0, square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "# erode the square array\neroded = scipy.ndimage.erosion(square, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# dilate the eroded array\ndilated = scipy.ndimage.dilation(eroded, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# find the non-zero elements in the dilated array\nnonzero = np.where(dilated != 0)\n# set the non-zero elements to 0\ndilated[nonzero] = 0\n# print the dilated array\nprint(dilated)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_val = np.max(col.toarray())\nmin_val = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "# Convert the sparse matrix to a dense array\ndarr = np.array(sA.toarray())\n\n# Get the median and mode of the dense array\nmedian = np.median(darr)\nmode = np.mode(darr)\n\n# Print the median and mode\nprint(\"Median: \", median)\nprint(\"Mode: \", mode)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *a):\n    return np.sum([a[i]*np.cos(np.pi*x/(2*tau)) for i in range(len(a))], axis=0)\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\narray_1d = np.ravel(example_array)\n# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.pdist(array_1d, 'euclidean')\n# Convert distances tometres\ndistances_m = distances * 0.001\n# Convert distances to a 2D array\ndistances_2d = np.array(distances_m).reshape(len(array_1d), len(array_1d))\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distances = np.min(distances_2d, axis=1)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_1d).reshape(len(array_1d), len(array_1d))\n# Convert the minimum distances to a 1D array\nmin_distances_1d = np.ravel(min_distances_2d)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.array(min_distances_", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Calculate pairwise Manhattan distances between all regions\ndistances = scipy.spatial.distance.cdist(np.arange(example_array.shape[0]), np.arange(example_array.shape[1]), metric='manhattan')\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distances = np.min(distances, axis=1)\n# Create a result array with the minimum distance separating the nearest edges of each raster patch\nresult = np.stack([np.arange(example_array.shape[0]), np.arange(example_array.shape[1]), min_distances], axis=1)\nprint(result)", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate pairwise Euclidean distances between all regions\n    # to obtain the minimum distance separating the nearest edges of each raster patch\n    # Convert distances from cells tometres\n    # result = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n    #                   [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n    #                   [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n    # # Convert distances from cells tometres\n    # result = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n    #                   [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n    #                   [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(x.shape[1]):\n    tck = interpolate.splrep(x[:, i], y[:, i], s=4)\n    result[i] = interpolate.splev(x_val, tck)\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array([38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0])\nx2 = np.array([39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8])\nx3 = np.array([34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0])\nx4 = np.array([34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8])\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\n\nresult = ss.anderson_ksamp(x1, x2)\nprint(result)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(df['A']) # keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = np.rollapply(df['B'], 3, lambda x: tau1(x))\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nresult = is_csr_matrix_only_zeroes(sa)", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n\nresult = is_lil_matrix_only_zeroes(sa)\nprint(result)", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n    ### END SOLUTION ###\n\nprint(f(example_pre_course_scores, example_during_course_scores))\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "kurtosis_result = np.mean(a**3) / np.mean(a**3)**2\nprint(kurtosis_result)", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=True)\nprint(kurtosis_result)", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "def interpolated(s, t):\n    z = (s + t)*np.exp(-6.0 * (s * s + t * t))\n    return z\n\nresult = interpolated(s, t)\nprint(result)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a 2D grid of points\n    x_grid, y_grid = np.meshgrid(s, t, indexing='ij')\n    # Create a 2D grid of values\n    z_grid = z(x_grid, y_grid)\n    # Create a 2D grid of indices\n    x_indices, y_indices = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t)), indexing='ij')\n    # Create a 2D grid of indices\n    x_indices_grid, y_indices_grid = np.meshgrid(np.arange(len(s)), np.arange(len(t", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_map = {}\nfor i, region in enumerate(vor.regions):\n    for point in region:\n        region_map[tuple(point)] = i\n\n# Create a dictionary to map the extra points to their corresponding region\nregion_", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "# find the voronoi region for each extra point\nregion_indices = []\nfor point in extraPoints:\n    region_indices.append(vor.point_region(point))\n\n# convert the list of region indices to numpy array\nregion_indices = np.array(region_indices)\n\n# count the number of points in each region\nregion_count = np.bincount(region_indices)\n\n# print the result\nprint(region_count)\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "max_size = max([v.size for v in vectors])\nresult = sparse.csc_matrix(np.pad(vectors, ((0, 0), (0, max_size - v.size)), 'constant'))", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=(3, 3), origin=(0, 0))\nprint(b)\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = np.array([M[row[i], column[i]] for i in range(len(row))])", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "dev = abs((x-u)/o2)\nP_inner = scipy.integrate.integrate(NDfx,-dev,dev)\nP_outer = 1 - P_inner\nP = P_inner + P_outer/2\nprint(P)", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(x = 2.5, u = 1, o2 = 3))\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a 2D DCT matrix\ndct_matrix = np.zeros((N, N))\nfor i in range(N):\n    for j in range(N):\n        dct_matrix[i, j] = sf.dctn(np.ones(N), 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "result = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result)", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "result = np.array([scipy.stats.binom.pmf(i, N, p) for i in range(N+1)])\nresult = np.tile(result, (N+1, 1))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.arange(N+1)))\nresult = np.multiply(result, np.multiply(np.arange(N+1), np.", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x))\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=1)\n\n# create a new dataframe with zscore and data\nresult = pd.DataFrame(data=np.c_[df.values, zscore], index=df.index, columns=np.append(df.columns, 'zscore'))\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=0)\n# round zscore to 3 decimal places\nzscore = np.round(zscore, 3)\n# create a new dataframe with zscore and original data\nresult = pd.DataFrame({'data': df.values, 'zscore': zscore})\n# print the result\nprint(result)\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y, x = np.mgrid[0:6, 0:6]\ncenter = np.array([3, 3])\nresult = distance.cdist(np.dstack((y, x)), center, 'euclidean')", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\ny = y - shape[0] / 2\nx = x - shape[1] / 2\nresult = distance.cdist(np.dstack((y, x)), np.array([[5., 4., 3., 3., 4., 5.],\n                                                      [4., 3., 2., 2., 3., 4.],\n                                                      [3., 2., 1., 1., 2., 3.],\n                                                      [3., 2., 1., 1., 2., 3.],\n                                                      [4., 3., 2., 2., 3., 4.],\n                                                      [5., 4., 3., 3., 4., 5.]]))\nprint(result)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.mgrid[0:shape[0], 0:shape[1]]\n    center = np.array([(shape[0] - 1) / 2, (shape[1] - 1) / 2])\n    return distance.cdist(np.dstack((y, x)), center)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint result", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\n\nx0 = np.array([2, 3, 1, 4, 20])\nout = scipy.optimize.minimize(residual, x0, args=(a, y))\nprint(out.x)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return (y - func(x)) ** 2\n\nout = scipy.optimize.minimize(residual, x0, bounds=[x_lower_bounds])\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t-np.sin(t)) if 0 < t < 2*np.pi else 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "sa = sa.toarray()\nsb = sb.toarray()\nresult = sparse.vstack((sa, sb))", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "sa = sa.toarray()\nsb = sb.toarray()\nresult = np.vstack((sa, sb))", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\ndef integrand(x, c):\n    return 2*c*x\n\nc = 5\nlow = 0\nhigh = 1\n\nresult, error = scipy.integrate.quad(lambda x: integrand(x, c), low, high)\nprint(result)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    def fun(x):\n        return 2*x*c\n    result, error = scipy.integrate.quad(fun, low, high)\n    return result\n\n# call the function with different values of c\nc = [5, 10, 15]\nfor i in range(len(c)):\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c[i]))\n    # print the result\n    print(f(c=c", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.reshape(1, 1)\nV_x = V + x\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "x = x.reshape(1, 1)\nV_x = V + x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "x = x.reshape(1, 1)\ny = y.reshape(1, 1)\nV += x\nV += y", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].toarray()\n    # calculate the length of the column\n    length = np.sqrt(np.sum(np.square(column)))\n    # normalize the column\n    column_normalized = column / length\n    # update the original column of the matrix\n    sa[:, col] = column_normalized\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(np.square(column)))\n    # normalize the column\n    column_normalized = column / length\n    # update the original column of the matrix\n    sa[:, col] = column_normalized\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = np.where(a > 0, 1, 0)", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = np.where(a == 0, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest element to each centroid\nresult = np.argmin(distances, axis=1)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest centroid for each data point\nclosest_centroid_index = np.argmin(distances, axis=1)\n# extract the closest point to each cluster\nresult = data[closest_centroid_index]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the k-th closest element to each centroid\nclosest_elements = np.argpartition(distances, k, axis=0)[:k]\n# find the index of the k-th closest element in original data for each cluster\nresult = np.argwhere(closest_elements == np.arange(data.shape[0]))\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result = fsolve(lambda x: eqn(x, a, b), x0=x)\n    print(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result = fsolve(lambda x, a: eqn(x, a, x), x0=x, args=(a,))\n    result = np.sort(result)\n    print(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Define the function to calculate the K-S test\ndef kstest(x, y, a, m, d):\n    return stats.kstest(y, bekkers_func(x, a, m, d))\n\n# Fit the function to the sample data\nresult = sp.optimize.curve_fit(bekkers_func, range_start, range_end, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Get the K-S test result\nresult = kstest(range_start, range_end, estimated_a, estimated_m, estimated_d)\nprint(result)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# calculate the estimated values of the function\nestimated_values = bekkers(range_start, estimated_a, estimated_m, estimated_d)\n# calculate the K-S test\nresult = sp.stats.kstest(sample_data, 'norm')\n# check the p-value at 95% confidence level\np_value = result.pvalue\nif p_value < 0.05:\n    result = True\nelse:\n    result = False\nprint(result)", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def rolling_integral(x):\n    return x.sum()\n\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral)\nprint(integral_df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "x = np.array([(2,2), (1,2), (2,3), (3,2), (2,1)])\ny = np.array([5,7,8,10,3])\neval = np.array([(2.7, 2.3)])\n\n# Create a grid data object\ngrid_data = scipy.interpolate.GridData(x, y, eval)\n\n# Use griddata to find the value at the sub-pixel value\nresult = grid_data.griddata(eval, y, x, method='linear')\nprint(result)", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "weights = sciopt.maximum_likelihood_solve(lambda x: x, a.values, method='max')\nweights = weights / np.sum(weights)\nprint(weights)", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "# find the indices of the elements that are less or equal to the neighbouring n elements\nindices = np.where(np.greater_equal(arr, np.roll(arr, n)))\n# find the indices of the elements that are less or equal to the neighbouring n elements forwards\nindices_forward = np.where(np.greater_equal(arr, np.roll(arr, n + 1)))\n# find the indices of the elements that are less or equal to the neighbouring n elements backwards\nindices_backward = np.where(np.greater_equal(arr, np.roll(arr, -n)))\n# combine the indices of the elements that are less or equal to the neighbouring n elements forwards and backwards\nindices = np.concatenate((indices[0], indices_forward[0], indices_backward[0]))\n# sort the indices in ascending order\nindices = np.sort(indices)\nresult = indices", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "# find the absolute indices of the elements\nabs_indices = np.where(arr == np.max(arr, axis=1))\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "num_cols = df.select_dtypes(include=[np.float64, np.int64]).columns\noutlier_cols = [col for col in num_cols if np.abs(stats.zscore(df[col])) > 3]\ndf = df[outlier_cols]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\nprint(data1)", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    data_df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                           columns=data['feature_names'] + ['target'])\n    return data_df\n    ### END SOLUTION ###\n\nsolve(data)", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a list of unique elements in the column\nunique_elements = np.unique(df['Col3'].values)\n\n# Create a new dataframe with one-hot-encoded values\nout_df = pd.DataFrame(np.zeros((len(unique_elements), len(df))), columns=unique_elements)\n\n# Iterate over the unique elements and set the value to 1 for the corresponding row in the original dataframe\nfor i, element in enumerate(unique_elements):\n    out_df.loc[i, df['Col3'].values == element] = 1\n\n# Print the output dataframe\nprint(out_df)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "# Convert the list of strings to a one-hot-encoded array\none_hot_encoded = pd.get_dummies(df['Col3'].apply(lambda x: x.split(',')))\n\n# Create a new dataframe with the one-hot-encoded array\ndf_out = pd.DataFrame(one_hot_encoded.to_numpy(), columns=one_hot_encoded.columns, index=one_hot_encoded.index)\n\n# Remove the original column\ndf_out = df_out.drop(columns='Col3')\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((len(unique_elements), len(df.columns))), columns=unique_elements)\n\n# One-hot-encode the last column\nfor i, element in enumerate(unique_elements):\n    new_df.iloc[:, i] = (df.iloc[:, -1] == element).astype(int)\n\n# Remove last column\nnew_df = new_df.drop(columns=[unique_elements[-1]])\n\n# Set the last column as index\nnew_df.set_index(unique_elements[-1], inplace=True)\n\n# Rename the last column\nnew_df.columns = unique_elements\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with one-hot-encoded values\nfor element in unique_elements:\n    df_out[element] = np.where(df.iloc[:, -1] == element, 1, 0)\n\n# Drop the last column\ndf_out = df_out.drop(columns=[unique_elements[-1]])\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with one-hot-encoded values\nfor element in unique_elements:\n    df_out[element] = np.where(df.iloc[:, -1] == element, 1, 0)\n\n# Drop the last column\ndf_out = df_out.drop(columns=[unique_elements[-1]])\n\n# Print the output dataframe\nprint(df_out)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\nproba = np.array([1 / (1 + np.exp(-x)) for x in svmmodel.decision_function(x_test)])\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\nproba = np.max(proba, axis=1)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "# Convert the transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\nnew_df = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the new_df to a pandas DataFrame\nnew_df = pd.DataFrame(new_df)\n\n# Print the new_df\nprint(new_df)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "# Convert the transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new DataFrame with the original data and the transform_output\nnew_df = pd.DataFrame(np.concatenate((df_origin, transform_output), axis=1))\n\n# Print the new DataFrame\nprint(new_df)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # transform the transform_output back to a dataframe\n    transform_output = pd.DataFrame(transform_output.toarray())\n    # merge the original dataframe and the transform_output\n    result = pd.concat([df, transform_output], axis=1)\n    return result\n    ### END SOLUTION ###\n\n# call the function\nsolve(df_origin, transform_output)", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the steps\nprint(clf.steps)", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline\nprint(clf)", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a new step\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.append(new_step)\n\n# Insert a new step at a specific position\nposition = 1\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.insert(position, new_step)\n\n# Delete a step\ndel clf.steps[1]\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\n# Insert a new step\nnew_step = ('new_step', SVC())\nclf.steps.append(new_step)\n\n# Insert a new step at a specific position\nposition = 1\nnew_step = ('new_step', SVC())\nclf.steps.insert(position, new_step)\n\n# Delete a step\nposition = 0\ndel clf.steps[position]\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\n# Insert ('t1919810', PCA()) right before 'svdm'\nsteps = clf.named_steps\nsteps.insert(1, ('t1919810', PCA()))\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[1] = ('t1919810', PCA())\nsteps[", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"early_stopping_rounds\": [10, 20, 30, 40],\n    \"learning_rate\": [0.01, 0.001, 0.0001],\n    \"max_depth\": [3, 4, 5],\n    \"n_estimators\": [100, 200, 300],\n    \"min_child_weight\": [1, 2, 3],\n    \"gamma\": [0.1, 0.01, 0.001],\n    \"subsample\": [0.5, 0.75, 1.0],\n    \"colsample_bytree\": [0.5, 0.75, 1.0],\n    \"colsample_bylevel\": [0.5, 0.75, 1.0],\n    \"colsample_byrow\": [0.5, 0.75, 1.0],\n    \"n_iter_no_boosting\": [100, 200, 300],\n    \"learning_rate_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"alpha\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"lambda_boost\": [0.1, 0.01, 0.001],\n    \"", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"learning_rate\" : [0.01, 0.1, 0.2, 0.3],\n    \"max_depth\" : [3, 4, 5],\n    \"n_estimators\" : [100, 200, 300],\n    \"gamma\" : [0.1, 0.2, 0.3],\n    \"alpha\" : [0.01, 0.1, 0.2, 0.3],\n    \"max_iter\" : [100, 200, 300]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n\n# Fit the grid search\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the prediction\nc = gridsearch.predict(testX)\n\n# Get the score on the validation set\nb = gridsearch.score(testX, testY)\n\nprint(best_params)\nprint(best_score)\nprint(c)\nprint(b)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "for train_index, test_index in cv:\n    logreg.fit(X[train_index], y[train_index])\n    proba.append(logreg.predict_proba(X[test_index]))", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "for train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_predicted = scaler.inverse_transform(scaled[:, 0])\n    # check score\n    score = np.mean((t_predicted - data[:, 0])**2)\n    return score # return score\n\nscore = solve(data, scaler, scaled)\nprint(score) # print score ### END SOLUTION ###\n", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = np.mean(scores)\nprint(f'Name model: {model_name}, Mean score: {mean_score}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = np.mean(scores)\nprint(f'Name model: {model_name}, Mean score: {mean_score}')", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\n# Define the GridSearchCV object\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV object\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n# Create a new DecisionTreeClassifier with the best parameters\ndt_best = DecisionTreeClassifier(max_depth=best_params['base_estimator__max_depth'], max_samples=best_params['max_samples'])\n\n# Create a new BaggingClassifier with the best parameters\nbc_best = BaggingClassifier(dt_best, n_estimators=20, max_samples=best_params['max_samples'], max_features=best_params['max_features'])\n\n# Fit the new BaggingClassifier\nbc_best.fit(X_train, y_train)\n\n# Get the probabilities\nproba = bc_best.predict_proba(X_test)", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef solve():\n    tfidf = TfidfVectorizer(preprocessor=preprocess)\n    tfidf.fit(['This is a test', 'This is another test'])\n    print(tfidf.transform(['This is a test', 'This is another test']))\n\nsolve()\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# example variable\ntext = \"This is a test.\"\n\n# apply preprocessor\ntext_transformed = tfidf.transform([text])\n\nprint(text_transformed.toarray())\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Apply preprocessing.scale to the data\nscaled_data = preprocessing.scale(data)\n\n# Create a new DataFrame from the scaled data\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Save the scaled DataFrame to a new variable\nscaled_data_out = scaled_data_df\n\n# Print the scaled DataFrame\nprint(scaled_data_out)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a new DataFrame with the scaled values\nscaled_data = preprocessing.scale(data)\nscaled_data = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n\n# Save the scaled data to a new DataFrame\nscaled_data.to_csv('scaled_data.csv')\n\n# Load the scaled data from the new DataFrame\nscaled_data = pd.read_csv('scaled_data.csv')\nscaled_data = pd.DataFrame(scaled_data, index=scaled_data.index, columns=scaled_data.columns)\n\n# Print the scaled data\nprint(scaled_data)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "import numpy as np\n\n# Get the feature names\nfeature_names = np.array(X.columns)\n\n# Get the feature importances\nimportances = clf.feature_importances_\n\n# Get the indices of the features with importance greater than the mean\nindices = np.where(importances > np.mean(importances))[0]\n\n# Get the feature names for the selected features\nselected_feature_names = feature_names[indices]\n\n# Print the selected feature names\nprint(selected_feature_names)", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "import numpy as np\n\n# get feature names\nfeature_names = np.array(X.columns)\n\n# get feature importances\nimportances = clf.feature_importances_\n\n# get selected feature names\nselected_feature_names = feature_names[np.where(importances > 0.05)[0]]\n\n# save selected feature names\nselected_feature_names = selected_feature_names.tolist()\n\nprint(selected_feature_names)\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = np.array([X.columns[i] for i in np.where(clf.feature_importances_ > 0.05)[0]])", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    centers = centers[p]\n    return centers\n    ### END SOLUTION ###\n\n# call the function\nsamples = get_samples(p, X, km)\nprint(samples)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\nfrom sklearn.kernel import GaussianKernel\n\n# create a svm model\nmodel = svm.SVM()\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"poly\", degree=2)\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarities(query, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    matrix = np.zeros((len(documents), len(documents)))\n    for i, doc in enumerate(documents):\n        matrix[i, :] = np.dot(query_tfidf.toarray(), tfidf.transform([doc]).toarray())\n    return matrix\n\ncosine_similarities_of_queries = get_cosine_similarities(queries, documents)\nprint(cosine_similarities_of_queries)", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarities(query, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = np.dot(query_tfidf, tfidf.transform(documents).T)\n    return cosine_similarities\n\ncosine_similarities_of_queries = np.array([get_cosine_similarities(query, documents) for query in queries])\nprint(cosine_similarities_of_queries)", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_matrix = tfidf.transform(documents)\n    query_matrix = tfidf.transform(queries)\n    cosine_similarities_of_queries = np.dot(query_matrix, tfidf_matrix.T)\n    return cosine_similarities_of_queries\n    ### END SOLUTION ###\n\n# Test the function\nsolve(queries, documents)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).astype(int)\n\n# Convert the 2D array to a 1D array\nnew_features = new_features.flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the ", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "f = np.array(f)\nnew_f = np.zeros((f.shape[0], f.shape[1]))\nfor i in range(f.shape[0]):\n    for j in range(f.shape[1]):\n        new_f[i, j] = f[i, j]\nnew_f = new_f.astype(int)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).astype(int)\n\n# Convert the 2D array to a 1D array\nnew_features = new_features.flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the 2D array to a 1D array with one-hot encoding\nnew_features = np.array(new_features).flatten()\n\n# Convert the 1D array to a 2D array with one-hot encoding\nnew_features = np.array(new_features).reshape(-1, 1)\n\n# Convert the ", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one-hot encoding\n    one_hot_encoded_features = np.zeros((features_array.shape[0], features_array.shape[1], features_array.shape[1]))\n    for i in range(features_array.shape[0]):\n        for j in range(features_array.shape[1]):\n            one_hot_encoded_features[i, j, j] = 1\n    # Convert the one-hot encoded features to a 2D array\n    one_hot_encoded_features_array = np.zeros((one_hot_encoded_features.shape[0], one_hot_encoded_features.shape[1]))\n    for i in range(one_hot_encoded_features.shape[0]):\n        for j in range(one_hot_encoded_features.shape[1]):\n            one_hot_encoded_features_array[i, j] = one_hot_encoded_features[i, j, j]\n    # Convert the one-hot encoded features to a 2D array\n    one_hot_encoded_features_array = np.zeros((one_hot_encoded_features.shape[0], one_hot_encoded_features.shape[1]))\n    for i in range(one_hot_encoded_features.shape[0]):\n        for j in range(one_hot_encoded_features.shape[1]):\n            one_hot_encoded_features_array[i, j] = one_hot_encoded_features[i, j, j]\n    # Convert the one-hot encoded features to a 2D array\n    one_hot_encoded_features_array = np.zeros((one_hot_encoded_features.shape[0], one_hot_encoded_features.shape[1]))\n    for i in range(one_hot_encoded_features.shape[0]):\n        for j in range(one_hot_encoded_features.shape[1]):\n            one_hot_encoded_features_array[i, j] = one_hot_encoded_features[i, j, j]\n    # Convert the one-hot encoded features to a 2D array\n    one_hot_encoded_features_array = np.zeros((one_hot_encoded_features.shape[0], one_hot_encoded_features.shape[1]))\n    for i in range(one_hot_encoded_features.shape[0]):\n        for j in range(one_hot_encoded_features.shape[1]):\n            one_hot_encoded_features_array[i, j] = one_hot_encoded_features[i, j, j]\n    # Convert the one-hot encoded features to a 2D array\n    one_hot_encoded_features_array = np.zeros((one_hot_encoded_features.shape[0], one_hot_encoded_features.shape[1]))\n    for i in range(one_hot_encoded_features.shape[0]):\n        for j in range(one_hot_encoded_features.shape[1]):\n            one_hot_encoded_features_array[i, j] = one_hot_encoded_features[i, j, j]\n    # Convert the one-hot encoded features to a 2D array\n    one_hot_encoded_features_array = np.zeros((one_hot_encoded_features.shape[0], one_hot_encoded_features.shape[1]))\n    for i in range(one_hot_encoded_features.shape[0]):\n        for j in range(one_hot_encoded_features.shape[1]):\n            one_hot_encoded_features_array[i, j] = one_hot_encoded_features[i, j, j]\n    # Convert the one-hot encoded features to a 2D array\n    one_hot_encoded_features_array = np.zeros((one_hot_encoded_features.shape[0], one_hot_encoded_features.shape[1]))\n    for i in range(one_hot_encoded_features.shape[0]):\n        for j in", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "# Convert the list of features to a 2D array\nnew_features = np.array(features).astype(int)\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.column_stack([new_features])\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a numpy array from the data matrix\ndata_matrix = np.array(data_matrix)\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2)\n\n# Fit the model to the data\ncluster.fit(data_matrix)\n\n# Get the cluster labels\ncluster_labels = cluster.labels_\n\n# Print the cluster labels\nprint(cluster_labels)", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage_method='average')\n\n# Fit the clustering model\nclustering.fit(dist_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a numpy array from the similarity matrix\nsimM = np.array(simM)\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage_method='average')\n\n# Fit the model\ncluster.fit(simM)\n\n# Get the cluster labels\ncluster_labels = cluster.labels_\n\n# Print the cluster labels\nprint(cluster_labels)", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage\n\n# calculate the linkage matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# get the cluster labels\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maximum')\ncluster_labels = cluster_labels.tolist()\nprint(cluster_labels)", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage\n\n# calculate the linkage matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# get the cluster labels\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maximum')\ncluster_labels = cluster_labels.tolist()\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a numpy array from the similarity matrix\nsimM = np.array(simM)\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.linkage(simM, method='average')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\n\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnson\n\n# Create a YeoJohnson transformer\ntransformer = YeoJohnson()\n\n# Apply the transformer to the data\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer()\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n\n# define x and y\nx_train_all = x_train.values\ny_train_all = y_train.values\nx_test_all = x_test.values\ny_test_all = y_test.values\n\n# split each of those sets so that we can define x and y\nx_train_all_x, x_test_all_x = x_train_all[:, :-1], x_test_all[:, :-1]\ny_train_all_y, y_test_all_y = x_train_all[:, -1], x_test_all[:, -1]\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features and the target\nx = data.drop(columns=['target'])\ny = data['target']\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.3, random_state=42)\n\n# define x and y\nx_train_all = x_train.values\ny_train_all = y_train.values\nx_test_all = x_test.values\ny_test_all = y_test.values\n\n# split each of those sets so that we can define x and y\nx_train_all_x = x_train_all[:, :-1]\nx_train_all_y = x_train_all[:, -1]\nx_test_all_x = x_test_all[:, :-1]\nx_test_all_y = x_test_all[:, -1]\n\n# define x and y\nx_train_x = x_train_all_x\nx_train_y = x_train_all_y\nx_test_x = x_test_all_x\nx_test_y = x_test_all_y\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": " ###\n    # split the data into training and testing sets\n    train_data, test_data = np.split(data, [int(data.shape[0]*0.8)])\n    # define x and y\n    x_train = train_data[:, :-1]\n    y_train = train_data[:, -1]\n    x_test = test_data[:, :-1]\n    y_test = test_data[:, -1]\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION ###\n\n# call the function\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "mse = df['mse'].values\nlabels = KMeans(n_clusters=2, n_init=10).fit(mse.reshape(-1, 1)).predict(mse.reshape(-1, 1))\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "f1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2, n_init=10).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "# Create a LinearSVC object\nsvc = LinearSVC(penalty='l1', loss='l1', solver='liblinear')\n# Fit the model\nsvc.fit(X, y)\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.get_support())]\nprint(selected_feature_names)", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "# Use LinearSVC to select features\nsvc = LinearSVC(penalty='l1', loss='l1', max_iter=10000)\nsvc.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\nprint(selected_feature_names)", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    # Use LinearSVC with penalty='l1'\n    svc = LinearSVC(penalty='l1', loss='l1', max_iter=10000, random_state=42)\n    # Fit the model\n    svc.fit(X, y)\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names()\n    # Get the selected feature indices\n    selected_feature_indices = np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np.asarray(feature_names)[np", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "series = np.array([]) #blank list to append result\n\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] #removes NaN values for each column to apply sklearn function\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y) # either this or the next line\n    m = slope.coef_[0]\n    series= np.concatenate((series, m), axis = 0)\nslopes = series\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "for col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    series= np.concatenate((series, m), axis = 0)\nslopes = series\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column\ndf['Sex'] = le.transform(df['Sex'])\n\n# Print the transformed data\nprint(df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column into a new column 'Sex_encoded'\ndf['Sex_encoded'] = le.transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ### END SOLUTION ###\n    return df\n\ndef load_data():\n    df = pd.DataFrame({\n        'Sex': ['male', 'female', 'male', 'female'],\n        'Age': [20, 25, 30, 35],\n        'PassengerId': [1, 2, 3, 4],\n        'PassengerClass': [1, 2, 3, 4]\n    })\n    return df\n\ntransformed_df = Transform(load_data())\nprint(transformed_df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION ###\n\n# Test the function\nnp_array = np.array([[1, 2], [3, 4], [5, 6]])\nprint(Transform(np_array))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "close_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict([close_buy1, m5, m10, ma20])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array([['dsa', '2'], ['sato', '3']])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n# print(X)\n# print(y)\n\nlogReg = LogisticRegression()\nlogReg.fit(X[:None],y)\n# logReg.fit(dataframe.iloc[-1:],dataframe.iloc[:,-1])\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the data to numpy array\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:, -1]\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model\nlogReg.fit(X[:None], y)\n# Predict the class of the last animal\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "train_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "train_dataframe = features_dataframe[features_dataframe['date'] <= np.mean(features_dataframe['date'])]\ntest_dataframe = features_dataframe[features_dataframe['date'] > np.mean(features_dataframe['date'])]\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": " ###\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=0.2, shuffle=False)\n    return train_dataframe, test_dataframe\n    ### END SOLUTION ###\n\n# Call the function\nsolve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "scaler = MinMaxScaler()\n\ncols = df.columns[2:4]\ndf[cols + '_scale'] = scaler.fit_transform(df[cols])\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "myData['new_A2'] = scaler.fit_transform(myData[['A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names())\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = GridSearch_fitted.fit(X, y)\nprint(full_results.best_score_)\nprint(full_results.best_params_)\nprint(full_results.cv_results_)\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = GridSearch_fitted.fit(X, y)\nfull_results.best_score_\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nfrom sklearn.ensemble import IsolationForest\n\n# Load the data\ndata = np.load('data.npy')\n\n# Fit the model\nmodel = IsolationForest()\nmodel.fit(data)\n\n# Save the model\nnp.save('sklearn_model', model)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "# tokenizing each description\ndf['description'] = df['description'].apply(lambda x: x.split())\n\n# transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(df['description'])\n\n# calculate cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix)\n\n# print the result\nprint(cosine_similarity_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n# Define the optimizer\noptimizer = torch.optim.SGD(optim.parameters(), lr=0.01)\n\n# Define the learning rate scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define the epoch count\nepochs = 100\n\n# Loop over the epochs\nfor epoch in range(epochs):\n    # Perform the training steps\n    for step in range(100):\n        # Perform the training steps\n        optimizer.step()\n        # Update the learning rate\n        scheduler.step()\n\n    # Update the learning rate\n    optimizer.param_groups[0]['lr'] = 0.001\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define the optimizer\noptimizer = torch.optim.SGD(optim.parameters(), lr=0.005)\n\n# Define the scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define the loss function\nloss_func = torch.nn.MSELoss()\n\n# Define the training data\ntrain_data = np.random.rand(100, 10)\ntrain_labels = np.random.rand(100, 1)\n\n# Define the epoch count\nepochs = 100\n\n# Training loop\nfor epoch in range(epochs):\n    # Get the current learning rate\n    lr = scheduler.get_lr()\n    print(f'Epoch {epoch}, Learning rate: {lr}')\n\n    # Perform the training\n    optimizer.zero_grad()\n    output = model(train_data)\n    loss = loss_func(output, train_labels)\n    loss.backward()\n    optimizer.step()\n\n    # Update the scheduler\n    scheduler.step()\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# convert the word2vec model to a numpy array\nembedding_weights = word2vec.wv.vectors\n# convert the numpy array to a PyTorch tensor\nembedding_weights_tensor = torch.tensor(embedding_weights, dtype=torch.float32)\n# create a embedding layer with the same shape as the embedding weights\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_weights_tensor)\n# embed the input data using the embedding layer\nembedded_input = embedding_layer(input_Tensor)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # get the embedding weights from gensim\n    embedding_weights = word2vec.wv.vectors\n    # convert the embedding weights to PyTorch tensor\n    embedding_weights_tensor = torch.tensor(embedding_weights, dtype=torch.float32)\n    # return the embedding weights tensor\n    return embedding_weights_tensor\n    ### END SOLUTION ###\n\n# call the function\nembedding_weights_tensor = get_embedded_input(input_Tensor)\nprint(embedding_weights_tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "x = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\nprint(px)", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "x = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\nprint(px)\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "x = torch.rand(6,6)\npx = pd.DataFrame(x.numpy())\nprint(px)", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "A_logical = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical]\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "A_log = torch.ByteTensor([1, 1, 0]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "A_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    ### END SOLUTION ###\n    return C\n\ndef load_data():\n    A_log = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_log, B\n\nsolve(A_log, B)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "A_log = torch.ByteTensor([0, 0, 1])\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log]\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Convert the index to numpy array\nidx = idx.numpy()\n\n# Use numpy index_select to get the desired result\nC = np.array(B)[idx]\n\n# Convert the numpy array to PyTorch tensor\nC = torch.from_numpy(C)\n\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    # Convert the numpy array to torch tensor\n    t = torch.tensor(a, dtype=torch.float16)\n    return t\n    ### END SOLUTION ###\n\n# Test the function\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=np.object)\n\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones(lens.shape, dtype=torch.long)\nmask[lens < 5] = 0\nmask", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "lens = torch.tensor([1, 9, 3, 5])\nmask = torch.ones(lens.shape[0], 10)\nmask[torch.arange(lens.shape[0]), lens - 1] = 0\nmask", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones(lens.shape, dtype=torch.long)\nmask[lens < 3] = 0\nmask[lens >= 3] = 1\nprint(mask)", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    mask = torch.zeros(len(lens), max(lens)).long()\n    for i in range(len(lens)):\n        mask[i, :lens[i]] = 1\n    return mask\n    ### END SOLUTION ###\n\n# Test the function\nlens = [3, 5, 4]\nprint(get_mask(lens))\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with diagonal elements\ndiag_ele = torch.ones(index_in_batch, 1)\n\n# Create a 2D tensor with drag_ele\ndrag_ele = torch.ones(index_in_batch, 1)\n\n# Create a 2D tensor with index_in_batch\nindex_in_batch = torch.ones(index_in_batch, 1)\n\n# Create a 2D tensor with index_in_batch * drag_ele\nindex_in_batch_drag_ele = index_in_batch * drag_ele\n\n# Create a 2D tensor with index_in_batch * diag_ele\nindex_in_batch_diag_ele = index_in_batch * diag_ele\n\n# Create a 3D tensor with index_in_batch * Matrix (who is a diagonal matrix, construct by drag_ele)\nindex_in_batch_matrix = index_in_batch_drag_ele * index_in_batch_diag_ele\n\n# Print the 3D tensor\nprint(index_in_batch_matrix)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    # Create a 2D tensor with the same shape as t\n    t_2D = torch.zeros_like(t)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag = torch.diag(t)\n    # Create a 3D tensor with the same shape as t\n    t_3D = t_2D_diag.unsqueeze(0).unsqueeze(0)\n    ### END SOLUTION ###\n    return t_3D\n\n# Test the function\nt = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(Convert(t))\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\nab = torch.stack((a, b), 0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = torch.tensor(a)\nb = torch.tensor(b)\nab = torch.stack((a, b), 0)\nprint(ab.shape)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n    # stack the two tensors along the first axis\n   ", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[..., lengths:] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[..., lengths] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    return torch.tensor(lt)\n    ### END SOLUTION ###\n\n# Call the function\nlist_of_tensors = Convert(list_of_tensors)\nprint(list_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "scores = x.max(1)\nids = torch.argmax(scores, 1, True)\nresult = x.gather(1, ids.unsqueeze(1)).squeeze(1)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "scores = x.mean(axis=1)\nids = torch.argmax(scores, 1, True)\nresult = x.gather(1, ids.unsqueeze(1)).squeeze(1)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask with the indices of the selected elements\nmask = (x[:, 2] == 1).astype(int)\n\n# Gather the selected slices\nresult = x[mask].sum(axis=0)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "# Convert the softmax output to a numpy array\nsoftmax_output = softmax_output.numpy()\n\n# Find the maximum value in each row\nmax_values = np.max(softmax_output, axis=1)\n\n# Create a tensor with the maximum value\nmax_tensor = torch.tensor(max_values, dtype=torch.int64)\n\n# Create a tensor with the class index of the maximum value\nmax_class = torch.argmax(softmax_output, axis=1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "# Convert the softmax output to a numpy array\nsoftmax_output = softmax_output.numpy()\n\n# Find the maximum value in each row\ny = np.argmax(softmax_output, axis=1)\n\n# Convert the numpy array to a tensor\ny = torch.tensor(y)\n\nprint(y)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "# Convert the softmax output to a numpy array\nsoftmax_output = softmax_output.numpy()\n\n# Find the index of the minimum value in each row\ny = np.argmin(softmax_output, axis=1)\n\n# Convert the numpy array to a tensor\ny = torch.tensor(y)\n\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    # Find the maximum value in each row\n    max_values = np.argmax(softmax_output, axis=1)\n    # Create a tensor with the maximum value\n    max_tensor = torch.tensor(max_values, dtype=torch.int64)\n    # Create a tensor with the class number\n    class_tensor = torch.arange(3)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor = class_tensor.unsqueeze(0)\n    # Create a tensor with the class number\n    class_tensor", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "# Find the index of the minimum value in the softmax output\ny = torch.argmax(softmax_output, 1)\n# Create a tensor with the same shape as the softmax output, with the minimum value at the corresponding index\ny = torch.where(softmax_output == softmax_output.min(), y, 0)\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "# Convert labels to one-hot encoding\nlabels = torch.LongTensor(labels)\nlabels = F.one_hot(labels, num_classes=7)\n# Convert labels to one-hot encoding\nlabels = labels.view(-1, 7, 1, 1)\nlabels = labels.permute(0, 3, 2, 1).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.view(-1, 7)\n# Calculate CrossEntropy loss\nloss = F.nll_loss(torch.log(labels), labels, reduction='none')\n# Calculate mean of loss\nmean_loss = loss.mean()\nprint(mean_loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = np.sum(A == B)", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = np.sum(A == B)", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = np.sum(A != B)", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = np.sum(A == B)\n    ### END SOLUTION ###\n    return cnt_equal\n\n# Call the function\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "x = A.shape[0]\ncnt_equal = np.sum(A[-x:] == B[-x:])", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = np.sum(A[-x:] != B[-x:])", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\n# Create a tensor with the shape of a\na = torch.randn(1, 3, 10, 40, 1)\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\n# Create a tensor with the shape of a\na = torch.randn(1, 3, 10, 40, 1)\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n\n# Create a tensor with the shape of a\nchunk_dim = 10\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output[mask == 1] = clean_input_spectrogram[mask == 1]", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask == 0] = clean_input_spectrogram[mask == 0]", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x), torch.abs(y))\nmin_y = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.where(x < y, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\nsigned_max = torch.where(x > y, max_x, max_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x), torch.abs(y))\n    min_y = torch.min(torch.abs(x), torch.abs(y))\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y,", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\nprint(f\"Confidence score: {confidence_score}, Class name: {class_name}\")", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "a = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\na = a[:, :-1]\nb = b[:, 1:]\n\nresult = torch.cat([a, b], axis=1)\nresult = result.mean(axis=1)\nresult = result.unsqueeze(1)\nresult = result.unsqueeze(0)\nresult = result.unsqueeze(0)\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a.reshape(-1, 1)\n    b = b.reshape(1, -1)\n    a = a.repeat(1, 2)\n    b = b.repeat(1, 2)\n    a = a.reshape(-1, 2)\n    b = b.reshape(-1, 2)\n    a = a.sum(axis=1)\n    b = b.sum(axis=1)\n    result = a + b\n    return result\n    ### END SOLUTION ###\n\n# Test the function\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\nprint(solve(a, b))\n", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[0., 0., 0.,0.]])\nr = torch.stack([t,new])\nprint(r)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "t = torch.tensor([[1, 2], [3, 4]])\nnew = torch.tensor([[0., 0., 0.,0.]])\nr = torch.stack([t,new])\nprint(r)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
