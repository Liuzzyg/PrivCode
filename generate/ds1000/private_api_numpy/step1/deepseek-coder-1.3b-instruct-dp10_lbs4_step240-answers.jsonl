{"id": 0, "code": "result = df.iloc[List]\nresult", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.loc[List]\nresult['Type'] = df['Type'].values[List]\nresult", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a new column with the values of Qu1 that have at least two occurrences\ndf['new_Qu1'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) >= x else x)\n\n# Create a new column with the values of Qu2 that have at least two occurrences\ndf['new_Qu2'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df.Qu2) >= x else x)\n\n# Create a new column with the values of Qu3 that have at least two occurrences\ndf['new_Qu3'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df.Qu3) >= x else x)\n\n# Create a new dataframe with the new columns\nresult = df[['new_Qu1', 'new_Qu2', 'new_Qu3']]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\ndef change_values(df):\n    for col in df.columns:\n        if df[col].nunique() >= 3:\n            df[col] = df[col].apply(lambda x: 'other' if x in df[col].value_counts().index[2:] else x)\n    return df\n\nresult = change_values(df)\nprint(result)\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    for col in df.columns:\n        if col in ['Qu1', 'Qu2', 'Qu3']:\n            counts = df[col].value_counts()\n            for index, value in counts.items():\n                if value >= 2:\n                    df.loc[df[col] == index, col] = 'other'\n    return df\n\nprint(f())\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a function to change values in column Qu1\ndef change_values_in_column_Qu1(series):\n    counts = series.value_counts()\n    return series.where(counts >= 3, 'other')\n\n# Create a function to change values in column Qu2 and Qu3\ndef change_values_in_column_Qu2_and_Qu3(series):\n    counts = series.value_counts()\n    return series.where(counts >= 2, 'other')\n\n# Apply the functions to the columns\ndf['Qu1'] = df['Qu1'].apply(change_values_in_column_Qu1)\ndf['Qu2'] = df['Qu2'].apply(change_values_in_column_Qu2_and_Qu3)\ndf['Qu3'] = df['Qu3'].apply(change_values_in_column_Qu2_and_Qu3)\n\n# Create the final result\nresult = df\nresult", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a function to replace values in column Qu1\ndef replace_values(series):\n    counts = series.value_counts()\n    result = series.copy()\n    for index, value in series.items():\n        if value in counts[counts >= 3].index:\n            result[index] = 'other'\n        elif value in counts[counts >= 2].index:\n            result[index] = value\n    return result\n\n# Apply the function to column Qu1\ndf['Qu1'] = replace_values(df['Qu1'])\n\n# Create a function to replace values in column Qu2\ndef replace_values_two(series):\n    counts = series.value_counts()\n    result = series.copy()\n    for index, value in series.items():\n        if value in counts[counts >= 2].index:\n            result[index] = value\n        elif value == 'apple':\n            result[index] = 'other'\n    return result\n\n# Apply the function to column Qu2\ndf['Qu2'] = replace_values_two(df['Qu2'])\n\n# Create a function to replace values in column Qu3\ndef replace_values_three(series):\n    counts = series.value_counts()\n    result = series.copy()\n    for index, value in series.items():\n        if value in counts[counts >= 2].index:\n            result[index] = value\n        elif value == 'apple':\n            result[index] = 'other'\n    return result\n\n# Apply the function to column Qu3\ndf['Qu3'] = replace_values_three(df['Qu3'])\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "result = df.drop_duplicates(subset='url', keep='last')\nresult", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndef df_to_dict(df):\n    result = {}\n    for name, v1, v2, v3 in zip(df['name'], df['v1'], df['v2'], df['v3']):\n        if name not in result:\n            result[name] = {}\n        result[name][v1] = {}\n        result[name][v1][v2] = v3\n    return result\n\nresult = df_to_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n\nresult = f(example_df)\nprint(result)\n### END SOLUTION ###\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\ndf['datetime'] = df['datetime'].apply(lambda x: x.replace('-06:00', ''))\ndf['datetime'] = pd.to_datetime(df['datetime'])\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.replace('-06:00', ''))\ndf['datetime'] = df['datetime'].apply(lambda x: x.replace('-06:00', ''))\ndf['datetime'] = df['datetime'].apply(lambda x: x.replace('-06:00', ''))\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf = df.sort_values('datetime')\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# Convert the message to a dictionary\nmessage_dict = df['message'].apply(lambda x: eval(x))\n\n# Create a new dataframe from the dictionary\nnew_df = pd.DataFrame(message_dict)\n\n# Reset the index of the new dataframe\nnew_df = new_df.reset_index(drop=True)\n\n# Convert the dictionary to a dataframe\nnew_df = pd.DataFrame(new_df.to_dict())\n\n# Reset the index of the new dataframe\nnew_df = new_df.reset_index(drop=True)\n\n# Convert the dictionary to a dataframe\nnew_df = pd.DataFrame(new_df.to_dict())\n\n# Reset the index of the new dataframe\nnew_df = new_df.reset_index(drop=True)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df['score'] * 10\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10\ndf['score'] = df['score'].where(df['product'].isin(products), 10)\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "for product in products:\n    df.loc[df['product'].between(product[0], product[1]), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\n# Create a new column with the minimum and maximum scores for each product\ndf['min_score'] = df['score'].min()\ndf['max_score'] = df['score'].max()\n\n# Create a new column with the normalized scores for each product\ndf['normalized_score'] = (df['score'] - df['min_score']) / (df['max_score'] - df['min_score'])\n\n# Filter the rows where the product is in the list of products\ndf = df[df['product'].isin(products)]\n\n# Drop the original score column\ndf = df.drop(columns='score')\n\n# Rename the new column to score\ndf = df.rename(columns={'normalized_score': 'score'})\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = ''\nfor i in range(len(df)):\n    if df.iloc[i, 0] == 1:\n        df.loc[i, 'category'] = 'A'\n    if df.iloc[i, 1] == 1:\n        df.loc[i, 'category'] += 'B'\n    if df.iloc[i, 2] == 1:\n        df.loc[i, 'category'] += 'C'\n    if df.iloc[i, 3] == 1:\n        df.loc[i, 'category'] += 'D'\n\ndf['category'] = df['category'].astype('category').cat.remove_unused_categories()\ndf['category'] = df['category'].astype('category').cat.reorder_categories(['A', 'B', 'C', 'D'])\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = 'A'\ndf.loc[df['A'] == 1, 'category'] = 'B'\ndf.loc[df['B'] == 1, 'category'] = 'C'\ndf.loc[df['C'] == 1, 'category'] = 'D'\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndef convert_to_categories(df):\n    categories = []\n    for col in df.columns:\n        if df[col].sum() > 0:\n            categories.append(col)\n    return categories\n\ndf['category'] = df.apply(convert_to_categories, axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-01', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-02', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-03', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-04', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-05', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-06', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-07', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-08', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-09', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-10', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-11', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-12', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-1', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-2', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-3', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-4', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-5', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-6', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-7', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-8', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-9', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-10', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-11', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-12', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-13', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-14', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-15', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-16', '') + ' ' + str(x.year))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-17', '') + '", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = df.index\nshifted_df.loc['1980-01-01'] = df.loc['1980-01-01']\nshifted_df.loc['1980-01-05'] = df.loc['1980-01-05']\nshifted_df.loc['1980-01-01', '#1'] = df.loc['1980-01-01', '#1']\nshifted_df.loc['1980-01-05', '#2'] = df.loc['1980-01-05', '#2']\nshifted_df", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = df.index\nshifted_df.loc['1980-01-01'] = df.loc['1980-01-01']\nshifted_df.loc['1980-01-02'] = df.loc['1980-01-02']\nshifted_df.loc['1980-01-03'] = df.loc['1980-01-03']\nshifted_df.loc['1980-01-04'] = df.loc['1980-01-04']\nshifted_df.loc['1980-01-05'] = df.loc['1980-01-05']\nshifted_df.loc['1980-01-06'] = df.loc['1980-01-06']\nshifted_df.loc['1980-01-07'] = df.loc['1980-01-07']\nshifted_df.loc['1980-01-08'] = df.loc['1980-01-08']\nshifted_df.loc['1980-01-09'] = df.loc['1980-01-09']\nshifted_df.loc['1980-01-10'] = df.loc['1980-01-10']\nshifted_df.loc['1980-01-11'] = df.loc['1980-01-11']\nshifted_df.loc['1980-01-12'] = df.loc['1980-01-12']\nshifted_df.loc['1980-01-13'] = df.loc['1980-01-13']\nshifted_df.loc['1980-01-14'] = df.loc['1980-01-14']\nshifted_df.loc['1980-01-15'] = df.loc['1980-01-15']\nshifted_df.loc['1980-01-16'] = df.loc['1980-01-16']\nshifted_df.loc['1980-01-17'] = df.loc['1980-01-17']\nshifted_df.loc['1980-01-18'] = df.loc['1980-01-18']\nshifted_df.loc['1980-01-19'] = df.loc['1980-01-19']\nshifted_df.loc['1980-01-20'] = df.loc['1980-01-20']\nshifted_df.loc['1980-01-21'] = df.loc['1980-01-21']\nshifted_df.loc['1980-01-22'] = df.loc['1980-01-22']\nshifted_df.loc['1980-01-23'] = df.loc['1980-01-23']\nshifted_df.loc['1980-01-24'] = df.loc['1980-01-24']\nshifted_df.loc['1980-01-25'] = df.loc['1980-01-25']\nshifted_df.loc['1980-01-26'] = df.loc['1980-01-26']\nshifted_df.loc['1980-01-27'] = df.loc['1980-01-27']\nshifted_df.loc['1980-01-28'] = df.loc['1980-01-28']\nshifted", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = df.index\nshifted_df.loc['1980-01-01'] = df.loc['1980-01-01']\nshifted_df.loc['1980-01-05'] = df.loc['1980-01-05']\nshifted_df.loc['1980-01-01', '#1'] = df.loc['1980-01-01', '#1']\nshifted_df.loc['1980-01-05', '#2'] = df.loc['1980-01-05', '#2']\nshifted_df", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\ndf['#1'] = df['#1'].shift(-1)\n\n# shift the last row of the first column up 1 row\ndf['#2'] = df['#2'].shift(1)\n\n# drop the first row of the first column\ndf = df.drop(0)\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'X' for col in df.columns]\nprint(df)\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col for col in df.columns]\nprint(df)\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "# Create a list of column names\ncolumn_names = ['HeaderA', 'HeaderB', 'HeaderC', 'HeaderX']\n\n# Create a list of new column names\nnew_column_names = ['XHeader' + column for column in column_names]\n\n# Rename the columns\ndf.columns = new_column_names\n\n# Print the dataframe\nprint(df)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "result = df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\nresult", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' in col})\nresult", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndef get_mean_val(df, col):\n    return df[col].mean()\n\ndef get_sum_val(df, col):\n    return df[col].sum()\n\ndef get_mean_val42(df, col):\n    return df[col].mean()\n\ndef get_sum_val42(df, col):\n    return df[col].sum()\n\ndef get_mean_val3(df, col):\n    return df[col].mean()\n\ndef get_sum_val3(df, col):\n    return df[col].sum()\n\ndef get_mean_val2(df, col):\n    return df[col].mean()\n\ndef get_sum_val2(df, col):\n    return df[col].sum()\n\ndef get_mean_val1(df, col):\n    return df[col].mean()\n\ndef get_sum_val1(df, col):\n    return df[col].sum()\n\ndef get_mean_val(df, col):\n    return df[col].mean()\n\ndef get_sum_val(df, col):\n    return df[col].sum()\n\ndef get_mean_val42(df, col):\n    return df[col].mean()\n\ndef get_sum_val42(df, col):\n    return df[col].sum()\n\ndef get_mean_val3(df, col):\n    return df[col].mean()\n\ndef get_sum_val3(df, col):\n    return df[col].sum()\n\ndef get_mean_val2(df, col):\n    return df[col].mean()\n\ndef get_sum_val2(df, col):\n    return df[col].sum()\n\ndef get_mean_val1(df, col):\n    return df[col].mean()\n\ndef get_sum_val1(df, col):\n    return df[col].sum()\n\ndef get_mean_val(df, col):\n    return df[col].mean()\n\ndef get_sum_val(df, col):\n    return df[col].sum()\n\ndef get_mean_val42(df, col):\n    return df[col].mean()\n\ndef get_sum_val42(df, col):\n    return df[col].sum()\n\ndef get_mean_val3(df, col):\n    return df[col].mean()\n\ndef get_sum_val3(df, col):\n    return df[col].sum()\n\ndef get_mean_val2(df, col):\n    return df[col].mean()\n\ndef get_sum_val2(df, col):\n    return df[col].sum()\n\ndef get_mean_val1(df, col):\n    return df[col].mean()\n\ndef get_sum_val1(df, col):\n    return df[col].sum()\n\ndef get_mean_val(df, col):\n    return df[col].mean()\n\ndef get_sum_val(df, col):\n    return df[col].sum()\n\ndef get_mean_val42(df, col):\n    return df[col].mean()\n\ndef get_sum_val42(df, col):\n    return df[col].sum()\n\ndef get_mean_val3(df, col):\n    return df[col].mean()\n\ndef get_sum_val3(df, col):\n    return df[col].sum()\n\ndef get_mean_val2(df, col):\n    return df[col].mean()\n\ndef get_sum_val2(df, col):\n    return df[col].sum()\n\ndef get_mean_val1(df, col):\n    return df[col].mean()\n\ndef get_sum_val1(df, col):\n    return df[col].sum()\n\ndef get_mean_val(df, col):\n    return df[col].mean()\n\ndef get_sum_val(df, col):\n    return df[col].sum()\n\ndef get_mean_val42(df, col):\n    return df[col].mean()\n\ndef get_sum_val42(df, col):\n    return df[col].sum()\n\ndef get_mean_val3(df, col", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "result = df.loc[row_list, column_list].mean(axis=1)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "result = df.loc[row_list, column_list].sum()\nprint(result)\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "result = df.loc[row_list, column_list].sum(axis=1)\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.apply(lambda x: x.value_counts() if x.nunique() > 1 else x.value_counts()[0])\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "result = df.apply(lambda x: x.value_counts())\nresult", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0,1]].combine_first(df.iloc[[2,3]])\nresult.columns = ['Sample type', 'Concentration', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\nprint(result)\n", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0,1]].combine_first(df.iloc[[2,3]])\nresult.columns = ['Nanonose', 'Concentration', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\nprint(result)\n", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "result = df.where(df.notnull(), df.mean()).where(df.isnull(), np.nan)\nresult", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "result = df.apply(lambda x: x.where(x.isnull(), x.iloc[0]), axis=1)\nresult", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "result = df.where(df.notnull(), df.mean())\nresult = result.where(result.notnull(), np.nan)\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that are smaller than the threshold\ndf['value'] = df['value'].apply(lambda x: x if x < thresh else x + df.loc[df['value'] < thresh, 'value'].sum())\n\n# Create a new column with the sum of the values of the rows that", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "# Create a new column with the threshold\ndf['thresh'] = thresh\n\n# Create a new column with the average of the values\ndf['avg'] = df['value'].mean()\n\n# Create a new column with the sum of the values\ndf['sum'] = df['value'].sum()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new column with the sum of the values where the value is greater than the threshold\ndf['sum_gt_thresh'] = df['value'][df['value'] > thresh].sum()\n\n# Create a new column with the average of the values where the value is greater than the threshold\ndf['avg_gt_thresh'] = df['value'][df['value'] > thresh].mean()\n\n# Create a new", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].apply(lambda x: x if x < section_left or x > section_right else (x + section_left + section_right) / 2)\n\n# Create a new", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "result = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\nresult", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndef exp_column(df, column_name):\n    return df[column_name]**(1/df[column_name])\n\nresult = df.apply(lambda x: exp_column(df, x))\nresult.columns = [f'exp_{x}' for x in df.columns]\nresult", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "result = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\nresult", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = sigmoid(df[col])\n\nresult = df\nresult", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the maximum value in each column\nmax_idx = df.idxmax()\n\n# Create a mask that is True for the maximum value in each column and False for the minimum value in each column\nmask = (max_idx == min_idx)\n\n# Get the index of the maximum value in each column after the minimum occurrence\nresult = max_idx[mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "# Create a new dataframe with all dates and fill with 0\nnew_df = pd.DataFrame(pd.date_range(start=df['dt'].min(), end=df['dt'].max()), columns=['dt'])\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fill(0)\n# Reset the index\nnew_df = new_df.reset_index(drop=True)\n# Create a new dataframe with all users and fill with 0\nnew_df_user = pd.DataFrame(new_df['user'].unique(), columns=['user'])\nnew_df_user = new_df_user.merge(new_df, on='user', how='left')\nnew_df_user['val'] = new_df_user['val'].fill(0)\n# Reset the index\nnew_df_user = new_df_user.reset_index(drop=True)\n# Combine the two dataframes\nresult = new_df_user.merge(new_df, on=['user', 'dt'], how='left')\nresult", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "# Create a new dataframe with all dates from the minimum date to the maximum date\nnew_df = pd.DataFrame(pd.date_range(df['dt'].min(), df['dt'].max()), columns=['dt'])\n\n# Create a new dataframe with all users and all dates\nnew_df = new_df.merge(df[['user', 'dt']], on='dt')\n\n# Fill the new dataframe with 0\nresult = new_df.merge(new_df.groupby(['user', 'dt']).zero_like(), on=['user', 'dt'])\nresult['val'] = result['val'].fillna(0)\nresult", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "# Create a new dataframe with all dates and a value of 233\nnew_df = pd.DataFrame(pd.date_range(start=df['dt'].min(), end=df['dt'].max()), columns=['dt'])\nnew_df['val'] = 233\n\n# Merge the new dataframe with the original dataframe\nresult = pd.merge(new_df, df, on=['dt'], how='left')\n\n# Fill the NaN values with 233\nresult['val'] = result['val'].fillna(233)\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a range of dates between the minimum and maximum date\ndates = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with the minimum and maximum date\nresult = pd.DataFrame({'dt': dates, 'user': ['a','a','b','b'], 'val': [1,33,2,1]})\n\n# Expand the date column to have all the dates there\nresult['dt'] = result['dt'].apply(lambda x: pd.to_datetime(x))\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index()\n\n# Fill in the maximum val of the user for the val column\nresult = result.groupby(['dt', 'user']).max().reset_index", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum date to the maximum date\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with the date range and the maximum value for each user\nresult = pd.DataFrame({'dt': date_range, 'user': df['user'], 'val': df['val'].max(axis=1)})\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Convert the date to the desired format\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "result = df.groupby('name').apply(lambda x: x.assign(name=x.index + 1)).reset_index(drop=True)\nresult", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "df['a'] = df['a'].astype(int)\nresult = df.groupby('name').apply(lambda x: x.assign(a=x['a'].astype(int) + 1))\nresult", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    df['name'] = df['name'].astype('int')\n    df['name'] += 1\n    df['name'] = df['name'].astype('str')\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\n# Create a unique ID for each name\ndf['ID'] = df['name'].apply(lambda x: df[df['name'] == x].index[0])\n\n# Combine a and b into a single column\ndf['combined'] = df['a'] + df['b']\n\n# Replace each of the name and combined columns with a unique ID\ndf['name'] = df['ID']\ndf['combined'] = df['ID']\n\n# Drop the ID column\ndf = df.drop(['ID'], axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.pivot_table(index='user', columns='date', values='value', dropna=False)\ndf = df.reset_index()\ndf.columns = ['user', 'date', 'value']\ndf['date'] = df['date'].apply(lambda x: pd.to_datetime(x, format='%d/%m/%y'))\ndf['someBool'] = df['someBool'].apply(lambda x: x.astype(bool))\ndf", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'01/12/15': 'others', 'value': 'value'})\ndf = df.drop(columns='02/12/15')\ndf = df.rename(columns={'user': 'user', 'others': '01/12/15'})\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.pivot_table(index='user', columns='date', values='value', fill_value=None)\ndf = df.reset_index()\ndf.columns = ['user', 'date', 'value']\ndf['date'] = df['date'].apply(lambda x: x.strftime('%m/%d/%Y'))\ndf['value'] = df['value'].apply(lambda x: x if x is not None else 0)\ndf['someBool'] = df['someBool'].apply(lambda x: x if x is not None else False)\ndf", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df.c > 0.5][columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df.c > 0.45][columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": " ###\n    # Use the loc function to select the rows where the value in column 'c' is greater than 0.5\n    # Then select the columns you want\n    # Finally, convert the result to a numpy array\n    # result = df.loc[df['c'] > 0.5, columns].to_numpy()\n    ### END SOLUTION ###\n    return df.loc[df['c'] > 0.5, columns].to_numpy()\n\n# Create a DataFrame\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# Print the original DataFrame\nprint(df)\n\n# Call the function\nresult = f(df, columns=['b', 'e'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": " ###\n    # select the rows where the value for column 'c' is greater than 0.5\n    df_selected = df[df.c > 0.5][columns]\n    # compute the sum of the two columns for each element to the right of original columns\n    df_selected['sum'] = df_selected[columns[1]].values + df_selected[columns[0]].values\n    # return the result\n    return df_selected\n\n# create a dataframe\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df, columns=['b', 'e']))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndef get_dates(row, X):\n    dates = []\n    for i in range(1, X):\n        dates.append(row['date'] + pd.DateOffset(days=i))\n    return dates\n\nresult = df[~df.apply(get_dates, X=X, axis=1)]\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef get_dates(row, X):\n    dates = []\n    for i in range(1, X):\n        dates.append(row['date'] + pd.DateOffset(months=i))\n    return dates\n\ndef get_overlapping_dates(row, X):\n    dates = get_dates(row, X)\n    return dates\n\ndef remove_overlapping_dates(row, X):\n    dates = get_overlapping_dates(row, X)\n    return not row['date'].isin(dates)\n\nresult = df[~df.apply(remove_overlapping_dates, X=X, axis=1)]\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef get_dates(row, X):\n    dates = []\n    for i in range(1, X):\n        dates.append(row['date'] + pd.DateOffset(months=i))\n    return dates\n\ndef remove_overlapping_dates(df, X):\n    dates = get_dates(df.iloc[0], X)\n    df = df[~df['date'].isin(dates)]\n    return df\n\nresult = remove_overlapping_dates(df, X)\nprint(result)\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the binning\ndf['col1_bin'] = df['col1'].rolling(window=3).mean()\n\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "result = df.groupby(df.index // 3).mean()\nresult['col1'] = result['col1'].astype(int)\nresult", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n# Create a new column with the sum of the first 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\ndf['col1'] = df['col1'].rolling(window=4).sum()\n\n# Create a new column with the sum of the next 4 rows\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "result = df.iloc[-3::-3].mean()\nresult", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-2:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-1:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[3:5, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[2:3, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first_first'] = df.loc[6:8, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first_first'] = df.loc[5:6, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_last_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first_first_first'] = df.loc[9:11, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first_first_first'] = df.loc[8:9, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last_last_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_last_last_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first_first_first_first'] = df.loc[11:13, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first_first_first_first'] = df.loc[10:11, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last_last_last_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_last_last_last_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first_first_first_first_first'] = df.loc[13:15, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first_first_first_first_first'] = df.loc[12:13, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last_last_last_last_last'] =", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_2'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_2'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_3'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_3'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_4'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_4'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_5'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_5'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_6'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_6'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_7'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_7'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_8'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_8'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_9'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_9'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_10'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_10'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_11'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_11'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_12'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_12'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_13'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_13'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0, method='nearest'))\ndf = df.replace(0, df.replace(0, 0", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df = df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.replace(0, df.", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "df['A'] = df['A'].where(df['A'] == 0, df['A'].rolling(window=2).max())\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_days'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    df['time'] = df['duration'].str.split(' ').str[0]\n    df['number'] = df['duration'].str.split(' ').str[1]\n    df['time_days'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['number'] = df['number'].astype(int)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number']\ndf", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.all(np.where(np.all(np.array([df1[column] == df2[column] for column in columns_check_list]), axis=0), True, False), axis=0)\nprint(result)\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = [all(df1[column] == df2[column]) for column in columns_check_list]\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Create a new column with the date, x and y values\n    df['date_x_y'] = df['date'].values.reshape(-1, 1)\n    # Create a numpy array of date, x and y\n    output = df['date_x_y'].values\n    ### END SOLUTION ###\n    return output\n\n# Test the function\ndf = pd.DataFrame({'id': ['abc'], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']}, index=[0])\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index(['id', 'date'])\n    df = df.swaplevel(0, 1)\n    return df\n    ### END SOLUTION ###\n\n# Test the function\ndf = pd.DataFrame({'x': ['abc'], 'y': [100], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot(index='Country', columns='year', values='value')\ndf.columns = df.columns.get_level_values(1)\ndf = df.reset_index()\ndf.columns = ['Country', 'Variable', 'year', 'value']\ndf = df[['Country', 'Variable', 'year', 'value']]\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1', 'variable': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\n# Create a list of column names\ncolumn_names = [f'Value_{i}' for i in range(1, 11)]\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\n# Create a list of column names that start with 'Value'\ncolumn_names = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the rows where any of the columns in column_names is greater than 1\ndf_filtered = df[column_names].abs().max(axis=1) > 1\n\n# Create a new dataframe with only the rows where the condition is True\ndf_filtered = df[df_filtered]", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a list of column names that start with 'Value'\ncolumn_names = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the columns that have a value greater than 1\nnew_df = df[column_names].apply(lambda x: x[x > 1])\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df = df.replace({'&LT;': '<'}, regex=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    df['A'] = df['A'].str.replace('&AMP;', '&')\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: None if validate_single_space_name(x) is None else x.split(' ')[1])\n\ndf = df[['first_name', 'last_name']]\nprint(df)\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x if validate_single_space_name(x) else None)\n\ndf = df[df['1_name'].notna()]\n\nprint(df)\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^(.*) (.*) (.*)$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return match_obj.groups()\n    else:\n        return None\n\ndf['first_name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(validate_single_space_name))\n\nprint(df)\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge_asof(df2, df1, on='Timestamp', direction='nearest')\nresult['data'] = df1['data'][df1['Timestamp'] == result['Timestamp']]\nresult = result.drop(['Timestamp'], axis=1)\nresult = result.sort_values('Timestamp')\nresult.columns = ['Timestamp', 'stuff', 'data']\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1['Timestamp'], df2['Timestamp'], direction='forward')\nresult = df1.merge(df2, on='Timestamp', how='left')\nresult = result.drop(['Timestamp_x', 'Timestamp_y'], axis=1)\nresult = result.rename(columns={'Timestamp_x': 'Timestamp'})\nresult = result.sort_values('Timestamp')\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 else row['col1'] if row['col3'] <= 50 else row['col1'], axis=1)\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\nerrors = []\nfor index, value in df['Field1'].iteritems():\n    if not isinstance(value, int):\n        errors.append(value)\n\nprint(errors)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndef get_integer_values(row):\n    values = row['Field1'].split(',')\n    integer_values = []\n    for value in values:\n        if value.isnumeric():\n            integer_values.append(int(value))\n        else:\n            integer_values.append(value)\n    return integer_values\n\ndf['Field1'] = df.apply(get_integer_values, axis=1)\n\nprint(df['Field1'])\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    result = []\n    for index, value in df['Field1'].items():\n        if not pd.to_numeric(value, errors='coerce').is_integer():\n            result.append(value)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)\nprint(result)\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "# calculate the pairwise distances between cars\ndistances = pd.DataFrame(np.sqrt((df['x'] - df['x'].values[:, None])**2 + (df['y'] - df['y'].values[:, None])**2), columns=df['car'], index=df['car'])\n\n# find the nearest neighbour for each car\nnearest_neighbour = distances.apply(lambda x: x.argmin(), axis=1)\n\n# calculate the average of the distances for each frame\nmean_distances = distances.mean(axis=1)\n\n# create a new dataframe with the nearest neighbour and the mean distance\ndf2 = pd.DataFrame({'car': df['car'], 'nearest_neighbour': nearest_neighbour, 'euclidean_distance': mean_distances})\n\n# print the result\nprint(df2)\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "# Create a new column with the farthest car for each time point\ndf['farthest_car'] = df.groupby('time')['car'].apply(lambda x: x[x == x.max()].index[0])\n\n# Create a new column with the euclidean distance between each car and its farthest neighbour\ndf['euclidean_distance'] = df.groupby('time')['farthest_car'].apply(lambda x: np.sqrt((x[0]-x[1])**2))\n\n# Calculate the average of the euclidean distance for each time point\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply(lambda x: x[x == x.max()])\n\n# Create a new column with the average distance for each car\ndf['average_distance'] = df.groupby('car')['euclidean_distance'].mean()\n\n# Create a new column with the farthest neighbour for each car\ndf['farthest_neighbour'] = df.groupby('car')['farthest_car'].apply", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: ', '.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "\ndef concat_keywords(df):\n    keywords = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]\n    df[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: \"-\".join(cols), axis=1)\n    return df\n\ndf = concat_keywords(df)\nprint(df)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndef concat_keywords(df):\n    df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.tolist()), axis=1)\n    return df\n\ndf = concat_keywords(df)\nprint(df)\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Set Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change value of ProductId to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = range(len(sample_df))\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# Create a new column with the number of rows for each user\ndf['num_rows'] = df.groupby('UserId').size()\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.20\n\n# Create a new column with the number of rows to select\ndf['num_to", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\nresult", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\nresult", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    # create a new column with the index of the first duplicate\n    df['index_original'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n    # return the dataframe with the new column\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\nresult", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n# Create a new column 'index_original' that contains the index of the last occurrence of each row\ndf['index_original'] = df.duplicated", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# Create a group by object\ngroup = df.groupby(['Sp', 'Mt'])\n\n# Find the maximum count for each group\nmax_count = group['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = group[group['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "result = df[df['count'] == df.groupby(['Sp','Mt'])['count'].max()]", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "result = df[df.groupby(['Sp', 'Mt'])['count'].transform('min') == df['count']]", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# Create a group by object\ngroup = df.groupby(['Sp','Value'])\n\n# Find the maximum count in each group\nmax_count = group['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = group[group['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")\nprint(result)\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category!='\" + \"'.join(filter_list) + \"'\")", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "result = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                  ('A', 'B', 'F'),\n                                  ('A', 'C', 'G'),\n                                  ('A', 'C', 'H'),\n                                  ('A', 'D', 'I'),\n                                  ('A', 'D', 'J')])\nresult", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "result = pd.melt(df, value_vars=[list('BBCCDD'), list('EFGHIJ')], var_names=['variable_0', 'variable_1', 'variable_2'])\nresult['variable_0'] = result['variable_0'].str[0]\nresult['variable_1'] = result['variable_1'].str[1]\nresult['variable_2'] = result['variable_2'].str[2]\nresult = result[['variable_0', 'variable_1', 'variable_2', 'value']]\nresult", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] < 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != np.nan else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: x.sum() if x.sum() != np.nan else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != np.nan else np.nan)\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship_type(column1, column2):\n    if column1.nunique() == column1.shape[0] and column2.nunique() == column2.shape[0]:\n        return 'one-to-one'\n    elif column1.nunique() == column1.shape[0] and column2.nunique() == column2.shape[0] - 1:\n        return 'one-to-many'\n    elif column1.nunique() == column1.shape[0] - 1 and column2.nunique() == column2.shape[0]:\n        return 'many-to-one'\n    elif column1.nunique() == column1.shape[0] - 1 and column2.nunique() == column2.shape[0] - 1:\n        return 'many-to-many'\n    else:\n        return 'unknown'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {relationship_type(df[column1], df[column2])}')\n\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-2-many'\n    elif column1.min() == column2.max():\n        return 'one-2-one'\n    elif column1.min() == column2.min():\n        return 'many-2-one'\n    elif column1.max() == column2.max():\n        return 'many-2-many'\n    else:\n        return 'unknown'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {relationship(df[column1], df[column2])}')\n\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def relationship(x, y):\n    if x.shape[0] == y.shape[0]:\n        return 'one-to-one'\n    elif x.shape[0] == y.shape[0] and x.shape[0] == y.shape[0]:\n        return 'one-to-many'\n    elif x.shape[0] == y.shape[0] and x.shape[0] == y.shape[0]:\n        return 'many-to-one'\n    elif x.shape[0] == y.shape[0] and x.shape[0] == y.shape[0]:\n        return 'many-to-many'\n    else:\n        return 'unknown'\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor c1 in df.columns:\n    for c2 in df.columns:\n        result.loc[c1, c2] = relationship(df[c1], df[c2])\nresult", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship(x, y):\n    if x.shape[0] == y.shape[0]:\n        return 'one-2-many'\n    elif x.shape[0] == y.shape[0] - 1:\n        return 'one-2-one'\n    elif x.shape[0] == y.shape[0] + 1:\n        return 'many-2-one'\n    elif x.shape[0] == y.shape[0] + 1 and y.shape[0] == x.shape[0] - 1:\n        return 'many-2-many'\n    else:\n        return 'none'\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor c1 in df.columns:\n    for c2 in df.columns:\n        result.loc[c1, c2] = relationship(df[c1], df[c2])\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\nresult = dfiban_uniq\nresult\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install keras\n!pip install numpy\n!pip install sc", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\n# Create a new column that indicates whether the passenger has a family\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\n\n# Group by the new column and calculate the mean of the 'Survived' column\nresult = df.groupby('Family')['Survived'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\n# Create a new column that indicates whether the passenger has a family\ndf['HasFamily'] = (df['Survived'] > 0) | (df['Parch'] > 0)\n\n# Create a new column that indicates whether the passenger has no family\ndf['NoFamily'] = (df['Survived'] == 0) & (df['Parch'] == 0)\n\n# Calculate the mean of the two groups\nresult = df.groupby(['HasFamily', 'NoFamily'])['SibSp'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndef create_group(sibsp, parch):\n    if sibsp == 1 and parch == 1:\n        return 'Has Family'\n    elif sibsp == 0 and parch == 0:\n        return 'No Family'\n    elif sibsp == 0 and parch == 1:\n        return 'New Family'\n    elif sibsp == 1 and parch == 0:\n        return 'Old Family'\n\ndf['Group'] = df.apply(lambda row: create_group(row['SibSp'], row['Parch']), axis=1)\n\nresult = df.groupby('Group')['Survived'].mean()\nprint(result)\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\ndf", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\ndf", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\ndf", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "def std_mean(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('a').apply(std_mean)\nresult.columns = ['mean', 'std']\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "def std_mean(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('b').apply(std_mean)\nresult.columns = ['mean', 'std']\nprint(result)\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x, axis=0)\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = df['b'].apply(softmax)\ndf['min-max'] = df['b'].apply(min_max)\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "result = df.loc[:, (df.sum(axis=0) != 0)]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != ", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "result = df[df.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "result = df[df.max(axis=1) != 2]", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "result = df.applymap(lambda x: x if x==2 else 0)\nresult", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=[True, False]).sort_index()\nprint(result)\n", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "sorted_s = s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.rename(columns={'index': '1', 0: '2'})\nsorted_s = sorted_s.rename(columns={'1': 'index', '2': '1'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns={'1': 'index'})\nsorted_s = sorted_s.rename(columns={'index': '1'})\nsorted_s = sorted_s.rename(columns", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str) == 's']\nresult", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# Create a group by object\ngroup = df.groupby(['Sp', 'Mt'])\n\n# Find the maximum count for each group\nmax_count = group['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = group[group['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "result = df[df['count'] == df.groupby(['Sp','Mt'])['count'].transform('max')]", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "result = df[df.groupby(['Sp', 'Mt'])['count'].transform('min') == df['count']]", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# Create a group by object\ngroup = df.groupby(['Sp','Value'])\n\n# Find the maximum count in each group\nmax_count = group['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = group[group['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(np.datetime64('17/8/1926'))\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": " ###\n    df['Date'] = df['Member'].map(dict)\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(pd.to_datetime('17/8/1926'))\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\nprint(df)\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['Count_m'] = df.Date.map(df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size())\ndf['Count_y'] = df.Date.map(df.groupby([df['Date'].dt.year]).size())\nprint(df)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_w'] = df.groupby([df['Date'].dt.weekday]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date')['B'].apply(lambda x: x.sum() if x.sum() == 0 else x.sum()).reset_index()\nresult2 = df.groupby('Date')['B'].apply(lambda x: x.sum() if x.sum() == 0 else x.sum()).reset_index()\nresult1['B'] = result1['B'].apply(lambda x: x if x == 0 else 1)\nresult2['B'] = result2['B'].apply(lambda x: x if x == 0 else 1)\nresult1['C'] = result1['C'].apply(lambda x: x if x == 0 else 1)\nresult2['C'] = result2['C'].apply(lambda x: x if x == 0 else 1)\nresult1 = result1.rename(columns={'B': 'zero'})\nresult2 = result2.rename(columns={'B': 'non-zero'})\nresult1\nresult2", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "result1 = df.groupby('Date')['B'].apply(lambda x: x.sum()).reset_index()\nresult1['B'] = result1['B'].apply(lambda x: x % 2 == 0)\nresult1['C'] = result1['C'].apply(lambda x: x % 2 == 0)\n\nresult2 = df.groupby('Date')['C'].apply(lambda x: x.sum()).reset_index()\nresult2['B'] = result2['B'].apply(lambda x: x % 2 == 1)\nresult2['C'] = result2['C'].apply(lambda x: x % 2 == 1)\n\nresult1 = result1.rename(columns={'B': 'Even'})\nresult2 = result2.rename(columns={'C': 'Even'})\n\nresult1 = result1.merge(result2, on='Date')\nresult1['Even'] = result1['Even'].apply(lambda x: x % 2 == 0)\nresult2['Even'] = result2['Even'].apply(lambda x: x % 2 == 1)\n\nresult1 = result1.rename(columns={'Even': 'Even'})\nresult2 = result2.rename(columns={'Even': 'Even'})\n\nresult1 = result1.set_index('Date')\nresult2 = result2.set_index('Date')\n\nresult1 = result1.sort_index()\nresult2 = result2.sort_index()\n\nresult1 = result1.reset_index()\nresult2 = result2.reset_index()\n\nresult1 = result1.rename(columns={'index': 'Date'})\nresult2 = result2.rename(columns={'index': 'Date'})\n\nresult1 = result1.sort_values(by='Date')\nresult2 = result2.sort_values(by='Date')\n\nresult1 = result1.set_index('Date')\nresult2 = result2.set_index('Date')\n\nresult1 = result1.reset_index()\nresult2 = result2.reset_index()\n\nresult1 = result1.rename(columns={'index': 'Date'})\nresult2 = result2.rename(columns={'index': 'Date'})\n\nresult1 = result1.sort_values(by='Date')\nresult2 = result2.sort_values(by='Date')\n\nresult1 = result1.set_index('Date')\nresult2 = result2.set_index('Date')\n\nresult1 = result1.reset_index()\nresult2 = result2.reset_index()\n\nresult1 = result1.rename(columns={'index': 'Date'})\nresult2 = result2.rename(columns={'index': 'Date'})\n\nresult1 = result1.sort_values(by='Date')\nresult2 = result2.sort_values(by='Date')\n\nresult1 = result1.set_index('Date')\nresult2 = result2.set_index('Date')\n\nresult1 = result1.reset_index()\nresult2 = result2.reset_index()\n\nresult1 = result1.rename(columns={'index': 'Date'})\nresult2 = result2.rename(columns={'index': 'Date'})\n\nresult1 = result1.sort_values(by='Date')\nresult2 = result2.sort_values(by='Date')\n\nresult1 = result1.set_index('Date')\nresult2 = result2.set_index('Date')\n\nresult1 = result1.reset_index()\nresult2 = result2.reset_index()\n\nresult1 = result1.rename(columns={'index': 'Date'})\nresult2 = result2.rename(columns={'index': 'Date'})\n\nresult1 = result1.sort_values(by='Date')\nresult2 = result2.sort_values(by='Date')\n\nresult1 = result1.set_index('Date')\nresult2 = result2.set_index('Date')\n\nresult1 = result1.reset_index()\nresult2 = result2.reset_index()\n\nresult1 = result1.rename(columns={'index': 'Date'})\nresult2 = result2.rename(columns={'index': 'Date'})\n\nresult1 = result1.", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=np.mean)\nresult['E'] = pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=np.mean)\nresult", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['mean'] = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nresult", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=np.mean)\nresult['E'] = pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=np.mean)\nresult", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.max)\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.min)\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\nimport dask.dataframe as dd\n\n# Convert the dataframe to a dask dataframe\ndask_df = dd.from_pandas(df, npartitions=1)\n\n# Split the column into multiple rows\nresult = dask_df.assign(var1=lambda x: x['var1'].str.split(','))\n\n# Flatten the result\nresult = result.flatten()\n\n# Convert the result to a pandas dataframe\nresult = result.compute()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "import dask.dataframe as dd\n\n# Convert the dataframe to a dask dataframe\ndask_df = dd.from_pandas(df, npartitions=1)\n\n# Split the column into multiple rows\nresult = dask_df.assign(var2=dask_df['var2'].str.split(','))\n\n# Convert the dask dataframe back to a pandas dataframe\nresult = result.compute()\n\nprint(result)\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\nimport dask.dataframe as dd\n\n# Convert the dataframe to a dask dataframe\ndask_df = dd.from_pandas(df, npartitions=1)\n\n# Split the column into multiple rows\nresult = dask_df.assign(var2=dask_df['var2'].str.split('-'))\n\n# Flatten the result\nresult = result.flatten()\n\n# Convert the result to a pandas dataframe\nresult = result.compute()\n\nprint(result)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf['fips'] = df['fips'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf['row'] = df['row'].str.split(' ', 1, expand=True)[1]\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\n# calculate the average for each row\ndf = df.groupby('Name').apply(lambda x: x.replace(0, x.mean()))\n\n# calculate the cumulative average for each row\ndf = df.groupby('Name').apply(lambda x: x.cumsum() / x.sum())\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\n# Create a new column with the cumulative average of the values\ndf['cumulative_average'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\ndf['cumulative_average_ignore_zero'] = df['cumulative_average_ignore_zero'].where(df.iloc[:, 1:] != 0, df['cumulative_average_ignore_zero'])\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\ndf['cumulative_average_ignore_zero'] = df['cumulative_average_ignore_zero'].where(df.iloc[:, 1:] != 0, df['cumulative_average_ignore_zero'])\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\ndf['cumulative_average_ignore_zero'] = df['cumulative_average_ignore_zero'].where(df.iloc[:, 1:] != 0, df['cumulative_average_ignore_zero'])\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\ndf['cumulative_average_ignore_zero'] = df['cumulative_average_ignore_zero'].where(df.iloc[:, 1:] != 0, df['cumulative_average_ignore_zero'])\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\ndf['cumulative_average_ignore_zero'] = df['cumulative_average_ignore_zero'].where(df.iloc[:, 1:] != 0, df['cumulative_average_ignore_zero'])\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\ndf['cumulative_average_ignore_zero'] = df['cumulative_average_ignore_zero'].where(df.iloc[:, 1:] != 0, df['cumulative_average_ignore_zero'])\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\ndf['cumulative_average_ignore_zero'] = df['cumulative_average_ignore_zero'].where(df.iloc[:, 1:] != 0, df['cumulative_average_ignore_zero'])\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum(axis=1) / df.iloc[:, 1:].sum(axis=1)\ndf['cumulative_average_ignore_", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    df = df.set_index('Name')\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n   ", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\n# Create a new column with the cumulative sum of the values\ndf['cumulative_sum'] = df.iloc[:, 1:].cumsum()\n\n# Create a new column with the cumulative average of the values\ndf['cumulative_average'] = df.iloc[:, 1:].cumsum() / df.iloc[:, 1:].sum()\n\n# Create a new column with the cumulative average of the values, ignoring zeros\ndf['cumulative_average_ignore_zero'] = df.iloc[:, 1:].cumsum() / (df.iloc[:, 1:].sum() - df.iloc[:, 1:].sum(axis=1))\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)) > 0\ndf['label'] = df['label'].apply(lambda x: 1 if x else 0)\ndf['label'] = df['label'].replace(False, 0)\ndf['label'] = df['label'].replace(True, 1)\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\n# Create a new column with the difference between the Close column and the previous row\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n\n# Set the label for the first row to 1\ndf.loc[0, 'label'] = 1\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = df.departure_time.diff().apply(lambda x: x.days*24*60*60 + x.seconds)\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit='s')) if x != 0 else 'NaT')\ndf['Duration'] = df.Duration.apply(lambda x: str(pd.Timedelta(x, unit=", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds)\ndf['Duration'] = df.Duration.replace(0, np.nan)\ndf['Duration'] = df.Duration.replace(np.nan, df.Duration.mean())\ndf", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['Duration'] = df.departure_time.apply(lambda x: (x - pd.to_datetime(x)) / 1000)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.apply(lambda x: (x - df.arrival_time).seconds)\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df.groupby('key1')['key2'].apply(lambda x: x.endswith('e')).sum()\nresult", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.index[df.value.argmax()]\nmin_result = df.index[df.value.argmin()]\nprint(max_result, min_result)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.mode(axis=0)[0][0]\nmedian_result = df.median()[0]\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df[~(99 <= df['closing_price'] <= 101)]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "result = df.groupby(\"item\")[\"diff\"].min()\nresult = result.reset_index()\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\nprint(df)\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df['SOURCE_NAME']\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isnull().sum()\n\n# Calculate the number of NaN values to fill\nnan_to_fill = nan_count // 2\n\n# Create a mask for the NaN values to fill\nmask = df['Column_x'].isnull()\n\n# Fill the NaN values with the mode of the column\ndf['Column_x'][mask] = df['Column_x'].mode()[0]\n\n# Fill the first 50% of NaN values with 0 and the last 50% with 1\ndf['Column_x'][mask[:nan_to_fill]] = 0\ndf['Column_x'][mask[-nan_to_fill:]] = 1\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isnull().sum()\n\n# Calculate the number of NaN values to fill\nnan_to_fill = nan_count * 0.3\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill_first = nan_to_fill // 3\n\n# Calculate the number of NaN values to fill in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill // 3\n\n# Calculate the number of NaN values to fill in the last 30% of the column\nnan_to_fill_last = nan_to_fill // 3\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill_first = nan_to_fill_first\n\n# Calculate the number of NaN values to fill in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill_middle\n\n# Calculate the number of NaN values to fill in the last 30% of the column\nnan_to_fill_last = nan_to_fill_last\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill_first = nan_to_fill_first\n\n# Calculate the number of NaN values to fill in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill_middle\n\n# Calculate the number of NaN values to fill in the last 30% of the column\nnan_to_fill_last = nan_to_fill_last\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill_first = nan_to_fill_first\n\n# Calculate the number of NaN values to fill in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill_middle\n\n# Calculate the number of NaN values to fill in the last 30% of the column\nnan_to_fill_last = nan_to_fill_last\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill_first = nan_to_fill_first\n\n# Calculate the number of NaN values to fill in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill_middle\n\n# Calculate the number of NaN values to fill in the last 30% of the column\nnan_to_fill_last = nan_to_fill_last\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill_first = nan_to_fill_first\n\n# Calculate the number of NaN values to fill in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill_middle\n\n# Calculate the number of NaN values to fill in the last 30% of the column\nnan_to_fill_last = nan_to_fill_last\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill_first = nan_to_fill_first\n\n# Calculate the number of NaN values to fill in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill_middle\n\n# Calculate the number of NaN values to fill in the last 30% of the column\nnan_to_fill_last = nan_to_fill_last\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill_first = nan_to_fill_first\n\n# Calculate the number of NaN values to fill in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill_middle\n\n# Calculate the number of NaN values to fill in the last 30% of the column\nnan_to_fill_last = nan_to_fill_last\n\n# Calculate the number of NaN values to fill in the first 30% of the column\nnan_to_fill", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "# Create a mask for the NaN values\nmask = df['Column_x'].isna()\n\n# Create a binary array of 0 and 1\nbinary = np.array([0, 1])\n\n# Create a random binary array of 0 and 1\nrandom_binary = np.random.choice(binary, size=(len(mask), 2))\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 1, 1, 0)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 1, 1, 0)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 1, 1, 0)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 1, 1, 0)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 1, 1, 0)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 1, 1, 0)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 1, 1, 0)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 1, 1, 0)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0 and a 50% chance of 1\nrandom_binary = np.where(random_binary == 0, 0, 1)\n\n# Create a binary array of 0 and 1 with a 50% chance of 0", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame([(a.iloc[0, 0], b.iloc[0, 0]), (a.iloc[1, 0], b.iloc[1, 0])], columns=['one', 'two'])\nresult", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.DataFrame([(a.iloc[0, 0], b.iloc[0, 0], c.iloc[0, 0]), (a.iloc[1, 0], b.iloc[1, 0], c.iloc[1, 0])], columns=['one', 'two'])\nresult", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "result = pd.DataFrame(np.array([(a.iloc[0, 0], b.iloc[0, 0])]), columns=['one', 'two'])\nfor i in range(1, a.shape[0]):\n    result = result.append(pd.DataFrame(np.array([(a.iloc[i, 0], b.iloc[i, 0])]), columns=['one', 'two']))\n\nfor i in range(a.shape[0]):\n    if a.shape[0] > a.shape[0]:\n        result = result.append(pd.DataFrame(np.array([(a.iloc[i, 0], np.nan)]), columns=['one', 'two']))\n    if b.shape[0] > a.shape[0]:\n        result = result.append(pd.DataFrame(np.array([(np.nan, b.iloc[i, 0])]), columns=['one', 'two']))\n\nresult = result.fillna(np.nan)\nresult", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(',')))\nresult = pd.DataFrame({'text': result})\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).values.reshape(-1, 1)\nresult = pd.DataFrame(result, columns=['text'])\nresult", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df['text'].values.reshape(-1, 1).tolist()\nresult = [', '.join(row) for row in result]\nresult = pd.DataFrame({'text': result})\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].values.reshape(-1, 1).tolist()\nresult = [item for sublist in result for item in sublist]\nresult = pd.DataFrame(result, columns=['text'])\nresult", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].values.reshape(-1, 1).tolist()\nresult = ['-'.join(x) for x in result]\nresult = pd.DataFrame(result, columns=['text'])\nresult['text'] = result['text'].str.strip()\nresult", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.concat([df1, df2], axis=0)\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult['date'] = result['date'].apply(lambda x: pd.to_datetime(x))\nresult = result.sort_values(['id', 'city', 'district', 'date'])\nresult = result.pivot(index='id', columns=['city', 'district'], values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city': 'city_district', 'district': 'district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename(columns={'city_district': 'city_district'})\nresult = result.sort_values(['id', 'city_district', 'date'])\nresult = result.pivot(index='id', columns='city_district', values='value')\nresult = result.fillna(0)\nresult = result.reset_index()\nresult = result.rename", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n# Create a new column 'date_formatted' with the date in the format '01-Jan-2019'\ndf1['date_formatted'] = df1['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n\n# Create a new column 'date_formatted' with the date in the format '01-Jan-2019'\ndf2['date_formatted'] = df2['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n\n# Concatenate the two dataframes on the 'id' column\nresult = pd.concat([df1, df2], axis=0)\n\n# Fill the NaN values with the smallest date for each id\nresult = result.groupby('id').apply(lambda x: x.fillna(x.loc[x.index.min(), 'date_formatted']))\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Drop the 'date_formatted' column\nresult = result.drop(columns='date_formatted')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "result = pd.concat([df1, df2], axis=0)\nresult = result.groupby(['id', 'city', 'district', 'date']).first().reset_index()\nresult['date'] = result['date'].apply(lambda x: pd.to_datetime(x))\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x']\nresult['B_y'] = D['B']\nresult = result.sort_values('A')\nresult['B_y'] = result['B_y'].replace(D['B'])\nresult = result.sort_values('A')\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['B_x'] = C['B']\nresult['B_y'] = D['B']\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result[result['B_y'].notna()]\nresult = result[result['B_x'].notna()]\nresult = result", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['dulplicated'] = result['A'].apply(lambda x: x in D['A'])\nresult", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).reset_index(drop=True)\nresult = result.sort_values(by=['time', 'amount'])\nresult", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).reset_index(drop=True)\nresult = result.sort_values(by=['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).reset_index(drop=True)\nresult = result.sort_values(by=['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'time': 'time-tuple', 'amount': 'amount-tuple'})\nresult = result[['user', 'time-tuple', 'amount-tuple']]\nresult", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\n# Create a new dataframe with the index as the file names and the values as the numpy arrays\ndf = pd.DataFrame(series.values, index=series.index, columns=series.index)\n\n# Flatten the dataframe\ndf_flatten = df.values.flatten()\n\n# Create a new dataframe with the flattened numpy arrays\ndf_concatenated = pd.DataFrame(df_flatten.reshape(-1, 1), index=series.index, columns=['concatenated'])\n\n# Print the concatenated dataframe\nprint(df_concatenated)\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\n# Create a new dataframe with the index as the name of the file and the values as the array of values\ndf = pd.DataFrame(series.index, index=series.index, columns=series.values)\n\n# Flatten the dataframe\ndf_flatten = df.values.flatten()\n\n# Create a new dataframe with the index as the name of the file and the values as the array of values\ndf_concatenated = pd.DataFrame(df_flatten, index=series.index, columns=series.index)\n\nprint(df_concatenated)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[df.columns.str.contains(s)]\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.columns[df.columns.str.contains(s)]\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "result = df.columns[df.columns.str.contains(s)]\nresult = result.to_frame()\nresult.columns = [s + str(i) for i in range(1, len(result.columns) + 1)]\nresult", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new column for each code in the original column\nfor i in range(len(df['codes'])):\n    new_df[f'code_{i}'] = df['codes'][i]\n\n# Fill the NaN values with 0\nnew_df = new_df.fillna(0)\n\n# Create a new column for each code in the original column\nfor i in range(len(df['codes'])):\n    new_df[f'code_{i}'] = df['codes'][i]\n\n# Fill the NaN values with 0\nnew_df = new_df.fillna(0)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new column for each code in the original column\nfor i in range(1, len(df['codes'])):\n    new_df[f'code_{i}'] = df['codes'][i]\n\n# Fill the NaN values with 0\nnew_df = new_df.fillna(0)\n\n# Create a new column for the first code in the original column\nnew_df['code_1'] = df['codes'][0]\n\n# Fill the NaN values with 0\nnew_df = new_df.fillna(0)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# Create a new dataframe with the first column as the index\ndf_new = df.set_index(0)\n\n# Create a new dataframe with the second column as the index\ndf_new_2 = df.set_index(1)\n\n# Create a new dataframe with the third column as the index\ndf_new_3 = df.set_index(2)\n\n# Create a new dataframe with the fourth column as the index\ndf_new_4 = df.set_index(3)\n\n# Create a new dataframe with the fifth column as the index\ndf_new_5 = df.set_index(4)\n\n# Create a new dataframe with the sixth column as the index\ndf_new_6 = df.set_index(5)\n\n# Create a new dataframe with the seventh column as the index\ndf_new_7 = df.set_index(6)\n\n# Create a new dataframe with the eighth column as the index\ndf_new_8 = df.set_index(7)\n\n# Create a new dataframe with the ninth column as the index\ndf_new_9 = df.set_index(8)\n\n# Create a new dataframe with the tenth column as the index\ndf_new_10 = df.set_index(9)\n\n# Create a new dataframe with the eleventh column as the index\ndf_new_11 = df.set_index(10)\n\n# Create a new dataframe with the twelfth column as the index\ndf_new_12 = df.set_index(11)\n\n# Create a new dataframe with the thirteenth column as the index\ndf_new_13 = df.set_index(12)\n\n# Create a new dataframe with the fourteenth column as the index\ndf_new_14 = df.set_index(13)\n\n# Create a new dataframe with the fifteenth column as the index\ndf_new_15 = df.set_index(14)\n\n# Create a new dataframe with the sixteenth column as the index\ndf_new_16 = df.set_index(15)\n\n# Create a new dataframe with the seventeenth column as the index\ndf_new_17 = df.set_index(16)\n\n# Create a new dataframe with the eightteenth column as the index\ndf_new_18 = df.set_index(17)\n\n# Create a new dataframe with the nineteenth column as the index\ndf_new_19 = df.set_index(18)\n\n# Create a new dataframe with the twentieth column as the index\ndf_new_20 = df.set_index(19)\n\n# Create a new dataframe with the twenty-first column as the index\ndf_new_21 = df.set_index(20)\n\n# Create a new dataframe with the twenty-second column as the index\ndf_new_22 = df.set_index(21)\n\n# Create a new dataframe with the twenty-third column as the index\ndf_new_23 = df.set_index(22)\n\n# Create a new dataframe with the twenty-fourth column as the index\ndf_new_24 = df.set_index(23)\n\n# Create a new dataframe with the twenty-fifth column as the index\ndf_new_25 = df.set_index(24)\n\n# Create a new dataframe with the twenty-sixth column as the index\ndf_new_26 = df.set_index(25)\n\n# Create a new dataframe with the twenty-seventh column as the index\ndf_new_27 = df.set_index(26)\n\n# Create a new dataframe with the twenty-eighth column as the index\ndf_new_28 = df.set_index(27)\n\n# Create a new dataframe with the twenty-ninth column as the index\ndf_new_29 = df.set_index(28)\n\n# Create a new dataframe with the thirdteenth column as the index\ndf_new_30 = df.set_index(29)\n\n# Create a new dataframe with the fourthteenth column as the index\ndf_new_31 = df.set_index(30)\n\n# Create a new dataframe with", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "result = df['col1'].apply(lambda x: x[0])\nresult = result.values.tolist()\nresult = [item for sublist in result for item in sublist]\nprint(result)\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x[::-1])))\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x)))\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype(int) / 60).astype(int)\n\n# Create a new column for the bin\ndf['bin'] = df['Time'].apply(lambda x: x // 2)\n\n# Average the values in each bin\ndf = df.groupby('bin').mean()\n\n# Create a new column for the bin start time\ndf['bin_start'] = df['bin'].apply(lambda x: pd.to_datetime(str(x) + ':00'))\n\n# Create a new column for the bin end time\ndf['bin_end'] = df['bin_start'].apply(lambda x: x + pd.to_timedelta(2, unit='m'))\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['bin_end'].apply(lambda x: x.minute())\n\n# Create a new column for the bin start time in minutes\ndf['bin_start_min'] = df['bin_start'].apply(lambda x: x.minute())\n\n# Create a new column for the bin end time in minutes\ndf['bin_end_min'] = df['", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype(int) / 60).astype(int)\n\n# Create a new column for the bin\ndf['Bin'] = df['Time'].apply(lambda x: x // 3)\n\n# Sum the values in each bin\ndf = df.groupby('Bin')['Value'].sum().reset_index()\n\n# Interpolate the values in each bin\ndf['Value'] = df.groupby('Bin')['Value'].apply(lambda x: x.interpolate())\n\n# Convert the bin back to time\ndf['Time'] = df['Bin'] * 3\n\n# Convert the time to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Set the time as index\ndf.set_index('Time', inplace=True)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[np.where(np.isnan(df).any(axis=1))[0]]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.columns[np.all(df.isnull(), axis=0)]\nresult", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[df.isnull().all(axis=1)]", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(x, y) for x in df.iloc[0].values if x != y or np.isnan(x) or np.isnan(y) for y in df.iloc[8].values if x != y or np.isnan(x) or np.isnan(y)]\nresult", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\n", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf.columns = df.columns.str.replace('_6', '')\ndf.columns = df.columns.str.replace('_7', '')\ndf.columns = df.columns.str.replace('_8', '')\ndf.columns = df.columns.str.replace('_9', '')\ndf.columns = df.columns.str.replace('_10', '')\ndf.columns = df.columns.str.replace('_11', '')\ndf.columns = df.columns.str.replace('_12', '')\ndf.columns = df.columns.str.replace('_13', '')\ndf.columns = df.columns.str.replace('_14', '')\ndf.columns = df.columns.str.replace('_15', '')\ndf.columns = df.columns.str.replace('_16', '')\ndf.columns = df.columns.str.replace('_17', '')\ndf.columns = df.columns.str.replace('_18', '')\ndf.columns = df.columns.str.replace('_19', '')\ndf.columns = df.columns.str.replace('_20', '')\ndf.columns = df.columns.str.replace('_21', '')\ndf.columns = df.columns.str.replace('_22', '')\ndf.columns = df.columns.str.replace('_23', '')\ndf.columns = df.columns.str.replace('_24', '')\ndf.columns = df.columns.str.replace('_25', '')\ndf.columns = df.columns.str.replace('_26', '')\ndf.columns = df.columns.str.replace('_27', '')\ndf.columns = df.columns.str.replace('_28', '')\ndf.columns = df.columns.str.replace('_29', '')\ndf.columns = df.columns.str.replace('_30', '')\ndf.columns = df.columns.str.replace('_31', '')\ndf.columns = df.columns.str.replace('_32', '')\ndf.columns = df.columns.str.replace('_33', '')\ndf.columns = df.columns.str.replace('_34', '')\ndf.columns = df.columns.str.replace('_35', '')\ndf.columns = df.columns.str.replace('_36', '')\ndf.columns = df.columns.str.replace('_37', '')\ndf.columns = df.columns.str.replace('_38', '')\ndf.columns = df.columns.str.replace('_39', '')\ndf.columns = df.columns.str.replace('_40', '')\ndf.columns = df.columns.str.replace('_41', '')\ndf.columns = df.columns.str.replace('_42', '')\ndf.columns = df.columns.str.replace('_43', '')\ndf.columns = df.columns.str.replace('_44', '')\ndf.columns = df.columns.str.replace('_45', '')\ndf.columns = df.columns.str.replace('_46', '')\ndf.columns = df.columns.str.replace('_47', '')\ndf.columns = df.columns.str.replace('_48', '')\ndf.columns = df.columns.str.replace('_49', '')\ndf.columns = df.columns.str.replace('_50', '')\ndf.columns = df.columns.str.replace('_51', '')\ndf.columns = df.columns.str.replace('_52', '')\ndf.columns = df.columns.str.replace", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].round(2)\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\nprint(df)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\nprint(df)</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "for column in list_of_my_columns:\n    df[column] = df[column].astype(float)\n    df[column] = df[column].mean()\n\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by=['time'])\nresult", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(['time', 'VIM'], ascending=[True, True])", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "# Create a list of dates to delete\ndates_to_delete = ['2020-02-17', '2020-02-18']\n\n# Create a new column with the date index\ndf['Date'] = df.index\n\n# Delete the dates from the new column\ndf = df[~df['Date'].isin(dates_to_delete)]\n\n# Set the date index back to the original column\ndf.set_index('Date', inplace=True)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Create a new column with the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column with the day of the year\ndf['Day_of_Year'] = df['Date'].dt.dayofyear\n\n# Create a new column with the week of the year\ndf['Week_of_Year'] = df['Date'].dt.weekofyear\n\n# Create a new column with the week of the year\ndf['Week'] = df['Date'].dt.week\n\n# Create a new column with the month of the year\ndf['Month_of_Year'] = df['Date'].dt.month\n\n# Create a new column with the month of the year\ndf['Month'] = df['Date'].dt.month_name()\n\n# Create a new column with the year of the date\ndf['Year'] = df['Date'].dt.year\n\n# Create a new column with the hour of the date\ndf['Hour'] = df['Date'].dt.hour\n\n# Create a new column with the minute of the date\ndf['Minute'] = df['Date'].dt.minute\n\n# Create a new column with the second of the date\ndf['Second'] = df['Date'].dt.second\n\n# Create a new column with the day of the week\ndf['Day_of_Week'] = df['Date'].dt.day_name()\n\n# Create a new column with the day of the year\ndf['Day_of_Year'] = df['Date'].dt.dayofyear\n\n# Create a new column with the week of the year\ndf['Week_of_Year'] = df['Date'].dt.weekofyear\n\n# Create a new column with the week of the year\ndf['Week'] = df['Date'].dt.week\n\n# Create a new column with the month of the year\ndf['Month_of_Year'] = df['Date'].dt.month\n\n# Create a new column with the month of the year\ndf['Month'] = df['Date'].dt.month_name()\n\n# Create a new column with the year of the date\ndf['Year'] = df['Date'].dt.year\n\n# Create a new column with the hour of the date\ndf['Hour'] = df['Date'].dt.hour\n\n# Create a new column with the minute of the date\ndf['Minute'] = df['Date'].dt.minute\n\n# Create a new column with the second of the date\ndf['Second'] = df['Date'].dt.second\n\n# Create a new column with the day of the week\ndf['Day_of_Week'] = df['Date'].dt.day_name()\n\n# Create a new column with the day of the year\ndf['Day_of_Year'] = df['Date'].dt.dayofyear\n\n# Create a new column with the week of the year\ndf['Week_of_Year'] = df['Date'].dt.weekofyear\n\n# Create a new column with the week of the year\ndf['Week'] = df['Date'].dt.week\n\n# Create a new column with the month of the year\ndf['Month_of_Year'] = df['Date'].dt.month\n\n# Create a new column with the month of the year\ndf['Month'] = df['Date'].dt.month_name()\n\n# Create a new column with the year of the date\ndf['Year'] = df['Date'].dt.year\n\n# Create a new column with the hour of the date\ndf['Hour'] = df['Date'].dt.hour\n\n# Create a new column with the minute of the date\ndf['Minute'] = df['Date'].dt.minute\n\n# Create a new column with the second of the date\ndf['Second'] = df['Date'].dt.second\n\n# Create a new column with the day of the week\ndf['Day_of_Week'] = df['Date'].dt.day_name()\n\n# Create a new column with the day of the year\ndf['Day_of_Year'] = df['Date'].dt.dayofyear\n\n# Create a new column with the week of the year\ndf['Week_of_Year'] = df['Date'].dt.weekofyear\n\n# Create a new column with the week of the year\ndf['Week'] = df['Date'].dt.week\n\n# Create a new column with the month of the year\ndf['Month_of_Year'] = df['Date'].dt.month\n\n# Create a new column with the month of the year", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr[np.abs(corr) > 0.3].stack().reset_index(level=1, drop=True).reset_index(level=0, drop=True)\nresult.columns = ['Col1', 'Col2']\nresult.index = range(1, 5)\nresult", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr.where(corr > 0.3, 0)\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0, drop=True).reset_index(level=1, drop=True)\nresult = result.rename(columns={0: 'value'})\nresult = result.set_index(['index', 'value'])\nresult = result.stack().reset_index(level=0,", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "result = df.rename(columns={df.columns[-1]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "result = df.rename(columns={df.columns[0]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndef find_frequent(row):\n    return row.value_counts().max()\n\ndef find_freq_count(row):\n    return row.value_counts().max()\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndef find_frequent(row):\n    return row.value_counts().index[0]\n\ndef find_freq_count(row):\n    return row.value_counts().values[0]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    values = row.values.tolist()\n    unique_values = list(set(values))\n    counts = [values.count(value) for value in unique_values]\n    return unique_values, counts\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df['frequent'].apply(lambda x: len(x[0]))\n\nprint(df)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "result = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nresult.columns = ['foo', 'bar']\nresult", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "\n# Convert the bar column to numeric if it's not NULL\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce')\n\n# Fill NULL values with 0\ndf['bar'] = df['bar'].fillna(0)\n\n# Group by id1 and id2 and calculate the mean of foo and bar\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\n\n# Reset the index\nresult = result.reset_index()\n\n# Convert the bar column to float\nresult['bar'] = result['bar'].astype(float)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum','foo','a_col']]\nresult", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result.drop(['a_col'], axis=1)\nresult", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[~np.isnan(x)]", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = x.tolist()\nresult = [i for i in result if not np.isnan(i)]\nresult = np.array(result)\nresult = result.tolist()\nresult", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((a.shape[0], 4))\nb[:, 0] = np.where(a == 0, 1, 0)\nb[:, 1:] = np.where(a == 1, 1, 0)\nb[:, 3] = np.where(a == 3, 1, 0)\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    b[i, a[i]] = 1\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n        else:\n            b[i, j] = 0\n", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n        else:\n            b[i, j] = 0\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i, j] = a[i, j]\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "def vec2matrix(A, ncol):\n    return A.reshape(-1, ncol)\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    return A.reshape(-1, ncol)\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\nresult", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=1)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=0)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    return max_value, max_index\n\nmax_value, max_index = f(example_a)\nprint(max_value)\nprint(max_index)\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2)[1], a.shape)\nprint(result)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "z = np.any(np.isnan(a), axis=0)\na = np.delete(a, z, axis=1)\nprint(a)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\nprint(result)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "result = np.unravel_index(np.argmin(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a, axis=None), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.argwhere(a == a.min())\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.degrees(degree))\nprint(result)", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.degrees(degree))\nprint(result)", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "def is_degree(number):\n    return number >= 0 and number <= 90\n\ndef is_radian(number):\n    return number > 90 and number <= 180\n\ndef is_degree_or_radian(number):\n    if is_degree(number):\n        return 0\n    elif is_radian(number):\n        return 1\n    else:\n        return None\n\nresult = is_degree_or_radian(number)\nprint(result)\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * 180 / np.pi\nresult = result / 180 * 360\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult = result % 360\nresult = result * 10\nresult = result / 10\nresult = result * 10\nresult", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "result = np.pad(A, (0, length - A.shape[0]), 'constant')\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "print(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint(np.power(a, power))\nprint", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(example_a, 2))\nprint(f(example_a, 3))\nprint(f(example_a, 4))\nprint(f(example_a, 5))\nprint(f(example_a, 6))\nprint(f(example_a, 7))\nprint(f(example_a, 8))\nprint(f(example_a, 9))\nprint(f(example_a, 10))\nprint(f(example_a, 11))\nprint(f(example_a, 12))\nprint(f(example_a, 13))\nprint(f(example_a, 14))\nprint(f(example_a, 15))\nprint(f(example_a, 16))\nprint(f(example_a, 17))\nprint(f(example_a, 18))\nprint(f(example_a, 19))\nprint(f(example_a, 20))\nprint(f(example_a, 21))\nprint(f(example_a, 22))\nprint(f(example_a, 23))\nprint(f(example_a, 24))\nprint(f(example_a, 25))\nprint(f(example_a, 26))\nprint(f(example_a, 27))\nprint(f(example_a, 28))\nprint(f(example_a, 29))\nprint(f(example_a, 30))\nprint(f(example_a, 31))\nprint(f(example_a, 32))\nprint(f(example_a, 33))\nprint(f(example_a, 34))\nprint(f(example_a, 35))\nprint(f(example_a, 36))\nprint(f(example_a, 37))\nprint(f(example_a, 38))\nprint(f(example_a, 39))\nprint(f(example_a, 40))\nprint(f(example_a, 41))\nprint(f(example_a, 42))\nprint(f(example_a, 43))\nprint(f(example_a, 44))\nprint(f(example_a, 45))\nprint(f(example_a, 46))\nprint(f(example_a, 47))\nprint(f(example_a, 48))\nprint(f(example_a, 49))\nprint(f(example_a, 50))\nprint(f(example_a, 51))\nprint(f(example_a, 52))\nprint(f(example_a, 53))\nprint(f(example_a, 54))\nprint(f(example_a, 55))\nprint(f(example_a, 56))\nprint(f(example_a, 57))\nprint(f(example_a, 58))\nprint(f(example_a, 59))\nprint(f(example_a, 60))\nprint(f(example_a, 61))\nprint(f(example_a, 62))\nprint(f(example_a, 63))\nprint(f(example_a, 64))\nprint(f(example_a, 65))\nprint(f(example_a, 66))\nprint(f(example_a, 67))\nprint(f(example_a, 68))\nprint(f(example_a, 69))\nprint(f(example_a, 70))\nprint(f(example_a, 71))\nprint(f(example_a, 72))\nprint(f(example_a, 73))\nprint(f(example_a, 74))\nprint(f(example_a, 75))\nprint(f(example_a, 76))\nprint(f(example_a, 77))\nprint(f(example_a, 78))\nprint(f(example", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), denominator)\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.divide(numerator, denominator)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = np.divide(numerator, denominator)\nresult = (result.numerator, result.denominator)\nresult", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = np.array([(a[i] + b[i] + c[i]) / 3 for i in range(len(a))])\nprint(result)", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(a, np.maximum(b, c))", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a[-1, :])\nprint(result)\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a[-1, :])\nprint(result)\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.concatenate((np.diag(a.flatten()), np.diag(a.flatten(), k=-1)), axis=1)\nresult = result.reshape(a.shape[0], -1)\nresult", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(a.shape[0]), np.diag(a.shape[1]-1, -1)), axis=1)\nresult = np.reshape(result, (a.shape[0], a.shape[1]))\nresult", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = np.array([x for x in X.flatten()])\nprint(result)", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.empty(X.shape[0], dtype=object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = []\n    for i in range(len(X)):\n        for j in range(len(X[i])):\n            result.append(X[i][j])\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty(X.shape[0], dtype=np.object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\nprint(result)", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)\n", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)\n", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "def get_max_independent_set(a):\n    # Get the number of rows in the matrix\n    n = a.shape[0]\n\n    # Create a matrix of zeros with the same shape as a\n    b = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    c = np.ones_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    d = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    e = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    f = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    g = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    h = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    i = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    j = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    k = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    l = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    m = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    n = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    o = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    p = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    q = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    r = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    s = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    t = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    u = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    v = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    w = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    x = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    y = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    z = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    a = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    b = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    c = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    d = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    e = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    f = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    g = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    h = np.ones_like(a)\n\n    # Create a matrix of zeros with the same shape as a\n    i = np.zeros_like(a)\n\n    # Create a matrix of ones with the same shape as a\n    j = np.ones_like(a)\n\n    # Create a matrix of zeros with the same", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "# calculate the t-statistic\nt_statistic = np.mean(a) - np.mean(b)\n\n# calculate the p-value\np_value = scipy.stats.ttest_ind(a, b, equal_var=True, alternative='two-sided')[1]\nprint(p_value)\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "# calculate the t-test\nt_statistic, p_value = scipy.stats.ttest_ind(a, b, equal_var=True, nan_policy='omit')\nprint(p_value)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the weighted mean and variance\nweighted_mean = (anobs * amean + bnobs * bmean) / (anobs + bnobs)\nweighted_var = np.sqrt((anobs * avar + bnobs * bvar) / (anobs + bnobs))\n\n# calculate the t-value\nt_value = weighted_mean - bmean\n\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_value, anobs + bnobs - 2)\n\n# print the p-value\nprint(p_value)", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.setdiff1d(np.setdiff1d(A, B), B)\nprint(output)\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape)\nprint(c)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape)\nprint(c)\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape)\nprint(c)\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)\nresult = np.argsort(a, axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)\nprint(a)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\nprint(result)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\nprint(a)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(a == a[0], axis=1)", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(a == a[0], axis=0)", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.equal(a[0], a[1:]), axis=1)\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\nprint(result)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    # return (np.cos(x))**4 + (np.sin(y))**2\n    # return np.cos(x)**4 + np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.cos(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.cos(x)*np.sin(y)**2\n    # return np.sin(x)*np.cos(y)**2\n    # return np.sin(x)*np.sin(y)**2\n    # return np.", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\n# compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow = np.min(grades)\nhigh = np.max(grades)\nwhile high - low > threshold:\n  mid = (low + high) / 2\n  if ecdf(grades)[mid] < threshold:\n    high = mid\n  else:\n    low = mid\n\nprint(low, high)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "randomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.choice([0, 1], size, p=[one_ratio, 1-one_ratio])\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.tensor(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\nprint(a_tf)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\nresult = result.tolist()\nresult = [result[i] for i in range(len(result))]\nprint(result)\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argpartition(a, -N)[:N]\nresult = result[np.argsort(a[result])][:N]\nresult = result.tolist()\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])\nresult = result.reshape(-1, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result.shape[0], 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\nresult = result.reshape(result", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "patch_size = 2\na_shape = a.shape\npatch_shape = (a_shape[0] // patch_size, a_shape[1] // patch_size)\nresult = np.array([a[i:i+patch_shape[0], j:j+patch_shape[1]] for i in range(patch_shape[0]) for j in range(patch_shape[1])])\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result)", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "patch_size = 2\na_shape = a.shape\npatch_shape = (a_shape[0] // patch_size, a_shape[1] // patch_size)\nresult = np.array([a[i:i+patch_shape[0], j:j+patch_shape[1]] for i in range(patch_shape[0]) for j in range(patch_shape[1])])\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch_shape[1]]\nresult = result[:patch_shape[0], :patch", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "import numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(string)\nprint(a)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.random.loguniform(min, max, n)\nprint(result)", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.random.loguniform(min, max, n)\nprint(result)", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.log(np.random.uniform(min, max, n))\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))\nprint(result)\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))\nprint(result)\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "def sub2ind(dims, index):\n    return np.prod(dims) * index[0] + np.prod(dims[:-1]) * index[1] + index[2]\n\nresult = sub2ind(dims, index)\nprint(result)\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df.values.dtype)\nprint(df)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.sum(a[accmap])\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])\nprint(result)\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0], axis=0)\nprint(result)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.minimum(a[index], a[index[index>=0]])\nresult = result[index[index>=0]]\nresult", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.array([elementwise_function(x[i], y[i]) for i in range(len(x))])\nprint(z)\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, probabilit)\nprint(result)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "result = np.pad(a, ((0, 0), (low_index, high_index)), 'constant')\nresult[1:3, 1:3] = 0\nresult[-1:2, -1:2] = 0\nresult", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.logical_not(np.iscomplex(x))]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_max = np.max(bin_data, axis=1)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (1 + 3*(x - x_min)**2 - 2*(x - x_min)**3)\n\nx_smooth = smoothclamp(x, x_min, x_max)\nprint(x_smooth)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0, 1)\n    t = t * t * (3 - 2 * t)\n    return x_min + (x_max - x_min) * t * (1 - t)\n\nx_smooth = smoothclamp(x, x_min, x_max, N)\nprint(x_smooth)\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "result = np.correlate(a, b, mode='full')\nprint(result)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.values.reshape(4, 15, 5)\nprint(result)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array(df.groupby(['major', 'timestamp']).mean()).reshape(15, 4, 5)\nprint(result)\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.unpackbits(np.uint8(a))\nresult = np.reshape(result, (a.shape[0], m))\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:m]\nresult = np.reshape(result, (a.shape[0], m))\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "result = np.unpackbits(np.uint8(a))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concatenate((result, np.zeros((1, m))))\nresult = np.concatenate((result, np.ones((1, m))))\nresult = np.concaten", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc -= 3*\u03c3\n\u03bc += 3*\u03c3\nresult = (\u03bc-\u03c3, \u03bc+\u03c3)\nresult", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc - 2\u03c3\n\u03bc + 2\u03c3", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std = np.std(a)\n    third_std_dev = np.mean(a) - 3*std\n    start = mean - third_std_dev\n    end = mean + third_std_dev\n    return start, end\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd = np.std(a)\nlower_bound = mean - 2*std\nupper_bound = mean + 2*std\nresult = a < lower_bound or a > upper_bound\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.data, percentile)\nprint(prob)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, :] = 0\na[:, 0] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.all(a == np.max(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "result = np.corrcoef(post, distance)[0, 1]", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.dot(X, X.T)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((Y.shape[0], Y.shape[1]))\nfor i in range(Y.shape[1]):\n    X[:, i] = np.dot(Y[:, i], Y[:, i].T)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = np.any(a == number)", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\nC = A[~np.isin(A, B)]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = np.array([x for x in A if x in B])\n", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "# Create a boolean array that indicates whether each element in A is in B\nmask = np.in1d(A, B)\n# Use the mask to index A and get the result\nC = A[mask]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.max(rankdata(a)) - np.min(rankdata(a)) + 1 - np.min(rankdata(a))\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult =", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.max(rankdata(a)) - np.min(rankdata(a)) + 1 - np.min(rankdata(a))\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult =", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    result = np.max(a) - np.min(a) - 1\n    return result\n\nprint(f())\n### END SOLUTION #!!!", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()\nprint(result)\n", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))\nprint(arr.shape)", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.linalg.norm(X, ord=1, axis=1)\nresult = X / l1.reshape(-1, 1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.linalg.norm(X, ord=2, axis=1)\nresult = X / x[:, np.newaxis]\nprint(result)\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = np.divide(X, x[:, np.newaxis])\nprint(result)\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].str.contains(target)\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "result = np.sqrt(np.sum((a[:, None, :] - a[None, :, :])**2, axis=2))", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "import numpy as np\n\ndef calculate_distance(a):\n    n = a.shape[0]\n    result = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                result[i, j] = np.sqrt(np.sum((a[i] - a[j]) ** 2))\n    return result\n\nresult = calculate_distance(a)\nprint(result)\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "import numpy as np\n\ndef calculate_distance(a):\n    n = a.shape[0]\n    result = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            result[i, j] = np.sqrt(np.sum((a[i] - a[j]) ** 2))\n    return result\n\nresult = calculate_distance(a)\nprint(result)\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "# Remove the adjacent (before removing) duplicate non-zero value and all the zero value\nresult = np.unique(a[a != 0])\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "# Remove adjacent (before removing) duplicate non-zero value and all the zero value\na = np.where(a[:-1] != 0, a[1:], a)\n\n# Remove all the zero value\na = a[a != 0]\n\n# Reshape the array to the original shape\nresult = a.reshape(-1, 1)\n\nprint(result)\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    df = df.sort_values(['lat', 'lon'])\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "lat_lon_val = np.stack((lat, lon, val), axis=1)\nmax_values = np.max(lat_lon_val, axis=1)\ndf = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val, 'maximum': max_values})\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, size[0])\n    strides = a.strides + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape, strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, size[0])\n    strides = a.strides + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape, strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:-1] # for 2D array\nresult = Z[:,:,:-1] # for 3D array\n", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = c in CNTS\nprint(result)", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = c in CNTS\nprint(result)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\ndf", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.matrix([[12.22151125, 0, 0, 0],[0,4.92815942, 0, 0],[0,0,2.06380839,0 ],[0,0,0,0.29766152]])\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a = np.where(np.triu(a, 1) == 0, 0, a)", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\n\n# Calculate the number of days between the start and end dates\nnum_days = (end_date - start_date).days\n\n# Create a linear space of dates\ndates = np.linspace(start_date, end_date, num_days)\n\n# Convert the dates to pandas DatetimeIndex\nresult = pd.DatetimeIndex(dates)\n\nprint(result)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    result = -1\nelse:\n    result = result\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))\nresult = indices[0]", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "# calculate the coefficients a, b and c using the numpy.polyfit function\nresult = np.polyfit(x, y, 2)\n# print the result\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "# Create a matrix with the x values and a column of ones\nx_matrix = np.column_stack((np.ones(len(x)), x))\n# Calculate the coefficients using the numpy linalg.solve function\ncoeff = np.linalg.solve(x_matrix, y)\n# Reverse the coefficients to get the highest order to lowest order\nresult = np.flip(coeff)\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = np.array([0, 1, 2, 3])\ntemp_df = df - temp_arr[:, None]\ntemp_df = temp_df.apply(lambda x: x.tolist())\nprint(temp_df)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)\nprint(result)\n", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled.mean(axis=0)\nresult", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\nprint(result)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\nprint(result)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "mask = arr < -10\narr[mask] = 0\nmask = arr >= 15\narr[mask] = 30\nmask = arr >= -10\narr[mask] += 5\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr_temp = arr.copy()\nmask = arr_temp < n1\nmask2 = arr_temp < n2\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arr_temp[mask3] + 5\narr[~mask2] = 30 \n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "result = np.nonzero(s1 != s2)[0].shape[0]\nprint(result)\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "result = np.nonzero(s1 != s2)[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) == np.isnan(s2))[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a[1]) # compare the first array with the second array\nfor i in range(1, len(a)):\n    result = result and np.array_equal(a[0], a[i]) # compare the first array with the i-th array\nprint(result) # print the result\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = np.all(np.isnan(a), axis=1)", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum shape\n    max_shape = np.", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum shape\nmax_shape = np.array(shape)\n# calculate the maximum", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0]), b[:, 0], b[:, 1]]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b[:, 0]], axis=1)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b[:, 0]], axis=2)\nprint(result)\n", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = np.where(df['a'] > 1, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "# Create a mask that is True for all rows except the first and last row, and all columns except the first and last column\nmask = np.logical_and(np.logical_not(np.all(im[1:-1, 1:-1] == 0, axis=1)), np.logical_not(np.all(im[1:-1, 1:-1] == 0, axis=0)))\n\n# Create a new array with the rows and columns that are not masked\nresult = im[np.logical_not(mask)]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "# Find the indices of the non-zero elements\nindices = np.where(A != 0)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin(A, axis=1)\n# Get the indices of the maximum value in each row and column\nmax_indices = np.argmax(A, axis=0)\nmax_indices = np.argmax(A, axis=1)\n# Get the indices of the minimum value in each row and column\nmin_indices = np.argmin(A, axis=0)\nmin_indices = np.argmin", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "def remove_peripheral_zeros(im):\n    rows, cols = im.shape\n    mask = np.ones((rows, cols), dtype=bool)\n    for i in range(rows):\n        if np.all(im[i, :] == 0):\n            mask[i, :] = False\n        if np.all(im[i, :] == 1):\n            mask[i, :] = False\n    for j in range(cols):\n        if np.all(im[:, j] == 0):\n            mask[:, j] = False\n        if np.all(im[:, j] == 1):\n            mask[:, j] = False\n    return im[mask]\n\nresult = remove_peripheral_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "# Create a mask that is True for all rows except the first and last row, and all columns except the first and last column\nmask = np.logical_and(np.logical_not(np.all(im[1:-1, 1:-1] == 0, axis=1)), np.logical_not(np.all(im[1:-1, 1:-1] == 0, axis=0)))\n\n# Create a new array with the rows and columns that are not masked\nresult = im[np.logical_not(mask)]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\nsns.scatterplot(x, y, label='x-y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "# create a line with random y\ny = np.random.rand(10)\n\n# create a line with random y\ny_random = np.random.rand(10)\n\n# create a line with random y\ny_random_2 = np.random.rand(10)\n\n# create a line with random y\ny_random_3 = np.random.rand(10)\n\n# create a line with random y\ny_random_4 = np.random.rand(10)\n\n# create a line with random y\ny_random_5 = np.random.rand(10)\n\n# create a line with random y\ny_random_6 = np.random.rand(10)\n\n# create a line with random y\ny_random_7 = np.random.rand(10)\n\n# create a line with random y\ny_random_8 = np.random.rand(10)\n\n# create a line with random y\ny_random_9 = np.random.rand(10)\n\n# create a line with random y\ny_random_10 = np.random.rand(10)\n\n# create a line with random y\ny_random_11 = np.random.rand(10)\n\n# create a line with random y\ny_random_12 = np.random.rand(10)\n\n# create a line with random y\ny_random_13 = np.random.rand(10)\n\n# create a line with random y\ny_random_14 = np.random.rand(10)\n\n# create a line with random y\ny_random_15 = np.random.rand(10)\n\n# create a line with random y\ny_random_16 = np.random.rand(10)\n\n# create a line with random y\ny_random_17 = np.random.rand(10)\n\n# create a line with random y\ny_random_18 = np.random.rand(10)\n\n# create a line with random y\ny_random_19 = np.random.rand(10)\n\n# create a line with random y\ny_random_20 = np.random.rand(10)\n\n# create a line with random y\ny_random_21 = np.random.rand(10)\n\n# create a line with random y\ny_random_22 = np.random.rand(10)\n\n# create a line with random y\ny_random_23 = np.random.rand(10)\n\n# create a line with random y\ny_random_24 = np.random.rand(10)\n\n# create a line with random y\ny_random_25 = np.random.rand(10)\n\n# create a line with random y\ny_random_26 = np.random.rand(10)\n\n# create a line with random y\ny_random_27 = np.random.rand(10)\n\n# create a line with random y\ny_random_28 = np.random.rand(10)\n\n# create a line with random y\ny_random_29 = np.random.rand(10)\n\n# create a line with random y\ny_random_30 = np.random.rand(10)\n\n# create a line with random y\ny_random_31 = np.random.rand(10)\n\n# create a line with random y\ny_random_32 = np.random.rand(10)\n\n# create a line with random y\ny_random_33 = np.random.rand(10)\n\n# create a line with random y\ny_random_34 = np.random.rand(10)\n\n# create a line with random y\ny_random_35 = np.random.rand(10)\n\n# create a line with random y\ny_random_36 = np.random.rand(10)\n\n# create a line with random y\ny_random_37 = np.random.rand(10)\n\n# create a line with random y\ny_random_38 = np.random.rand(10)\n\n# create a line with random y\ny_random_39 = np.random.rand(10)\n\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "# create a line with random y\ny = np.random.rand(10)\n\n# create a line with random y\ny_random = np.random.rand(10)\n\n# create a line with random y\ny_random_2 = np.random.rand(10)\n\n# create a line with random y\ny_random_3 = np.random.rand(10)\n\n# create a line with random y\ny_random_4 = np.random.rand(10)\n\n# create a line with random y\ny_random_5 = np.random.rand(10)\n\n# create a line with random y\ny_random_6 = np.random.rand(10)\n\n# create a line with random y\ny_random_7 = np.random.rand(10)\n\n# create a line with random y\ny_random_8 = np.random.rand(10)\n\n# create a line with random y\ny_random_9 = np.random.rand(10)\n\n# create a line with random y\ny_random_10 = np.random.rand(10)\n\n# create a line with random y\ny_random_11 = np.random.rand(10)\n\n# create a line with random y\ny_random_12 = np.random.rand(10)\n\n# create a line with random y\ny_random_13 = np.random.rand(10)\n\n# create a line with random y\ny_random_14 = np.random.rand(10)\n\n# create a line with random y\ny_random_15 = np.random.rand(10)\n\n# create a line with random y\ny_random_16 = np.random.rand(10)\n\n# create a line with random y\ny_random_17 = np.random.rand(10)\n\n# create a line with random y\ny_random_18 = np.random.rand(10)\n\n# create a line with random y\ny_random_19 = np.random.rand(10)\n\n# create a line with random y\ny_random_20 = np.random.rand(10)\n\n# create a line with random y\ny_random_21 = np.random.rand(10)\n\n# create a line with random y\ny_random_22 = np.random.rand(10)\n\n# create a line with random y\ny_random_23 = np.random.rand(10)\n\n# create a line with random y\ny_random_24 = np.random.rand(10)\n\n# create a line with random y\ny_random_25 = np.random.rand(10)\n\n# create a line with random y\ny_random_26 = np.random.rand(10)\n\n# create a line with random y\ny_random_27 = np.random.rand(10)\n\n# create a line with random y\ny_random_28 = np.random.rand(10)\n\n# create a line with random y\ny_random_29 = np.random.rand(10)\n\n# create a line with random y\ny_random_30 = np.random.rand(10)\n\n# create a line with random y\ny_random_31 = np.random.rand(10)\n\n# create a line with random y\ny_random_32 = np.random.rand(10)\n\n# create a line with random y\ny_random_33 = np.random.rand(10)\n\n# create a line with random y\ny_random_34 = np.random.rand(10)\n\n# create a line with random y\ny_random_35 = np.random.rand(10)\n\n# create a line with random y\ny_random_36 = np.random.rand(10)\n\n# create a line with random y\ny_random_37 = np.random.rand(10)\n\n# create a line with random y\ny_random_38 = np.random.rand(10)\n\n# create a line with random y\ny_random_39 = np.random.rand(10)\n\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, 'o-')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.plot(x, y, 'd-', linewidth=2, markersize=10, marker='d')\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "highlight_range = np.array([2, 4])\nhighlight_color = 'r'\nhighlight_linewidth = 2\nhighlight_label = 'Highlight'\n\nplt.plot(highlight_range, np.zeros_like(highlight_range), color=highlight_color, linewidth=highlight_linewidth, label=highlight_label)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = x * 2 + 2\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    style=\"Gender\",\n    alpha=0.6,\n    s=50,\n    legend=False,\n)\n\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set(style=\"whitegrid\")\nsns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "# create a dataframe from x and y\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# draw a line plot\nsns.lineplot(x='x', y='y', data=df)\n\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.legend(x, x, title='xyz', fontsize=20)\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, facecolor='none')\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"black\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), x, 'r-', lw=5, markersize=30)\nplt.plot(range(10), y, 'r-', lw=5, markersize=30)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.arange(0, 2 * np.pi, 2)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='best')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticks(g.get_xticks(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# fit a very long title myTitle into multiple lines\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = np.sort(y)[::-1]\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.array([0, 1.5])\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "y_ticks = np.array([-1, 1])\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "x_cover = np.concatenate((x, y))\ny_cover = np.concatenate((y, z))\nz_cover = np.concatenate((z, x))\n\nplt.figure(figsize=(10, 10))\nplt.plot(x_cover, label='x')\nplt.plot(y_cover, label='y')\nplt.plot(z_cover, label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().yaxis.set_major_formatter(plt.matplotlib.ticker.ScalarFormatter())\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax = sns.lineplot(x=x, y=y, linestyle='-')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[0].legend()\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y2')\naxs[1].legend()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\nax[0].plot(x, y1, label='sin(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y1')\nax[0].set_title('sin(x)')\nax[0].legend()\n\nax[1].plot(x, y2, label='cos(x)')\nax[1].set_xlabel('x')\nax[1].set_ylabel('y2')\nax[1].set_title('cos(x)')\nax[1].legend()\n\n# remove the frames from the subplots\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\n\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks([])\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4), ['x'])\nplt.grid(axis='x', which='both', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 4))\nplt.xticks(np.arange(1, 2))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), constrained_layout=True)\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlim(0, 4)\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().tick_params(axis='y')\nplt.gca().tick_params(axis='x')\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\nplt.xlabel('X')\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green', line_color='blue')\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', line_color='g')\n\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x=tips['total_bill'], y=tips['tip'])\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 5))\nfor celltype in df[\"celltype\"].unique():\n    data = df[df[\"celltype\"] == celltype]\n    plt.bar(data[\"s1\"], data[\"s2\"], label=celltype)\n\nplt.xlabel(\"s1\")\nplt.ylabel(\"s2\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y over X')\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, cmap='hot', aspect='auto')\nplt.xticks(range(4), xlabels, rotation=90)\nplt.yticks(range(4), ylabels)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(211)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax.set_ylim(-20, 100)\n\nax2 = fig.add_subplot(212)\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax2.legend(loc=0)\nax2.grid()\nax2.set_xlabel(\"Time (h)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax2.set_ylim(-20, 100)\n\nplt.show()\nplt.clf()\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(x, y, label='Y')\n\naxs[0].set_title('Y')\naxs[1].set_title('Y')\n\naxs[0].legend()\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.scatter(a, b, c)\nfor i, j in enumerate(c):\n    plt.annotate(j, (a[i], b[i]))\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(labelspacing=0.0, title='Legend')\nplt.legend(title='Legend', title_fontsize=14, title_fontweight='bold')\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, alpha=0.5, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, ax1 = plt.subplots(3, 1, figsize=(10, 10))\n\n# Make the first subplot.\nax1.plot(x, y, 'r')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('First subplot')\n\n# Make the second subplot.\nax2 = ax1.twinx()\nax2.plot(x, y, 'g')\nax2.set_ylabel('y')\nax2.set_title('Second subplot')\n\n# Make the third subplot.\nax3 = ax1.twinx()\nax3.plot(x, y, 'b')\nax3.set_ylabel('y')\nax3.set_title('Third subplot')\n\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Set the legend of the chart\nplt.legend()\n\n# Set the title of the chart\nplt.title('Histogram of x and y')\n\n# Set the x-axis label\nplt.xlabel('x')\n\n# Set the y-axis label\nplt.ylabel('Count')\n\n# Show the chart\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist([x, y], bins=10, alpha=0.5, label=['x', 'y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "x = np.linspace(0, 5, 100)\ny = a*x + b\n\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n# create two colormaps\ncmap_x = plt.get_cmap('viridis')\ncmap_y = plt.get_cmap('viridis')\n\n# create two subplots\nax[0].imshow(x, cmap=cmap_x)\nax[1].imshow(y, cmap=cmap_y)\n\n# create a colorbar for the two subplots\ncbar_x = plt.colorbar(ax[0].imshow(x, cmap=cmap_x), ax=ax[0])\ncbar_y = plt.colorbar(ax[1].imshow(y, cmap=cmap_y), ax=ax[1])\n\n# set the colorbar for the two subplots\ncbar_x.set_label('x')\ncbar_y.set_label('y')\n\n# set the title of the two subplots\nax[0].set_title('x')\nax[1].set_title('y')\n\n# set the title of the entire figure\nfig.set_title('x and y')\n\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[0].plot(x, z, label='Z')\naxs[1].plot(a, y, label='Y')\naxs[1].plot(a, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].set_title('Y and Z')\naxs[0].legend()\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.plot(points)\nplt.yscale('log')\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'r')\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "plt.plot(c[:, 0], c[:, 1], c[:, 2], c[:, 3])\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.plot(x, y)\nplt.loglog(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-log plot')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\n\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "# Create a histogram of the data\nplt.hist(data, bins=np.arange(0, 21000, 1000), edgecolor='black')\n\n# Set the y tick labels as percentage\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\npl", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, alpha=0.5, linestyle='-')\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[0].legend()\naxs[1].plot(x, a, label='a')\naxs[1].legend()\nfig.legend(axs[0], ['y'], loc='center')\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=False, sharey=False, figsize=(10, 10))\n\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\n\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['second'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "x_ticks = np.arange(0, 10, 2)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\nplt.tight_layout()\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.yticks(np.arange(-60, 60, 10))\nplt.xticks(np.arange(2010, 2020, 1))\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.xticks(np.arange(2010, 2020), np.arange(2010, 2020), alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[1].plot(x, y, label='y')\n\naxs[0].set_title('Subplot 1')\naxs[1].set_title('Subplot 2')\n\naxs[0].set_xlabel('x')\naxs[1].set_xlabel('x')\n\naxs[0].set_ylabel('y')\naxs[1].set_ylabel('y')\n\naxs[0].legend()\naxs[1].legend()\n\nfig.suptitle('Figure')\n\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df['Type A'], df['Type B'])\nplt.xlabel('Type A')\nplt.ylabel('Type B')\nplt.title('Line Chart of Type A and Type B')\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='||')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none')\nplt.plot(x, y, '|', color='black')\nplt.show()\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*!', s=100, hatch='v')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "xlim = [1, 5]\nylim = [1, 4]\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.heatmap(data, xlim=xlim, ylim=ylim)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.legend(['cutoff'])\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 10))\nplt.pie(height, labels=labels, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedge_width=0.4)\n\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b-')\nplt.grid(True, linestyle='-', color='0.6')\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle=':')\nplt.grid(which='major', linestyle='-')\nplt.show()\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True, explode=(0.1, 0.1, 0.1, 0.1), textprops={'color': 'white'}, pctdistance=0.85, center=(0.5, 0.5), shadow=True)\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True, explode=(0.1, 0.1, 0.1, 0.1), textprops={'color': 'white'}, pctdistance=0.85, center=(0.5, 0.5), shadow=True)\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'r-', alpha=0.5, linewidth=2, edgecolor='b')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color='green')\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Create a bar plot\nplt.figure(figsize=(10, 5))\n\n# Create a bar plot\nplt.bar(np.arange(3), blue_bar, alpha=0.5, color='b', label='Blue bars')\nplt.bar(np.arange(3), orange_bar, alpha=0.5, color='orange', label='Orange bars')\n\n# Set the x-axis labels\nplt.xlabel('Index')\n# Set the y-axis labels\nplt.ylabel('Height')\n# Set the title of the plot\nplt.title('Blue and Orange bars side by side')\n\n# Add a legend to the plot\nplt.legend()\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, ax1 = plt.subplots()\n\nax1.plot(x, y, 'r-')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Line chart of y over x')\n\nax2 = ax1.twinx()\nax2.plot(a, z, 'b-')\nax2.set_ylabel('z')\nax2.set_title('Line chart of z over a')\n\nax1.legend(['y over x', 'z over a'], loc='upper right')\n\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.axis([0, 10, 0, 10])\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(data=df, x=\"bill_length_mm\", y=\"species\", hue=\"sex\", col=\"species\", sharey=False, col_wrap=3, height=5, aspect=0.8, kind=\"bar\")\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(10, 10))\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(loc='center right', bbox_to_anchor=(0.9, 0.5), shadow=True, ncol=1, handlelength=0.3)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.plot(x, y, marker=\"*\", label=\"Line\")\nplt.legend(loc='best')\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.plot(x, y)\nplt.title('Figure 1')\nplt.bold('Figure')\nplt.bold('1')\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n\n# Create a scatter plot of x over y\nplt.scatter(x, y, s=100)\n\n# Create a line plot of x over y\nplt.plot(x, y, 'r')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'g')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'b')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'y')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'm')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'c')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'k')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'r')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'g')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'b')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'y')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'm')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'c')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'k')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'r')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'g')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'b')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'y')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'm')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'c')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'k')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'r')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'g')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'b')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'y')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'm')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'c')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'k')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'r')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'g')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'b')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'y')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'm')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'c')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'k')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'r')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'g')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'b')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'y')\n\n# Create a line plot of x over y\nplt.plot(x, y, 'm')\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='r', edgecolor='k')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax in axs:\n    ax.plot(x, y)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 10, 2), edgecolor='black')\nplt.xlabel('x')\nplt.ylabel('Count')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'o-')\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 0], 'w')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(z, a, label='Z')\naxs[0].set_title('Y')\naxs[1].set_title('Z')\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_position([", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs:\n    ax.plot(x, y)\n    ax.set_xticks(np.arange(10))\n    ax.set_yticks(np.arange(10))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1)\nplt.figure(fignum=1, figsize=(8, 8))\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 6))\nplt.table(\n    df,\n    col_width=[1.0, 1.0, 1.0, 1.0, 1.0],\n    col_names=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    row_names=df.index,\n    bbox=[0, 0, 1, 1],\n    cell_contents=df.values,\n    cell_linewidth=1,\n    cell_edge_color=\"white\",\n    edge_color=\"white\",\n    linewidth=1,\n    linewidth_linewidth=1,\n    linewidth_edge_color=\"white\",\n    linewidth_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color=\"white\",\n    linewidth_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_edge_color_", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\",", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind_time = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse_time = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time_kind = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind_time_diet = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse_time_kind = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time_kind_pulse = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind_time_diet_pulse = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse_time_kind_diet = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time_kind_pulse_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind_time_diet_pulse_diet = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse_time_kind_diet_pulse = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time_kind_pulse_diet_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind_time_diet_pulse_diet_diet = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse_time_kind_diet_pulse_diet = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time_kind_pulse_diet_pulse = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind_time_diet_pulse_diet_pulse = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse_time_kind_diet_pulse_diet_pulse = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time_kind_pulse_diet_pulse_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind_time_d", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.7, kind=\"point\")\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Plot of y over x')\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='upper right', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 6))\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=0.1, edgecolor=\"black\")\nstripplot.set_axis_labels(\"Sex\", \"Bill Length (mm)\")\nstripplot.set_title(\"Bill Length by Sex and Species\")\nstripplot.set_xlabel(\"Sex\")\nstripplot.set_ylabel(\"Bill Length (mm)\")\nstripplot.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\nsns.FaceGrid(axs, gs=np.array([[1, 1]]), margin=0.1, margin_left=0.1, margin_right=0.1, margin_top=0.1, margin_bottom=0.1, width_ratios=[1, 1], height_ratios=[1, 1], aspect=1)\nsns.pointplot(x=\"a\", y=\"c\", hue=\"b\", data=df, join=False, palette=\"Set2\")\naxs.set_xticks(np.arange(1, 31, 2))\naxs.set_xticklabels(np.arange(1, 31, 2))\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_view_angle(30, 30)\nax.set_view_angle(100, 50)\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.hidexticks()\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[1, 1], height_ratios=[1, 1])\n\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x[i, j])\n        ax.axis('off')\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\nprint(x.numpy()) # prints 1\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.numpy()) # print the value of x\nprint(x.", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nresult = tf.cast(result, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nresult = tf.reshape(result, [-1, 10])\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nresult = tf.reshape(result, [-1, 10])\nresult = tf.cast(result, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    # Create a tensor of zeros with the same shape as the labels\n    result = tf.zeros_like(labels)\n\n    # Set the value of the first class to 1\n    result[0] = 1\n\n    # Set the value of the last class to 1\n    result[-1] = 1\n\n    # Return the result\n    return result\n\n# Call the function\nresult = f(example_labels)\nprint(result)\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nresult = tf.reshape(result, [-1, 10])\nresult = tf.cast(result, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.compat.v1.py_func(\n    func=lambda x: [x, x+1, x+2],\n    inp=[i],\n    Tout=[tf.int64]\n  )\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=my_map_func)\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return tf.concat([input, input + 1, input + 2], axis=0)\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(f)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\npadding = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.where(padding < lengths, tf.ones_like(lengths), tf.zeros_like(lengths))\n\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths), 8))\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones(shape=(len(lengths),", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "lengths = tf.constant([4, 3, 5, 2])\ntotal_length = 8\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s corresponding to the lengths\nmask = tf", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([a, b], axis=1)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    # Use tf.stack to create a 2D tensor with the elements of a and b\n    # Then use tf.transpose to transpose the tensor\n    # Finally use tf.reshape to reshape the tensor to a 2D tensor\n    # result = tf.reshape(tf.transpose(tf.stack([a, b])), [-1, 2])\n    ### END SOLUTION ###\n\n    # return result\n    # return tf.reshape(tf.transpose(tf.stack([a, b])), [-1, 2])\n\nprint(f())\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "result = tf.reshape(a, (50, 100, 512))\nprint(result)\n", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.reshape(a, (50, 100, 1, 512))\nprint(result)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "result = tf.reshape(a, (1, 50, 100, 1, 512))\nprint(result)\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.math.prod(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "# calculate the L2 distance d(A,B) element-wise\nresult = tf.square(a - b)\n# then reduce by row\nresult = tf.reduce_sum(result, axis=0)\nprint(result)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "# calculate the L2 distance d(A,B) column-wise\nl2_distance = tf.square(a - b)\n# reduce by column\nresult = tf.reduce_sum(l2_distance, axis=0)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    # calculate the L2 distance d(A,B) element-wise\n    # then reduce by row\n    # result = tf.reduce_sum(tf.square(tf.sub(A, B)), axis=0)\n    # result = tf.reduce_sum(result, axis=1)\n    # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce_sum(result, axis=1)\n    # # result = tf.reduce_sum(result, axis=0)\n    # # result = tf.reduce", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = x[y,z]\nresult = m.numpy()\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = x[row, col]\nresult = m.numpy()\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    return x[y,z]\n\nresult = f(example_x,example_y,example_z)\nprint(result)\n### END SOLUTION #!!!", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\n# Create a tensor of shape [Batch_size, Batch_size, Num_vectors]\nC = tf.constant(np.dot(A, B))\n\n# Reshape the tensor to [Batch_size, Batch_size, Num_vectors]\nC = tf.reshape(C, [A.shape[0], A.shape[0], A.shape[1]])\n\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "result = tf.einsum('ijk,ik->ijk', A, B)\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "x = tf.constant(x)\nresult = tf.strings.unicode_transpose(x)\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    # Convert the bytes to strings\n    result = tf.strings.unicode_transpose(x)\n    # Return the result\n    return result\n\n# Call the function\nprint(f())\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "# Get the number of non-zero entries in the second to last dimension\nnum_non_zero = tf.reduce_sum(x, axis=-2, keepdims=True)\n\n# Divide by the number of non-zero entries\nresult = tf.divide(x, num_non_zero)\nresult = tf.reshape(result, x.shape)\nprint(result)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "x_nonzero = tf.where(x != 0, x, tf.zeros_like(x))\nresult = tf.reduce_mean(x_nonzero, axis=-1)\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nprint(result)\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": " ###\n    # calculate the sum of the non-zero entries\n    sum_non_zero = tf.reduce_sum(x, axis=-1)\n    # calculate the average of the non-zero entries\n    average = tf.reduce_mean(x, axis=-1)\n    # calculate the result\n    result = tf.where(sum_non_zero > 0, average, x)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\n# Create two random matrices\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Calculate the sum of the matrix multiplication\nresult = tf.reduce_sum(tf.matmul(A, B))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "result = tf.argmax(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "result = tf.argmax(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    # Get the maximum value in each row\n    max_values = tf.argmax(a, axis=1)\n    # Convert the maximum value to the index of the highest value in each row\n    result = tf.where(a == max_values[:, None], max_values, 0)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_a))\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n\n# Save the model\nmodel.save('my_model')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "# generate 10 random integers from the uniform distribution\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "# generate 114 random integers from the uniform distribution\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=5, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": " ###\n    tf.random.set_seed(seed_x)\n    random_integers = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n    return random_integers\n\nresult = f(seed_x=10)\nprint(result)\n### END SOLUTION ###\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "import tensorflow as tf\n\n# get the version of tensorflow\nresult = tf.__version__\nprint(result)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "# Fit the polynomial of order 2\nA, B = np.polyfit(x, y, 2)\nresult = np.array([A, B])\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the polynomial of order 1\nA1, B1 = np.polyfit(x, y, 1)\n# Fit the polynomial of order 2\nA2, B2 = np.polyfit(x, y, 2)\n# Fit the polynomial of order 3\nA3, B3 = np.polyfit(x, y, 3)\n# Fit the polynomial of order 4\nA4, B4 = np.polyfit(x, y, 4)\n\n# Create a list of the coefficients\ncoeffs = [A1, B1, A2, B2, A3, B3, A4, B4]\n\n# Create a list of the orders\norders = [1, 2, 3, 4]\n\n# Create a list of the results\nresults = [coeffs, orders]\n\n# Print the results\nprint(results)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, A, B, C):\n    return A * np.exp(B * x) + C\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array([result[0], result[1], result[2]])\nprint(result)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\nresult", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c - 2) ** 2 + ((3 * a - b - c) ** 2 + sin(b) + cos(b) + 4))\n\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.norm.cdf(z_scores, 0, 1)\np_values = 1 - p_values\np_values = p_values\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = scipy.stats.norm.cdf(z_scores, mu, sigma)\np_values = 1 - p_values\np_values = p_values\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = np.mean([scipy.stats.zscore(p_values) for _ in range(10000)])", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)\nprint(result)</code!", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(stddev, scale=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\nprint(expected_value, median)", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb\nprint(result)\nprint(result.toarray())\n", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    result = sA * sB\n    return result\n\nresult = f(example_sA, example_sB)\nprint(result)\nprint(result.toarray())\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a LinearNDInterpolator object\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\n\n# Get the interpolated value\nresult = interpolator(request)\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid of points\nx = np.linspace(points[:, 0].min(), points[:, 0].max(), 100)\ny = np.linspace(points[:, 1].min(), points[:, 1].max(), 100)\nz = np.linspace(points[:, 2].min(), points[:, 2].max(), 100)\n\n# Create a 3D grid of points\nx, y, z = np.meshgrid(x, y, z, indexing='ij')\n\n# Create a 3D grid of values\nV = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of interpolated values\nresult = np.zeros((request.shape[0], 100, 100))\n\n# Interpolate the values\nfor i in range(request.shape[0]):\n    # Create a 3D grid of interpolated values\n    result[i] = scipy.interpolate.LinearNDInterpolator((x, y, z), V)(request[i])\n\nprint(result)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, reshape=False)\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\nxrot, yrot = np.round(xrot), np.round(yrot)\nprint(xrot, yrot)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = np.diagonal(M.toarray())\n", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, 'uniform')\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # compute the K-S test\n    result = stats.kstest(times, 'uniform')\n    # return the result\n    return result\n    ### END SOLUTION ###\n\n# call the function\nresult = f(example_times, example_rate, example_T)\nprint(result)\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, 'uniform')\nprint(result.pvalue)", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = c1.concatenate(c2)\nprint(Feature)\n", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = c1.join(c2)\nprint(Feature)\n", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points2\npoints2_matrix = np.ones((N, 1))\nfor i in range(N):\n    points2_matrix[i, 0] = points2[i, 0]\n# Create a matrix of points with a 1 in the corresponding position for each point in points1\npoints1_matrix = np.ones((N, 1))\nfor i in range(N):\n    points1_matrix[i, 0] = points1[i, 0]\n# Create a matrix of points with a 1 in the corresponding", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\nb = b.toarray()\nb = sparse.csr_matrix(b)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "result = np.where(img > threshold, 1, 0)\nresult = ndimage.label(result)[0]\nresult = np.unique(result)\nresult = np.sum(result)\nprint(result)\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "result = np.where(img < threshold, 1, 0)\nresult = ndimage.label(result)[0]\nresult = np.unique(result, return_counts=True)[1]\nresult = np.sum(result)\nprint(result)\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Create a binary image where the values are greater than the threshold\n    binary_img = img > threshold\n    # Use ndimage.label function to find the regions of the binary image\n    labels = ndimage.label(binary_img)\n    # Return the number of regions\n    return labels[1].max()\n    ### END SOLUTION ###\n\nprint(f(example_img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the center of mass of such regions\ncenter_of_mass = ndimage.center_of_mass(img, mask)\n\n# Find the distance between the center of mass of such regions and the top left corner\ndistance = np.sqrt((center_of_mass[0]-0)**2 + (center_of_mass[1]-0)**2)\n\n# Output the distances as a list\nresult = distance.tolist()\nprint(result)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "# Create a lil_matrix from the sparse matrix\nlil_matrix_M = lil_matrix(M)\n\n# Make the matrix symmetric\nlil_matrix_M.set_shape(lil_matrix_M.shape[1], lil_matrix_M.shape[0])\nlil_matrix_M = lil_matrix_M.transpose()\n\n# Convert back to sparse matrix\nM_symmetric = lil_matrix_M.toarray()\n\n# Print the symmetric matrix\nprint(M_symmetric)\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    # Create a new lil_matrix with the same shape as sA\n    sA_symmetry = lil_matrix(sA.shape)\n\n    # Fill in the symmetric elements\n    for i in range(sA.shape[0]):\n        for j in range(sA.shape[1]):\n            if i == j:\n                sA_symmetry[i, j] = sA[i, j]\n            else:\n                sA_symmetry[i, j] = sA[j, i]\n\n    # Return the symmetric matrix\n    return sA_symmetry\n\n# Call the function\nsA_symmetry = f(example_sA)\nprint(sA_symmetry)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "square = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scipy.ndimage.binary_dilation(square, 1)\nsquare = scipy.ndimage.binary_erosion(square, 1)\nsquare = scip", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n# Create a mask with the same shape as the square array\nmask = np.zeros_like(square, dtype=bool)\n\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_value = np.max(col.toarray())\nmin_value = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Get the median of the column\nmedian = np.median(col.toarray())\n\n# Get the mode of the column\nmode = np.mode(col.toarray())\n\nprint(median)\nprint(mode)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *a):\n    return sum(a[i]*np.cos(x*i*np.pi/tau) for i in range(1, degree+1))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=np.ones(degree+1))\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\narray_1d = example_array.flatten()\n# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.pdist(array_1d, 'euclidean')\n# Convert the distances to a 2D array\ndistances_2d = np.reshape(distances, (len(array_1d), len(array_1d)))\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distance = np.min(distances_2d, axis=1)\n# Convert the minimum distance to meters\nmin_distance_meters = min_distance * np.mean(example_array)\n# Create a 2D array with the minimum distance separating the nearest edges of each raster patch\nresult = np.stack((array_1d, min_distance_meters), axis=1)\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique ID patches\nunique_ids = np.unique(example_array)\n# Create a 2D array with the unique", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate the pairwise Euclidean distances between all regions\n    # Convert the distances to meters by multiplying by the resolution\n    # Return the result in the form of \"from ID, to ID, distance\"\n    ### END SOLUTION ###\n    # return result\n    # result = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n    #                   [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n    #                   [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n    # return result\n    # result = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n    #                   [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n    #                   [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n    #                   [0, 0, 0, 0, 0, 0, ", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(5):\n    x_int = np.interp(x_val, x[:, i], y[:, i])\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    y_int = interpolate.splev(x_val, tck, der = 0)\n    result[i] = y_int\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nx3 = np.array(x3)\nx4 = np.array(x4)\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\nprint(statistic)\nprint(critical_values)\nprint(significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nresult = ss.anderson_ksamp(x1, x2)\nprint(result)\nprint(result[0]) # this will print the test statistic\nprint(result[1]) # this will print the p-value\nprint(result[2]) # this will print the critical values\nprint(result[3]) # this will print the significance level\nprint(result[4]) # this will print the distribution of the test statistic\nprint(result[5]) # this will print the distribution of the critical values\nprint(result[6]) # this will print the distribution of the p-value\nprint(result[7]) # this will print the distribution of the significance level\nprint(result[8]) # this will print the distribution of the critical values\nprint(result[9]) # this will print the distribution of the test statistic\nprint(result[10]) # this will print the distribution of the p-value\nprint(result[11]) # this will print the distribution of the significance level\nprint(result[12]) # this will print the distribution of the critical values\nprint(result[13]) # this will print the distribution of the test statistic\nprint(result[14]) # this will print the distribution of the p-value\nprint(result[15]) # this will print the distribution of the significance level\nprint(result[16]) # this will print the distribution of the critical values\nprint(result[17]) # this will print the distribution of the test statistic\nprint(result[18]) # this will print the distribution of the p-value\nprint(result[19]) # this will print the distribution of the significance level\nprint(result[20]) # this will print the distribution of the critical values\nprint(result[21]) # this will print the distribution of the test statistic\nprint(result[22]) # this will print the distribution of the p-value\nprint(result[23]) # this will print the distribution of the significance level\nprint(result[24]) # this will print the distribution of the critical values\nprint(result[25]) # this will print the distribution of the test statistic\nprint(result[26]) # this will print the distribution of the p-value\nprint(result[27]) # this will print the distribution of the significance level\nprint(result[28]) # this will print the distribution of the critical values\nprint(result[29]) # this will print the distribution of the test statistic\nprint(result[30]) # this will print the distribution of the p-value\nprint(result[31]) # this will print the distribution of the significance level\nprint(result[32]) # this will print the distribution of the critical values\nprint(result[33]) # this will print the distribution of the test statistic\nprint(result[34]) # this will print the distribution of the p-value\nprint(result[35]) # this will print the distribution of the significance level\nprint(result[36]) # this will print the distribution of the critical values\nprint(result[37]) # this will print the distribution of the test statistic\nprint(result[38]) # this will print the distribution of the p-value\nprint(result[39]) # this will print the distribution of the significance level\nprint(result[40]) # this will print the distribution of the critical values\nprint(result[41]) # this will print the distribution of the test statistic\nprint(result[42]) # this will print the distribution of the p-value\nprint(result[43]) # this will print the distribution of the significance level\nprint(result[44]) # this will print the distribution of the critical values\nprint(result[45]) # this will print the distribution of the test statistic\nprint(result[46]) # this will print the distribution of the p-value\nprint(result[47]) # this will print the distribution of the significance level\nprint(result[48]) # this will print the distribution of the critical values\nprint(result[49]) # this will print the distribution of the test statistic\nprint(result[50]) # this will print the distribution of the p-value\nprint(result[51]) # this will print the distribution of the significance level\nprint(result[52]) # this will print the distribution of the critical values\nprint(result[53]) # this will print the distribution of the test statistic\nprint(result", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(df['A']) # keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['AC'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['BC'] = df['B'].rolling(3).apply(lambda x: tau1(x))\n\nprint(df)\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nresult = is_csr_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n\nresult = is_lil_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\nprint(result)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores)[1]", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n\nprint(f())\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "import scipy.stats\n\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "def fuelConsumption(speed, temperature):\n    return (speed + temperature) * np.exp(-6.0 * (speed * speed + temperature * temperature))\n\ndef interpolated(s, t):\n    return np.array([fuelConsumption(s[i], t[i]) for i in range(len(s))])\n\nresult = interpolated(s, t)\nprint(result)\n", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a grid of points for the interpolation\n    x_grid, y_grid = np.meshgrid(s, t, indexing='ij')\n    # Create a function that will be used for the interpolation\n    z_grid = z\n    # Create the interpolation function\n    interpolated = scipy.interpolate.interp2d(s, t, z_grid, kind='cubic')\n    # Compute the values on the grid\n    result = interpolated(x_grid, y_grid)\n    # # END SOLUTION ##!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "# Create a list of points that are not in the voronoi tessellation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\nextraPoints = np.array(extraPoints)\n# Get the voronoi regions for the extra points\nextraPointsRegion = vor.point_contains_region(extraPoints)\n# Count the number of points in each region\nresult = np.sum(extraPointsRegion, axis=1)\nprint(result)\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\nextraPoints = np.array(extraPoints)\n# Get the voronoi regions\nregions = vor.regions\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = np.array(extraPoints)\n# Create a list of points that are not in the", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "# Find the maximum size of the vectors\nmax_size = max([len(v) for v in vectors])\n\n# Create a sparse matrix with zeros\nresult = sparse.csc_matrix(max_size, dtype=np.float64)\n\n# Fill the matrix with the vectors\nfor i, v in enumerate(vectors):\n    result[i, :len(v)] = v\n\n# Fill the rest of the matrix with zeros\nresult[result.shape[0]:, :] = 0\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant')\nb = np.roll(b, 1, axis=1)\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[i, j] for i, j in zip(row, column)]\nprint(result)\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, :, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u,o2,x)\nprint(prob)", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n\n# print the solution\nprint(f(x = 2.5, u = 1, o2 = 3))\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a 2D array with random values\narr = np.random.rand(N, N)\n\n# Compute the DCT matrix\ndct_matrix = sf.dctn(arr, norm='ortho')\n\n# Compute the inverse DCT matrix\ninv_dct_matrix = sf.idctn(dct_matrix, norm='ortho')\n\n# Compare the original array with the inverse DCT matrix\nresult = np.allclose(arr, inv_dct_matrix)", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "result = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result)\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "result = np.array([[scipy.stats.binom.pmf(i, j, p) for j in range(i+1)] for i in range(N+1)])\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x))\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=0)\n\n# create a new dataframe with zscore and data\nresult = pd.DataFrame(zscore, index=df.index, columns=df.columns)\n\n# add a new column for data\nresult['data'] = df.values\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=0)\n\n# round to 3 decimal places\nzscore = np.round(zscore, 3)\n\n# create a new dataframe with zscore\nzscore_df = pd.DataFrame(zscore, index=df.index, columns=df.columns)\n\n# create a new dataframe with data\ndata_df = pd.DataFrame(df.values, index=df.index, columns=df.columns)\n\n# combine the two dataframes\nresult = pd.concat([data_df, zscore_df], axis=1)\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y, x = np.mgrid[0:6, 0:6]\nmid = np.array([[3, 3], [3, 3]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\nmid = np.array([[5., 4., 3., 3., 4., 5.],\n               [4., 3., 2., 2., 3., 4.],\n               [3., 2., 1., 1., 2., 3.],\n               [3., 2., 1., 1., 2., 3.],\n               [4., 3., 2., 2., 3., 4.],\n               [5., 4., 3., 3., 4., 5.]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.mgrid[0:shape[0], 0:shape[1]]\n    mid = np.array([[0, 0], [shape[0] / 2, shape[1] / 2]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint result", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return np.dot(a, x ** 2)\n\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\n\ndef main():\n    x0 = np.array([2, 3, 1, 4, 20])\n    fit_params = scipy.optimize.Parameters()\n    fit_params.add('x', value=x0)\n    out = scipy.optimize.minimize(residual, fit_params, args=(a, y))\n    print(out.x)\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return (y - func(x)) ** 2\n\nout = scipy.optimize.minimize(residual, x0, method='L-BFGS-B', bounds=[x_lower_bounds])\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) * (t < 2*np.pi) * 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])\nprint(result)", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.hstack([sa, sb])\nprint(result)", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\ndef integrand(x, c):\n    return 2*c*x\n\nc = 5\nlow = 0\nhigh = 1\n\nresult, error = scipy.integrate.quad(integrand, low, high, args=(c,))\n\nprint(result)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    def eqn(x):\n        return 2*x*c\n    result, error = scipy.integrate.quad(eqn, low, high)\n    return result\n\n# create an array of c values\nc = array([5, 10, 15])\n# create an array to store the solutions\nI = array([])\n# loop over c values\nfor n in range(len(c)):\n    # calculate the solution for each c value\n    result = f(c=c[n])\n    # append the solution to the array\n    I = append(I, result)\n# print the solutions\nprint(I)\n", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.reshape(-1, 1)\nV_reshaped = V.reshape(-1, 1)\nV_reshaped += x\nV = V_reshaped\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "x = x.reshape(-1, 1)\nV_reshaped = V.reshape(-1, 1)\nV_reshaped += x\nV = V_reshaped\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V += x\nV += y", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    # get the column length\n    Len = math.sqrt(sum(List))\n    # here I assumed dot(number,Column) would do a basic scalar product\n    dot((1/Len),Column)\n    # now what? how do I update the original column of the matrix, everything that have been returned are copies, which drove me nuts and missed pointers so much\n    # you can modify the original column of the matrix by assigning a new value to the column\n    sa[:,Col] = Column\nEND SOLUTION\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor Col in xrange(sa.shape[1]):\n    # get the column\n    Column = sa[:,Col].data\n    # get the column length\n    Len = math.sqrt(sum(Column**2))\n    # normalize the column\n    Column = Column / Len\n    # update the original column of the matrix\n    sa[:,Col] = Column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = np.where(a > 0, 1, 0)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = np.where(a == 0, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# Create a distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# Find the index of the closest element to each centroid\nclosest_indices = np.argmin(distances, axis=1)\n# Find the index of the closest element to each cluster\nresult = np.argmin(closest_indices, axis=0)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# Create a distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# Create a linkage matrix\nlinkage = scipy.spatial.distance.linkage(distances, method='ward')\n# Use the cut_tree function to assign each sample to a cluster\ncluster_assignments = scipy.spatial.distance.cut_tree(linkage, n_clusters=5)\n# Create a dictionary to store the closest points to each cluster\nclosest_points = {}\n# Loop over the clusters\nfor cluster in np.unique(cluster_assignments):\n    # Get the indices of the samples in the cluster\n    cluster_indices = np.where(cluster_assignments == cluster)[0]\n    # Get the centroid of the cluster\n    centroid = centroids[cluster]\n    # Get the closest point to the centroid\n    closest_point = data[cluster_indices[np.argmin(np.linalg.norm(data[cluster_indices] - centroid, axis=1))]]\n    # Add the closest point to the dictionary\n    closest_points[cluster] = closest_point\n# Get the closest points to each cluster\nresult = np.array([closest_points[cluster] for cluster in np.unique(cluster_assignments)])\n", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# Create a distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# Create a list of indices for each cluster\nindices = scipy.spatial.distance.argpartition(distances, k, axis=0)[:k, :]\n# Get the index of the k-closest element to the centroid of each cluster\nresult = indices[:, 0]<result<indices[:, 1]<result<indices[:, 2]<result<indices[:, 3]<result<indices[:, 4]<result<indices[:, 5]<result<indices[:, 6]<result<indices[:, 7]<result<indices[:, 8]<result<indices[:, 9]<result<indices[:, 10]<result<indices[:, 11]<result<indices[:, 12]<result<indices[:, 13]<result<indices[:, 14]<result<indices[:, 15]<result<indices[:, 16]<result<indices[:, 17]<result<indices[:, 18]<result<indices[:, 19]<result<indices[:, 20]<result<indices[:, 21]<result<indices[:, 22]<result<indices[:, 23]<result<indices[:, 24]<result<indices[:, 25]<result<indices[:, 26]<result<indices[:, 27]<result<indices[:, 28]<result<indices[:, 29]<result<indices[:, 30]<result<indices[:, 31]<result<indices[:, 32]<result<indices[:, 33]<result<indices[:, 34]<result<indices[:, 35]<result<indices[:, 36]<result<indices[:, 37]<result<indices[:, 38]<result<indices[:, 39]<result<indices[:, 40]<result<indices[:, 41]<result<indices[:, 42]<result<indices[:, 43]<result<indices[:, 44]<result<indices[:, 45]<result<indices[:, 46]<result<indices[:, 47]<result<indices[:, 48]<result<indices[:, 49]<result<indices[:, 50]<result<indices[:, 51]<result<indices[:, 52]<result<indices[:, 53]<result<indices[:, 54]<result<indices[:, 55]<result<indices[:, 56]<result<indices[:, 57]<result<indices[:, 58]<result<indices[:, 59]<result<indices[:, 60]<result<indices[:, 61]<result<indices[:, 62]<result<indices[:, 63]<result<indices[:, 64]<result<indices[:, 65]<result<indices[:, 66]<result<indices[:, 67]<result<indices[:, 68]<result<indices[:, 69]<result<indices[:, 70]<result<indices[:, 71]<result<indices[:, 72]<result<indices[:, 73]<result<indices[:, 74]<result<indices[:, 75]<result<indices[:, 76]<result<indices[:, 77]<result<indices[:, 78]<result<indices[:, 79]<result<indices[:, 80]<result<indices[:, 81]<result<indices[:, 82]<result<indices[:, 83]<result<indices[:, 84]<result<indices[:, 85]<result<", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result = fsolve(lambda x: eqn(x, x, b), x0=x)\n    print(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result.append(fsolve(eqn, x0=x, args=(a,)))\nresult = np.array(result)\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Fit the function to the sample data\nparams, params_cov = sp.optimize.curve_fit(bekkers_func, range_start, range_end, sample_data, estimated_a, estimated_m, estimated_d)\n\n# Calculate the K-S test\nstatistic, pvalue = stats.kstest(sample_data, bekkers_func(range_start, estimated_a, estimated_m, estimated_d))\n\n# Print the result\nprint(statistic, pvalue)\nresult = (statistic, pvalue)\n", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# generate the sample data\nsample_data = np.random.normal(loc=1.5, scale=0.1, size=1000)\n\n# estimate the parameters of the function\nestimated_a, estimated_m, estimated_d = 1,1,1\n\n# generate the experimental data\nx = np.linspace(range_start, range_end, 1000)\ny = bekkers(x, estimated_a, estimated_m, estimated_d)\n\n# generate the distribution function\ndistribution_function = lambda x: bekkers(x, estimated_a, estimated_m, estimated_d)\n\n# perform the K-S test\nresult = stats.kstest(y, distribution_function)\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def rolling_integral(x, y):\n    return integrate.trapz(x, y)\n\nrolling_integral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral)\nprint(rolling_integral_df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "# Create a 2D array with the points and values\nx_values = np.array([[2, 2], [1, 2], [2, 3], [3, 2], [2, 1]])\ny_values = np.array([5, 7, 8, 10, 3])\n\n# Create a 2D array with the evaluation points\neval_values = np.array([[2.7, 2.3]])\n\n# Use griddata to interpolate the values\nresult = scipy.interpolate.griddata(x_values, y_values, eval_values, method='linear')\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "# calculate the number of categories\nn_categories = a.max() + 1\n\n# calculate the weights\nweights = np.ones(n_categories) / n_categories\n\n# calculate the maximum likelihood estimation\ndef max_likelihood_estimation(weights):\n    return np.sum(weights * a)\n\n# find the best parameters\nbest_weights = sciopt.maximum(weights, max_likelihood_estimation(weights))\n\nprint(best_weights)\n", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "result = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nprint(result)\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "# find the indices of the elements that are less or equal to the neighbouring n elements\nindices = np.where((arr <= np.roll(arr, n)) | (np.roll(arr, -n) <= arr))\n# find the indices of the elements that are less or equal to the neighbouring n elements forwards and backwards\nforward_indices = np.where((arr <= np.roll(arr, n)) | (np.roll(arr, -n) <= arr))\nbackward_indices = np.where((arr >= np.roll(arr, n)) | (np.roll(arr, -n) >= arr))\n# combine the indices of the elements that are less or equal to the neighbouring n elements forwards and backwards\nresult = np.unique(np.concatenate((forward_indices[0], backward_indices[0])))\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "result = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if j == 0:\n            if arr[i, j] <= arr[i, j + 1]:\n                result.append([i, j])\n        elif j == arr.shape[1] - 1:\n            if arr[i, j] <= arr[i, j - 1]:\n                result.append([i, j])\n        else:\n            if arr[i, j] <= arr[i, j - 1] and arr[i, j] <= arr[i, j + 1]:\n                result.append([i, j])\nresult = np.array(result)\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult = result.tolist()\nresult", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "num_cols = df.select_dtypes(include=[np.number]).columns\noutlier_cols = [col for col in num_cols if np.abs(stats.zscore(df[col])) > 3]\ndf = df[outlier_cols]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], data['target']))\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + data['target_names'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], 'target'))\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    # Convert the Bunch object to a DataFrame\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                      columns=np.append(data['feature_names'], data['target_names']))\n    return df\n\n# Call the function\ndata1 = solve(data)\nprint(data1)\n### END SOLUTION #!!", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a list of unique elements in the column\nunique_elements = np.unique(df['Col3'])\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'], values=unique_elements, drop_first=True)\n\n# Add the one-hot-encoded values to the original dataframe\ndf_out = df.join(df_out)\n\n# Drop the original column\ndf_out = df_out.drop(['Col3'], axis=1)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# Create a list of unique names\nnames = np.unique(np.concatenate(df['Col3'].values))\n\n# Create a DataFrame with the unique names as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(names))), columns=names)\n\n# Set the values to 1 for the names that exist in the row\nfor i in range(df.shape[0]):\n    for j in range(len(names)):\n        if names[j] in df.values[i, 2]:\n            df_out.values[i, j] = 1\n\n# Print the DataFrame\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Create a list of unique elements in the last column\nunique_elements = np.unique(df['Col4'])\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col4'], values=unique_elements, drop_first=True)\n\n# Add the one-hot-encoded values to the original dataframe\ndf_out = pd.concat([df, df_out], axis=1)\n\n# Drop the original column\ndf_out = df_out.drop(['Col4'], axis=1)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# Create a list of unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a dataframe with one-hot-encoded values for each unique element\ndf_out = pd.get_dummies(df.iloc[:, -1], values=unique_elements)\n\n# Add the one-hot-encoded values to the original dataframe\ndf_out = df.join(df_out)\n\n# Drop the last column\ndf_out = df_out.drop(columns=[df.columns[-1]])\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# Create a list of unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a dataframe with one-hot-encoded values for each unique element\ndf_out = pd.get_dummies(df.iloc[:, -1], values=unique_elements)\n\n# Add the one-hot-encoded values to the original dataframe\ndf_out = df.join(df_out)\n\n# Drop the last column\ndf_out = df_out.drop(columns=[df.columns[-1]])\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\nproba = np.array([1 / (1 + np.exp(-x)) for x in svmmodel.decision_function(x_test)])\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\nproba = np.array([1 / (1 + np.exp(-x)) for x in model.decision_function(x_predict)])\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "# Convert the transform_output to a DataFrame\ntransform_output_df = pd.DataFrame(transform_output.toarray())\n\n# Create a new DataFrame with the original data and the transform_output_df\ndf = pd.concat([df_origin, transform_output_df], axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "# Create a sparse matrix from the transform_output\ntransform_matrix = csr_matrix(transform_output)\n\n# Create a DataFrame from the transform_matrix\ntransform_df = pd.DataFrame(transform_matrix.toarray())\n\n# Merge the original DataFrame with the transform_df\ndf = pd.concat([df_origin, transform_df], axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # Create a new dataframe with the original data and the transformed output\n    new_df = pd.concat([df, transform_output], axis=1)\n    # Return the new dataframe\n    return new_df\n    ### END SOLUTION ###\n\n# Call the function with your data\nresult = solve(df_origin, transform_output)\nprint(result)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete a step\ndel clf.steps[1]\n# Insert a step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\n# Delete a step\ndel clf.steps[1]\n# Insert a step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline\nprint(clf)", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a new step\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.append(new_step)\n\n# Print the new pipeline\nprint(clf)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\nprint(clf.steps)\n# Delete a step\ndel clf.steps[1]\nprint(clf.steps)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\n# Insert ('t1919810', PCA()) right before 'svdm'\nclf.steps.insert(1, ('t1919810', PCA()))\n\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# Define the parameter grid\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.001],\n    'max_depth': [3, 4, 5],\n    'n_estimators': [100, 200, 300]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the GridSearchCV\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, iid=False)\n\n# Fit the GridSearchCV\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the best iteration\nbest_iteration = gridsearch.best_iteration_\n\n# Get the best prediction\nbest_prediction = gridsearch.best_predictor_\n\n# Get the best score\nb = best_score\n\n# Get the best prediction\nc = best_prediction\n\n# Print the best parameters\nprint(best_params)\n\n# Print the best score\nprint(best_score)\n\n# Print the best iteration\nprint(best_iteration)\n\n# Print the best prediction\nprint(best_prediction)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\n# Define the grid of parameters to search\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.001],\n    'max_depth': [3, 4, 5],\n    'n_estimators': [100, 200, 300]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the fit parameters\nfit_params = {\"early_stopping_rounds\": 42,\n              \"eval_metric\" : \"mae\",\n              \"eval_set\" : [[testX, testY]]}\n\n# Perform the grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\ngridsearch.fit(trainX, trainY, **fit_params)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the prediction\nc = gridsearch.predict(testX)\n\n# Print the best parameters, best score and prediction\nprint(\"Best parameters: \", best_params)\nprint(\"Best score: \", best_score)\nprint(\"Prediction: \", c", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\nproba = np.mean(proba, axis=0)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_pred = scaler.inverse_transform(scaled[:, 0])\n    # return t_pred\n    return t_pred\n\n# solve(data, scaler, scaled)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nproba = grid_search.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)\nprint(predict)", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef preprocessor(s):\n    return preprocess(s)\n\ntfidf = TfidfVectorizer(preprocessor=preprocessor)\n\n# example variable\ntext = [\"This is a test.\", \"This is another test.\", \"Another test.\"]\n\n# create a dataframe\ndf = pd.DataFrame(text, columns=[\"text\"])\n\n# apply tfidf\ntfidf.fit(df[\"text\"])\n\n# print the tfidf matrix\nprint(tfidf.transform(df[\"text\"]))\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# example variable\ntext = [\"Hello World\", \"Python is a great language\"]\n\n# create a dataframe\ndf = pd.DataFrame(text, columns=[\"text\"])\n\n# create tfidf vectorizer\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# fit the vectorizer\ntfidf.fit(df['text'])\n\n# print the tfidf vectorizer\nprint(tfidf.get_feature_names())\nprint(tfidf.transform(df['text']))\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Convert the DataFrame to numpy array\ndata_array = data.values\n\n# Apply preprocessing.scale to the array\nscaled_array = preprocessing.scale(data_array)\n\n# Convert the scaled array back to DataFrame\nscaled_data = pd.DataFrame(scaled_array, columns=data.columns)\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index\n\n# Set the columns of the scaled DataFrame\nscaled_data.columns = data.columns\n\n# Set the index of the scaled DataFrame\nscaled_data.index = data.index", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a numpy array from the DataFrame\narray = data.values\n\n# Apply preprocessing.scale to the array\narray_out = preprocessing.scale(array)\n\n# Create a new DataFrame from the scaled array\ndf_out = pd.DataFrame(array_out, index=data.index, columns=data.columns)\n\n# Print the output DataFrame\nprint(df_out)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = np.array(X.columns)[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "column_names = np.array(X.columns)[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "column_names = np.array(X.columns)[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest_50_samples[:50]\nclosest_50_samples = np.sort(closest_50_samples, axis=0)\nclosest_50_samples = closest", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]\nclosest_50_samples = np.array(closest_50_samples)\nclosest_50_samples = np.reshape(closest_50_samples, (1, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=0)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=1)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=0)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=1)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=0)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=1)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=0)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=1)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=0)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=1)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=0)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=1)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=0)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=1)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50))\nclosest_50_samples = np.repeat(closest_50_samples, 50, axis=0)\nclosest_50_samples = np.reshape(closest_50_samples, (50, 50", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_1", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    center = km.cluster_centers_[p]\n    distances = np.linalg.norm(X - center, axis=1)\n    closest_samples = np.argpartition(distances, 50)[:50]\n    return X[closest_samples]\n    ### END SOLUTION ###\n\nsamples = get_samples(p, X, km)\nprint(samples)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = pd.concat([X_train, pandas.DataFrame(y_train)], axis=1)\n\n# create GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train[0], X_train[1])\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = pd.concat([X_train, pandas.DataFrame(y_train)], axis=1)\n\n# create GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# Create a SVR model\nmodel = SVR(kernel='rbf')\n\n# Fit the model\nmodel.fit(X, y)\n\n# Predict the output\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\nfrom sklearn.kernel import GaussianKernel\n\n# Create a SVM with Gaussian kernel\nsvm_model = svm.SVC(kernel=GaussianKernel())\n\n# Fit the model\nsvm_model.fit(X, y)\n\n# Predict the output\npredict = svm_model.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# Create a SVR model\nmodel = SVR(kernel='poly', degree=2)\n\n# Fit the model\nmodel.fit(X, y)\n\n# Predict the output\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# Create a SVM regression model\nmodel = SVR(kernel='poly', degree=2)\n\n# Fit the model\nmodel.fit(X, y)\n\n# Predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarities_of_queries(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    document_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.dot(query_matrix, document_matrix.T) / (np.linalg.norm(query_matrix, axis=1)[:, None] * np.linalg.norm(document_matrix, axis=1))\n    return cosine_similarities_of_queries", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarities_of_queries(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    document_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.dot(query_matrix, document_matrix.T) / (np.linalg.norm(query_matrix, axis=1)[:, None] * np.linalg.norm(document_matrix, axis=1))\n    return cosine_similarities_of_queries", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_matrix = tfidf.transform(documents)\n    query_matrix = tfidf.transform(queries)\n    cosine_similarities_of_queries = np.dot(query_matrix, tfidf_matrix.T)\n    return cosine_similarities_of_queries\n\n# Test the function\nqueries = [\"This is a test query\", \"This is another test query\", \"This is a third test query\"]\ndocuments = [\"This is a test document\", \"This is another test document\", \"This is a third test document\"]\nprint(solve(queries, documents))\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Create a 2D array with zeros\nzero_array = np.zeros((new_features.shape[0], new_features.shape[1]))\n\n# Create a 2D array with ones\none_array = np.ones((new_features.shape[0], new_features.shape[1]))\n\n# Create a 2D array with two ones\ntwo_array = np.ones((new_features.shape[0], new_features.shape[1]))\ntwo_array[0, 0] = 2\n\n# Create a 2D array with three ones\nthree_array = np.ones((new_features.shape[0], new_features.shape[1]))\nthree_array[0, 0] = 3\n\n# Create a 2D array with four ones\nfour_array = np.ones((new_features.shape[0], new_features.shape[1]))\nfour_array[0, 0] = 4\n\n# Create a 2D array with five ones\nfive_array = np.ones((new_features.shape[0], new_features.shape[1]))\nfive_array[0, 0] = 5\n\n# Create a 2D array with six ones\nsix_array = np.ones((new_features.shape[0], new_features.shape[1]))\nsix_array[0, 0] = 6\n\n# Create a 2D array with seven ones\nseven_array = np.ones((new_features.shape[0], new_features.shape[1]))\nseven_array[0, 0] = 7\n\n# Create a 2D array with eight ones\neight_array = np.ones((new_features.shape[0], new_features.shape[1]))\neight_array[0, 0] = 8\n\n# Create a 2D array with nine ones\nnine_array = np.ones((new_features.shape[0], new_features.shape[1]))\nnine_array[0, 0] = 9\n\n# Create a 2D array with ten ones\nten_array = np.ones((new_features.shape[0], new_features.shape[1]))\nten_array[0, 0] = 10\n\n# Create a 2D array with one hundred ones\none_hundred_array = np.ones((new_features.shape[0], new_features.shape[1]))\none_hundred_array[0, 0] = 100\n\n# Create a 2D array with one hundred one ones\none_hundred_one_array = np.ones((new_features.shape[0], new_features.shape[1]))\none_hundred_one_array[0, 0] = 101\n\n# Create a 2D array with one hundred one two ones\none_hundred_one_two_array = np.ones((new_features.shape[0], new_features.shape[1]))\none_hundred_one_two_array[0, 0] = 102\n\n# Create a 2D array with one hundred one two three ones\none_hundred_one_two_three_array = np.ones((new_features.shape[0], new_features.shape[1]))\none_hundred_one_two_three_array[0, 0] = 103\n\n# Create a 2D array with one hundred one two three four ones\none_hundred_one_two_three_four_array = np.ones((new_features.shape[0], new_features.shape[1]))\none_hundred_one_two_three_four_array[0, 0] = 104\n\n# Create a 2D array with one hundred one two three four five ones\none_hundred_one_two_three_four_five_array = np.ones((new_features.shape[0], new_features.shape[1]))\none_hundred_one_two_three_four_five_array[0, 0] = 105\n\n# Create a 2D array with one hundred one two three four five six ones\none_hundred_one_two_three_four_five_six_array = np.ones((new_features.shape[0], new_features.shape[1]))\none_hundred_one", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# Convert the list of features to a 2D array\nf_array = np.array(f)\n\n# Create a 2D array with zeros\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nf_array = np.array(f)\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_f = np.zeros((f_array.shape[0], f_array.shape[1]))\n\n# Convert the list of features to a 2D array\nnew_", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Create a 2D array with zeros\nzero_array = np.zeros((new_features.shape[0], new_features.shape[1]))\n\n# Create a 2D array with ones\none_array = np.ones((new_features.shape[0], new_features.shape[1]))\n\n# Create a 2D array with two ones\ntwo_array = np.ones((new_features.shape[0], new_features.shape[1]))\ntwo_array[0, 0] = 0\n\n# Create a 2D array with three ones\nthree_array = np.ones((new_features.shape[0], new_features.shape[1]))\nthree_array[0, 0] = 0\nthree_array[0, 1] = 0\n\n# Create a 2D array with four ones\nfour_array = np.ones((new_features.shape[0], new_features.shape[1]))\nfour_array[0, 0] = 0\nfour_array[0, 1] = 0\nfour_array[0, 2] = 0\n\n# Create a 2D array with five ones\nfive_array = np.ones((new_features.shape[0], new_features.shape[1]))\nfive_array[0, 0] = 0\nfive_array[0, 1] = 0\nfive_array[0, 2] = 0\nfive_array[0, 3] = 0\n\n# Create a 2D array with six ones\nsix_array = np.ones((new_features.shape[0], new_features.shape[1]))\nsix_array[0, 0] = 0\nsix_array[0, 1] = 0\nsix_array[0, 2] = 0\nsix_array[0, 3] = 0\nsix_array[0, 4] = 0\n\n# Create a 2D array with seven ones\nseven_array = np.ones((new_features.shape[0], new_features.shape[1]))\nseven_array[0, 0] = 0\nseven_array[0, 1] = 0\nseven_array[0, 2] = 0\nseven_array[0, 3] = 0\nseven_array[0, 4] = 0\nseven_array[0, 5] = 0\n\n# Create a 2D array with eight ones\neight_array = np.ones((new_features.shape[0], new_features.shape[1]))\neight_array[0, 0] = 0\neight_array[0, 1] = 0\neight_array[0, 2] = 0\neight_array[0, 3] = 0\neight_array[0, 4] = 0\neight_array[0, 5] = 0\neight_array[0, 6] = 0\n\n# Create a 2D array with nine ones\nnine_array = np.ones((new_features.shape[0], new_features.shape[1]))\nnine_array[0, 0] = 0\nnine_array[0, 1] = 0\nnine_array[0, 2] = 0\nnine_array[0, 3] = 0\nnine_array[0, 4] = 0\nnine_array[0, 5] = 0\nnine_array[0, 6] = 0\nnine_array[0, 7] = 0\n\n# Create a 2D array with ten ones\nten_array = np.ones((new_features.shape[0], new_features.shape[1]))\nten_array[0, 0] = 0\nten_array[0, 1] = 0\nten_array[0, 2] = 0\nten_array[0, 3] = 0\nten_array[0, 4] = 0\nten_array[0, 5] = 0\nten_array[0, 6] = 0\nten_array[0, 7] = 0\nten_array[0, 8] = 0\n\n# Create a 2D array with eleven ones\neleven_array = np.ones((new_features.shape[0], new_", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Create a 2D array with zeros\n    new_features_array = np.zeros((features_array.shape[0], features_array.shape[1]))\n    # Convert the features to a 2D array\n    for i in range(features_array.shape[0]):\n        for j in range(features_array.shape[1]):\n            new_features_array[i, j] = features_array[i, j]\n    # Return the new features array\n    return new_features_array\n    ### END SOLUTION ###\n\n# Test the function\nfeatures = [\n    ['f1', 'f2', 'f3'],\n    ['f2', 'f4', 'f5', 'f6'],\n    ['f1', 'f2']\n]\nprint(solve(features))\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to one hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Create a DataFrame from the new features\nnew_features_df = pd.DataFrame(new_features, columns=['t1', 't2', 't3', 't4', 't5', 't6', 't7'])\n\n# Use sklearn's feature selection utilities to select the features\nselected_features = sklearn.feature_selection.select_features(new_features_df, n_features=3)\n\n# Print the selected features\nprint(selected_features)\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2D array with the data\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data\ncluster_labels = cluster.fit_predict(data_matrix)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model\ncluster_labels = cluster.fit_predict(dist_matrix)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2D array with the similarity matrix\nsimM = np.array([[0, 0.6, 0.8], [0.6, 0, 0.111], [0.8, 0.111, 0]])\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model to the similarity matrix\ncluster_labels = cluster.fit_predict(simM)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "import scipy.cluster.hierarchy as shc\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create a linkage matrix\nlinkage_matrix = shc.linkage(dist_matrix, method='ward')\n\n# Get the cluster labels\ncluster_labels = shc.fcluster(linkage_matrix, 2, criterion='max')\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create a linkage matrix\nlinkage_matrix = linkage(dist_matrix, method='ward')\n\n# Create a cluster labels\ncluster_labels = fcluster(linkage_matrix, 2, criterion='maxclust')\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "import scipy.cluster.hierarchy as shc\n\n# Create a distance matrix\ndistMatrix = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Create a linkage matrix\nlinkageMatrix = shc.linkage_matrix(distMatrix, method='ward')\n\n# Perform hierarchical clustering\ncluster_labels = shc.fcluster(linkageMatrix, 2, criterion='maximum')\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\nbox_cox_transformer = BoxCoxTransformer()\nbox_cox_data = box_cox_transformer.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\nbox_cox_data = transformer.transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\n\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(analyzer='word')\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\ntransformed_text = np.concatenate((transformed_text, text), axis=1)\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.to", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=[-1]), dataset[-1], test_size=0.2, random_state=42)\n\n# Define x and y\nx_train_x = x_train.drop(columns=[-1])\nx_train_y = x_train[-1]\nx_test_x = x_test.drop(columns=[-1])\nx_test_y = x_test[-1]\n\n# Print the shapes of x_train_x and x_train_y\nprint(x_train_x.shape)\nprint(x_train_y.shape)\nprint(x_test_x.shape)\nprint(x_test_y.shape)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features and the target\nx = data.drop(columns=[-1]) # drop the last column\ny = data[-1] # get the last column\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=[-1]), dataset[-1], test_size=0.2, random_state=42)\n\n# Define x and y\nx_train_x = x_train.drop(columns=[-1])\nx_train_y = x_train[-1]\nx_test_x = x_test.drop(columns=[-1])\nx_test_y = x_test[-1]\n\n# Print the shapes of x_train_x and x_train_y\nprint(x_train_x.shape)\nprint(x_train_y.shape)\nprint(x_test_x.shape)\nprint(x_test_y.shape)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": " ###\n    # split the data into training and testing sets\n    train_size = int(0.8 * len(data))\n    train_data = data[:train_size]\n    test_data = data[train_size:]\n\n    # define x and y\n    x_train = train_data[:, :-1]\n    y_train = train_data[:, -1]\n    x_test = test_data[:, :-1]\n    y_test = test_data[:, -1]\n\n    return x_train, y_train, x_test, y_test\n\n# call the function\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "mse_values = df['mse'].values\nlabels = KMeans(n_clusters=2, n_init=10).fit(mse_values.reshape(-1, 1)).predict(mse_values.reshape(-1, 1))\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# Reshape the data to 2D array\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans object with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit the model to the data\nkmeans.fit(X)\n\n# Get the labels of the clusters\nlabels = kmeans.predict(X)\n\n# Get the centroid of the clusters\ncentroids = kmeans.cluster_centers_\n\n# Print the labels and centroids\nprint(labels)\nprint(centroids)\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "# Create a LinearSVC object with l1 penalty\nsvc = LinearSVC(penalty='l1', solver='liblinear')\n# Fit the model\nsvc.fit(X, y)\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\nprint(selected_feature_names)", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "# Create a LinearSVC object\nsvc = LinearSVC(penalty='l1', loss='l2', max_iter=10000, random_state=42)\n# Fit the model\nsvc.fit(X, y)\n# Get the selected features\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\nprint(selected_feature_names)\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    svc = LinearSVC(penalty='l1', solver='liblinear')\n    svc.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\n    return selected_feature_names\n    ### END SOLUTION ###\n\n# Test the function\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nfeature_names = vectorizer.get_feature_names()\nfeature_names.sort()\nprint(feature_names)\nBEGIN SOLUTION\n<code>\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vocabulary = ['Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX']\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vocabulary = ['Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX']\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "series = np.array([]) #blank list to append result\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] #removes NaN values for each column to apply sklearn function\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y) # either this or the next line\n    m = slope.coef_[0]\n    series= np.concatenate((series, m), axis = 0)\nslopes = series\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "series = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    series= np.concatenate((series, m), axis = 0)\nslopes = series\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column\ndf['Sex'] = le.transform(df['Sex'])\n\n# Print the transformed data\nprint(df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column into a numerical value\ndf['Sex'] = le.transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ### END SOLUTION ###\n    return df\n\ndef load_data():\n    # load the data\n    df = pd.read_csv('titanic.csv')\n    return df\n\n# call the function\ndf = load_data()\nTransform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION ###\n\n# Test the function\nnp_array = np.array([[1, 2], [3, 4]])\nprint(Transform(np_array))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close, ma50, ma100, ma200])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array([['dsa', '2'], ['sato', '3']])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:,:-1].astype(float)\ny = dataframe.iloc[:,-1]\n# print(X)\n# print(y)\n\nlogReg = LogisticRegression()\nlogReg.fit(X,y)\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the dataframe to numpy array\nX = dataframe.iloc[-1:].astype(float)\n# Get the target variable\ny = dataframe.iloc[:,-1]\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model\nlogReg.fit(X[:None], y)\n# Predict the target variable\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "train_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=train_size, shuffle=False)\n\n# Sort the dataframe by date\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last date in the train dataframe\nlast_date_index_in_train = train_dataframe.index[-1]\n\n# Get the index of the last date in the test dataframe\nlast_date_index = test_dataframe.index[-1]\n\n# Get the index of the last", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Calculate the number of rows to be in the test set\ntest_size = int(len(features_dataframe) * 0.2)\n\n# Get the rows to be in the test set\ntest_dataframe = features_dataframe[-test_size:]\n\n# Get the rows to be in the train set\ntrain_dataframe = features_dataframe[:-test_size]\n\n# Print the train and test dataframes\nprint(train_dataframe)\nprint(test_dataframe)\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": " ###\n    # sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(by=\"date\")\n    # calculate the number of rows to be in the test set\n    test_size = int(len(features_dataframe) * 0.8)\n    # split the dataframe into train and test sets\n    train_dataframe = features_dataframe[:test_size]\n    test_dataframe = features_dataframe[test_size:]\n    ### END SOLUTION ###\n    return train_dataframe, test_dataframe\n\n# call the function\ntrain_dataframe, test_dataframe = solve(features_dataframe)\nprint(train_dataframe)\nprint(test_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2_scale', 'X3_scale']] = scaler.fit_transform(df[['X2', 'X3']])\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "myData['new_A2'] = scaler.fit_transform(myData[['A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = GridSearch_fitted.fit(X, y)\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.cv_results_\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_results.cv_results_['params']\nfull_results.cv_results_['mean_test_score']\nfull_", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = GridSearch_fitted.fit(X, y)\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.cv_results_\nfull_results.cv_results_['mean_fit_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean_score_time']\nfull_results.cv_results_['mean", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# Load your data\ndata = np.load('data.npy')\n\n# Fit the model\nmodel = IsolationForest()\nmodel.fit(data)\n\n# Save the model\nnp.save('sklearn_model', model)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = np.ones((len(df), len(df)))\nfor i in range(len(df)):\n    for j in range(len(df)):\n        cosine_similarity_matrix[i, j] = 1 - sklearn.metrics.cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])\ncosine_similarity_matrix = pd.DataFrame(cosine_similarity_matrix, index=df.index, columns=df.index)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\noptim.reset_parameters()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n# Define the learning rate scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define a function to update the learning rate\ndef update_learning_rate(epoch):\n    if epoch > 10:\n        scheduler.step()\n    return scheduler.get_lr()\n\n# Set the learning rate\noptim.param_groups[0]['lr'] = 0.001\n\n# Update the learning rate after each epoch\nfor epoch in range(100):\n    optim.step()\n    print(update_learning_rate(epoch))\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\noptim.zero_grad()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define the learning rate scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define a function to update the learning rate\ndef update_learning_rate(epoch):\n    if epoch > 10:\n        scheduler.step()\n\n# Call the function to update the learning rate\nfor epoch in range(100):\n    update_learning_rate(epoch)\n    optimizer.step()\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the word2vec model to a PyTorch embedding layer\nembedding_weights = torch.tensor(word2vec.wv.vectors)\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_weights)\n\n# Embedding the input data\nembedded_input = embedding_layer(input_Tensor)\nprint(embedded_input)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # get the weights from the word2vec model\n    weights = word2vec.wv.vectors\n    # create a PyTorch embedding layer with the weights\n    embedding_layer = torch.nn.Embedding.from_pretrained(weights)\n    # embed the input data\n    embedded_input = embedding_layer(input_Tensor)\n    # ### END SOLUTION ###\n    return embedded_input\n\ndef load_data():\n    # create a random tensor of size 10000 x 100\n    input_Tensor = np.random.randint(0, 100, (10000, 100))\n    return input_Tensor\n\n# call the function\nembedded_input = get_embedded_input(load_data())\nprint(embedded_input)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # This works\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "A_logical = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical] # This works because B is a 2D tensor and A_logical is a 1D tensor\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "A_log = torch.ByteTensor([1, 1, 0]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # This works\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "A_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # This works\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    return C\n\ndef load_data():\n    A_log = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_log, B\n\nA_log, B = load_data()\nC = solve(A_log, B)\nprint(C)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "A_log = torch.ByteTensor([0, 0, 1]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # This works because B is a LongTensor\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Create a tensor with the index\nidx = torch.LongTensor([1, 2])\n\n# Create a tensor with the original data\nB = torch.LongTensor([[2, 1, 3], [5, 4, 6]])\n\n# Use index_select to get the expected output\nC = torch.index_select(B, 1, idx)\n\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    # Convert the numpy array to a torch tensor\n    t = torch.tensor(a, dtype=torch.float16)\n    return t\n    ### END SOLUTION ###\n\n# Test the function\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=np.object)\n\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.arange(lens.numel()).unsqueeze(0) < lens.unsqueeze(1)\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "lens = torch.tensor([1, 9, 3, 5])\nmask = torch.arange(lens.numel()).unsqueeze(0).expand(lens.numel(), lens.numel())\nmask = (mask >= lens.unsqueeze(1)) & (mask >= lens.unsqueeze(0))\nmask = (mask.t() == torch.arange(lens.numel()).unsqueeze(0).expand(lens.numel(), lens.numel()))\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.arange(lens.numel()).unsqueeze(0).expand(lens.numel(), lens.numel())\nmask = (mask >= lens.unsqueeze(1)) & (mask >= lens.unsqueeze(0))\nmask = (mask.t() == torch.arange(lens.numel()).unsqueeze(0).expand(lens.numel(), lens.numel()))\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    max_len = np.max(lens)\n    mask = np.zeros((len(lens), max_len))\n    for i in range(len(lens)):\n        mask[i, lens[i]-1:] = 1\n    return mask\n\nlens = np.array([3, 5, 4])\nmask = get_mask(lens)\nprint(mask)\n## END SOLUTION ###\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with diagonal elements\ndiagonal_ele = np.array([1, 2, 3, 4, 5])\n\n# Create a 2D tensor with index in batch\nindex_in_batch = np.array([1, 2, 3, 4, 5])\n\n# Create a 2D tensor with drag elements\ndrag_ele = np.array([1, 2, 3, 4, 5])\n\n# Create a 2D tensor with diagonal elements\ndiagonal_ele_2D = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n\n# Create a 2D tensor with index in batch\nindex_in_batch_2D = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n\n# Create a 2D tensor with drag elements\ndrag_ele_2D = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n\n# Create a 2D tensor with diagonal elements\ndiagonal_ele_2D_2D = np.array([[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]], [[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]])\n\n# Create a 2D tensor with index in batch\nindex_in_batch_2D_2D = np.array([[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]], [[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]])\n\n# Create a 2D tensor with drag elements\ndrag_ele_2D_2D = np.array([[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]], [[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]])\n\n# Create a 2D tensor with diagonal elements\ndiagonal_ele_2D_2D_2D = np.array([[[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]], [[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]], [[[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]], [[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]]])\n\n# Create a 2D tensor with index in batch\nindex_in_batch_2D_2D_2D = np.array([[[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]], [[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]], [[[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]], [[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]]])\n\n# Create a 2D tensor with drag elements\ndrag_ele_2D_2D_2D = np.array([[[[1, 2, 3, 4, 5], [6, 7, 8, ", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    index_in_batch = t.shape[0]\n    diag_ele = np.ones((index_in_batch, 1))\n    Matrix = np.diag(diag_ele)\n    Matrix = torch.tensor(Matrix)\n    return Matrix\n    ### END SOLUTION ###\n\n# Test the function\nt = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(Convert(t))\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\nab = torch.stack((a, b), 0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = a.reshape(-1, 1)\nb = b.reshape(-1, 1)\nab = np.vstack((a, b))\nab = torch.tensor(ab)\nprint(ab.shape) # prints (138, 514)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    # stack the tensors along the first dimension\n    # ab = torch.stack((a, b), 0)\n    # # convert the tensor to numpy\n    # a_np = a.numpy()\n    # b_np = b.numpy()\n    # # stack the numpy arrays along the first dimension\n    # ab_np = np.stack((a_np, b_np), 0)\n    # # convert the numpy array to a tensor\n    # ab_tensor = torch.from_numpy(ab_np)\n    # # return the tensor\n    # return ab_tensor\n    ### END SOLUTION ###\n\n# a and b are numpy arrays\na = np.array([[1, 2, 3], [4, 5, 6]])\nb = np.array([[7, 8, 9], [10, 11, 12]])\n\n# a and b are tensors\na = torch.tensor(a)\nb = torch.tensor(b)\n\n# solve(a, b) # this will return a tensor of shape (3, 11)\n# print(solve(a, b))\n# print(a)\n# print(b)\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b))\n# print(solve(a, b", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[ : , lengths : , : ] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[ : , lengths : , : ] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\nprint(new_tensors)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.tensor(lt, dtype=torch.float32)\n    return tensor_of_tensors\n    ### END SOLUTION ###\n\n# Call the function\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = t[np.arange(t.shape[0]), idx]", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = t[np.arange(t.shape[0]), idx]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = t[idx]\nprint(result)\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\n# Get the maximum ids\nids = torch.argmax(x, 1)\n\n# Gather the selected slices\nresult = x.gather(1, ids.unsqueeze(-1)).squeeze(1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\n# Get the maximum ids\nmax_ids = torch.argmax(scores, 1)\n\n# Gather the selected slices\nresult = x[max_ids]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask with the ids\nmask = torch.tensor(ids == 1, dtype=torch!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "softmax_output = torch.tensor([[0.2, 0.1, 0.7], [0.6, 0.2, 0.2], [0.1, 0.8, 0.1]])\ny = torch.argmax(softmax_output, dim=1)\nprint(y)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "softmax_output = torch.tensor([[0.7, 0.2, 0.1], [0.2, 0.6, 0.2], [0.1, 0.1, 0.8]])\ny = torch.argmax(softmax_output, dim=1)\nprint(y)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "softmax_output = torch.tensor([[0.2, 0.1, 0.7], [0.6, 0.3, 0.1], [0.15, 0.8, 0.05]])\ny = torch.argmax(softmax_output, 1)\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    # Get the maximum value from the softmax output\n    max_value = np.max(softmax_output, axis=1)\n    # Create a tensor with the maximum value\n    max_tensor = torch.tensor(max_value, dtype=torch.int32)\n    # Create a tensor with the class index of the maximum value\n    class_index = np.argmax(softmax_output, axis=1)\n    # Create a tensor with the class index\n    class_tensor = torch.tensor(class_index, dtype=torch.int32)\n    # Return the class tensor\n    return class_tensor\n    ### END SOLUTION ###\n\n# Test the function\nsoftmax_output = np.array([[0.2, 0.1, 0.7], [0.6, 0.2, 0.2], [0.1, 0.8, 0.1]])\nprint(solve(softmax_output))\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "# Get the maximum value in each row\nmax_values = torch.max(softmax_output, 1)\n# Get the index of the maximum value\ny = torch.argmax(softmax_output, 1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = max_values.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the maximum value\ny = y.unsqueeze(1)\n# Create a tensor with the index of the maximum value\ny = y.unsqueeze(1)\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "# Convert labels to one-hot encoding\nlabels = F.one_hot(labels, num_classes=7)\n# Calculate CrossEntropy loss\nloss = F.cross_entropy(images, labels)\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = np.sum(A == B)\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = np.sum(A == B)\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = np.sum(A != B)\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = np.sum(A == B)\n    ### END SOLUTION ###\n    return cnt_equal\n\nA = np.random.rand(1000, 1)\nB = np.random.rand(1000, 1)\n\nprint(Count(A, B))\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "cnt_equal = np.sum(A[-x:] == B[-x:])", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = np.sum(A[-x:] != B[-x:])", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensors_31.append(a[:, :, i:i+10, :, :])\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (31, 1, 10, 10, 1)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensors_31.append(a[:, :, i:i+1, :, :])\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 31, 10, 1)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output[mask == 1] = clean_input_spectrogram[mask == 1]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask == 0] = clean_input_spectrogram[mask == 0]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x))\nmin_y = torch.min(torch.abs(y))\nsigned_min = torch.where(x < y, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\nsigned_max = torch.where(x > y, max_x, max_y)\nsigned_max = torch.where(x > y, sign_x, sign_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x), torch.abs(y))\n    min_y = torch.min(torch.abs(x), torch.abs(y))\n    min_x_sign = torch.where(torch.abs(x) == min_x, sign_x, sign_y)\n    min_y_sign = torch.where(torch.abs(y) == min_y, sign_y, sign_x)\n    return min_x_sign, min_y_sign\n\nx = torch.tensor([1, 2, 3, 4, 5])\ny = torch.tensor([6, 7, 8, 9, 10])\n\nmin_x_sign, min_y_sign = solve(x, y)\nprint(min_x_sign, min_y_sign)\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\nprint(f\"Confidence score: {confidence_score}, Class name: {class_name}\")\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# Create a new tensor with the same shape as a and b\nresult = torch.zeros_like(a)\n\n# Calculate the average of the last column of a and the first column of b\nresult[:, 1:] = (a[:, 1:] + b[:, :-1]) / 2\n\n# Calculate the average of the first column of a and the last column of b\nresult[:, 0] = (a[:, 0] + b[:, -1]) / 2\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    # Create a new tensor with the same shape as a and b\n    result = torch.zeros_like(a)\n\n    # Calculate the average of the last column of a and the first column of b\n    average = (a[:, -1] + b[:, 0]) / 2\n\n    # Set the last column of a to the average\n    result[:, -1] = average\n\n    # Set the first column of b to the average\n    result[:, 0] = average\n\n    # Return the result\n    return result\n\n# Call the function with the given inputs\nresult = solve(a, b)\nprint(result)\n", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "t = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\nnew = torch.tensor([[0, 0, 0, 0]])\nresult = torch.stack([t, new])\nprint(result)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "t = torch.tensor([[1, 2], [3, 4]])\nnew = torch.tensor([[0, 0, 0, 0]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
