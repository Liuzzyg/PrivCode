{"id": 0, "code": "result = df.iloc[List]", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.loc[List]\nprint(result)\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a new column with the values of Qu1 that have at least two occurrences\ndf['Qu1_new'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) < x else x)\n\n# Create a new column with the values of Qu2 that have at least two occurrences\ndf['Qu2_new'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df.Qu2) < x else x)\n\n# Create a new column with the values of Qu3 that have at least two occurrences\ndf['Qu3_new'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df.Qu3) < x else x)\n\n# Create a new column with the values of Qu1 that have at least two occurrences\ndf['Qu1_new'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) < x else x)\n\n# Create a new column with the values of Qu2 that have at least two occurrences\ndf['Qu2_new'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df.Qu2) < x else x)\n\n# Create a new column with the values of Qu3 that have at least two occurrences\ndf['Qu3_new'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df.Qu3) < x else x)\n\n# Create a new column with the values of Qu1 that have at least two occurrences\ndf['Qu1_new'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) < x else x)\n\n# Create a new column with the values of Qu2 that have at least two occurrences\ndf['Qu2_new'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df.Qu2) < x else x)\n\n# Create a new column with the values of Qu3 that have at least two occurrences\ndf['Qu3_new'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df.Qu3) < x else x)\n\n# Create a new column with the values of Qu1 that have at least two occurrences\ndf['Qu1_new'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) < x else x)\n\n# Create a new column with the values of Qu2 that have at least two occurrences\ndf['Qu2_new'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df.Qu2) < x else x)\n\n# Create a new column with the values of Qu3 that have at least two occurrences\ndf['Qu3_new'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df.Qu3) < x else x)\n\n# Create a new column with the values of Qu1 that have at least two occurrences\ndf['Qu1_new'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) < x else x)\n\n# Create a new column with the values of Qu2 that have at least two occurrences\ndf['Qu2_new'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df.Qu2) < x else x)\n\n# Create a new column with the values of Qu3 that have at least two occurrences\ndf['Qu3_new'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df.Qu3) < x else x)\n\n# Create a new column with the values of Qu1 that have at least two occurrences\ndf['Qu1_new'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) < x else x)\n\n# Create a new column with the values of Qu2 that have at least two occurrences\ndf['Qu2_new'] = df['Qu", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a new column with the value of the first column if the value count is greater or equal to 3\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) < x else x)\n\n# Create a new column with the value of the first column if the value count is greater or equal to 3\ndf['Qu2'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df.Qu2) < x else x)\n\n# Create a new column with the value of the first column if the value count is greater or equal to 3\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df.Qu3) < x else x)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    # create a dictionary with the values of column Qu1 and Qu2\n    dict_values = dict(zip(df.columns[1:], df.values[:, 1:]))\n    # create a dictionary with the values of column Qu3\n    dict_values_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu1 and Qu3\n    dict_values_other_other = dict(zip(df.columns[1:], df.values[:, 1:]))\n    # create a dictionary with the values of column Qu2 and Qu3\n    dict_values_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu1 and Qu2\n    dict_values_other_other_other_other = dict(zip(df.columns[1:], df.values[:, 1:]))\n    # create a dictionary with the values of column Qu3 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu1 and Qu3\n    dict_values_other_other_other_other_other = dict(zip(df.columns[1:], df.values[:, 1:]))\n    # create a dictionary with the values of column Qu2 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu3 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu1 and Qu2\n    dict_values_other_other_other_other_other = dict(zip(df.columns[1:], df.values[:, 1:]))\n    # create a dictionary with the values of column Qu2 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu3 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu1 and Qu3\n    dict_values_other_other_other_other_other = dict(zip(df.columns[1:], df.values[:, 1:]))\n    # create a dictionary with the values of column Qu2 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu3 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu1 and Qu2\n    dict_values_other_other_other_other_other = dict(zip(df.columns[1:], df.values[:, 1:]))\n    # create a dictionary with the values of column Qu2 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu3 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu1 and Qu3\n    dict_values_other_other_other_other_other = dict(zip(df.columns[1:], df.values[:, 1:]))\n    # create a dictionary with the values of column Qu2 and Qu4\n    dict_values_other_other_other_other_other = dict(zip(df.columns[2:], df.values[:, 2:]))\n    # create a dictionary with the values of column Qu3 and Qu", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a function to change values in column Qu1\ndef change_values_in_column_Qu1(data):\n    value_counts = data.Qu1.value_counts()\n    data.Qu1[value_counts >= 3] = 'other'\n    return data\n\n# Create a function to change values in column Qu2 and Qu3\ndef change_values_in_column_Qu2_and_Qu3(data):\n    value_counts = data.Qu2.value_counts()\n    data.Qu2[value_counts >= 2] = 'other'\n    value_counts = data.Qu3.value_counts()\n    data.Qu3[value_counts >= 2] = 'other'\n    return data\n\n# Apply the functions to the data\ndata = change_values_in_column_Qu1(data)\ndata = change_values_in_column_Qu2_and_Qu3(data)\n\n# Print the result\nprint(data)\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'apple': 'other', 'potato': 'other', 'banana': 'other', 'sausage': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu3\ndict_values_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'cheese': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values_other_other_other_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other', 'egg': 'other'}", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\n# Create a new column 'keep_if_dup_bool' to convert 'keep_if_dup' to boolean\ndf['keep_if_dup_bool'] = df['keep_if_dup'] == 'Yes'\n\n# Drop duplicates\nresult = df.drop_duplicates(subset='url', keep='last')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndef convert_to_nested_dict(df):\n    result = {}\n    for name, v1, v2, v3 in zip(df['name'], df['v1'], df['v2'], df['v3']):\n        if name not in result:\n            result[name] = {}\n        result[name][v1] = {}\n        result[name][v1][v2] = v3\n    return result\n\nresult = convert_to_nested_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    # Use the tz_convert function to convert the timezone to UTC\n    df['datetime'] = df['datetime'].apply(lambda x: x.tz_convert('UTC'))\n    # Use the str function to get the timezone information\n    df['datetime'] = df['datetime'].apply(lambda x: str(x).split('-')[0])\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# Convert the message to a dictionary\nmessage_dict = {}\nfor i in range(len(df)):\n    message_dict[i] = eval(df['message'][i])\n\n# Create a new dataframe from the dictionary\nnew_df = pd.DataFrame(list(message_dict.items()), columns=['index', 'message'])\n\n# Convert the index to a string\nnew_df['index'] = new_df['index'].astype(str)\n\n# Expand the dataframe\nresult = new_df.set_index('index').transpose()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "\nfor product in products:\n    df.loc[df['product'] == product, 'score'] *= 10\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10\ndf['score'] = df['score'].where(df['product'].isin(products), 10)\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "for product in products:\n    df.loc[df['product'].between(product[0], product[1]), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\nfor product in products:\n    df.loc[df['product'] == product, 'score'] = (df.loc[df['product'] == product, 'score'] - df.loc[df['product'] == product, 'score'].min()) / (df.loc[df['product'] == product, 'score'].max() - df.loc[df['product'] == product, 'score'].min())\n\nprint(df)\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "\n# Create a new column 'category'\ndf['category'] = ''\n\n# Iterate over the columns and assign the category\nfor col in df.columns:\n    if col == 'A':\n        df.loc[df[col] == 1, 'category'] = 'A'\n    elif col == 'B':\n        df.loc[df[col] == 1, 'category'] = 'B'\n    elif col == 'C':\n        df.loc[df[col] == 1, 'category'] = 'C'\n    elif col == 'D':\n        df.loc[df[col] == 1, 'category'] = 'D'\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "\n# Create a new column with the binary values as integers\ndf['category'] = df[['A', 'B', 'C', 'D']].apply(lambda x: 'A' if x.all() else 'B' if x[0] == 1 else 'C' if x[1] == 1 else 'D', axis=1)\n\n# Convert the categorical column to a categorical column\ndf['category'] = df['category'].astype('category')\n\nprint(df)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndef convert_to_list(df):\n    return df.apply(lambda x: [i for i in df.columns if x[i] == 1], axis=1)\n\ndf['category'] = convert_to_list(df)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.to_period('M')\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%b-%Y'))\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime(\"%d-%b-%Y\")\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime(\"%d-%b-%Y\")\ndf = df[df['Date'].between(List[0], List[1])]\nprint(df)\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nprint(shifted_df)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nprint(shifted_df)\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\ndf['#1'] = df['#1'].shift(-1)\n\n# shift the last row of the first column up 1 row\ndf['#2'] = df['#2'].shift(1)\n\n# reset the index\ndf = df.reset_index(drop=True)\n\n# print the dataframe\nprint(df)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "new_cols = [f'Header{i}' for i in range(1, 11)]\ndf.columns = new_cols\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "new_columns = [f'XHeader{i}' for i in range(1, 11)]\ndf.columns = new_columns\nprint(df)\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "# Create a dictionary with the new column names\nnew_cols = {'HeaderA': 'HeaderAX',\n            'HeaderB': 'HeaderBX',\n            'HeaderC': 'HeaderCX',\n            'HeaderX': 'HeaderX'}\n\n# Use the dictionary to rename the columns\ndf = df.rename(columns=new_cols)\nprint(df)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "result = df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' not in col})\nprint(result)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' not in col})\nprint(result)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndef get_mean_and_sum(group):\n    mean_val1 = df.loc[group, 'val1'].mean()\n    mean_val2 = df.loc[group, 'val2'].mean()\n    mean_val42 = df.loc[group, 'val42'].mean()\n    return mean_val1, mean_val2, mean_val42\n\nresult = df.groupby('group').apply(get_mean_and_sum)\nresult", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "def meanAdvance(row_list, column_list, axis=0):\n    return df.loc[row_list, column_list].mean(axis=axis)\n\nresult = meanAdvance(row_list, column_list)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "def sumAdvance(row_list, column_list, axis=0):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list)\nprint(result)\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "def sumAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list, axis=0)\nresult = result.dropmax()\nprint(result)\n", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.apply(lambda x: x.value_counts())\nresult = result.fillna(0)\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "result = df.apply(lambda x: x.value_counts())\nresult", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0,1]]\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0,1]]\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "result = df.where(df.notnull(), 1).where(df.isnull(), 2)\nresult", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "result = df.where(df.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "result = df.where(df.isnull(), np.nan)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "# Find rows where value is less than the threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# Sum the values of these rows\nresult = rows_less_than_thresh.sum()\n# Replace the rows with the sum with the original values\ndf = df.replace(rows_less_than_thresh, 0)\n# Add a new row with the sum of the original values\ndf = df.append(result)\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "# find rows with value less than threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# find rows with value greater than threshold\nrows_greater_than_thresh = df[df['value'] >= thresh]\n# calculate average of rows greater than threshold\navg_greater_than_thresh = rows_greater_than_thresh['value'].mean()\n# replace rows with value less than threshold with average of rows greater than threshold\nresult = df.loc[rows_less_than_thresh.index]\nresult.loc['X'] = avg_greater_than_thresh\nprint(result)\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "# find the rows in the section\nrows_in_section = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\n# calculate the average of the rows in the section\nresult = rows_in_section.mean()\n# replace the rows in the section with the average\ndf.loc[section_left:section_right] = result\n# print the result\nprint(result)\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\n# Create a dictionary with the inverse of each column\ninv_dict = {col: 1 / val for col, val in df.iteritems()}\n\n# Create a new dataframe with the inverse of each column\nresult = pd.DataFrame(inv_dict)\n\n# Add the original column names to the result\nresult.columns = df.columns\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndef exp_column(df, column):\n    return df[column]**(1/df[column].count())\n\nfor column in df.columns:\n    df[f'exp_{column}'] = exp_column(df, column)\n\nresult = df\nprint(result)\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "inv_A = df['A'].map(lambda x: 1/x if x != 0 else 0)\ninv_B = df['B'].map(lambda x: 1/x if x != 0 else 0)\n\nresult = pd.DataFrame({\"A\": df['A'], \"B\": df['B'], \"inv_A\": inv_A, \"inv_B\": inv_B})\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = df[col].apply(sigmoid)\n\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = df.apply(lambda x: x[x == x.max()].index[-1], axis=1)\n\n# Create a mask to ignore the last occurrence of the column-wise maximum\nmask = np.ones(len(last_max_index), dtype=bool)\nmask[-1] = False\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Create a mask where the maximum value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Apply the mask to the index of the minimum value\nresult = min_index[mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum to the maximum\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with all dates and fill with 0\nresult = pd.DataFrame(date_range, index=date_range, columns=['dt'])\nresult['val'] = 0\n\n# Expand the date column to have all the dates there\nresult['dt'] = date_range\n\n# Fill in 0 for the val column\nresult['val'] = result['val'].fillna(0)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum to the maximum\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with all dates and fill with 0\nnew_df = pd.DataFrame(date_range, columns=['dt'])\nnew_df['val'] = 0\n\n# Join the new data frame with the original data frame\nresult = pd.merge(new_df, df, on='dt', how='left')\nresult", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum to the maximum\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with all dates and fill in 233 for the val column\nresult = pd.DataFrame({'dt': date_range, 'val': 233}, index=date_range)\n\n# Join the original data frame with the new data frame\nresult = pd.merge(df, result, on='dt', how='left')\n\n# Fill the NaN values with 233\nresult['val'] = result['val'].fillna(233)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum date to the maximum date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while simultaneously filling in the maximum val of the user for the val column\nresult = new_df.merge(df, on='dt', how='left').fillna(df['val'].max())\nresult", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum date to the maximum date\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with the date range and the maximum value for each user\nresult = pd.DataFrame({'dt': date_range, 'user': df['user'].unique(), 'val': df['val'].max()})\n\n# Expand the date column to have all the dates there\nresult['dt'] = date_range\n\n# Convert the date column to the desired format\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n# Convert the data frame to the desired format\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.unstack(level=1).reset_index()\nresult = result.set_index(['dt', 'user'])\nresult = result.stack(level=1).reset_index()\nresult", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "result = df.groupby('name').apply(lambda x: x.assign(name=x.index + 1)).reset_index(drop=True)\nresult", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "\n# Create a unique ID for each name\nunique_id = df['name'].apply(lambda x: x[0])\n\n# Create a new column with unique ID\ndf['unique_id'] = unique_id\n\n# Replace each a with a unique ID\ndf['a'] = df['unique_id']\n\n# Drop the unique_id column\ndf = df.drop(columns='unique_id')\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', '", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\n# Create a unique ID for each name\nunique_id = df['name'].apply(lambda x: x + str(df['a'].values[df['name'] == x].sum()))\n\n# Create a new column 'ID' with unique ID\ndf['ID'] = unique_id\n\n# Replace each name with a unique ID\ndf['ID'] = df['ID'].apply(lambda x: x.replace('Aaron', '1'))\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user', '01/12/15', '02/12/15']].rename(columns={'01/12/15': 'date', '02/12/15': 'value'})\n\n# Create a new dataframe with the date and value columns\nnew_df = df[['user',", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value')\ndf = df.rename(columns=lambda x: x.replace('/', ''))\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'others'})\ndf = df.set_index('others')\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'value'})\ndf = df.set_index('value')\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'user'})\ndf = df.set_index('user')\ndf = df.reset_index()\nprint(df)\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.pivot_table(index='user', columns='date', values='value', aggfunc='first')\ndf = df.reset_index()\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'01/12/15': 'date'})\ndf = df.rename(columns={'02/12/15': 'value'})\ndf = df.rename(columns={'someBool': 'someBool'})\nprint(df)\n", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df.c > 0.5][columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df.c > 0.45][columns].to_numpy()\n", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": " ###\n    # Use the loc function to select the rows where the value in column 'c' is greater than 0.5\n    # Then select the columns you want\n    # Finally, convert the result to a numpy array\n    ### END SOLUTION ###\n    return df.loc[df['c'] > 0.5, columns].to_numpy()\n\n# Create a DataFrame\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# Call the function\nprint(f(df, columns=['b', 'e']))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": " ###\n    # select the rows where column 'c' is greater than 0.5\n    # then select the columns 'b' and 'e'\n    # then compute the sum of these columns\n    # return the result\n    ### END SOLUTION ###\n    return df[df.c > 0.5][columns].sum()\n\n# create a dataframe\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# call the function\nprint(f(df, columns=['b', 'e']))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": " ###\n    return df[df['c'] > 0.5][columns]\n    ### END SOLUTION ###\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndef find_overlap(row, X):\n    date_range = pd.date_range(start=row['date'], periods=X)\n    return date_range.isin(row['date'])\n\nresult = df[~df.apply(find_overlap, X=X, axis=1)]\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef find_overlap(row, X):\n    date = row['date']\n    date_format = pd.to_datetime(date)\n    dates = [date_format + pd.DateOffset(months=i) for i in range(X)]\n    return dates\n\ndef find_overlap_row(row, X):\n    dates = find_overlap(row, X)\n    return dates\n\ndef find_overlap_dates(df, X):\n    return df.apply(find_overlap_row, X=X, axis=1)\n\nresult = find_overlap_dates(df, X)\nprint(result)\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef find_overlapping_dates(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates\n\ndef find_overlapping_dates_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks(row, X):\n    dates = row['date'].split('/')\n    dates = [int(date[1:3]) for date in dates]\n    return dates[-X:]\n\ndef find_overlapping_dates_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with_x_weeks_with", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\n# Create a new column with the bin edges\nbin_edges = [0, 0.5, 1]\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column with the bin numbers\nbin_nums = pd.cut(df['col1'], bin_edges, labels=[0, 0.5, 1])\n\n# Create a new column", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\n# Create a new column with the number of rows in each bin\ndf['bin'] = (df['col1'].astype(int) // 3).astype(int)\n\n# Group by the bin column and count the number of rows in each bin\nresult = df.groupby('bin').size()\n\nprint(result)\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n# Create a new column with the bin number\ndf['bin'] = (df['col1'] // 4).astype(int)\n\n# Group by bin and sum the values in each bin\nresult = df.groupby('bin')['col1'].sum()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\n# Create a new column with the number of rows in each group of 3 rows\ndf['group'] = (df.index // 3).astype(int)\n\n# Create a new column with the mean of the values in each group of 3 rows\nresult = df.groupby('group')['col1'].mean()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column for the sum of the first 3 rows\ndf['sum'] = df.iloc[:3].sum(axis=1)\n\n# Create a new column for the average of the first 2 rows\ndf['avg'] = df.iloc[:2].mean(axis=1)\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:].sum(axis=1)\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:].mean(axis=1)\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:].sum(axis=1)\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:].mean(axis=1)\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:].sum(axis=1)\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:].mean(axis=1)\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:].sum(axis=1)\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:].mean(axis=1)\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:].sum(axis=1)\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:].mean(axis=1)\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:].sum(axis=1)\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:].mean(axis=1)\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:].sum(axis=1)\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:].mean(axis=1)\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:].sum(axis=1)\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:].mean(axis=1)\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:].sum(axis=1)\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:].mean(axis=1)\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:].sum(axis=1)\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:].mean(axis=1)\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:].sum(axis=1)\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:].mean(axis=1)\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:].sum(axis=1)\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:].mean(axis=1)\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows_sum = last_3_rows['col1'].sum()\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows_avg = first_2_rows['col1'].mean()\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows_sum_2 = last_3_rows['col1'].sum()\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows_avg_2 = first_2_rows['col1'].mean()\n\n# Create a new dataframe with the sums and averages\nresult = pd.DataFrame({'col1':[last_3_rows_sum, first_2_rows_avg, last_3_rows_sum_2, first_2_rows_avg_2]})\n\nprint(result)\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(method='ffill')\ndf = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\n# Find the index of the first zero after the first non-zero value\nfirst_zero_index = df[df['A'] == 0].index[0]\n\n# Find the index of the last zero before the last non-zero value\nlast_zero_index = df[df['A'] == 0].index[-1]\n\n# Find the maximum value between the last zero and the first zero\nmax_value = max(df['A'].loc[first_zero_index:last_zero_index])\n\n# Fill the zeros with the maximum value\ndf['A'] = df['A'].fillna(max_value)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['time_days'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_days'] = df['time_days'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "\n# Create a dictionary to map the duration to the number of days\nduration_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n\n# Create a new column 'number' to store the number of days\ndf['number'] = df['duration'].map(duration_to_days)\n\n# Create a new column 'time_day' to store the number of days\ndf['time_day'] = df['duration'].map(duration_to_days)\n\nprint(df)\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    df['time'] = df['duration'].str.split(' ', 1).str[0]\n    df['number'] = df['duration'].str.split(' ', 1).str[1]\n    df['time_days'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "\n# Create a dictionary to map the duration to the number of days in a year\nduration_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n\n# Create a new column 'number' that maps the duration to the number of days\ndf['number'] = df['duration'].map(duration_to_days)\n\n# Create a new column 'time' that maps the duration to the number of days\ndf['time'] = df['duration'].map(duration_to_days)\n\n# Create a new column 'time_day' that maps the duration to the number of days\ndf['time_day'] = df['duration'].map(duration_to_days)\n\n# Print the new data frame\nprint(df)\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.where(np.all(np.array([df1[column] == df2[column] for column in columns_check_list]), axis=1))\nprint(result)\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = np.all(np.where((df1[columns_check_list] == df2[columns_check_list]), True, False), axis=1)", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x)))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    # parse date index using pd.to_datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # swap the two levels\n    df = df.set_index(['id', 'date'])\n    # return the solution in this function\n    return df\n    ### END SOLUTION ###\n\n# test the function\ndf = pd.DataFrame({'x': ['abc'], 'y': [100], 'date': ['3/1/1994']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot_table(index=['Country', 'Variable'], columns='year', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns=['Variable'])\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Country': 'Country', 'var2", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.sort_values(['Country', 'Variable', 'year'], ascending=[True, True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='value', fill_value=0)\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns=['Variable'])\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'year': 'var2'})\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var1'], ascending=[True", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\n# Create a list of all column names that start with 'Value'\ncolumn_names = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the columns that have a value less than 1\nnew_df = df[column_names]\n\n# Create a new dataframe with only the rows where all values are less than 1\nnew_df = new_df[new_df.abs() < 1]\n\nprint(new_df)\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\n# Create a list of all column names that start with 'Value'\ncolumn_names = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the rows where all columns are more than 1\nnew_df = new_df.loc[new_df.all(axis=1)]\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a list of all column names that start with 'Value'\nvalue_columns = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df =", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df = df.replace({'&LT;': '<'}, regex=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    # replace &AMP; with '&'\n    df = df.replace({'&AMP;': '&'})\n    # return the result\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x.split(' ')[1] if ' ' in x else None)\n\nprint(df)\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['name'] = df['name'].apply(validate_single_space_name)\n\n# Create a new DataFrame with the first and last names\nfirst_names = df[df['name'].notna()]['name'].apply(lambda x: x.split(' ', 1)[0])\nlast_names = df[df['name'].notna()]['name'].apply(lambda x: x.split(' ', 1)[1])\n\n# Create a new DataFrame with the first and last names\nnew_df = pd.DataFrame({'first_name': first_names, 'last_name': last_names})\n\n# Print the new DataFrame\nprint(new_df)\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge_asof(df2, df1, on='Timestamp', direction='nearest')\nresult['data'] = df1['data'].values[result['Timestamp'].values]\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1, df2, on='Timestamp', direction='nearest')\nresult['Timestamp'] = result['Timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\nresult['data'] = result['data'].astype(str)\nresult['stuff'] = result['stuff'].astype(str)\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= 50, df['col1'], np.max(df[['col1', 'col2', 'col3']], axis=1))\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "\ndef state(row):\n    if row['col2'] > 50 and row['col3'] > 50:\n        return row['col1']\n    else:\n        return row['col1'] + row['col2'] + row['col3']\n\ndf['state'] = df.apply(state, axis=1)\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\n# Create a list of errors\nerrors = [\"and\"]\n\n# Iterate over each row\nfor index, row in df.iterrows():\n    # Check if the value is not an integer\n    if not pd.to_numeric(row[\"Field1\"], errors='coerce') == row[\"Field1\"]:\n        errors.append(row[\"Field1\"])\n\n# Print the errors\nprint(errors)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndef get_integer_values(row):\n    return [val for val in row if isinstance(val, int)]\n\ndf['Field1'] = df['Field1'].apply(get_integer_values)\n\nprint(df['Field1'].to_list())\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(x))\n    # create a new column with the result of the isnumeric function\n    df['isnumeric'] = df['Field1'].apply(lambda x: not pd.isnull(x) and not pd.isnull(x) and not pd.isnull(", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndef compute_percentage(df):\n    total = df.sum()\n    return df / total * 100\n\ndf['val1'] = compute_percentage(df['val1'])\ndf['val2'] = compute_percentage(df['val2'])\ndf['val3'] = compute_percentage(df['val3'])\ndf['val4'] = compute_percentage(df['val4'])\n\nprint(df)\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "\n# calculate total for each category\ntotal = df['val1'].sum()\n\n# calculate percentage for each category\npercentage = df['val1'] / total\n\n# create a new dataframe with percentage\nresult = pd.DataFrame({'cat': df['cat'], 'val1': percentage})\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)\nprint(result)\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "# calculate the pairwise distances between cars\ndistances = pd.DataFrame(np.sqrt((df['x'].values[:, None] - df['x'].values[None, :])**2 + (df['y'].values[:, None] - df['y'].values[None, :])**2))\n\n# find the nearest neighbour for each car\nnearest_neighbour = np.argmin(distances, axis=1)\n\n# calculate the average of the distances for each frame\nmean_distances = distances.groupby('time').mean()\n\n# create a new dataframe with the nearest neighbour and the mean distance for each time point\ndf2 = pd.DataFrame({'car': df['car'].values, 'nearest_neighbour': nearest_neighbour, 'mean_distance': mean_distances})\n\n# print the result\nprint(df2)\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "# Create a new column 'farmost_neighbour' that contains the farthest car for each time point\ndf['farmost_neighbour'] = df.groupby('time')['car'].transform('max')\n\n# Create a new column 'euclidean_distance' that contains the euclidean distance between each car and its farthest neighbour\ndf['euclidean_distance'] = df.groupby('time')['car'].transform(lambda x: (x - df.loc[x.index, 'farmost_neighbour']).abs().mean())\n\n# Group by 'time' and calculate the average of 'euclidean_distance'\ndf = df.groupby('time').mean()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "\ndef concat_columns(df):\n    return \", \".join(df.columns)\n\ndf[\"keywords_all\"] = df.apply(concat_columns, axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "\ndef concat_columns(df):\n    return \"-\".join(df.columns)\n\ndf[\"keywords_all\"] = df.apply(concat_columns, axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "\ndef concat_keywords(df):\n    keywords_all = []\n    for col in df.columns[1:]:\n        keywords_all.append(df[col].apply(lambda x: \"-\".join(x.split()) if not np.isnan(x) else x))\n    return \"-\".join(keywords_all)\n\ndf['keywords_all'] = concat_keywords(df)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndef concat_last_rows(df):\n    last_rows = df.iloc[-1:]\n    last_rows['keywords_all'] = last_rows.apply(lambda row: \"-\".join(row.values[1:]), axis=1)\n    return last_rows\n\ndf = concat_last_rows(df)\nprint(df)\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_rows = df.sample(n=int(0.20 * len(df)), random_state=0)\n\n# Set Quantity to zero\nsample_rows['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_rows.index = range(len(sample_rows))\n\n# Print the result\nprint(sample_rows)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_rows = df.sample(n=int(0.20 * len(df)), random_state=0)\n\n# Change value of ProductId to zero\nsample_rows['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_rows.index = range(len(sample_rows))\n\n# Print the result\nprint(sample_rows)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# Create a new DataFrame with the 20% of rows of each user\ndf_sample = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n\n# Set the Quantity column of these rows to zero\ndf_sample['Quantity'] = 0\n\n# Keep the indexes of the altered rows\ndf_sample.index = df.index\n\n# Print the resulting DataFrame\nprint(df_sample)\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    # find the duplicates\n    duplicates = df.duplicated(subset=['col1','col2'], keep='first')\n    # find the index of the first duplicate\n    index_original = duplicates.index[duplicates]\n    # add a new column with the index of the first duplicate\n    df['index_original'] = index_original\n    # return the result\n    return df\n\n# call the function\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# Find the maximum count for each group\nmax_count = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\nprint(result)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "min_count = df['count'].min()\nresult = df[df['count'] == min_count]", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# Find the maximum count for each group\nmax_count = df.groupby(['Sp','Value'])['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")\nprint(result)\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")\nprint(result)\n", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "result = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                  ('A', 'B', 'F'),\n                                  ('A', 'C', 'G'),\n                                  ('A', 'C', 'H'),\n                                  ('A', 'D', 'I'),\n                                  ('A', 'D', 'J')])\nprint(result)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "result = pd.melt(df, value_vars=[list('BBCCDD'), list('EFGHIJ')], var_names=['variable_0', 'variable_1', 'variable_2'])\nresult['variable_0'] = result['variable_0'].apply(lambda x: x[0])\nresult['variable_1'] = result['variable_1'].apply(lambda x: x[1])\nresult['variable_2'] = result['variable_2'].apply(lambda x: x[2])\nresult = result[['variable_0', 'variable_1', 'variable_2', 'value']]\nresult", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id').val.cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] < 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(np.nanmean(x)) else np.nan)\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship_type(column1, column2):\n    if column1.equals(column2):\n        return 'one-to-one'\n    elif column1.equals(column2.unique()):\n        return 'one-to-many'\n    elif column2.equals(column2.unique()):\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\nresult = []\nfor i in range(df.shape[1]):\n    for j in range(i + 1, df.shape[1]):\n        result.append(f'{df.columns[i]} {df.columns[j]} {relationship_type(df[df.columns[i]], df[df.columns[j]])}')\n\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship_type(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-2-many'\n    elif column1.min() == column2.max():\n        return 'one-2-one'\n    elif column1.min() == column2.min():\n        return 'many-2-one'\n    elif column1.max() == column2.max():\n        return 'many-2-many'\n    else:\n        return 'none'\n\nresult = []\nfor i in range(df.shape[1]):\n    for j in range(i + 1, df.shape[1]):\n        result.append(f'{df.columns[i]} {df.columns[j]} {relationship_type(df[df.columns[i]], df[df.columns[j]])}')\n\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def relationship(column1, column2, column3, column4, column5):\n    if column1.equals(column2) and column1.equals(column3) and column1.equals(column4) and column1.equals(column5):\n        return 'one-to-one'\n    elif column2.equals(column3) and column2.equals(column4) and column2.equals(column5):\n        return 'one-to-many'\n    elif column3.equals(column4) and column3.equals(column5):\n        return 'many-to-one'\n    elif column4.equals(column5):\n        return 'many-to-many'\n    else:\n        return 'none'\n\nresult = pd.DataFrame({\n    'Column1': relationship(df['Column1'], df['Column2'], df['Column3'], df['Column4'], df['Column5']),\n    'Column2': relationship(df['Column2'], df['Column3'], df['Column4'], df['Column5'], df['Column1']),\n    'Column3': relationship(df['Column3'], df['Column4'], df['Column5'], df['Column1'], df['Column2']),\n    'Column4': relationship(df['Column4'], df['Column5'], df['Column1'], df['Column2'], df['Column3']),\n    'Column5': relationship(df['Column5'], df['Column1'], df['Column2'], df['Column3'], df['Column4'])\n})\nprint(result)\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship(x, y):\n    if x.shape[0] == y.shape[0]:\n        return 'one-2-many'\n    elif x.shape[0] == y.shape[0] + 1:\n        return 'one-2-one'\n    elif x.shape[0] == y.shape[0] + 2:\n        return 'many-2-one'\n    elif x.shape[0] == y.shape[0] + 2:\n        return 'many-2-many'\n    else:\n        return 'none'\n\nresult = pd.DataFrame(columns=df.columns, index=df.columns)\nfor c1 in df.columns:\n    for c2 in df.columns:\n        result.loc[c1, c2] = relationship(df[c1], df[c2])\n\nprint(result)\n", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\nresult = dfiban_uniq\nprint(result)\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install sklearn\n!pip install pand", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\n# Create a new column 'Family'\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\n\n# Group by 'Family' and calculate the mean of 'Survived'\nresult = df.groupby('Family')['Survived'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\n# Create a new column 'Family'\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\n\n# Group by 'Family' and calculate the mean of 'SibSp'\nresult = df.groupby('Family')['SibSp'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndef group_condition(sibsp, parch):\n    if sibsp == 1 and parch == 1:\n        return 'Has Family'\n    elif sibsp == 0 and parch == 0:\n        return 'No Family'\n    elif sibsp == 0 and parch == 1:\n        return 'New Family'\n    elif sibsp == 1 and parch == 0:\n        return 'Old Family'\n    else:\n        return 'Unknown'\n\n\ngroup_names = ['Has Family', 'No Family', 'New Family', 'Old Family']\n\ngroup_names_dict = dict(zip(group_names, group_names))\n\ndf['Group'] = df.apply(lambda row: group_names_dict[group_condition(row['SibSp'], row['Parch'])], axis=1)\n\nresult = df.groupby('Group')['Survived'].mean()\nprint(result)\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\nprint(df)\n", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\nprint(df)\n", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "def std_mean(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('a').apply(std_mean)\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "def mean_std(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('b').a.apply(mean_std)\nresult", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x, axis=0)\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = df['b'].apply(softmax)\ndf['min-max'] = df['b'].apply(min_max)\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "result = df.loc[~(df == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "result = df[df.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "result = df[df.max(axis=1) != 2]", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "result = df.where(df == 2, 0)\nresult", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=True).sort_index()\nprint(result)\n", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "sorted_s = s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].apply(lambda x: x.isdigit())]\nprint(result)\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# Find the maximum count for each group\nmax_count = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\nprint(result)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "min_count = df['count'].min()\nresult = df[df['count'] == min_count]", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# Find the maximum count for each group\nmax_count = df.groupby(['Sp','Value'])['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(np.nan)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": " ###\n    # map the dict to the date column\n    df['Date'] = df['Member'].map(dict)\n    # fill NaN with the date column\n    df['Date'] = df['Date'].fillna(df['Date'])\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17-Aug-1926')\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\nprint(df)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\n\n# Group by year, month and weekday\ndf_group = df.groupby([df['Date'].dt.year, df['Date'].dt.month, df['Date'].dt.weekday])\n\n# Count the number of dates per year, month and weekday\ndf_count = df_group.size()\n\n# Create a new dataframe with the count of dates per year, month and weekday\ndf_count_d = df_group.size().unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).unstack(1).un", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date')['B'].apply(lambda x: x.sum())\nresult2 = df.groupby('Date')['C'].apply(lambda x: x.sum())\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "even_values = df[df['B'] % 2 == 0]\nodd_values = df[df['B'] % 2 != 0]\n\nresult1 = even_values.groupby('Date').count()\nresult2 = odd_values.groupby('Date').count()\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nprint(result)\n\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.mean)\nprint(result)\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult_mean = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nresult_mean['E'] = result_mean['E'] * 2\nresult_mean['D'] = result_mean['D'] * 2\nresult_mean</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nprint(result)\n\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nprint(result)\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\ndef split_string_to_rows(df):\n    df = df.replace(',', ', ', regex=True)\n    df = df.replace(' ', ', ', regex=True)\n    df = df.replace('\\n', ', ', regex=True)\n    return df\n\nresult = split_string_to_rows(df)\nprint(result)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\ndef split_string_to_rows(s):\n    return s.split(',')\n\ndf['var2'] = df['var2'].apply(split_string_to_rows)\n\nresult = df\nresult", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\ndef split_into_rows(df):\n    return df.assign(var1=lambda x: x.var1.str.split('-'))\n\nresult = split_into_rows(df)\nprint(result)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df['fips'] = df['row'].str[:-1]\ndf['row'] = df['row'].str[-1]\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "\ndf['fips'] = df['row'].str[:4]\ndf['medi'] = df['row'].str[4:]\n\nprint(df)\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        if col != 'Name':\n            df[col] = df[col].apply(lambda x: x / (1 if x != 0 else 1))\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        if col != 'Name':\n            df[col] = df[col].rolling(window=len(df[col])).mean()\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    # calculate the cumulative average for each row using pandas\n    # ignore zero values\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n   ", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        if col != 'Name':\n            df[col] = df[col].rolling(window=len(df[col])).mean()\n            df[col] = df[col].where(df[col] != 0, df[col])\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)\ndf['label'] = df['label'].replace([0, 1], [1, 0])\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\n# Create a new column 'label'\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n\n# Set label for the first row to 1\ndf.loc[0, 'label'] = 1\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime(\"%d-%b-%Y\")\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_datetime(df.departure_time) - pd.to_datetime(df.arrival_time)\ndf['Duration'] = df.Duration.apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = pd.to_datetime(df.Duration)\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds)\nprint(df)\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "\n# Convert the arrival and departure times to datetime format\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n\n# Calculate the duration in seconds\ndf['Duration'] = (df['departure_time'] - df['arrival_time']).apply(lambda x: x.seconds)\n\n# Convert the duration to seconds\ndf['Duration'] = df['Duration'].apply(lambda x: x/60/60)\n\n# Convert the duration to minutes\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda x: x/60)\n\n# Convert the duration to hours\ndf['Duration'] = df['Duration'].apply(lambda", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df.groupby('key1').apply(lambda x: x.loc[x['key2'].endswith('e'), 'key2'].size()).reset_index(drop=True)\nprint(result)\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.index[df.value.argmax()]\nmin_result = df.index[df.value.argmin()]\nprint(min_result,max_result)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df['value'].mode()\nmedian_result = df['value'].median()\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(99 <= df['closing_price']) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df[~(99 <= df['closing_price'] <= 101)]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "result = df.groupby(\"item\")[\"diff\"].min()\nresult = result.reset_index()\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "\ndef get_last_underscore(s):\n    return s.split('_')[-1] if '_' in s else s\n\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(get_last_underscore)\nprint(df)\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "\ndef get_last_underscore(s):\n    return s.split('_')[-1] if '_' in s else s\n\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(get_last_underscore)\nprint(df)\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# Calculate the number of NaN values in the column\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# Calculate the number of NaN values to fill\nnan_to_fill = nan_count // 2\n\n# Create a mask for the NaN values to fill\nmask = np.isnan(df['Column_x'])\n\n# Fill the NaN values with the mode of the column\ndf['Column_x'][mask] = df['Column_x'].mode()[0]\n\n# Fill the first half of NaN values with 0 and the last half with 1\ndf['Column_x'][~mask] = np.where(mask, 0, 1)\n\n# Print the original and transformed data\nprint(df)\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values in the column\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# Calculate the number of NaN values to be filled\nnan_to_fill = nan_count * 0.3\n\n# Calculate the number of NaN values to be filled in the first 30% of the column\nnan_to_fill_first = np.floor(nan_to_fill)\n\n# Calculate the number of NaN values to be filled in the middle 30% of the column\nnan_to_fill_middle = np.floor(nan_to_fill * 0.7)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = nan_to_fill - nan_to_fill_first - nan_to_fill_middle\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# Create a mask for the NaN values\nmask = np.isnan(df['Column_x'])\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame([(a.iloc[0, 0], b.iloc[0, 0]), (a.iloc[1, 0], b.iloc[1, 0])], columns=['one', 'two'])\nprint(result)\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0], c.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0], c.iloc[1,0])], columns=['one', 'two'])\nresult", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "# Create a new dataframe with the same shape as a and b\na_b = pd.DataFrame(np.nan, index=a.index, columns=a.columns)\n\n# Iterate over the rows of a and b\nfor i in range(a.shape[0]):\n    # Iterate over the columns of a and b\n    for j in range(a.shape[1]):\n        # If the row and column are not nan, assign the tuple to the corresponding position in a_b\n        if not np.isnan(a.iloc[i, j]) and not np.isnan(b.iloc[i, j]):\n            a_b.iloc[i, j] = (a.iloc[i, j], b.iloc[i, j])\n\n# Print the result\nprint(a_b)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).reset_index()\nresult = groups.set_index('username')\nresult = result.sort_values(by='views')\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).reset_index()\nresult = groups.set_index('username')\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).fillna(0)\nresult = groups.reset_index()\nresult.columns = ['username', 'views']\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', ')))\nresult = result.to_frame('text')\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame()\nresult.columns = ['text']\nresult", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', '))).to_frame()\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].to_series().str.split(', ', expand=True).apply(lambda x: x.str.strip())\nresult = result.apply(lambda x: ', '.join(x))\nresult = result.to_frame('text')\nresult", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame()\nresult.columns = ['text']\nresult", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.concat([df1, df2], axis=0)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.set_index(['id', 'city', 'district']))\nresult = result.reset_index()\nresult = result.fillna(value={'city': 'NaN', 'district': 'NaN'})\nresult = result.sort_values(['id', 'date'])\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n# Create a new column in df2 that represents the date in the format 01-Jan-2019\ndf2['date'] = pd.to_datetime(df2['date']).dt.strftime('%d-%b-%Y')\n\n# Concatenate df1 and df2 on the id column\nresult = pd.concat([df1, df2], axis=0)\n\n# Sort the result by id and date\nresult = result.sort_values(['id', 'date'])\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\n# merge df1 and df2 on id\nmerged_df = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with city and district from df2\nmerged_df = merged_df.fillna({'city': '', 'district': ''})\n\n# sort by id and date\nmerged_df = merged_df.sort_values(['id', 'date'])\n\n# group by id and date, then concatenate the values\nresult = merged_df.groupby(['id', 'date']).apply(lambda x: x.sort_values(['date']))\n\n# reset index\nresult = result.reset_index(drop=True)\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x']\nresult['B_y'] = D['B']\nresult = result.sort_values('A')\nresult['B'] = result['B_x']\nresult.drop(['B_x'], axis=1, inplace=True)\nresult.reset_index(drop=True, inplace=True)\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x']\nresult['B_y'] = D['B']\nresult = result.sort_values('A')\nresult['B_y'] = result['B_y'].replace(np.nan, 5)\nresult", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['dulplicated'] = result['A'].apply(lambda x: x in D['A'])\nresult", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).tolist())\nprint(result)\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).tolist()\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time), tuple(amount))] for time, amount in result]\nresult = [list(x) for x in result]\nresult = [[(tuple(time),", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist())\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'time': 'amount-time-tuple'})\nresult = result.sort_values(['user'])\nresult", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\n# Convert the series to a dataframe\ndf = series.to_frame()\n\n# Expand the dataframe to a 2D dataframe\ndf_concatenated = df.transpose().reset_index(drop=True).transpose()\n\nprint(df_concatenated)\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\n# Create a new dataframe with the same index as the series\ndf = pd.DataFrame(series.index, index=series.index, columns=series.index)\n\n# Expand the series to the dataframe\ndf = df.join(series)\n\n# Rename the columns\ndf.columns = ['0', '1', '2', '3']\n\n# Print the dataframe\nprint(df)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[~df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.columns[~df.columns.str.contains(s)].tolist()[0]\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "# Find the column names that contain the string 'spike'\nmatch_columns = [col for col in df.columns if s in col]\n\n# Find the column names that contain the string 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike'\nmatch_columns += [col for col in df.columns if col.startswith(s)]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s + '-')]\n\n# Find the column names that contain the string 'spike' but not exactly 'spike-'\nmatch_columns += [col for col in df.columns if col.startswith(s", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# Convert the list to a single column\ndf['codes'] = df['codes'].apply(lambda x: x[0] if x else None)\n\n# Split the list into columns\ndf = df.join(df['codes'].apply(lambda x: pd.Series(x)).fillna(0))\n\n# Drop the original column\ndf = df.drop(columns='codes')\n\n# Sort the columns\ndf = df.sort_values(by=list(df.columns)[0])\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "result = df['col1'].apply(lambda x: x[0])\nresult = result.tolist()\nprint(result)\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x[::-1])))\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x)))\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "\n# Convert the Time column to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Create a new column that represents the bin of the time\ndf['Time_bin'] = df['Time'].apply(lambda x: x.hour * 60 + x.minute)\n\n# Group by the bin and mean the Value column\nmean_values = df.groupby('Time_bin')['Value'].mean()\n\n# Create a new DataFrame with the mean values\nmean_values_df = pd.DataFrame({'Time': mean_values.index, 'Value': mean_values.values})\n\n# Interpolate the values\nmean_values_df['Time'] = mean_values_df['Time'].apply(lambda x: x.replace(minute=x.minute // 2 * 2))\n\n# Create a new column with the bin of the time\nmean_values_df['Time_bin'] = mean_values_df['Time'].apply(lambda x: x.hour * 60 + x.minute // 2 * 2)\n\n# Group by the bin and mean the Value column\nmean_values_df = mean_values_df.groupby('Time_bin')['Value'].mean()\n\n# Create a new DataFrame with the mean values\nmean_values_df = pd.DataFrame({'Time': mean_values_df.index, 'Value': mean_values_df.values})\n\n# Print the mean values\nprint(mean_values_df)\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype(int) / 60).astype(int)\n\n# Create a new column for the bin\ndf['bin'] = df['Time'].apply(lambda x: x // 3)\n\n# Group by bin and sum the values\nresult = df.groupby('bin').sum()\n\n# Convert the bin back to time\nresult['Time'] = result['bin'] * 3\n\n# Convert the time to datetime\nresult['Time'] = pd.to_datetime(result['Time'])\n\n# Drop the bin column\nresult = result.drop(columns='bin')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[np.where(np.isnan(df).any(axis=1))[0]]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.columns[np.all(df.isnull(), axis=1)]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[np.where(np.isnan(df).any(axis=1))[0]]", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(np.nan, np.nan) if np.isnan(x) and np.isnan(y) else (x, y) for x, y in zip(df.iloc[0], df.iloc[8])]\nprint(result)\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.str.split('_').str[1]\ndf = df.", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf = df.transpose().reset_index().transpose()\ndf.columns = df.columns.values.reshape(-1,1)\ndf", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\nprint(df)", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\nprint(df)", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by='time')", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(by=['treatment','dose','time'])\nprint(result)\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "# Create a date range for the dates you want to delete\ndate_range = pd.date_range(start='2020-02-17', end='2020-02-18')\n\n# Create a new dataframe with the dates you want to delete\ndate_range_df = pd.DataFrame(date_range, columns=['Date'])\n\n# Delete the dates from the original dataframe\nresult = df[~date_range_df['Date'].isin(df['Date'])]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Create a new column 'Day' to store the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr[np.abs(corr) > 0.3].stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr[np.abs(corr) > 0.3].values.flatten()\n", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "result = df.rename(columns={df.columns[-1]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "result = df.rename(columns={df.columns[0]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndef find_frequent(row):\n    return row.value_counts().max()\n\ndef find_freq_count(row):\n    return row.value_counts().values.max()\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndef find_frequent(row):\n    return row.value_counts().index[0]\n\ndef find_freq_count(row):\n    return row.value_counts().values[0]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    return [val for val in row if row[val] == row[val]]\n\ndef find_frequent_count(row):\n    return len([val for val in row if row[val] == row[val]])\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_frequent_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "res = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nres = res.reset_index()\nres", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "res = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nres['bar'] = res['bar'].apply(lambda x: x.replace('NULL', 0))\nres", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo']]\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'b_col']]\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = np.delete(x, np.where(np.isnan(x)))", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "x = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nx = x[~np.isnan(x)]\nresult = x.tolist()\nprint(result)\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((a.shape[0], 4))\nb[:, 0] = a == 0\nb[:, 1] = a == 1\nb[:, 2] = a == 2\nb[:, 3] = a == 3\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((a.shape[0], a.max() + 1))\nb[np.arange(a.shape[0]), a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((a.size, a.size))\nb[a, a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - 1] = 1\nb[a[:, 0] - 1, a[:, 1] - ", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (len(A), ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "def vec2matrix(A, nrow):\n    return np.reshape(A, (nrow, -1))\n\nB = vec2matrix(A, nrow)\nprint(B)\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "def shift(a, shift):\n    return np.roll(a, shift, axis=1)\n\nresult = shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape, order='C')\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape, order='C')\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a), a.shape)\n    return max_index\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2, axis=-1)[:-2, -2], a.shape)\nprint(result)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "z = np.any(np.isnan(a), axis=0)\na = np.delete(a, z, axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = np.delete(a, np.where(np.isnan(a)))\nprint(a)\n", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\nprint(result)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravel_index(np.argmin(a, axis=None), a.shape)\nresult = min_index\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "min_index = np.unravel_index(np.argmin(a, axis=0), a.shape)\nresult = np.array([[0, 1], [1, 0]])\nprint(min_index)\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.degrees(degree))\nprint(result)", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.deg2rad(degree))", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * 180 / np.pi\nresult = np.rad2deg(result)\nprint(result)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)\nprint(a)\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(a, 2))\nprint(f(a, 3))\nprint(f(a, 4))\nprint(f(a, 5))\nprint(f(a, 6))\nprint(f(a, 7))\nprint(f(a, 8))\nprint(f(a, 9))\nprint(f(a, 10))\nprint(f(a, 11))\nprint(f(a, 12))\nprint(f(a, 13))\nprint(f(a, 14))\nprint(f(a, 15))\nprint(f(a, 16))\nprint(f(a, 17))\nprint(f(a, 18))\nprint(f(a, 19))\nprint(f(a, 20))\nprint(f(a, 21))\nprint(f(a, 22))\nprint(f(a, 23))\nprint(f(a, 24))\nprint(f(a, 25))\nprint(f(a, 26))\nprint(f(a, 27))\nprint(f(a, 28))\nprint(f(a, 29))\nprint(f(a, 30))\nprint(f(a, 31))\nprint(f(a, 32))\nprint(f(a, 33))\nprint(f(a, 34))\nprint(f(a, 35))\nprint(f(a, 36))\nprint(f(a, 37))\nprint(f(a, 38))\nprint(f(a, 39))\nprint(f(a, 40))\nprint(f(a, 41))\nprint(f(a, 42))\nprint(f(a, 43))\nprint(f(a, 44))\nprint(f(a, 45))\nprint(f(a, 46))\nprint(f(a, 47))\nprint(f(a, 48))\nprint(f(a, 49))\nprint(f(a, 50))\nprint(f(a, 51))\nprint(f(a, 52))\nprint(f(a, 53))\nprint(f(a, 54))\nprint(f(a, 55))\nprint(f(a, 56))\nprint(f(a, 57))\nprint(f(a, 58))\nprint(f(a, 59))\nprint(f(a, 60))\nprint(f(a, 61))\nprint(f(a, 62))\nprint(f(a, 63))\nprint(f(a, 64))\nprint(f(a, 65))\nprint(f(a, 66))\nprint(f(a, 67))\nprint(f(a, 68))\nprint(f(a, 69))\nprint(f(a, 70))\nprint(f(a, 71))\nprint(f(a, 72))\nprint(f(a, 73))\nprint(f(a, 74))\nprint(f(a, 75))\nprint(f(a, 76))\nprint(f(a, 77))\nprint(f(a, 78))\nprint(f(a, 79))\nprint(f(a, 80))\nprint(f(a, 81))\nprint(f(a, 82))\nprint(f(a, 83))\nprint(f(a, 84))\nprint(f(a, 85))\nprint(f(a, 86))\nprint(f(a, 87))\nprint(f(a, 88))\nprint(f(a, 89))\nprint(f(a, 90))\nprint(f(a, 91))\nprint(f(a, 92))\nprint(f(a", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    # Use numpy's floor_divide function to divide the numerator and denominator by 100\n    # Then return the result as a tuple\n    # result = np.floor_divide([numerator, denominator], 100)\n    # return result\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(98, 42)) # Output: (7, 3)\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), result.astype(int))\nprint(result)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = np.mean([a, b, c], axis=0)\nprint(result)\n", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(a, np.maximum(b, c))", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.flatten())[::-1]\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a.flatten())[::-1]\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.concatenate((np.diag(a), np.diag(a.T)[::-1]))\nprint(result)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(np.arange(a.shape[0])), np.diag(np.arange(a.shape[1])-a.shape[0]+1)), axis=1)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = np.empty(X.shape[0], dtype=object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.empty(X.shape[0], dtype=object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i, j])\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty(X.shape[0], dtype=np.object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.fromstring(mystr, dtype=int, sep='')\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "# calculate the t-statistic\nt_statistic = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_statistic, 40, 50)\nprint(p_value)", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "# calculate the t-statistic\nt_statistic = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_statistic, a.shape[0] + b.shape[0])\nprint(p_value)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the weighted mean and variance\nweights = np.array([anobs, bnobs])\nweights = weights / np.sum(weights)\n\n# calculate the weighted mean and variance\nweights = weights[0] * anobs + weights[1] * bnobs\nweights = weights / np.sum(weights)\n\n# calculate the t-test\nt_statistic = (bmean - amean) / np.sqrt(weights[0] * (1 - weights[0]) / anobs + weights[1] * (1 - weights[1]) / bnobs)\np_value = scipy.stats.t.pdf(t_statistic, anobs + bnobs)\np_value", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.setdiff1d(np.setdiff1d(A, B), B)\nprint(output)\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)\nprint(result)\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)\nprint(a)\n", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, 0)\nprint(a)\n", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\nprint(result)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\nprint(a)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n\nprint(f(a=example_a, pos=2, element=66))\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.all(a == a[0], axis=1))", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.all(a == a[0], axis=1))", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.all(a == a[0], axis=1))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\nprint(result)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # using Simpson's rule\n    # result = np.sum(np.sum(np.sin(x)**2 + np.cos(y)**4, axis=1))\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using trapz\n    # result = np.trapz(np.sin(x)**2 + np.cos(y)**4, x)\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using simps\n    # result = np.simps(np.sin(x)**2 + np.cos(y)**4, x)\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using dblquad\n    # result = np.dblquad(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, lambda x: 0, lambda x: 1)\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using rectangles\n    # result = np.rectangle(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, 0, 1)\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using rectangles with weights\n    # result = np.rectangle(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, 0, 1, weights=[1, 1, 1, 1])\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using rectangles with weights\n    # result = np.rectangle(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, 0, 1, weights=[1, 1, 1, 1])\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using rectangles with weights\n    # result = np.rectangle(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, 0, 1, weights=[1, 1, 1, 1])\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using rectangles with weights\n    # result = np.rectangle(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, 0, 1, weights=[1, 1, 1, 1])\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using rectangles with weights\n    # result = np.rectangle(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, 0, 1, weights=[1, 1, 1, 1])\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using rectangles with weights\n    # result = np.rectangle(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, 0, 1, weights=[1, 1, 1, 1])\n    # # calculate the 2D integral of (cos(x))^4 + (sin(y))^2\n    # # using rectangles with weights\n    # result = np.rectangle(lambda x, y: np.sin(x)**2 + np.cos(y)**4, 0, 1, 0, 1, weights=[1, 1, 1, 1])\n   ", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\n# compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow = np.min(grades)\nhigh = np.max(grades)\nwhile high - low > threshold:\n  high = np.max(grades[grades < high])\n  low = np.min(grades[grades > low])\n  if high - low < threshold:\n    break\n\nprint(low, high)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.randint(2, size=size)\nnums[nums>=one_ratio] = 1\nnums[nums<one_ratio] = 0\nprint(nums)", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\nprint(a_pt)", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = np.array(a)", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\nprint(a_tf)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a) + 1\nresult = result.tolist()\nprint(result)\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[::-1][:N]\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "result = np.array([[a[i:i+2, j:j+2] for j in range(0, a.shape[1], 2)] for i in range(0, a.shape[0], 2)]).reshape(-1, 2, 2)", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\npatch_shape = a.shape[:-1] + (a.shape[-1] // patch_size, patch_size)\n\nresult = np.array([a[i:i + patch_shape[0], j:j + patch_shape[1]] for i in range(0, a.shape[0], patch_size) for j in range(0, a.shape[1], patch_size)])\n\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\nprint(result)\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\npatch_shape = a.shape[:-1] + (a.shape[-1] // patch_size, patch_size)\n\nresult = np.array([a[i:i + patch_shape[0], j:j + patch_shape[1]] for i in range(0, a.shape[0], patch_size) for j in range(0, a.shape[1], patch_size)])\n\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.fromstring(string, sep=' ', dtype=np.float64)\nprint(a)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.log(np.random.uniform(min, max, n))", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.log(np.random.uniform(min, max, n))", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.log(np.random.uniform(min, max, n))\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))\nprint(result)\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))\nprint(result)\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, index[0], index[1], index[2])\n", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "def sub2ind(dims, index):\n    return np.asscalar(np.prod(dims)) + np.asscalar(np.prod(index))\n\nresult = sub2ind(dims, index)\nprint(result)\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df.values.dtype)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.sum(a[accmap])\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0])\nprint(result)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.minimum.reduce(a[index], axis=0)\nprint(result)\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    z[i] = elementwise_function(x[i], y[i])\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, probabilit)\nprint(result)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "a = np.pad(a, ((0, 0), (low_index, high_index)), 'constant')\nresult = a[1:3, 1:3]\nprint(result)\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.logical_not(np.iscomplex(x))]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_max = np.max(bin_data, axis=1)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nbin_data_mean = [list(x) for x in bin_data_mean]\nprint(bin_data_mean)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (x - x_min) * (x - x_max)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    x_min = np.clip(x_min, x, x_max)\n    x_max = np.clip(x_max, x, x_min)\n    x_min = np.clip(x_min, x, x_max)\n    x_max = np.clip(x_max, x, x_min)\n    return np.clip(x, x_min, x_max)\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.correlate(a, b, mode='same')\n\n# Print the correlation matrix\nprint(corr_matrix)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(4)])\nprint(result)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(15)])\nprint(result)\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.unpackbits(np.uint8(a))\nresult = np.reshape(result, (a.shape[0], m))\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:m].reshape(-1, m)\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "result = np.unpackbits(np.uint8(a))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1, m)), result))\nresult = np.concatenate((np.ones((1, m)), result))\nresult = np.concatenate((np.zeros((1", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc -= 3*\u03c3\n\u03bc += 3*\u03c3\nresult = (\u03bc-3*\u03c3, \u03bc+3*\u03c3)\nprint(result)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc - 2*\u03c3\n\u03bc + 2*\u03c3\nresult = (\u03bc-2*\u03c3, \u03bc+2*\u03c3)\nresult", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std = np.std(a)\n    third_std_dev = np.mean(a) + 3*std\n    return third_std_dev\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd = np.std(a)\nlower_bound = mean - 2*std\nupper_bound = mean + 2*std\noutlier = a < lower_bound\nresult = outlier | a > upper_bound\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data, percentile)\nprint(prob)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[0, 1] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.all(a == np.amax(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1), axis=1)\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "result = np.corrcoef(post, distance)[0, 1]", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.dot(X, X.T)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.dot(Y.reshape(Y.shape[0], -1), Y.reshape(Y.shape[0], -1).T)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = np.any(a == number)", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = A[~np.isin(A, B)]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[np.in1d(A, B)]", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "# Create a boolean array that indicates whether each element in A is in B\nbool_array = np.in1d(A, B)\n# Use np.where to select elements in A that are in B\nC = np.where(bool_array, A, 0)\n# Remove zero values\nC = C[C != 0]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.max(rankdata(a)) - rankdata(a) + 1", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.max(rankdata(a)) - rankdata(a) + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    # Use the rankdata function to get the rank of each value in the array\n    # Then reverse the rank array to get the highest to lowest ranking\n    # Finally, convert the rank array to integer\n    # result = np.array(rankdata(a)).astype(int)\n    # # return the result\n    # return result\n    ### END SOLUTION ###\n\nf(example_a)\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.sum(np.abs(X), axis=1)\nresult = X / l1.reshape(-1, 1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.array([LA.norm(v,ord=2) for v in X])\nresult = np.array([v/x for v in X])\nprint(result)", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = np.divide(X, x[:, None])\nprint(result)", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = np.where(df['a'].str.contains(target), choices, np.nan)\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "result = np.sqrt(np.sum((a[:, None, :] - a[None, :, :])**2, axis=2))", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "result = np.zeros((dim, dim))\nfor i in range(dim):\n    for j in range(i + 1, dim):\n        result[i, j] = np.sqrt(np.sum((a[:, i] - a[:, j]) ** 2))\n        result[j, i] = result[i, j]\nresult = np.triu(result, k=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concaten", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "import numpy as np\n\ndef calculate_distance(a):\n    n = a.shape[0]\n    result = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            result[i, j] = np.sqrt(np.sum((a[i] - a[j]) ** 2))\n    return result\n\nresult = calculate_distance(a)\nprint(result)\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "# Remove all zero values\na = a[a!=0]\n\n# Remove adjacent duplicate values\na = np.concatenate([a[i] for i in range(len(a)) if i == 0 or a[i] != a[i-1]], a[i])\n\n# Remove all zero values after removing adjacent duplicate values\na = a[a!=0]\n\nresult = a\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# Find the indices of the non-zero values\nindices = np.where(a != 0)[0]\n\n# Find the indices of the zero values\nzero_indices = np.where(a == 0)[0]\n\n# Find the indices of the adjacent non-zero values\nadjacent_indices = np.where(np.diff(indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to the zero values\nzero_adjacent_indices = np.where(np.diff(zero_indices) == 1)[0]\n\n# Find the indices of the zero values adjacent to", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "lat_val = np.concatenate((lat, val), axis=1)\nlon_val = np.concatenate((lon, val), axis=1)\n\ndf = pd.DataFrame({'lat': lat_val[:, 0], 'lon': lon_val[:, 0], 'val': lat_val[:, 1]})\n\nprint(df)\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "lat_max = np.max(lat, axis=1)\nlon_max = np.max(lon, axis=1)\nval_max = np.max(val, axis=1)\n\nmax_values = np.maximum(lat_max, lon_max)\nmax_values = np.maximum(max_values, val_max)\n\ndf = pd.DataFrame({'lat': lat_max, 'lon': lon_max, 'val': val_max, 'maximum': max_values})\nprint(df)\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.array(shape) > size]\n    shape = shape[::-1]\n    shape = np.array(shape)\n    shape = shape[np.", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n</code!", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:,-1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = c in CNTS\nprint(result)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "print(c in CNTS) # True\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.matrix([[12.22151125, 0, 0, 0],[0,4.92815942, 0, 0],[0,0,2.06380839,0 ],[0,0,0,0.29766152]])\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a = np.where(np.triu(a, k=1, suppress_errors=True) == 0, 0, a)", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\n\n# Calculate the number of days between the start and end dates\ndays = (end_date - start_date).days\n\n# Create a linear space of dates\ndates = np.linspace(start_date, end_date, n)\n\n# Convert the dates to pandas DatetimeIndex\nresult = pd.DatetimeIndex(dates)\n\nprint(result)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No index found\")\nelse:\n    print(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))\nresult = indices[0]\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "# calculate the coefficients a, b and c\na = np.sum((y - np.sum(y)) * x) / np.sum(x ** 2)\nb = np.sum((y - np.sum(y)) * x) / np.sum(x)\nc = np.sum(y) - a * np.sum(x) ** 2 - b * np.sum(x)\nresult = np.array([a, b, c])\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "# calculate the coefficients\ncoeffs = np.polyfit(x, y, degree)\n# sort the coefficients in descending order\nresult = np.sort(coeffs)[::-1]\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = np.array([0, 1, 2, 3])\ntemp_df = pd.DataFrame(np.repeat([1, 2, 3, 4], 4).reshape(4, -1))\n\nresult = np.subtract(temp_df, temp_arr)\nprint(result)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na = scaler.fit_transform(a)\nresult = a", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\nprint(result)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "mask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr >= -10\narr[mask] = arr[mask] + 5\n\narr[~mask] = 30\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr = np.where(arr < n1, 0, np.where(arr >= n2, 30, arr + 5))", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "# compare the two arrays and find the number of different elements\nresult = np.nonzero(np.in1d(s1, s2))[0].shape[0]\nprint(result)\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.all(np.array(a) == np.array(a[0]))", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all(axis=1) for arr in a)\nprint(result)\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, 0), (0, shape[1]-a.shape[1])), 'constant')\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, 0), (0, shape[1]-a.shape[1])), 'constant')\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "# find the maximum shape\nmax_shape = np.max(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n# find the shape of the array\nshape_array = np.array(shape)\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3,3)\nprint(a)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=2)", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=2)", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = np.where(df['a'] > 1, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "# Create a mask that is True for all rows except the first and last row, and all columns except the first and last column\nmask = np.logical_not(np.logical_or(np.logical_not(np.all(im[1:-1, 1:-1] == 0, axis=1)), np.logical_not(np.all(im[1:-1, 1:-1] == 0, axis=0))))\n\n# Apply the mask to the image\nresult = np.where(mask, im, 0)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "# Find the indices of the non-zero elements\nindices = np.where(A != 0)\n# Find the minimum and maximum indices for each column\nmin_indices = np.min(indices, axis=1)\nmax_indices = np.max(indices, axis=1)\n# Slice the array to the bounding box of the non-zero elements\nresult = A[min_indices[0]:max_indices[0]+1, min_indices[1]:max_indices[1]+1]\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "def remove_peripheral_zeros(im):\n    row_count = np.sum(im, axis=1)\n    col_count = np.sum(im, axis=0)\n    row_count = np.where(row_count == 0, 1, row_count)\n    col_count = np.where(col_count == 0, 1, col_count)\n    return np.where(row_count == 1, np.where(col_count == 1, 0, im), im)\n\nresult = remove_peripheral_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "# Create a mask that is True for all rows except the first and last row, and all columns except the first and last column\nmask = np.all(im[1:-1, 1:-1] != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im != 0, axis=1)\n\n# Create a mask that is True for all", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 5))\nsns.lineplot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "for i in range(10):\n    y = np.random.randn(10) + i\n    plt.plot(x, y, label=f'line {i}')\n\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "for i in range(10):\n    y = np.random.randn(10) + i\n    plt.plot(x, y, label=f'line {i}')\n\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, '--', marker='d')\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 6))\nsns.lineplot(x, y, marker='d', linewidth=2)\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "highlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nplt.plot(x, color=highlight_color)\nplt.plot(highlight_range, color=highlight_color)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = x * 2 + 2\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1, 2])\ny = np.array([0, 2, 4])\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    style=\"Gender\",\n    height=5,\n    aspect=1,\n)\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "sns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "legend_title = 'xyz'\nlegend_font_size = 20\n\n# set the legend title and font size\nplt.legend(legend_title, fontsize=legend_font_size)\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, facecolor=\"none\")\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"k\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), x, \"r-\", lw=5, markersize=30, alpha=0.5)\nplt.plot(range(10), y, \"r-\", lw=5, markersize=30, alpha=0.5)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "x_rotated = np.rot90(x, 1, 1)\ny_rotated = np.rot90(y, 1, 1)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "x_rotated = np.rot90(x, 1, 1)\ny_rotated = np.rot90(y, 1, 1)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.linspace(0, 2 * np.pi, 11)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "xlabel = \"X\"\nplt.xlabel(xlabel)\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticks(g.get_xticks())\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# fit a very long title myTitle into multiple lines\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = np.sort(y)[::-1]\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.linspace(0, 1.5, 11)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "y_ticks = np.linspace(-1, 1, 11)\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "x_y_z = np.array([x, y, z]).T\n\n# plot x, then y then z\nplt.figure(figsize=(10, 5))\nplt.plot(x_y_z[0], x_y_z[1], label='x')\nplt.plot(x_y_z[0], x_y_z[2], label='y')\nplt.plot(x_y_z[1], x_y_z[2], label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "x = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.ticklabel_format(style='plain', axis='y')\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "# Create a new data frame with the x and y values\nnew_data = pd.DataFrame({'x': x, 'y': y})\n\n# Create a new line plot with a dashed line\nsns.lineplot(data=new_data, x='x', y='y', linestyle='-')\n\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[0].set_title('sin(x)')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y2')\naxs[1].set_title('cos(x)')\naxs[1].legend()\n\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\nax[0].plot(x, y1, label='sin(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y1')\nax[0].set_title('sin(x)')\nax[0].legend()\n\nax[1].plot(x, y2, label='cos(x)')\nax[1].set_xlabel('x')\nax[1].set_ylabel('y2')\nax[1].set_title('cos(x)')\nax[1].legend()\n\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4))\nplt.grid(axis='x', which='both')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.grid(True, which='y')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(min(y), max(y) + 1, 1))\nplt.xticks(np.arange(min(x), max(x) + 1, 1))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "# axes = axes.flatten()\n# for ax in axes:\n#     ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n#     ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n# plt.tight_layout()\n# plt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlabel(\"Column Labels\")\nax.set_ylabel(\"Row Labels\")\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.space(20)\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y, label='Y')\nplt.ylabel('Y')\nplt.xlabel('X')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "# Create a joint regression plot\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green')\n\n# Create a distribution plot\nsns.distplot(tips['total_bill'], kde=False, color='blue')\n\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "# Create a new dataframe with the regression line\nregression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "# Create a joint regression plot\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg')\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 5))\nfor celltype in df[\"celltype\"].unique():\n    data = df[df[\"celltype\"] == celltype]\n    plt.bar(data[\"s1\"], data[\"s2\"], label=celltype)\n\nplt.xlabel(\"s1\")\nplt.ylabel(\"s2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 5))\nfor celltype, s1, s2 in zip(df[\"celltype\"], df[\"s1\"], df[\"s2\"]):\n    plt.bar(celltype, s1, label=f\"s1 = {s1}\")\n    plt.bar(celltype, s2, label=f\"s2 = {s2}\")\n\nplt.xlabel(\"celltype\")\nplt.ylabel(\"s1 and s2\")\nplt.legend()\nplt.xticks(rotation=45)\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Y over X\")\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, aspect='auto')\nplt.xlabel(xlabels, rotation=90)\nplt.ylabel(ylabels, rotation=0)\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\n# copy the code of the above plot and edit it to have legend for all three cruves in the two subplots\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\n\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "# make scatter plot\nplt.scatter(a, b, c)\n\n# annotate each data point with correspond numbers in c\nfor i, j in enumerate(c):\n    plt.annotate(j, (a[i], b[i]))\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Line')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(labelcolor='black', title='Legend')\nplt.legend(title='Legend', title_fontsize=14, title_fontweight='bold')\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, bins=10, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, ax1 = plt.subplots(3, 1, figsize=(10, 10))\n\n# Make the first subplot.\nax1.plot(x, y, 'r')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\n\n# Make the second subplot.\nax2 = ax1.twin()\nax2.plot(x, y, 'g')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\n\n# Make the third subplot.\nax3 = ax1.twin()\nax3.plot(x, y, 'b')\nax3.set_xlabel('x')\nax3.set_ylabel('y')\n\n# Make the first subplot three times wider than the second subplot but they should have the same height.\nfig.set_size_inches(10, 10)\n\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Set the labels of the x and y axis\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Set the title of the chart\nplt.title('Histogram of x and y')\n\n# Set the legend of the chart\nplt.legend()\n\n# Show the chart\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 5))\nplt.hist([x, y], bins=10, alpha=0.5, color=['r', 'g'], label=['x', 'y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "x = np.linspace(0, 5, 100)\ny = a * x + b\n\n# draw the line\nplt.plot(x, y)\n\n# set the xlim and ylim to be between 0 and 5\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2, 1)\n\n# create two colormaps for x and y\ncmap_x = plt.get_cmap('viridis')\ncmap_y = plt.get_cmap('viridis')\n\n# create a colorbar for x and y\ncbar_x = plt.colorbar(plt.imshow(x, cmap=cmap_x), ax=ax[0])\ncbar_y = plt.colorbar(plt.imshow(y, cmap=cmap_y), ax=ax[1])\n\n# set the colorbar's position\ncbar_x.set_position([0.9, 0.1])\ncbar_y.set_position([0.9, 0.2])\n\n# set the colorbar's label\ncbar_x.set_label('x')\ncbar_y.set_label('y')\n\n# set the title of the plot\nplt.title('Colorbar for x and y')\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[0].plot(a, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].plot(x, z, label='Y and Z')\naxs[1].set_title('Y and Z')\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "# Create a DataFrame from points\ndf = pd.DataFrame(points, columns=['x', 'y'])\n\n# Create a log-scale y-axis\ny_log = np.log10(df['y'])\n\n# Create a line plot\nplt.plot(df['x'], y_log, '-')\n\n# Set the x-axis log scale\nplt.xscale('log')\n\n# Set the y-axis log scale\nplt.yscale('log')\n\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'r')\nplt.title('y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.loglog(x, y, basex=10, basey=10)\nplt.xlabel('x (log-log)')\nplt.ylabel('y (log-log)')\nplt.title('Log-log plot of y over x')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n\n# set the legend of the plot\nplt.legend()\n\n# set the title of the plot\nplt.title(\"Line plot of data\")\n\n# set the x-axis label\nplt.xlabel(\"Date\")\n\n# set the y-axis label\nplt.ylabel(\"Value\")\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "# Make a histogram of data\nplt.hist(data, bins=10, edgecolor='black')\n\n# Calculate the total sum of data\ntotal_sum = np.sum(data)\n\n# Calculate the percentage of each data point\npercentage = data / total_sum\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(percentage * 100, 1)\n\n# Set y tick labels as 10%, 20%, etc.\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as 10%, 20%, etc.\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100)", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, alpha=0.5, linestyle='-')\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\nfig.legend(loc='upper right')\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 10))\n\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\n\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n# Set the title of the plot\nplt.title(\"Regression Plots of Bill Length and Flipper Length\")\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "x_extra = np.arange(2.1, 7.6, 0.1)\nplt.xticks(x_extra)\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.xticks(rotation=-60)\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(x, alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, 'r')\naxs[0].set_title('First Subplot')\naxs[1].plot(x, y, 'g')\naxs[1].set_title('Second Subplot')\n\nfig.suptitle('Figure')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df[\"Type A\"], df[\"Type B\"], label=\"Type A\")\nplt.xlabel(\"Type A\")\nplt.ylabel(\"Type B\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='|')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none')\nplt.plot(x, y, '|', color='red')\nplt.plot(x, y, '|', color='red', hatch='-')\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*', hatch='v')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "xlim = [1, 5]\nylim = [1, 4]\n\nplt.figure(figsize=(10, 10))\nplt.imshow(data, extent=[xlim[0], xlim[1], ylim[0], ylim[1]], aspect='auto')\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.figure(figsize=(10, 5))\nplt.plot([3], [0], '--', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 5))\nplt.bar(labels, height, width=0.5, color='blue')\nplt.plot([0, 1], [0, 1], color='red')\nplt.plot([0, 1], [-1, -1], color='red')\nplt.plot([0, 1], [-1, 0], color='red')\nplt.plot([0, 1], [-1, 1], color='red')\nplt.plot([0, 1], [-1, 2], color='red')\nplt.plot([0, 1], [-1, 3], color='red')\nplt.plot([0, 1], [-1, 4], color='red')\nplt.plot([0, 1], [-1, 5], color='red')\nplt.plot([0, 1], [-1, 6], color='red')\nplt.plot([0, 1], [-1, 7], color='red')\nplt.plot([0, 1], [-1, 8], color='red')\nplt.plot([0, 1], [-1, 9], color='red')\nplt.plot([0, 1], [-1, 10], color='red')\nplt.plot([0, 1], [-1, 11], color='red')\nplt.plot([0, 1], [-1, 12], color='red')\nplt.plot([0, 1], [-1, 13], color='red')\nplt.plot([0, 1], [-1, 14], color='red')\nplt.plot([0, 1], [-1, 15], color='red')\nplt.plot([0, 1], [-1, 16], color='red')\nplt.plot([0, 1], [-1, 17], color='red')\nplt.plot([0, 1], [-1, 18], color='red')\nplt.plot([0, 1], [-1, 19], color='red')\nplt.plot([0, 1], [-1, 20], color='red')\nplt.plot([0, 1], [-1, 21], color='red')\nplt.plot([0, 1], [-1, 22], color='red')\nplt.plot([0, 1], [-1, 23], color='red')\nplt.plot([0, 1], [-1, 24], color='red')\nplt.plot([0, 1], [-1, 25], color='red')\nplt.plot([0, 1], [-1, 26], color='red')\nplt.plot([0, 1], [-1, 27], color='red')\nplt.plot([0, 1], [-1, 28], color='red')\nplt.plot([0, 1], [-1, 29], color='red')\nplt.plot([0, 1], [-1, 30], color='red')\nplt.plot([0, 1], [-1, 31], color='red')\nplt.plot([0, 1], [-1, 32], color='red')\nplt.plot([0, 1], [-1, 33], color='red')\nplt.plot([0, 1], [-1, 34], color='red')\nplt.plot([0, 1], [-1, 35], color='red')\nplt.plot([0, 1], [-1, 36], color='red')\nplt.plot([0, 1], [-1, 37], color='red')\nplt.plot([0, 1], [-1, 38], color='red')\nplt.plot([0, 1], [-1, 39], color='red')\nplt.plot([0, 1], [-1, 40], color='red')\nplt.plot([0, 1],", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedge_width=0.4)\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b-')\nplt.grid(True, which='both')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle='-', linewidth=0.5, color='gray')\nplt.grid(which='major', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, '--', alpha=0.5, edgecolor='none')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Create a figure and a set of axes\nfig, ax = plt.subplots()\n\n# Create a bar plot for the blue bars\nax.bar(np.arange(3), blue_bar, color='blue')\n\n# Create a bar plot for the orange bars\nax.bar(np.arange(3) + 0.5, orange_bar, color='orange')\n\n# Set the x-axis labels\nax.set_xlabel('Index')\n# Set the y-axis labels\nax.set_ylabel('Height')\n\n# Set the title of the plot\nax.set_title('Bar plot of blue and orange bars')\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[0].legend()\n\naxs[1].plot(x, z, label='z')\naxs[1].legend()\n\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.axis([0, 10, 0, 10])\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(data=df, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"species\", sharey=False, col_wrap=3, height=5, aspect=0.8, kind=\"bar\")\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(10, 10))\nplt.plot([0.5], [0.5], 'ro')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5,", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"upper right\", handlelength=0.3)\nplt.show()\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.plot(x, y, marker=\"*\", label=\"Line\")\nplt.plot(x, y, marker=\"*\", label=\"Line\")\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.imshow(data, aspect='auto')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(1)\nplt.plot(x, y)\nplt.title(\"Figure 1\")\nplt.bold(\"Figure\")\nplt.bold(\"1\")\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, fontsize=10)\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n\n# Create a scatter plot of x over y\nplt.scatter(x, y, s=100)\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, color='red', edgecolor='black')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax, x_val, y_val in zip(axs.flat, x, y):\n    ax.plot(x_val, y_val)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('y over x')\n\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('x')\nplt.ylabel('frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 5))\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.fill_between(x, y-error, y+error, alpha=0.2, color='blue')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Error over x')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 0], 'w')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True)\nax1.plot(x, y, label='Y')\nax1.set_title('Y')\nax2.plot(z, a, label='Z')\nax2.set_title('Z')\nax2.set_ylabel('A')\nax2.set_xlabel('Z')\nax2.set_ylim([0, 10])\nax2.set_xlim([0, 10])\nax2.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xticks(np.arange(0, 10, 1))\n        axs[i, j].set_yticks(np.arange(0, 10, 1))\n        axs[i, j].set_xlabel('x')\n        axs[i, j].set_ylabel('y')\n\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1, figsize=(8, 8))\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nplt.table(\n    df,\n    col_width=[1, 1, 1, 1],\n    col_names=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    row_names=range(1, 11),\n    cell_contents=df.values,\n    bbox=[0, 0, 1, 1],\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white\",\n    edgecolor=\"white\",\n    linewidth=1,\n    linewidths=1,\n    cell_linewidth=1,\n    cell_edgecolor=\"white", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks([])\nplt.yticks([])\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"diet\"] == \"No Fat\"]\n\n# Create a scatter plot with the x-axis as \"time\" and the y-axis as \"pulse\"\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=new_df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.title(\"Group: No Fat\")\nplt.show()\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "# Create a scatter plot of the data\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Plot of y over x')\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(borderless=True)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 6))\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=0.1, edgecolor=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis_params(axis_color=\"black\")\nstripplot.set_axis", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_names=[\"a\"], col_names=[\"b\"], margin_fill_color=\"white\")\n\n# Create a pointplot of \"c\" over \"a\"\nfacegrid.plot(sns.pointplot, x=\"a\", y=\"c\", hue=\"b\", palette=\"Set2\", legend=False)\n\n# Set xticks of intervals of 1\nfacegrid.ax.set_xticks(np.arange(1, 31, 2))\n\n# Set xtick labels with intervals of 2\nfacegrid.ax.set_xticklabels(np.arange(1, 31, 2))\n\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_zlim([0, 1])\nax.set_view_angle(30, 50)\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tick_params(axis='x', which='both', labelbottom=False, labeltop=False, labelleft=False, labelright=False, bottom=False, top=False, labelbottom=False)\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[1, 1], height_ratios=[1, 1])\n\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x[i, j])\n        ax.axis('off')\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nprint(x.numpy()) # prints 1\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nx = tf.cast(x, tf.int32)\nprint(x.numpy())\n", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    # Create a tensor of zeros with the same shape as the labels\n    result = tf.zeros_like(labels)\n\n    # Iterate over the labels and set the corresponding values to 1\n    for i in range(len(labels)):\n        result[i, labels[i]] = 1\n\n    return result\n\n# Call the function\nprint(f())\n### END SOLUTION ###\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.compat.v1.py_func(\n    func=lambda i: [i, i+1, i+2],\n    inp=[i],\n    Tout=[tf.int64]\n  )\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=my_map_func)\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\n\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return tf.concat([input, input + 1, input + 2], axis=0)\n\ndef map_func(input):\n    return tf.reshape(f(input), [-1])\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(map_func)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as lengths\nmask", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "lengths = tf.constant([4, 3, 5, 2])\nmask = tf.sequence_length_mask(lengths, 8)\nmask = tf.cast(mask, tf.float32)\nresult = tf.where(mask, tf.ones_like(lengths), tf.zeros_like(lengths))\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths),", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\n\nmask = tf.where(lengths_padded > 0, 1, 0)\n\nresult = tf.reshape(mask, (4, 8))\nprint(result)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([a, b], axis=1)\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    # Use tf.stack to create a 3D tensor with shape (a.shape[0], a.shape[0], b.shape[0])\n    # Then use tf.reshape to change the shape to (a.shape[0], a.shape[0]*b.shape[0])\n    # Finally use tf.gather_nd to get the elements of the 3D tensor\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(a, tf.stack([tf.range(a.shape[0]), tf.range(a.shape[0]*b.shape[0])], axis=1))\n    # result = tf.gather_nd(b, tf.stack([tf.range(a.shape[0]", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "a = tf.reshape(a, (50, 100, 512))\nresult = a", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "a = tf.reshape(a, (50, 100, 1, 512))\nresult = a", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "a = tf.reshape(a, (1, 50, 100, 1, 512))\nresult = a", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.math.prod(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.reciprocal(A)\nprint(result)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "# calculate the square of the difference between the two embeddings\ndiff_square = tf.square(a - b)\n\n# reduce the square by row\nresult = tf.reduce_sum(diff_square, axis=1)\n\n# calculate the square of the difference between the two embeddings\ndiff_square = tf.square(a - b)\n\n# reduce the square by row\nresult = tf.reduce_sum(diff_square, axis=1)\nprint(result)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "# calculate the square difference\ndiff = tf.square(a - b)\n# reduce the square difference by column\nresult = tf.reduce_sum(diff, axis=0)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    # calculate the square of the difference between A and B\n    square_diff = tf.square(A - B)\n    # reduce the square difference by row\n    result = tf.reduce_sum(square_diff, axis=1)\n    # return the result\n    return result\n\nprint(f(A=example_a,B=example_b))\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = x[y,z]\nresult = tf.gather(x, y, axis=1)\nresult = tf.gather(result, z, axis=0)\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = x[row, col]\nresult = tf.gather(x, col, axis=1)\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    return tf.gather(x, y, axis=1)\n\nresult = f(example_x, example_y, example_z)\nprint(result)\n### END SOLUTION ###\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "C = tf.math.multiply(A, B)\nresult = tf.reduce_sum(C, axis=2)\nprint(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "result = tf.einsum('ijk,ik->ijk', A, B)\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "x = tf.constant(x)\nresult = tf.strings.unicode_transpose(x)\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    # Convert the bytes to strings\n    result = tf.strings.unicode_transpose(x)\n    # Return the result\n    return result\n\n# Call the function\nprint(f(example_x))\n### END SOLUTION ###\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "x = tf.where(x > 0, x, tf.zeros_like(x))\nresult = tf.reduce_sum(x, axis=-1) / tf.reduce_sum(x, axis=-1, keepdims=True)\nresult = tf.where(result > 0, result, tf.zeros_like(result))\nprint(result)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "x = tf.where(x > 0, x, tf.zeros_like(x))\nresult = tf.reduce_mean(x, axis=-1)\nresult = tf.where(result > 0, result, tf.zeros_like(result))\nprint(result)\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": " ###\n    # get the sum of the non-zero elements in the second to last dimension\n    sum_non_zero = tf.reduce_sum(x, axis=-2, keepdims=True)\n    # divide the sum by the number of non-zero elements\n    result = tf.divide(x, sum_non_zero)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\n# Create two random matrices A and B\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Calculate the sum of the matrix multiplication\nresult = tf.reduce_sum(tf.matmul(A, B))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\n# Find the index of the maximum value in each row\nmax_index = tf.argmax(a, axis=1)\n\n# Convert the maximum index to the correct format\nresult = tf.reshape(max_index, (-1, 1))\n\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\n# Find the maximum value in each column\nmax_values = tf.argmax(a, axis=1)\n\n# Convert the maximum value to the index of the highest value in each column\nresult = tf.where(a == max_values[:, None], max_values, tf.constant(0))\n\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    # find the maximum value in each row\n    max_values = tf.argmax(a, axis=1)\n    # count the number of times the maximum value in each row is equal to the actual value\n    count = tf.reduce_sum(tf.cast(max_values == a, tf.int32))\n    # return the count\n    return count\n\nprint(f(example_a))\n### END SOLUTION ###\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1) + 1", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n\n# Save the model\nmodel.save('my_model')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "# generate 10 random integers from the uniform distribution\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "seed_x = 10\ntf.random.set_seed(seed_x)\n\n# generate 114 random integers from the uniform distribution\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=5, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": " ###\n    tf.random.set_seed(seed_x)\n    random_integers = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n    return random_integers\n\n# call the function\nprint(f())\n### END SOLUTION ###\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "import tensorflow as tf\n\n# get the version of tensorflow\nversion = tf.__version__\nprint(version)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "# Fit the data to a polynomial of order 2\nA, B = scipy.optimize.polyfit(x, y, 2)\nresult = np.array([A, B])\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the data using a polynomial of order 1\nA, B = scipy.optimize.polyfit(x, y, 1)\nresult = np.array([A, B])\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, A, B, C):\n    return A * np.exp(B * x) + C\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array([result[0], result[1], result[2]])\nprint(result)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + (3*a - b - c)**2 + sin(b) + cos(b) + 4)\n\ndef grad_f(x):\n    a, b, c = x\n    return [2*(a + b - c) - 2, 2*(3*a - b - c) - 2, sin(b) + cos(b) + 4]\n\ndef hess_f(x):\n    a, b, c = x\n    return [[-2, -2, -2], [-2, -2, -2], [-1, -1, -1]]\n\nresult = optimize.minimize(f, initial_guess, jac=grad_f, hess=hess_f)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.t.pdf(z_scores, 0, 1)", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores,", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = scipy.stats.zscore(p_values)\nprint(z_scores)", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)\nprint(result)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(stddev, mu)\nexpected_value = dist.mean()\nmedian = dist.median()\nprint(expected_value, median)", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    result = sA * sB\n    return result\n\nprint(f(example_sA, example_sB))\n<code>\n\nThis function should return the result of the matrix multiplication of the two input sparse matrices. The function f takes two sparse matrices as input and returns the result of the matrix multiplication. The result is a sparse matrix.\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid of points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid of V values\nV_grid = np.empty((x.shape[0], x.shape[1], x.shape[2]))\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        for k in range(x.shape[2]):\n            V_grid[i, j, k] = V[i]\n\n# Create a 3D grid of the request points\nrequest_grid = np.empty((request.shape[0], request.shape[1], request.shape[2]))\nfor i in range(request.shape[0]):\n    for j in range(request.shape[1]):\n        for k in range(request.shape[2]):\n            request_grid[i, j, k] = request[i]\n\n# Create a 3D grid of the interpolated values\ninterpolated_values = np.empty((x.shape[0], x.shape[1], x.shape[2]))\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        for k in range(x.shape[2]):\n            interpolated_values[i, j, k] = scipy.interpolate.LinearNDInterpolator(\n                (x[i, j, k], y[i, j, k], z[i, j, k]), V_grid[i, j, k])(request_grid[i, j, k])\n\n# Print the result\nprint(interpolated_values)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid of points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid of V values\nV_grid = np.empty((x.shape[0], x.shape[1], x.shape[2]))\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        for k in range(x.shape[2]):\n            V_grid[i, j, k] = V[i]\n\n# Create a 3D grid of request points\nrequest_grid = np.empty((request.shape[0], request.shape[1], request.shape[2]))\nfor i in range(request.shape[0]):\n    for j in range(request.shape[1]):\n        for k in range(request.shape[2]):\n            request_grid[i, j, k] = request[i]\n\n# Create a 3D grid of interpolated values\ninterpolated_values = np.empty((x.shape[0], x.shape[1], x.shape[2]))\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        for k in range(x.shape[2]):\n            interpolated_values[i, j, k] = scipy.interpolate.LinearNDInterpolator(request_grid, V_grid)(x[i, j, k], y[i, j, k], z[i, j, k])\n\n# Print the interpolated values\nprint(interpolated_values)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, reshape=False)\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\nxrot, yrot = np.round(xrot), np.round(yrot)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = np.diagonal(M.toarray())", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # Use scipy's kstest function to perform the K-S test\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n    # The result should be a K-S test result\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "# calculate the distance between points\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# assign points2 to points1\nresult = points2[min_indices]\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "# create a matrix of points with points1 as rows and points2 as columns\npoints = np.vstack([points1, points2])\n# calculate the Manhattan distance between each point in points1 and points2\ndistances = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\n# find the minimum distance for each point in points1\nmin_distances = np.min(distances, axis=1)\n# find the index of the minimum distance for each point in points1\nmin_indices = np.argmin(distances, axis=1)\n# assign points1 to points2 using the minimum distances and indices\nresult = points2[min_indices]\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b = b.toarray()\nb[0, 0] = 0\nb = sparse.csr_matrix(b)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "result = np.where(img > threshold)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "region_count = np.sum(img < threshold)\nprint(region_count)\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use numpy's where function to find the values above the threshold\n    # Then use numpy's unique function to find the unique values and their counts\n    # Finally, return the unique values and their counts\n    ### END SOLUTION ###\n    return np.unique(np.where(img > threshold), return_counts=True)\n\nprint(f(example_img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "# Find the regions of cells which value exceeds a given threshold\nregion_values = img[img > threshold]\n\n# Find the center of mass of such regions\ncenter_of_mass = np.mean(region_values, axis=(1, 2))\n\n# Find the distance between the center of mass of such regions and the top left corner\ndistance = np.sqrt((0 - center_of_mass[0])**2 + (0 - center_of_mass[1])**2)\n\n# Output the distances as a list\nresult = distance.tolist()\nprint(result)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = lil_matrix(M)\nM = M + M.T\nM = M.tocsr()\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    # make the matrix symmetric\n    sA = lil_matrix(sA)\n    # make the matrix symmetric\n    sA = sA.transpose()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    #", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "# erode the square by 1 pixel\neroded = scipy.ndimage.erosion(square, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# dilate the eroded square by 1 pixel\ndilated = scipy.ndimage.dilation(eroded, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# remove the isolated single cells\nsquare[dilated == 0] = 0\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "# erode the square array\neroded = scipy.ndimage.erosion(square, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# dilate the eroded array\ndilated = scipy.ndimage.dilation(eroded, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# find the non-zero elements in the dilated array\nnonzero = np.where(dilated != 0)\n# set the non-zero elements to 0\ndilated[nonzero] = 0\n# print the dilated array\nprint(dilated)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_val = np.max(col.toarray())\nmin_val = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Get the median and mode of the column\nmedian = np.median(col.toarray())\nmode = np.mode(col.toarray())\n\nprint(median)\nprint(mode)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15):\n    return a0 + a1 * np.cos(a2 * np.pi / tau * x) + a3 * np.cos(a4 * np.pi / tau * x) + a5 * np.cos(a6 * np.pi / tau * x) + a7 * np.cos(a8 * np.pi / tau * x) + a9 * np.cos(a10 * np.pi / tau * x) + a11 * np.cos(a12 * np.pi / tau * x) + a13 * np.cos(a14 * np.pi / tau * x) + a15 * np.cos(a15 * np.pi / tau * x)\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\narray_1d = example_array.flatten()\n# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.cdist(array_1d, array_1d, metric='euclidean')\n# Convert distances tometres\ndistances_metres = distances * np.mean(example_array)\n# Convert distances to a 2D array\nresult = np.array(np.where(distances_metres == np.inf, np.nan, distances_metres)).astype(int)\n# Print result\nprint(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Calculate pairwise Manhattan distances between all regions\ndistances = scipy.spatial.distance.cdist(example_array, example_array, metric='cityblock')\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distance = np.min(distances, axis=1)\n# Print the result\nprint(min_distance)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate pairwise Euclidean distances between all regions\n    # to obtain the minimum distance separating the nearest edges of each raster patch\n    # Convert distances from cells tometres\n    # result = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n    #                   [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n    #                   [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n    # # Convert distances from cells tometres\n    # result = np.multiply(result, 1000)\n    # # Return the result\n    # return result\n    ### END SOLUTION ###\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(x.shape[1]):\n    tck = interpolate.splrep(x[:, i], y[:, i], s=4)\n    result[i] = interpolate.splev(x_val, tck)\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array([38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0])\nx2 = np.array([39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8])\nx3 = np.array([34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0])\nx4 = np.array([34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8])\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\nprint(statistic)\nprint(critical_values)\nprint(significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\n\nresult = ss.anderson_ksamp(x1, x2)\nprint(result)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(df['A']) # keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nresult = is_csr_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n\nresult = is_lil_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue\nprint(p_value)", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n    ### END SOLUTION ###\n\nprint(f(example_pre_course_scores, example_during_course_scores))\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "kurtosis_result = np.mean(a**3) / np.mean(a**3)**2 - 3\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=True)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "def interpolated(s, t):\n    z = (s + t)*np.exp(-6.0 * (s * s + t * t))\n    return z\n\nresult = interpolated(s, t)\nprint(result)\n", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(s, t) for t in example_t] for s in example_s])\n    # Create a 2D array of the values of the function f(s, t)\n    z_values = np.array([[f(", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are in the voronoi regions and not in the voronoi regions\nextraPointsInVoronoiNotInVoronoi = [point for point in extraPoints if point in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi regions and not in the voronoi regions\nextraPointsNotInVoronoiNotInVoronoi = [point for point in extraPoints if point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi regions and in the voronoi regions\nextraPointsNotInVoronoiInVoronoi = [point for point in extraPoints if point not in vor.regions and point in vor.vertices]\n# Create a list of points that are in the voronoi regions and in the voronoi regions\nextraPointsInVoronoiInVoronoi = [point for point in extraPoints if point in vor.regions and point in vor.vertices]\n# Create a list of points that are not in the voronoi regions and in the voronoi regions\nextraPointsNotInVoronoiInVoronoi = [point for point in extraPoints if point not in vor.regions and point in vor.vertices]\n# Create a list of points that are not in the voronoi regions and not in the voronoi regions\nextraPointsNotInVoronoiNotInVoronoi = [point for point in extraPoints if point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi regions and in the voronoi regions\nextraPointsNotInVoronoiInVoronoi = [point for point in extraPoints if point not in vor.regions and point in vor.vertices]\n# Create a list of points that are not in the voronoi regions and not in the voronoi regions\nextraPointsNotInVoronoiNotInVoronoi = [point for point in extraPoints if point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi regions and in the voronoi regions\nextraPointsNotInVoronoiInVoronoi = [point for point in extraPoints if point not in vor.regions and point in vor.vertices]\n# Create a list of points that are not in the voronoi regions and not in the voronoi regions\nextraPointsNotInVoronoiNotInVoronoi = [point for point in extraPoints if point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi regions and in the voronoi regions\nextraPointsNotInVoronoiInVoronoi = [point for point in extraPoints if point not in vor.regions and point in vor.vertices]\n# Create a list of points that are not in the voronoi regions and not in the voronoi regions\nextraPointsNotInVoronoiNotInVoronoi = [point for point in extraPoints if point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi regions and in the voronoi regions\nextraPointsNotInVoronoiInVoronoi = [point for point in extraPoints if point not in vor.regions and point in vor.vertices]\n# Create a list of points that are not in the voronoi regions and not in the voronoi regions\nextraPointsNotInVoronoiNotInVoronoi = [point for point in extraPoints if point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voron", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "# Create a list of points that are not in the voronoi cell\nextraPointsNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions:\n        extraPointsNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell\nextraPointsInCell = []\nfor point in extraPoints:\n    if point in vor.regions:\n        extraPointsInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellNotInCell = []\nfor point in extraPoints:\n    if point in vor.regions and point not in vor.vertices:\n        extraPointsInCellNotInCell.append(point)\n\n# Create a list of points that are not in the voronoi cell and not in the voronoi cell\nextraPointsNotInCellNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions and point not in vor.vertices:\n        extraPointsNotInCellNotInCell.append(point)\n\n# Create a list of points that are not in the voronoi cell and not in the voronoi cell\nextraPointsNotInCellNotInCellNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices:\n        extraPointsNotInCellNotInCellNotInCell.append(point)\n\n# Create a list of points that are not in the voronoi cell and not in the voronoi cell and not in the voronoi cell\nextraPointsNotInCellNotInCellNotInCellNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices:\n        extraPointsNotInCellNotInCellNotInCellNotInCell.append(point)\n\n# Create a list of points that are not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell\nextraPointsNotInCellNotInCellNotInCellNotInCellNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "# Find the maximum size of the vectors\nmax_size = max([len(v) for v in vectors])\n\n# Create a sparse matrix with zeros\nresult = sparse.csc_matrix((max_size, max_size), dtype=np.float64)\n\n# Fill the sparse matrix with the vectors\nfor i, v in enumerate(vectors):\n    result[i, :len(v)] = v\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=(3, 3), origin=(0, 0))\nprint(b)\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = np.array([M[row[i], column[i]] for i in range(len(row))])", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, :, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u,o2,x)\nprint(prob)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(x = 2.5, u = 1, o2 = 3))\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a 2D array with random values\nA = np.random.rand(N, N)\n\n# Compute the DCT matrix\ndct_matrix = sf.dctn(A, norm='ortho')\n\n# Print the result\nprint(dct_matrix)\n", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "result = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result)\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "result = scipy.stats.binom.pmf(np.arange(N+1), N, p)", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x))\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=1)\n\n# create a new dataframe with zscore and data\nresult = pd.DataFrame(data=np.c_[df.values, zscore], index=df.index, columns=np.append(df.columns, 'zscore'))\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=0)\n\n# round zscore to 3 decimal places\nzscore = np.round(zscore, 3)\n\n# create a new dataframe with zscore and data\nresult = pd.DataFrame(np.c_[df.values, zscore], columns=np.concatenate([df.columns, ['zscore']]))\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y = np.arange(6)\nx = np.arange(6)\nmid = np.array([[0, 0], [3, 3], [3, 3]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\nmid = np.array([[5., 4., 3., 3., 4., 5.],\n               [4., 3., 2., 2., 3., 4.],\n               [3., 2., 1., 1., 2., 3.],\n               [3., 2., 1., 1., 2., 3.],\n               [4., 3., 2., 2., 3., 4.],\n               [5., 4., 3., 3., 4., 5.]])\n\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.mgrid[0:shape[0], 0:shape[1]]\n    center = np.array([shape[0] / 2, shape[1] / 2])\n    return distance.cdist(np.dstack((y, x)), center)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\n\ndef minimize_x(a, x0):\n    out = scipy.optimize.minimize(residual, x0, args=(a, y))\n    return out.x\n\nout = minimize_x(a, x0)\nprint(out)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return (y - func(x)) ** 2\n\nout = scipy.optimize.minimize(residual, x0, bounds=[x_lower_bounds, x_lower_bounds + x_true])\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t-np.sin(t)) if 0 < t < 2*np.pi else 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])\nprint(result)", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.vstack([sa, sb])\nprint(result.toarray())", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\ndef integrand(x, c):\n    return 2*c*x\n\nc = 5\nlow = 0\nhigh = 1\n\nresult, error = scipy.integrate.quad(lambda x: integrand(x, c), low, high)\nprint(result)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    def eqn(x):\n        return 2*x*c\n    result, error = scipy.integrate.quad(eqn, low, high)\n    return result\n\n# call the function with different values of c\nc = [5, 10, 15]\nfor n in range(len(c)):\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=c[n]))\n    # print the result\n    print(f(c=c[n]))\n    # print the error\n    print(f(c=", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.to_dense()\nV += x", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "x = x.reshape(1, 1)\nV = V + x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "x = x.reshape(1, 1)\ny = y.reshape(1, 1)\nV += x\nV += y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor Col in xrange(sa.shape[1]):\n    # extract column\n    Column = sa[:,Col].data\n    # calculate length\n    Len = math.sqrt(np.sum(Column**2))\n    # normalize column\n    Column = Column / Len\n    # update original column\n    sa[:,Col] = Column\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].toarray()\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    column /= length\n    # update the original column of the matrix\n    sa[:, col] = column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = np.where(a > 0, 1, 0)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = np.where(a == 0, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest centroid for each cluster\nresult = np.argmin(distances, axis=1)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest centroid for each data point\nclosest_centroid_index = np.argmin(distances, axis=1)\n# extract the closest centroid\nresult = centroids[closest_centroid_index]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the k-th closest element to each centroid\nclosest_elements = np.argpartition(distances, k, axis=0)[:k]\n# find the index of the k-th closest element in original data for each cluster\nresult = np.argwhere(closest_elements == np.arange(data.shape[0]))\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result = fsolve(lambda x: eqn(x, a, b), x0=x)\n    print(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result = fsolve(lambda x, a: eqn(x, a, x), x0=x, args=(a,))\n    print(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Define the function to calculate the K-S test\ndef kstest(x, y, a, m, d):\n    return stats.kstest(y, bekkers_func(x, a, m, d))\n\n# Use scipy.optimize.curve_fit to find the parameters of the function\nparams, params_cov = sp.optimize.curve_fit(bekkers_func, sample_data, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Use scipy.stats.kstest to calculate the K-S test\nresult = kstest(sample_data, sample_data, params[0], params[1], params[2])\nprint(result)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Estimate the parameters of the function\nestimated_a, estimated_m, estimated_d = sp.optimize.curve_fit(bekkers, range_start, range_end, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nkstest_result = stats.kstest(sample_data, bekkers(range_start, estimated_a, estimated_m, estimated_d))\n\n# Check if the K-S test result can reject the null hypothesis\nresult = kstest_result.pvalue < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def rolling_integral(x):\n    return integrate.trapz(x, x)\n\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral)\nprint(integral_df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "x = np.array([(2,2), (1,2), (2,3), (3,2), (2,1)])\ny = np.array([5,7,8,10,3])\neval = np.array([(2.7, 2.3)])\n\n# Create a grid of points\nx_grid = np.linspace(x.min(), x.max(), 100)\ny_grid = np.linspace(y.min(), y.max(), 100)\n\n# Create a grid of points for the evaluation\nx_eval = np.linspace(x.min(), x.max(), 100)\ny_eval = np.linspace(y.min(), y.max(), 100)\n\n# Use griddata to find the value at the evaluation points\nresult = scipy.interpolate.griddata(x, y, eval, x_grid, y_grid, method='linear')\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "weights = sciopt.maximum_likelihood_solver(a.values, np.ones(a.shape[0]))\nweights = weights.x\nweights = weights / np.sum(weights)\nweights = weights.tolist()\nweights = [i / np.sum(a.values) for i in weights]\nweights = np.array(weights)\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights[0]\nweights = weights.tolist()\nweights = weights", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\nprint(result)\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "# find the absolute values of the array\nabs_arr = np.abs(arr)\n# find the relative extrema\nrelative_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))\n# find the original indices of the relative extrema\noriginal_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_indices_abs = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)))[0]\n# find the original indices of the relative extrema\noriginal_indices_rel = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_indices_abs_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)))[0]\n# find the original indices of the relative extrema\noriginal_indices_rel_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_indices_abs_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)))[0]\n# find the original indices of the relative extrema\noriginal_indices_rel_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_indices_abs_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)))[0]\n# find the original indices of the relative extrema\noriginal_indices_rel_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_indices_abs_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)))[0]\n# find the original indices of the relative extrema\noriginal_indices_rel_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_indices_abs_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)))[0]\n# find the original indices of the relative extrema\noriginal_indices_rel_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_indices_abs_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)))[0]\n# find the original indices of the relative extrema\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "# find the absolute indices of the elements\nabs_indices = np.where(arr == np.max(arr, axis=1))\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "num_cols = df.select_dtypes(include=[np.number]).columns\noutlier_cols = [col for col in num_cols if np.abs(stats.zscore(df[col])) > 3]\ndf = df[outlier_cols]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data.data, data.target],\n                     columns=data.feature_names + ['target'])\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + data['target_names'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data.data, data.target],\n                     columns=data.feature_names + ['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    # Convert the data from a Bunch object to a DataFrame\n    data_df = pd.DataFrame(data=np.c_[data.data, data.target],\n                           columns=data.feature_names + ['target'])\n    return data_df\n    ### END SOLUTION ###\n\n# Call the function\nsolve(data)\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a list of unique elements in the column\nunique_elements = np.unique(df['Col3'].values)\n\n# Create a new dataframe with one-hot-encoded values\nout_df = pd.DataFrame(np.zeros((len(unique_elements), len(df))), columns=unique_elements, index=df.index)\n\n# Iterate over each row in the dataframe\nfor i, row in df.iterrows():\n    # Iterate over each element in the list\n    for element in row['Col3']:\n        # Get the index of the element in the unique elements list\n        index = np.where(unique_elements == element)[0][0]\n        # Set the value of the element in the one-hot-encoded dataframe\n        out_df.loc[i, element] = 1\n\n# Print the one-hot-encoded dataframe\nprint(out_df)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# Create a list of unique names\nnames = np.unique(np.concatenate(df['Col3'].values))\n\n# Create a dataframe with zeros\nout_df = pd.DataFrame(0, index=names, columns=names)\n\n# Iterate over each row in the dataframe\nfor index, row in df.iterrows():\n    # Iterate over each name in the row\n    for name in row['Col3']:\n        # Set the value to 1 for the name\n        out_df.loc[name, name] = 1\n\n# Print the output dataframe\nprint(out_df)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Create a list of unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a dataframe with one-hot-encoded values for each unique element\none_hot_encoded_values = np.zeros((len(unique_elements), len(unique_elements)))\nfor i, element in enumerate(unique_elements):\n    one_hot_encoded_values[i, np.where(np.array(df.iloc[:, -1]) == element)] = 1\n\n# Create a new dataframe with one-hot-encoded values\ndf_out = pd.DataFrame(one_hot_encoded_values, columns=unique_elements, index=unique_elements)\n\n# Print the new dataframe\nprint(df_out)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over the rows of the original dataframe\nfor i in range(df.shape[0]):\n    # Iterate over the unique elements\n    for j in range(unique_elements.shape[0]):\n        # If the element exists in the last column of the current row, set the value to 1\n        if unique_elements[j] in df.iloc[i, -1]:\n            new_df.iloc[i, j] = 1\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over the rows of the original dataframe\nfor i in range(df.shape[0]):\n    # Iterate over the unique elements\n    for j in range(unique_elements.shape[0]):\n        # If the element exists in the last column of the current row, set the value to 1\n        if unique_elements[j] in df.iloc[i, -1]:\n            new_df.iloc[i, j] = 1\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\nproba = np.array([1 / (1 + np.exp(-x)) for x in svmmodel.decision_function(x_test)])\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\nproba = np.max(proba, axis=1)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "# Convert the transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\nnew_df = pd.concat([df_origin, transform_output], axis=1)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "# Convert the transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a DataFrame from the transform_output\ntransform_output_df = pd.DataFrame(transform_output.toarray())\n\n# Merge the original DataFrame with the transform_output_df\nmerged_df = pd.concat([df_origin, transform_output_df], axis=1)\n\n# Print the merged DataFrame\nprint(merged_df)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # transform the transform_output back to a dataframe\n    transform_output = pd.DataFrame(transform_output.toarray(), columns=transform_output.columns)\n    # merge the original dataframe and the transform_output\n    result = pd.concat([df, transform_output], axis=1)\n    return result\n    ### END SOLUTION ###\n\n# call the function\nsolve(df_origin, transform_output)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the new pipeline\nprint(clf)", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the new pipeline\nprint(clf)", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline\nprint(clf)", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a new step\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.append(new_step)\n\n# Print the new pipeline\nprint(clf)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\n# Insert a new step\nnew_step = ('new_step', SVC())\nclf.steps.append(new_step)\n\n# Print the new pipeline\nprint(clf)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\n# Insert ('t1919810', PCA()) right before 'svdm'\nclf.steps.insert(1, ('t1919810', PCA()))\n\n# Print the new pipeline\nprint(clf)\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"early_stopping_rounds\": [10, 20, 30, 40],\n    \"learning_rate\": [0.01, 0.1, 0.2, 0.3],\n    \"max_depth\": [2, 3, 4, 5],\n    \"n_estimators\": [100, 200, 300, 400]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n\n# Fit the grid search\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the best iteration\nbest_iteration = gridsearch.best_iteration_\n\n# Print the best parameters\nprint(\"Best parameters: \", best_params)\n\n# Print the best score\nprint(\"Best score: \", best_score)\n\n# Print the best iteration\nprint(\"Best iteration: \", best_iteration)\n\n# Print the best model\nprint(\"Best model: \", gridsearch.best_estimator_)\n\n# Print the best model score\nprint(\"Best model score: \", gridsearch.best_estimator_.score(testX, testY))\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"learning_rate\" : [0.01, 0.1, 0.001],\n    \"max_depth\" : [3, 4, 5],\n    \"n_estimators\" : [100, 200, 300],\n    \"objective\" : [\"reg:squarederror\"],\n    \"n_jobs\" : [1, 2, 3],\n    \"iid\" : [True, False],\n    \"early_stopping_rounds\" : [10, 20, 30],\n    \"eval_metric\" : [\"mae\"],\n    \"eval_set\" : [[testX, testY]]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid)\n\n# Fit the grid search\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the score\nscore = gridsearch.best_score_\n\n# Get the prediction\nprediction = gridsearch.predict(testX)\n\n# Print the score and prediction\nprint(\"Score: \", score)\nprint(\"Prediction: \", prediction)\nprint(\"Best Params: \", best_params)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\nproba = np.mean(proba, axis=0)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\nproba = np.mean(proba, axis=0)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_pred = scaler.inverse_transform(scaled[:, 0])\n    # check score\n    score = np.mean(scaled[:, 0] - t_pred)\n    return score # return score\n\nscore = solve(data, scaler, scaled)\nprint(score) # print score #! END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = np.mean(scores)\nprint(f'Name model: {model_name}, Mean score: {mean_score}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\n# Define the GridSearchCV object\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV object to the training data\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters from the GridSearchCV object\nbest_params = grid_search.best_params_\n\n# Create a new DecisionTreeClassifier with the best parameters\ndt_best = DecisionTreeClassifier(max_depth=best_params['base_estimator__max_depth'], max_samples=best_params['max_samples'])\n\n# Create a new BaggingClassifier with the best parameters\nbc_best = BaggingClassifier(dt_best, n_estimators=20, max_samples=best_params['max_samples'], max_features=best_params['max_features'])\n\n# Fit the best BaggingClassifier to the training data\nbc_best.fit(X_train, y_train)\n\n# Get the probabilities of the best BaggingClassifier\nproba = bc_best.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef solve():\n    tfidf = TfidfVectorizer(preprocessor=preprocess)\n    tfidf.fit([\"This is a test\"])\n    print(tfidf.transform([\"This is a test\"]))\n\nsolve()\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# example variable\ntext = \"This is a test.\"\n\n# apply preprocessor\ntext_transformed = tfidf.transform([text])\n\nprint(text_transformed.toarray())\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Apply preprocessing.scale to the data\nscaled_data = preprocessing.scale(data)\n\n# Create a new DataFrame from the scaled data\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Print the scaled DataFrame\nprint(scaled_data_df)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a new DataFrame with the scaled values\nscaled_data = preprocessing.scale(data)\nscaled_data = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n\n# Save the scaled data to a new DataFrame\ndf_out = scaled_data\n\nprint(df_out)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "# get feature names\nfeature_names = X.columns\n\n# get feature importances\nimportances = clf.feature_importances_\n\n# create a dataframe with feature names and importances\nfeature_importances_df = pd.DataFrame({'feature_names': feature_names, 'importances': importances})\n\n# sort feature importances by importance\nfeature_importances_df = feature_importances_df.sort_values('importances', ascending=False)\n\n# get the names of the features\ncolumn_names = feature_importances_df['feature_names'].tolist()\n\nprint(column_names)\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]\n", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    # get the 50 samples closest to the p^th center\n    # you can use numpy's sort function to sort the centers by their distance to the p^th center\n    # then select the first 50 samples\n    # print(centers)\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.sort(np.linalg.norm(centers - p, axis=1)))\n    # print(np.", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = np.concatenate((X_train, X_train[0]), axis=1)\n\n# remove categorical variable\nX_train = X_train.drop([0], axis=1)\n\n# create GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\nfrom sklearn.kernel import GaussianKernel\n\n# create a Gaussian kernel\ngaussian_kernel = GaussianKernel(X, X)\n\n# create a SVM with Gaussian kernel\nsvm_model = svm.SVM(kernel=gaussian_kernel)\n\n# fit the model\nsvm_model.fit(X, y)\n\n# predict X\npredict = svm_model.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarities_of_queries(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    document_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.dot(query_matrix, document_matrix.T) / (np.linalg.norm(document_matrix, axis=1)[:, None] * np.linalg.norm(query_matrix, axis=1))\n    return cosine_similarities_of_queries", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarities_of_queries(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    document_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.dot(query_matrix, document_matrix.T) / (np.linalg.norm(document_matrix, axis=1)[:, None] * np.linalg.norm(query_matrix, axis=1))\n    return cosine_similarities_of_queries", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_matrix = tfidf.transform(documents)\n    query_matrix = tfidf.transform(queries)\n    cosine_similarities_of_queries = np.dot(query_matrix, tfidf_matrix.T)\n    return cosine_similarities_of_queries\n    ### END SOLUTION ###\n\n# Test the function\nsolve(queries, documents)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the feature names to a 2D array\nfeature_names = np.array(features[0]).reshape(-1, 1)\n\n# Create a DataFrame from the new features\nnew_features_df = pd.DataFrame(new_features, columns=feature_names)\n\n# Convert the DataFrame to a numpy array\nnew_features_array = new_features_df.to_numpy()\n\n# Convert the numpy array to a 2D array\nnew_features_array = np.array(new_features_array).reshape(-1, 1)\n\n# Print the new features array\nprint(new_features_array)\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "new_f = np.array([[1 if feature in sample else 0 for feature in f[i]] for i in range(len(f))])\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "# Convert the features to a 2D array\nnew_features = np.array([[0, 0, 0, 1, 1, 1], [1, 0, 1, 0, 0, 0], [0, 0, 1, 1, 1, 1]])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate((new_features, np.ones((new_features.shape[0], 1))), axis=1)\n\n# Convert the 2D array to a 2D array", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array\n    features_array = np.array(features_array).reshape(-1, 1)\n    # Convert the 2D array to a 1D array\n    features_array = features_array.flatten()\n    # Convert the 1D array to a 2D array", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "# Convert the list of features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n#", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the data matrix\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2)\n\n# Fit the clustering model to the data\nclustering.fit(data_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage_method='average')\n\n# Fit the clustering model\nclustering.fit(dist_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the similarity matrix\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2)\n\n# Fit the clustering model to the similarity matrix\nclustering.fit(simM)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.linkage(dist_matrix, method='average')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.linkage(dist_matrix, method='average')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a matrix with the similarity between all fruits\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Calculate the linkage matrix\nlinkage_matrix = linkage(simM, 'ward')\n\n# Get the cluster labels\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maximum')\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformer.transform(data)", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformer.transform(data)\nbox_cox_data = transformer.transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\n\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\n\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformer.transform(data)", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(analyzer='word')\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\nprint(transformed_text)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n\n# Define x and y\nx_train_all = x_train\nx_test_all = x_test\ny_train_all = y_train\ny_test_all = y_test\n\n# Split each of those sets so that we can define x and y\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x_test_all, y_train_all, y_test_all = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=42)\nx_train_all, x", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features and the target\nx = data.drop(columns=['target'])\ny = data['target']\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=[-1]), dataset[-1], test_size=0.3, random_state=42)\n\n# Define x and y\nx_train_all = x_train.values\ny_train_all = y_train.values\nx_test_all = x_test.values\ny_test_all = y_test.values\n\n# Define x and y\nx_train_all = x_train_all[:, :-1]\ny_train_all = y_train_all[:, -1]\nx_test_all = x_test_all[:, :-1]\ny_test_all = y_test_all[:, -1]\n\n# Define x and y\nx_train = x_train_all[:, :-1]\ny_train = y_train_all[:, -1]\nx_test = x_test_all[:, :-1]\ny_test = y_test_all[:, -1]\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": " ###\n    # split the data into training and testing sets\n    train_data, test_data = np.split(data, [int(data.shape[0]*0.8)])\n    # define x and y\n    x_train = train_data[:, :-1]\n    y_train = train_data[:, -1]\n    x_test = test_data[:, :-1]\n    y_test = test_data[:, -1]\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION ###\n\n# call the function\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)!", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "mse = df['mse'].values\nlabels = KMeans(n_clusters=2, n_init=10).fit(mse.reshape(-1, 1)).predict(mse.reshape(-1, 1))\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# Reshape the data to 2D array\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans object with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit the KMeans object to the data\nkmeans.fit(X)\n\n# Get the labels of the data\nlabels = kmeans.predict(X)\n\n# Get the centroids of the clusters\ncentroids = kmeans.cluster_centers_\n\n# Print the labels and centroids\nprint(labels)\nprint(centroids)\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "# Create a LinearSVC object\nsvc = LinearSVC(penalty='l1', loss='l1', dual=False)\n# Fit the model\nsvc.fit(X, y)\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(np.array(svc.coef_).flatten() != 0)]\nprint(selected_feature_names)", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "# Create a LinearSVC object\nsvc = LinearSVC(penalty='l1', loss='l2', max_iter=10000)\n# Fit the model\nsvc.fit(X, y)\n# Get the selected features\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\nprint(selected_feature_names)\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    # Use LinearSVC with penalty='l1'\n    svc = LinearSVC(penalty='l1', loss='l1', max_iter=10000, random_state=42)\n    # Fit the model\n    svc.fit(X, y)\n    # Get the feature names\n    feature_names = np.asarray(vectorizer.get_feature_names())[svc.get_support()]\n    return feature_names\n    ### END SOLUTION ###\n\n# Test the function\nsolve(corpus, y, vectorizer, X)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary_)\nprint(vectorizer.get_feature_names())\nprint(vectorizer.vocabulary", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nprint(np.transpose(X.toarray()))\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nprint(np.transpose(X.toarray()))\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "slopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\nslopes = slopes.tolist()\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "slopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\nslopes = slopes.tolist()\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ntransformed_df = df['Sex'].apply(le.fit_transform)\n\n# Print the transformed data\nprint(transformed_df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column into a numerical array\ntransformed_df = le.transform(df['Sex'])\n\n# Convert the transformed array to a DataFrame\ntransformed_df = pd.DataFrame(transformed_df, columns=['Sex'])\n\n# Print the transformed DataFrame\nprint(transformed_df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ### END SOLUTION ###\n    return df\n\ndef load_data():\n    df = pd.DataFrame({\n        'Sex': ['male', 'female', 'male', 'female'],\n        'Age': [20, 25, 30, 35],\n        'PassengerId': [1, 2, 3, 4],\n        'Survived': [0, 1, 0, 1]\n    })\n    return df\n\ntransformed_df = Transform(load_data())\nprint(transformed_df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    a_transformed = scaler.transform(a)\n    return a_transformed\n    ### END SOLUTION ###\n\n# Test the function\na = np.array([[1, 2], [3, 4]])\nprint(Transform(a))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close, ma50, ma100, ma200])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array([['dsa', '2'], ['sato', '3']])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n# print(X)\n# print(y)\n\nlogReg = LogisticRegression()\nlogReg.fit(X[:None],y)\n# logReg.fit(dataframe.iloc[-1:],dataframe.iloc[:,-1])\n\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the data to numpy array\nX = dataframe.iloc[-1:].astype(float)\n# Convert the class to numpy array\ny = dataframe.iloc[:, -1].values\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model\nlogReg.fit(X[:None], y)\n# Predict the class of the last animal\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "train_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "train_dataframe = features_dataframe[features_dataframe['date'] <= np.max(features_dataframe['date'])]\ntest_dataframe = features_dataframe[features_dataframe['date'] > np.max(features_dataframe['date'])]\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": " ###\n    # sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(by=\"date\")\n    # calculate the number of rows in the dataframe\n    n_rows = features_dataframe.shape[0]\n    # calculate the number of rows to be in the test set\n    test_size = n_rows * 0.8\n    # calculate the number of rows to be in the train set\n    train_size = n_rows - test_size\n    # split the dataframe into train and test set\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=test_size, train_size=train_size)\n    return train_dataframe, test_dataframe\n    ### END SOLUTION ###\n\n# call the function\nsolve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "scaler = MinMaxScaler()\n\ncols = df.columns[2:4]\ndf[cols + '_scale'] = scaler.fit_transform(df[cols])\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "myData['new_A2'] = scaler.fit_transform(myData[['A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = GridSearch_fitted.fit(X, y)\nprint(full_results.best_score_)\nprint(full_results.best_params_)\nprint(full_results.grid_scores_)\nprint(full_results.best_score_)\nprint(full_results.best_params_)\nprint(full_results.grid_scores_)\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = GridSearch_fitted.fit(X, y)\nfull_results.best_score_\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\ndef save_model(model):\n    # Save the model weights\n    weights = model.weights_\n    # Save the model means\n    means = model.means_\n    # Save the model covariance\n    covs = model.covs_\n    # Save the model components\n    components = model.components_\n    # Save all the attributes\n    attributes = np.concatenate((weights, means, covs, components))\n    np.save('sklearn_model', attributes)\n\n# Save the model\nsave_model(fitted_model)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "# tokenizing each description\ndf['description'] = df['description'].apply(lambda x: x.split())\n\n# transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(df['description'])\n\n# calculate cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = np.ones((len(df), len(df)))\nfor i in range(len(df)):\n    for j in range(len(df)):\n        if i == j:\n            cosine_similarity_matrix[i, j] = 1\n        else:\n            cosine_similarity_matrix[i, j] = 1 - sklearn.metrics.cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])\n\n# print the result\nprint(cosine_similarity_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\noptim.param_groups[0]['lr'] = 0.01\noptim.step()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n# Define the learning rate scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define a function to update the learning rate\ndef update_learning_rate(optimizer, epoch):\n    if epoch > 10:\n        optimizer.param_groups[0]['lr'] = 0.001\n    return optimizer\n\n# Update the learning rate after each epoch\nfor epoch in range(100):\n    optimizer = update_learning_rate(optim, epoch)\n    optimizer.step()\n    scheduler.step()\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\noptim.zero_grad()\noptim.step()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define the learning rate schedule\nlr_schedule = torch.optim.lr_scheduler.LambdaSchedule(lambda epoch: 0.0005)\n\n# Define the optimizer\noptimizer = torch.optim.SGD(optim.parameters(), lr=0.005)\n\n# Define the epoch where the learning rate should be changed\nepoch_change = 10\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr = 20\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_2 = 30\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_3 = 40\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_4 = 50\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_5 = 60\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_6 = 70\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_7 = 80\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_8 = 90\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_9 = 100\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_10 = 110\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_11 = 120\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_12 = 130\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_13 = 140\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_14 = 150\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_15 = 160\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_16 = 170\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_17 = 180\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_18 = 190\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_19 = 200\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_20 = 210\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_21 = 220\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_22 = 230\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_23 = 240\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_24 = 250\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_25 = 260\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_26 = 270\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_27 = 280\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_28 = 290\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_29 = 300\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_30 = 310\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_31 = 320\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_32 = 330\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_33 = 340\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_34 = 350\n\n# Define the epoch where the learning rate is changed\nepoch_change_lr_35 = 360\n\n# Define the", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the word2vec model to a PyTorch embedding layer\nembedding_weights = torch.tensor(word2vec.wv.vectors, dtype=torch.float32)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], -1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], ", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # get the weights from the word2vec model\n    weights = word2vec.wv.vectors\n    # convert the weights to PyTorch tensor\n    weights_tensor = torch.tensor(weights, dtype=torch.float32)\n    # return the weights tensor\n    return weights_tensor\n    ### END SOLUTION ###\n\n# call the function\nembedded_input = get_embedded_input(input_Tensor)\nprint(embedded_input)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "x = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\nprint(px)\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(np.array(x))\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "x = torch.rand(6,6)\npx = pd.DataFrame(x.numpy())\nprint(px)\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "A_logical = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical] # Throws error\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "A_log = torch.ByteTensor([1, 1, 0]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "A_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    ### END SOLUTION ###\n    return C\n\ndef load_data():\n    A_log = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_log, B\n\nsolve(A_log, B)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "A_log = torch.ByteTensor([0, 0, 1]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Convert the index to numpy array\nidx_np = idx.numpy()\n\n# Use numpy index_select to get the desired output\nC = np.array([np.array(B[i])[idx_np[i]] for i in range(len(idx_np))])\n\n# Convert the numpy array to PyTorch tensor\nC = torch.from_numpy(C)\n\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    # Convert the numpy array to a torch tensor\n    t = torch.tensor(a, dtype=torch.float16)\n    return t\n    ### END SOLUTION ###\n\n# Test the function\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=np.object)\n\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones_like(lens)\nmask[lens == 0] = 0\nmask[lens == 1] = 1\nmask[lens == 2] = 0\nmask[lens == 3] = 1\nmask[lens == 4] = 0\nmask[lens == 5] = 1\nmask[lens == 6] = 0\nmask[lens == 7] = 1\nmask[lens == 8] = 0\nmask[lens == 9] = 1\nmask[lens == 10] = 0\nmask[lens == 11] = 1\nmask[lens == 12] = 0\nmask[lens == 13] = 1\nmask[lens == 14] = 0\nmask[lens == 15] = 1\nmask[lens == 16] = 0\nmask[lens == 17] = 1\nmask[lens == 18] = 0\nmask[lens == 19] = 1\nmask[lens == 20] = 0\nmask[lens == 21] = 1\nmask[lens == 22] = 0\nmask[lens == 23] = 1\nmask[lens == 24] = 0\nmask[lens == 25] = 1\nmask[lens == 26] = 0\nmask[lens == 27] = 1\nmask[lens == 28] = 0\nmask[lens == 29] = 1\nmask[lens == 30] = 0\nmask[lens == 31] = 1\nmask[lens == 32] = 0\nmask[lens == 33] = 1\nmask[lens == 34] = 0\nmask[lens == 35] = 1\nmask[lens == 36] = 0\nmask[lens == 37] = 1\nmask[lens == 38] = 0\nmask[lens == 39] = 1\nmask[lens == 40] = 0\nmask[lens == 41] = 1\nmask[lens == 42] = 0\nmask[lens == 43] = 1\nmask[lens == 44] = 0\nmask[lens == 45] = 1\nmask[lens == 46] = 0\nmask[lens == 47] = 1\nmask[lens == 48] = 0\nmask[lens == 49] = 1\nmask[lens == 50] = 0\nmask[lens == 51] = 1\nmask[lens == 52] = 0\nmask[lens == 53] = 1\nmask[lens == 54] = 0\nmask[lens == 55] = 1\nmask[lens == 56] = 0\nmask[lens == 57] = 1\nmask[lens == 58] = 0\nmask[lens == 59] = 1\nmask[lens == 60] = 0\nmask[lens == 61] = 1\nmask[lens == 62] = 0\nmask[lens == 63] = 1\nmask[lens == 64] = 0\nmask[lens == 65] = 1\nmask[lens == 66] = 0\nmask[lens == 67] = 1\nmask[lens == 68] = 0\nmask[lens == 69] = 1\nmask[lens == 70] = 0\nmask[lens == 71] = 1\nmask[lens == 72] = 0\nmask[lens == 73] = 1\nmask[lens == 74] = 0\nmask[lens == 75] = 1\nmask[lens == 76] = 0\nmask[lens == 77] = 1\nmask[lens == 78] = 0\nmask[lens == 79] = 1\nmask[lens == 80] = 0\nmask[lens == 81] = 1\nmask[lens == 82] = 0\nmask[lens == 83] = 1\nmask[lens == 84] = 0\nmask[lens == 85] = 1\nmask[lens == 86] = 0\nmask[lens == 87] = 1\nmask[lens == 88] = 0\nmask[lens == 89] = 1\nmask[lens == 90] = 0\nmask[l", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "lens = torch.tensor([1, 9, 3, 5])\nmask = torch.ones_like(lens)\nmask[lens] = 0\nmask = torch.where(lens > 0, 1, 0)\nprint(mask)\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones_like(lens)\nmask[lens] = 0\nmask = torch.where(lens > 0, 1, 0)\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    mask = torch.zeros(lens.shape[0], lens.shape[1])\n    for i in range(lens.shape[0]):\n        mask[i, :lens[i]] = 1\n    return mask\n    ### END SOLUTION ###\n\nlens = torch.LongTensor([3, 5, 4])\nprint(get_mask(lens))\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with diagonal elements\ndiag_ele = torch.ones(Tensor_2D.shape[0])\n\n# Create a 2D tensor with index_in_batch\nindex_in_batch = torch.arange(Tensor_2D.shape[0])\n\n# Create a 2D tensor with drag_ele\ndrag_ele = torch.ones(Tensor_2D.shape[0])\n\n# Create a 3D tensor with index_in_batch and drag_ele\nindex_in_batch_drag_ele = torch.stack([index_in_batch, drag_ele], dim=1)\n\n# Create a 3D tensor with index_in_batch * diag_ele\nindex_in_batch_diag_ele = index_in_batch_drag_ele * diag_ele\n\n# Create a 3D tensor with index_in_batch * Matrix (who is a diagonal matrix, construct by drag_ele)\nindex_in_batch_matrix = torch.diag(drag_ele)\n\n# Create a 3D tensor with index_in_batch * Matrix (who is a diagonal matrix, construct by drag_ele)\nindex_in_batch_matrix_2D = index_in_batch_matrix.reshape(index_in_batch_matrix.shape[0], 1, 1)\n\n# Create a 3D tensor with index_in_batch * Matrix (who is a diagonal matrix, construct by drag_ele)\nindex_in_batch_matrix_3D = index_in_batch_matrix_2D.reshape(index_in_batch_matrix_2D.shape[0], 1, 1)\n\n# Print the 3D tensor\nprint(index_in_batch_matrix_3D)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    # Create a 2D tensor with the same shape as t\n    t_2D = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D = t_2D.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag = t_2D_diag.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix = t_2D_diag_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix = t_2D_diag_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix = t_2", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\nab = torch.stack((a, b), 0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = a.reshape(114, 514)\nb = b.reshape(24, 514)\nab = np.vstack((a, b))\nab = torch.tensor(ab)\nprint(ab.shape) # prints (138, 514)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    # stack the tensors a and b along the first dimension\n    # then stack the result with a and b along the second dimension\n    # the result should be a tensor of shape (3, 11)\n    # return a, b\n    ### END SOLUTION ###\n\n# a and b are numpy arrays\na = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]])\nb = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]])\n\n# convert numpy arrays to torch tensors\na = torch.from_numpy(a)\nb = torch.from_numpy(b)\n\n# call the function\nsolve(a, b)\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[ : , lengths : , : ] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[ : , lengths : , : ] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\nprint(new_tensors)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    # Convert the list of tensors to a tensor of tensors\n    tensor_of_tensors = torch.tensor(lt)\n    ### END SOLUTION ###\n    return tensor_of_tensors\n\n# Call the function\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = t[idx]\nprint(result)\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\nscores = x.reshape(70, 3)\nmax_scores = np.max(scores, axis=1)\nmax_ids = np.argmax(scores, axis=1)\nresult = x[max_ids]\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\nscores = x.mean(1) # calculate the mean of each row\nids = torch.argmax(scores, 1) # find the index of the maximum score\nresult = x[ids.unsqueeze(1), :] # gather the slices of x using the maximum ids\nresult = result.squeeze(1) # remove the extra dimension\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask with the indices of the selected elements\nmask = (ids == 1).reshape(-1, 1)\n\n# Gather the selected slices\nresult = x[mask].reshape(-1, 2)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "y = torch.argmax(softmax_output, 1)\nprint(y)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "y = torch.argmax(softmax_output, 1)\nprint(y)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "y = torch.argmax(softmax_output, 1)\ny = y.view(-1, 1)\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    # Find the maximum value in each row of the softmax output\n    max_values = torch.max(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum value from the softmax output\n    max_tensor = torch.ones_like(softmax_output) * np.inf\n    # Find the index of the maximum value in each row of softmax_output\n    max_index = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.ones_like(softmax_output) * np.inf\n    # Find the index of the maximum value in each row of softmax_output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum index from the softmax output\n    max_index_tensor = torch.argmax(softmax_output, 1)\n    # Create a tensor with the", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "# Find the index of the minimum value in the softmax output\ny = torch.argmax(softmax_output, 1)\n# Create a tensor with the same shape as the softmax output, with all values set to 0\ny = torch.zeros_like(softmax_output)\n# Set the value at the index of the minimum value to 1\ny[y == torch.argmax(softmax_output, 1)] = 1\nreturn y\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "images = Variable(torch.randn(5, 3, 4, 4))\nlabels = Variable(torch.LongTensor(5, 4, 4).random_(3))\nloss = cross_entropy2d(images, labels)\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = np.sum(A == B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = np.sum(A == B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = np.sum(A != B)\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = np.sum(A == B)\n    ### END SOLUTION ###\n    return cnt_equal\n\nA = np.random.rand(1000, 1)\nB = np.random.rand(1000, 1)\n\nprint(Count(A, B))\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "cnt_equal = np.sum(A[-x:] == B[-x:])", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = np.sum(A[-x:] != B[-x:])", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = torch.chunk(a, chunk_dim, dim=3)\ntensors_31 = tensors_31.reshape(tensors_31.shape[0], tensors_31.shape[1], tensors_31.shape[2], tensors_31.shape[3], 1)\nprint(tensors_31)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = torch.chunk(a, chunk_dim, dim=2)\ntensors_31 = tensors_31.reshape(tensors_31.shape[0], tensors_31.shape[1], tensors_31.shape[2], tensors_31.shape[3], 1)\nprint(tensors_31)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output[mask == 1] = clean_input_spectrogram[mask == 1]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask == 0] = clean_input_spectrogram[mask == 0]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x), torch.abs(y))\nmin_y = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.where(x < y, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\nsigned_max = torch.where(x > y, max_x, max_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x), torch.abs(y))\n    min_y = torch.min(torch.abs(x), torch.abs(y))\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, min_x_sign, min_y_sign)\n    return min_x_sign\n\n# call the function\nsolution = solve(x, y)\nprint(solution)\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\nprint(f\"Confidence score: {confidence_score}, Class name: {class_name}\")\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "a = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\na = a[:, :-1]\nb = b[:, 1:]\n\nresult = a + b\nresult = result.t()\nresult = result.reshape(-1, 2)\nresult = result.t()\n\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a[:, :-1]\n    b = b[:, 1:]\n    result = a + b\n    return result\n    ### END SOLUTION ###\n\n# Test the function\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\nprint(solve(a, b))\n# Output:\n# tensor([[1, 2, 4, 6, 7],\n#         [1, 2, 4, 6, 7]])\n#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "t = torch.tensor([[1, 2], [3, 4]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
