{"id": 0, "code": "result = df.iloc[List]", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.iloc[List]\nprint(result)\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a new column with the values from Qu1 that have at least two occurrences\ndf['new_Qu1'] = df['Qu1'].apply(lambda x: 'other' if pd.value_counts(df.Qu1) < x else x)\n\n# Create a new column with the values from Qu2 that have at least two occurrences\ndf['new_Qu2'] = df['Qu2'].apply(lambda x: 'other' if pd.value_counts(df.Qu2) < x else x)\n\n# Create a new column with the values from Qu3 that have at least two occurrences\ndf['new_Qu3'] = df['Qu3'].apply(lambda x: 'other' if pd.value_counts(df.Qu3) < x else x)\n\n# Create a new column with the values from Qu1 that have at least two occurrences\ndf['new_Qu1'] = df['new_Qu1'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu1) < x else x)\n\n# Create a new column with the values from Qu2 that have at least two occurrences\ndf['new_Qu2'] = df['new_Qu2'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu2) < x else x)\n\n# Create a new column with the values from Qu3 that have at least two occurrences\ndf['new_Qu3'] = df['new_Qu3'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu3) < x else x)\n\n# Create a new column with the values from Qu1 that have at least two occurrences\ndf['new_Qu1'] = df['new_Qu1'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu1) < x else x)\n\n# Create a new column with the values from Qu2 that have at least two occurrences\ndf['new_Qu2'] = df['new_Qu2'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu2) < x else x)\n\n# Create a new column with the values from Qu3 that have at least two occurrences\ndf['new_Qu3'] = df['new_Qu3'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu3) < x else x)\n\n# Create a new column with the values from Qu1 that have at least two occurrences\ndf['new_Qu1'] = df['new_Qu1'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu1) < x else x)\n\n# Create a new column with the values from Qu2 that have at least two occurrences\ndf['new_Qu2'] = df['new_Qu2'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu2) < x else x)\n\n# Create a new column with the values from Qu3 that have at least two occurrences\ndf['new_Qu3'] = df['new_Qu3'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu3) < x else x)\n\n# Create a new column with the values from Qu1 that have at least two occurrences\ndf['new_Qu1'] = df['new_Qu1'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu1) < x else x)\n\n# Create a new column with the values from Qu2 that have at least two occurrences\ndf['new_Qu2'] = df['new_Qu2'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu2) < x else x)\n\n# Create a new column with the values from Qu3 that have at least two occurrences\ndf['new_Qu3'] = df['new_Qu3'].apply(lambda x: 'other' if pd.value_counts(df.new_Qu3) < x else x)\n\n# Create a new column with the values from Qu1 that have at least two occurrences\ndf['new_Qu1'] = df['new_Qu1'].apply(lambda x: 'other' if p", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\ndef change_values(df):\n    for col in df.columns:\n        counts = df[col].value_counts()\n        if counts.max() >= 3:\n            df[col] = df[col].replace(counts[counts >= 3].index, 'other')\n    return df\n\nresult = change_values(df)\nprint(result)\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    for col in df.columns:\n        if df[col].nunique() >= 2:\n            df[col] = df[col].apply(lambda x: 'other' if x in ['apple', 'egg'] else x)\n    return df\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a dictionary with the values of Qu1 and Qu2 that have at least 3 occurrences\ndict_values = {k: v for k, v in df[['Qu1', 'Qu2']].values.tolist() if (pd.value_counts(v) >= 3).any()}\n\n# Create a dictionary with the values of Qu3 that have at least 2 occurrences\ndict_values_other = {k: v for k, v in df['Qu3'].values.tolist() if (pd.value_counts(v) >= 2).any()}\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least 3 occurrences\ndict_values_other.update({k: v for k, v in df[['Qu1', 'Qu2']].values.tolist() if (pd.value_counts(v) >= 3).any()})\n\n# Create a dictionary with the values of Qu3 that have at least 2 occurrences\ndict_values_other.update({k: v for k, v in df['Qu3'].values.tolist() if (pd.value_counts(v) >= 2).any()})\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least 3 occurrences\ndict_values_other.update({k: v for k, v in df[['Qu1', 'Qu2']].values.tolist() if (pd.value_counts(v) >= 3).any()})\n\n# Create a dictionary with the values of Qu3 that have at least 2 occurrences\ndict_values_other.update({k: v for k, v in df['Qu3'].values.tolist() if (pd.value_counts(v) >= 2).any()})\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least 3 occurrences\ndict_values_other.update({k: v for k, v in df[['Qu1', 'Qu2']].values.tolist() if (pd.value_counts(v) >= 3).any()})\n\n# Create a dictionary with the values of Qu3 that have at least 2 occurrences\ndict_values_other.update({k: v for k, v in df['Qu3'].values.tolist() if (pd.value_counts(v) >= 2).any()})\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least 3 occurrences\ndict_values_other.update({k: v for k, v in df[['Qu1', 'Qu2']].values.tolist() if (pd.value_counts(v) >= 3).any()})\n\n# Create a dictionary with the values of Qu3 that have at least 2 occurrences\ndict_values_other.update({k: v for k, v in df['Qu3'].values.tolist() if (pd.value_counts(v) >= 2).any()})\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least 3 occurrences\ndict_values_other.update({k: v for k, v in df[['Qu1', 'Qu2']].values.tolist() if (pd.value_counts(v) >= 3).any()})\n\n# Create a dictionary with the values of Qu3 that have at least 2 occurrences\ndict_values_other.update({k: v for k, v in df['Qu3'].values.tolist() if (pd.value_counts(v) >= 2).any()})\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least 3 occurrences\ndict_values_other.update({k: v for k, v in df[['Qu1', 'Qu2']].values.tolist() if (pd.value_counts(v) >= 3).any()})\n\n# Create a dictionary with the values of Qu3 that have at least 2 occurrences\ndict_values_other.update({k: v for k, v in df['Qu3'].values.tolist() if (pd.value_counts(v) >= 2).any()})\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least 3 occurrences\ndict_values_other.update({k: v for k, v in df", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a dictionary with the values and their counts\ncounts = df.value_counts()\n\n# Create a dictionary with the values and their replacement\nreplacement_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'egg': 'egg'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other_other_other_other_other_other_other_other_other_other_other_other_other_other_other = {'apple': 'other', 'potato': 'other', 'sausage': 'other', 'banana': 'other'}\n\n# Create a dictionary with the values and their replacement\nreplacement_dict_other", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "result = df.groupby('url').max().reset_index()\nresult['keep_if_dup'] = result['keep_if_dup'].apply(lambda x: 'Yes' if x == 'Yes' else 'No')\nresult", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndef df_to_dict(df):\n    result = {}\n    for name in df['name'].unique():\n        result[name] = {}\n        for v1 in df['v1'].unique():\n            result[name][v1] = {}\n            for v2 in df['v2'].unique():\n                result[name][v1][v2] = df[(df['name'] == name) & (df['v1'] == v1) & (df['v2'] == v2)]['v3'].values[0]\n    return result\n\nresult = df_to_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\ndf = df.sort_values('datetime')\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# Extract the key value pairs\nresult = df[['name', 'status', 'number']].copy()\n\n# Iterate over the message column\nfor i in range(len(df)):\n    # Extract the key value pairs from the message\n    message = df.loc[i, 'message']\n    message = message.replace('[', '').replace(']', '').replace(': ', ':')\n    message = message.split(', ')\n    # Create a dictionary with the key value pairs\n    dict_message = {}\n    for item in message:\n        key, value = item.split(': ')\n        dict_message[key] = value\n    # Add the dictionary to the result\n    result.loc[i, 'job'] = dict_message.get('job', 'none')\n    result.loc[i, 'money'] = dict_message.get('money', 'none')\n    result.loc[i, 'wife'] = dict_message.get('wife', 'none')\n    result.loc[i, 'group'] = dict_message.get('group', 'none')\n    result.loc[i, 'kids'] = dict_message.get('kids', 'none')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "\nfor product in products:\n    df.loc[df['product'] == product, 'score'] *= 10\n\nprint(df)\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10\ndf['score'] = df['score'].where(df['product'].isin(products), 0)\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "for product in products:\n    df.loc[df['product'].between(product[0], product[1]), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\nfor product in products:\n    df.loc[df['product'] == product, 'score'] = (df.loc[df['product'] == product, 'score'] - df.loc[df['product'] == product, 'score'].min()) / (df.loc[df['product'] == product, 'score'].max() - df.loc[df['product'] == product, 'score'].min())\n\nprint(df)\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = ''\nfor i in range(len(df)):\n    if df.iloc[i, 0] == 1:\n        df.iloc[i, 0] = 0\n    if df.iloc[i, 1] == 1:\n        df.iloc[i, 1] = 0\n    if df.iloc[i, 2] == 1:\n        df.iloc[i, 2] = 0\n    if df.iloc[i, 3] == 1:\n        df.iloc[i, 3] = 0\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 0] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 1] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 2] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 3] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 4] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 5] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 6] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 7] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 8] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 9] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 10] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 11] = 1\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n       ", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = 'A'\ndf.loc[df['A'] == 1, 'category'] = 'B'\ndf.loc[df['B'] == 1, 'category'] = 'C'\ndf.loc[df['C'] == 1, 'category'] = 'D'\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndef convert_to_list(df):\n    return df.apply(lambda x: list(x[x==1].index), axis=1)\n\ndf['category'] = convert_to_list(df)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf = df.set_index('Date')\ndf = df.reindex(List, fill_value=0)\ndf = df.reset_index()\ndf = df.rename(columns={'Date':'Date'})\nprint(df)\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_df.index = shifted_df.index.shift(1, axis=0)\nshifted_", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = df.index\nshifted_df.loc[0] = df.loc[0]\nshifted_df.loc[1] = df.loc[1]\nshifted_df.loc[2] = df.loc[2]\nshifted_df.loc[3] = df.loc[3]\nshifted_df.loc[4] = df.loc[4]\nshifted_df.loc[5] = df.loc[5]\nshifted_df.loc[6] = df.loc[6]\nshifted_df.loc[7] = df.loc[7]\nshifted_df.loc[8] = df.loc[8]\nshifted_df.loc[9] = df.loc[9]\nshifted_df.loc[10] = df.loc[10]\nshifted_df.loc[11] = df.loc[11]\nshifted_df.loc[12] = df.loc[12]\nshifted_df.loc[13] = df.loc[13]\nshifted_df.loc[14] = df.loc[14]\nshifted_df.loc[15] = df.loc[15]\nshifted_df.loc[16] = df.loc[16]\nshifted_df.loc[17] = df.loc[17]\nshifted_df.loc[18] = df.loc[18]\nshifted_df.loc[19] = df.loc[19]\nshifted_df.loc[20] = df.loc[20]\nshifted_df.loc[21] = df.loc[21]\nshifted_df.loc[22] = df.loc[22]\nshifted_df.loc[23] = df.loc[23]\nshifted_df.loc[24] = df.loc[24]\nshifted_df.loc[25] = df.loc[25]\nshifted_df.loc[26] = df.loc[26]\nshifted_df.loc[27] = df.loc[27]\nshifted_df.loc[28] = df.loc[28]\nshifted_df.loc[29] = df.loc[29]\nshifted_df.loc[30] = df.loc[30]\nshifted_df.loc[31] = df.loc[31]\nshifted_df.loc[32] = df.loc[32]\nshifted_df.loc[33] = df.loc[33]\nshifted_df.loc[34] = df.loc[34]\nshifted_df.loc[35] = df.loc[35]\nshifted_df.loc[36] = df.loc[36]\nshifted_df.loc[37] = df.loc[37]\nshifted_df.loc[38] = df.loc[38]\nshifted_df.loc[39] = df.loc[39]\nshifted_df.loc[40] = df.loc[40]\nshifted_df.loc[41] = df.loc[41]\nshifted_df.loc[42] = df.loc[42]\nshifted_df.loc[43] = df.loc[43]\nshifted_df.loc[44] = df.loc[44]\nshifted_df.loc[45] = df.loc[45]\nshifted_df.loc[46] = df.loc[46]\nshifted_df.loc[47] = df.loc[47]\nshifted_df.loc[48] = df.loc[48]\nshifted_df.loc[49] = df.loc[49]\nshifted_df.loc[50] = df.loc[50]\nshifted_df.loc[51] = df.loc[51]\nshifted_df.loc[52] = df.loc", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = df.index\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc[-1, '#1']\nshifted_df.loc[-1, '#2'] = df.loc[-1, '#2']\nshifted_df.loc[-1, '#1'] = df.loc", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\ndf = df.shift(1, axis=1)\n\n# shift the last row of the first column up 1 row\ndf = df.shift(-1, axis=1)\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'X' for col in df.columns]\nprint(df)\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "new_columns = [f'XHeader{i}' for i in range(1, 11)]\ndf.columns = new_columns\nprint(df)\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "# Create a list of column names\ncols = ['HeaderA', 'HeaderB', 'HeaderC', 'HeaderX']\n\n# Create a list of column names with 'X' at the end\ncols_with_x = [col + 'X' for col in cols]\n\n# Create a new dataframe with the column names\nnew_df = pd.DataFrame(columns=cols_with_x)\n\n# Add the original columns to the new dataframe\nnew_df[cols_with_x] = df[cols]\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "result = df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\nprint(result)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' in col})\nprint(result)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndef get_mean_val1_val2(group):\n    return df.loc[group, 'val1'] + df.loc[group, 'val2']\n\ndef get_sum_val42(group):\n    return df.loc[group, 'val42']\n\nresult = df.groupby('group').apply(lambda x: pd.Series({\n    'group_color': x['group_color'].first(),\n    'val1': x['val1'].sum(),\n    'val2': x['val2'].mean(),\n    'val42': x['val42'].sum()\n}))\n\nprint(result)\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "def meanAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].mean(axis=axis)\n\nresult = meanAdvance(row_list, column_list, axis=0)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "result = df.loc[row_list, column_list].sum()\nprint(result)\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "result = df.loc[row_list, column_list].sum(axis=1)\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.apply(lambda x: x.value_counts() if x.notnull().any() else x.value_counts(), axis=0)\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "result = df.groupby(df.columns).size()\nresult = result.to_string()\nprint(result)\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0,1]]\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0,1]]\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "result = df.where(df.notnull(), df.mean()).where(df.isnull(), np.nan)", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "result = df.where(df.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "result = df.where(df.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# Find the rows where value is less than the threshold\nrows_to_replace = df[df['value'] < thresh]\n\n# Sum the values of the rows to replace\nsum_values = rows_to_replace['value'].sum()\n\n# Create a new row with the sum of the values\nnew_row = pd.Series({'value': sum_values})\n\n# Replace the rows in the original dataframe with the new row\ndf = df.replace(rows_to_replace, new_row)\n\nresult = df\nprint(result)\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\n# Find rows where value is less than the threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n\n# Find rows where value is greater than the threshold\nrows_greater_than_thresh = df[df['value'] >= thresh]\n\n# Calculate the average of the values in the rows greater than the threshold\navg_value = rows_greater_than_thresh['value'].mean()\n\n# Create a new row with the average value\nnew_row = pd.Series({'value': avg_value})\n\n# Replace the rows in the original dataframe with the new row\ndf.loc[rows_less_than_thresh.index] = new_row\n\nresult = df", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "# find the rows in the section\nrows_in_section = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\n# calculate the average of the rows in the section\naverage_value = rows_in_section['value'].mean()\n# replace the rows in the section with the average value\nresult = df.replace(rows_in_section, average_value)\nresult", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "result = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\nresult", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndef exp_column(df, column):\n    return df[column]**(1/len(df[column]))\n\nresult = df.apply(lambda x: exp_column(df, x))\nresult.columns = [f'exp_{x}' for x in df.columns]\nresult", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "result = df.assign(inv_A=1/df['A'])\nresult = result.assign(inv_B=1/df['B'])\nresult", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = df[col].apply(sigmoid)\n\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Get the index of the last occurrence of the column-wise maximum\nmax_index = df.apply(lambda x: x.argmax(), axis=1)\n\n# Get the index of the last occurrence of the column-wise maximum up to the minimum index\nresult = min_index[max_index]\n\nprint(result)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Create a mask that is True for the rows where the column value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Apply the mask to the index of the minimum value in each column\nresult = min_index[mask]\n\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "# Create a new dataframe with all dates and all users\nall_dates = pd.DataFrame(df['dt'].values, index=df['dt'].values, columns=df['user'].values)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates.copy()\nall_dates_zero.replace(0, np.nan, inplace=True)\n\n# Find the minimum and maximum date\nmin_date = all_dates.min(axis=1)\nmax_date = all_dates.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date = all_dates_zero.max(axis=1)\n\n# Create a new dataframe with all dates and all users with 0 values\nall_dates_zero = all_dates_zero.replace(0, np.nan)\n\n# Find the minimum and maximum date\nmin_date = all_dates_zero.min(axis=1)\nmax_date", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "# Create a new dataframe with all dates from the minimum date to the maximum date\nnew_df = pd.DataFrame(pd.date_range(df['dt'].min(), df['dt'].max()), columns=['dt'])\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df, on='dt', how='left').fillna(0)\n\n# Create a new dataframe with all dates from the minimum date to the maximum date and fill with 0\nnew_df = new_df.merge(df,", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "# Create a new dataframe with all dates and all users\nall_dates = pd.date_range(start=df['dt'].min(), end=df['dt'].max())\nall_users = df['user'].unique()\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and all users\nall_dates_users = pd.MultiIndex.from_product([all_dates, all_users])\n\n# Create a new dataframe with all dates and", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "# Find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum to the maximum\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with the date range and the maximum value for each user\nresult = pd.DataFrame({'dt': date_range, 'user': df['user'].unique(), 'val': df['val'].max()})\n\n# Fill in the maximum value for each user for the date range\nresult = result.set_index(['dt', 'user']).unstack().fillna(result['val'].values)\n\n# Reset the index\nresult = result.reset_index()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "# Find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum to the maximum\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with the date range and the maximum value for each user\nresult = pd.DataFrame({'dt': date_range, 'user': df['user'].unique(), 'val': df['val'].max()})\n\n# Convert the date to the desired format\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n# Sort the result by date and user\nresult = result.sort_values(['dt', 'user'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "result = df.groupby('name').apply(lambda x: x.assign(name=x.index + 1)).reset_index(drop=True)\nresult", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "\n# Create a unique ID for each name\nunique_ids = df['name'].unique()\n\n# Create a dictionary to map unique IDs to their corresponding values\nid_map = dict(zip(unique_ids, range(1, len(unique_ids) + 1)))\n\n# Replace the 'a' column with the unique ID\ndf['a'] = df['name'].map(id_map)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    df['name'] = df['name'].astype('int')\n    df['name'] += 1\n    df['name'] = df['name'].astype('str')\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\n# Create a unique ID for each name\nid_dict = {}\nfor i in range(len(df)):\n    id_dict[df['name'].values[i]] = i + 1\n\n# Create a new column with unique ID\ndf['ID'] = df['name'].map(id_dict)\n\n# Replace the name column with unique ID\ndf['name'] = df['ID']\n\n# Drop the ID column\ndf = df.drop(['ID'], axis=1)\n\nresult = df\nprint(result)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = pd.pivot_table(df, values='value', index='user', columns='date', aggfunc='first')\ndf = df.reset_index()\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'01/12/15': 'date'})\ndf = df.rename(columns={'02/12/15': 'value'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.rename(columns={'someBool': 'someBool'})\ndf = df.rename(columns={'user': 'user'})\ndf = df.rename(columns={'value': 'value'})\ndf = df.rename(columns={'date': 'date'})\ndf = df.ren", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'01/12/15': 'others', 'value': 'value'})\ndf = df.drop(['user'], axis=1)\ndf = df.sort_values(['user', 'others'])\nprint(df)\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.pivot_table(index='user', columns='date', values='value', fill_value=None)\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'someBool':", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df.c > 0.5][columns].to_numpy()", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df.c > 0.45][columns].values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": " ###\n    # Use the loc function to select the rows where the value in column 'c' is greater than 0.5\n    # Then select the columns you want\n    # Finally, convert the result to a numpy array\n    # result = df.loc[df.c > 0.5, columns].to_numpy()\n    ### END SOLUTION ###\n    return result\n\n# Create a DataFrame\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# Call the function\nresult = f(df, columns=['b', 'e'])\nprint(result)\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": " ###\n    # Use the loc function to select the rows where the value in column 'c' is greater than 0.5\n    # Then select the columns 'b' and 'e'\n    # Finally, calculate the sum of the two columns for each element\n    # return the result\n    ### END SOLUTION ###\n\n    return df.loc[df.c > 0.5, columns].sum(axis=1)\n\n# Create a DataFrame\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# Call the function\nresult = f(df, columns=['b', 'e'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": " ###\n    return df[df.c > 0.5][columns]\n    ### END SOLUTION ###\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint f(df, ['b', 'e'])\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndef find_overlap(x, df):\n    dates = df['date'].values\n    dates = dates.reshape(-1, 1)\n    dates_to_compare = dates + x\n    dates_to_compare = dates_to_compare.reshape(-1, 1)\n    overlap = np.in1d(dates, dates_to_compare)\n    return overlap\n\noverlap = find_overlap(X, df)\n\nresult = df[~overlap]\nresult", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef find_overlap(row, X):\n    date = row['date']\n    date_range = pd.date_range(date, periods=X)\n    return date_range\n\ndef remove_overlap(df, X):\n    dates = df['date'].values\n    dates_range = pd.date_range(start=dates[0], end=dates[-1])\n    dates_range = dates_range.values\n    return dates_range\n\ndef remove_overlap_rows(df, X):\n    dates_range = remove_overlap(df, X)\n    return df[~df['date'].isin(dates_range)]\n\nresult = remove_overlap_rows(df, X)\nprint(result)\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef get_dates(date, X):\n    dates = []\n    for i in range(X):\n        date += pd.DateOffset(months=i)\n        dates.append(date)\n    return dates\n\ndates = get_dates(date, X)\n\nresult = df[~df['date'].isin(dates)]\nprint(result)\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\n# Create a new column with the bin values\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['col1'].apply(lambda x: x // 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x // 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x // 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x // 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x // 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x // 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x // 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3)\n\n# Create a new column with the bin values\ndf['bin'] = df['bin'].apply(lambda x: x / 3", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\n# Create a new column with the number of rows in each bin\ndf['bin'] = (df.index // 3).astype(int)\n\n# Group by the bin column and count the number of rows in each bin\nresult = df.groupby('bin').size()\n\nprint(result)\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n# Create a new column with the bin number\ndf['bin'] = (df['col1'].astype(int) // 4).astype(int)\n\n# Group by bin and sum the values\nresult = df.groupby('bin')['col1'].sum()\n\nprint(result)\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Create a new column with the mean of the last 3 rows\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum'] = df['col1'].iloc[:3].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg'] = df['col1'].iloc[:2].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df['col1'].iloc[-3:].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df['col1'].iloc[-2:].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df['col1'].iloc[3:].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df['col1'].iloc[2:].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last'] = df['col1'].iloc[-3:-2].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_last'] = df['col1'].iloc[-2:-1].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first_first'] = df['col1'].iloc[3:3].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first_first'] = df['col1'].iloc[2:2].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last_last'] = df['col1'].iloc[-3:-2].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_last_last'] = df['col1'].iloc[-2:-1].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first_first_first'] = df['col1'].iloc[3:3].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first_first_first'] = df['col1'].iloc[2:2].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last_last_last'] = df['col1'].iloc[-3:-2].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_last_last_last'] = df['col1'].iloc[-2:-1].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first_first_first_first'] = df['col1'].iloc[3:3].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first_first_first_first'] = df['col1'].iloc[2:2].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last_last_last_last'] = df['col1'].iloc[-3:-2].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_last_last_last_last'] = df['col1'].iloc[-2:-1].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first_first_first_first_first'] = df['col1'].iloc[3:3].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first_first_first_first_first'] = df['col1'].iloc[2:2].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_last_last_last_last_last'] = df['col1'].iloc", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_2'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_2'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_3'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_3'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_4'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_4'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_5'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_5'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_6'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_6'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_7'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_7'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_8'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_8'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_9'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_9'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_10'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_10'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_11'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_11'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_12'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_12'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_13'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_13'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(method='ffill')\ndf = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\n# Find the index of the first zero\nzero_index = df[df['A'] == 0].index[0]\n\n# Find the index of the last zero\nlast_zero_index = df[df['A'] == 0].index[-1]\n\n# Find the maximum value between the last zero and the next zero\nmax_value = max(df.loc[zero_index + 1:last_zero_index, 'A'].max(), df.loc[zero_index:last_zero_index, 'A'].max())\n\n# Replace the zero with the maximum value\ndf.loc[zero_index:last_zero_index, 'A'] = max_value\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_days'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\ndf", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    df['time'] = df.duration.str.split(' ').str[0]\n    df['number'] = df.duration.str.split(' ').str[1]\n    df['time_days'] = df.duration.str.split(' ').str[1]\n    df['time_days'] = df.time_days.apply(lambda x: {'year': 365, 'day': 1, 'week': 7, 'month': 30}[x])\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['number'] = df['number'].astype(int)\ndf['time_day'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\ndf['time_day'] *= df['number']\ndf", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.where(np.all(np.array([df1[column] == df2[column] for column in columns_check_list]), axis=1))\nprint(result)\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = np.where(np.all(df1[columns_check_list].values == df2[columns_check_list].values, axis=1))\nresult = result[0]\nresult = result.tolist()\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Create a new column with the date, x and y values\n    df['date'] = df['date'].apply(lambda x: x.date())\n    df['x'] = df['x'].values\n    df['y'] = df['y'].values\n    # Return the new dataframe\n    return df\n\n# Test the function\ndf = pd.DataFrame({'id': ['abc'], 'date': ['3/1/1994'], 'x': [100], 'y': [7]})\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    df = df.reset_index()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index(['id', 'date'])\n    df = df.swaplevel(0, 1)\n    return df\n    ### END SOLUTION ###\n\n# Test the function\ndf = pd.DataFrame({'x': ['abc'], 'y': [100], 'date': ['3/1/1994']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.melt(id_vars=['Country', 'year'], var_name='Variable')\ndf = df.pivot(index='Country', columns='Variable', values='value')\ndf = df.reset", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns='var2')\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var3'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var3')\ndf = df.reset_index()\ndf = df.rename(columns={'var3': 'var4'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var4')\ndf = df.reset_index()\ndf = df.rename(columns={'var4': 'var5'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var5')\ndf = df.reset_index()\ndf = df.rename(columns={'var5': 'var6'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var6')\ndf = df.reset_index()\ndf = df.rename(columns={'var6': 'var7'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var7')\ndf = df.reset_index()\ndf = df.rename(columns={'var7': 'var8'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var8')\ndf = df.reset_index()\ndf = df.rename(columns={'var8': 'var9'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var9')\ndf = df.reset_index()\ndf = df.rename(columns={'var9': 'var10'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var10')\ndf = df.reset_index()\ndf = df.rename(columns={'var10': 'var11'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var11')\ndf = df.reset_index()\ndf = df.rename(columns={'var11': 'var12'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var12')\ndf = df.reset_index()\ndf = df.rename(columns", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\n# Create a list of column names\ncols = [f'Value_{i}' for i in range(1, 11)]\n\n# Create a new dataframe with only the columns that have a value greater than 1\nnew_df = df[cols]\n\n# Create a new dataframe with only the rows where all values are greater than 1\nnew_df = new_df[new_df.max(axis=1) > 1]\n\nprint(new_df)\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\n# Create a list of column names\ncols = [f'Value_{i}' for i in range(1, 10)]\n\n# Create a new dataframe with only the columns that have a value greater than 1\nnew_df = df[cols].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the rows where all columns are greater than 1\nnew_df = new_df.all(axis=1)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a list of column names that start with 'Value'\ncolumn_names = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df = df.replace({'&LT;': '<'}, regex=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    df = df.replace('&AMP;', '&')\n    return df\n\nf(example_df)\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df = df.replace({'&AMP;': ' & '}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[1] if ' ' in x else None)\n\nprint(df)\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x if x is None else x.split(' ')[1])\n\nprint(df)\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^(.*) (.*) (.*)$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return match_obj.groups()\n    else:\n        return None\n\ndf['first_name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(validate_single_space_name))\n\nprint(df)\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge_asof(df2, df1, on='Timestamp', direction='nearest')\nresult['data'] = df1['data'].values[result['Timestamp'].astype(int)]\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1, df2, on='Timestamp', direction='forward')\nresult['stuff'] = df2['stuff'][result['Timestamp']]\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= 50, df['col1'], np.max(df[['col1', 'col2', 'col3']], axis=1))\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = np.where((df['col2'] > 50) & (df['col3'] > 50), df['col1'], np.sum(df[['col1', 'col2', 'col3']], axis=1))\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\nerrors = []\nfor index, value in df['Field1'].iteritems():\n    if not isinstance(value, int):\n        errors.append(value)\n\nprint(errors)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndef get_integer_values(df):\n    return [x for x in df['Field1'].values if isinstance(x, int)]\n\nresult = get_integer_values(df)\nprint(result)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    result = []\n    for index, value in df.iterrows():\n        if not pd.to_numeric(value, errors='coerce').is_integer():\n            result.append(value)\n    return result\n\nprint(f())\n### END SOLUTION ###\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndef compute_percentage(df):\n    total = df.sum()\n    return df / total * 100\n\ndf = compute_percentage(df)\nprint(df)\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)\nprint(result)\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\n# Create a new column 'nearest_neighbour' that contains the nearest neighbour for each car\ndf['nearest_neighbour'] = df.groupby('car')['time'].apply(lambda x: x.iloc[x.argmin(axis=1)]).values\n\n# Create a new column 'euclidean_distance' that contains the euclidean distance between each car and its nearest neighbour\ndf['euclidean_distance'] = df.groupby('car')['nearest_neighbour'].apply(lambda x: np.linalg.norm(x.values - df.loc[x.values, 'x'].values))\n\n# Calculate the average of the distances for each frame using groupby\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "# Create a new column 'farmost_neighbour' that contains the farthest car for each time point\ndf['farmost_neighbour'] = df.groupby('time')['car'].apply(lambda x: x.loc[x.argmax()])\n\n# Create a new column 'euclidean_distance' that contains the euclidean distance between each car and its farthest neighbour\ndf['euclidean_distance'] = df.groupby('time')['car'].apply(lambda x: np.sqrt((x - df.loc[x.argmax(), 'x']).pow(2).sum()))\n\n# Calculate the average of the distances for each time point\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "\ndef concat_columns(df):\n    return \",\".join(df.columns)\n\ndf[\"keywords_all\"] = df.apply(concat_columns, axis=1)\nprint(df)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "\ndef concat_cols(df):\n    return \"-\".join(df.columns)\n\ndf[\"keywords_all\"] = df.apply(concat_cols, axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "\ndef concat_keywords(df):\n    keywords_all = []\n    for col in df.columns[1:]:\n        keywords_all.append(df[col].dropna().to_list())\n    return \"-\".join(keywords_all)\n\ndf['keywords_all'] = df.apply(concat_keywords, axis=1)\nprint(df)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndef concat_columns(df):\n    cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n    df['keywords_all'] = df[cols].apply(lambda x: '-'.join(x.dropna().values.tolist()), axis=1)\n    return df\n\ndf = concat_columns(df)\nprint(df)\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Set Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index\n\n# Print the result\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change value of ProductId to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index\n\n# Print the result\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# Create a new DataFrame with the 20% of rows from each user\nsample_df = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n\n# Set the Quantity column of the sampled rows to zero\nsample_df['Quantity'] = 0\n\n# Get the indexes of the rows that were altered\nindexes = sample_df.index\n\n# Create a new DataFrame with the original rows and the sampled rows\nresult_df = pd.concat([df, sample_df])\n\n# Remove the rows that were altered\nresult_df = result_df.drop(indexes)\n\n# Print the result\nprint(result_df)\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = df.index[duplicate_bool]\nresult = duplicate\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = df.index[duplicate_bool]\nresult = duplicate", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    # find the duplicates\n    duplicates = df.duplicated(subset=['col1','col2'], keep='first')\n    # find the index of the first duplicate\n    index_original = duplicates.index[duplicates]\n    # add a new column with the index of the first duplicate\n    df['index_original'] = index_original\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\nresult", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\nresult", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# Find the maximum count for each group\nmax_count = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "result = df[df['count'] == df.groupby(['Sp','Mt'])['count'].transform('max')]", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\ngrouped = df.groupby(['Sp', 'Mt'])\nresult = grouped['count'].min().reset_index()\nresult = result[result['count'] == grouped['count'].min()]\nresult", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# Find the maximum count for each group\nmax_count = df.groupby(['Sp','Value'])['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")\nprint(result)\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")\nprint(result)\n", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "result = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                  ('A', 'B', 'F'),\n                                  ('A', 'C', 'G'),\n                                  ('A', 'C', 'H'),\n                                  ('A', 'D', 'I'),\n                                  ('A', 'D', 'J')])\nprint(result)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "result = pd.melt(df, value_vars=[list('BBCCDD'), list('EFGHIJ')], var_names=['variable_0', 'variable_1', 'variable_2'])\nresult['variable_0'] = result['variable_0'].str[0]\nresult['variable_1'] = result['variable_1'].str[1]\nresult['variable_2'] = result['variable_2'].str[2]\nresult = result[['variable_0', 'variable_1', 'variable_2', 'value']]\nresult", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id').val.cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id').val.cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id').val.cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id').val.cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] < 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship_type(column1, column2):\n    if column1.equals(column2):\n        return 'one-to-one'\n    elif column1.equals(column2.unique()):\n        return 'one-to-many'\n    elif column2.equals(column1.unique()):\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {relationship_type(column1, column2)}')\n\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship_type(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-2-many'\n    elif column1.min() == column2.max():\n        return 'one-2-one'\n    elif column1.min() == column2.min():\n        return 'many-2-one'\n    elif column1.max() == column2.max():\n        return 'many-2-many'\n    else:\n        return 'unknown'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 == column2:\n            continue\n        result.append(f'{column1} {column2} {relationship_type(df[column1], df[column2])}')\n\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def relationship(column1, column2, column3, column4, column5):\n    if column1.equals(column2) and column2.equals(column3) and column3.equals(column4) and column4.equals(column5):\n        return 'one-to-one'\n    elif column1.equals(column2) and column2.equals(column3) and column3.equals(column4) and column4.equals(column5):\n        return 'one-to-many'\n    elif column1.equals(column2) and column2.equals(column3) and column3.equals(column4) and column4.equals(column5):\n        return 'many-to-one'\n    elif column1.equals(column2) and column2.equals(column3) and column3.equals(column4) and column4.equals(column5):\n        return 'many-to-many'\n    else:\n        return 'unknown'\n\nresult = pd.DataFrame(columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5'],\n                      index=['one-to-one', 'one-to-many', 'many-to-one', 'many-to-many', 'unknown'])\n\nfor i in range(len(df.columns)):\n    for j in range(i + 1, len(df.columns)):\n        result.loc[relationship(df[df.columns[i]], df[df.columns[j]], df[df.columns[i]], df[df.columns[j]], df[df.columns[i]]), (i, j)] = 'yes'\n\nprint(result)\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship(x, y):\n    if x.shape[0] == y.shape[0]:\n        return 'one-2-many'\n    elif x.shape[0] == y.shape[0] + 1:\n        return 'one-2-one'\n    elif x.shape[0] == y.shape[0] + 1 and y.shape[0] == x.shape[0] + 1:\n        return 'many-2-many'\n    else:\n        return 'many-2-one'\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor c1 in df.columns:\n    for c2 in df.columns:\n        result.loc[c1, c2] = relationship(df[c1], df[c2])\n\nprint(result)\n", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\nresult = dfiban_uniq\nresult\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\ndef hello_world():\n    print(\"Hello, World!\")\n\nhello_world()!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\n# Create a new column 'Family'\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\n\n# Group by 'Family' and calculate the mean of 'Survived'\nresult = df.groupby('Family')['Survived'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\n# Create a new column 'Family'\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\n\n# Group by 'Family' and calculate the mean of 'SibSp'\nresult = df.groupby('Family')['SibSp'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndef group_condition(sibsp, parch):\n    if sibsp == 1 and parch == 1:\n        return 'Has Family'\n    elif sibsp == 0 and parch == 0:\n        return 'No Family'\n    elif sibsp == 0 and parch == 1:\n        return 'New Family'\n    elif sibsp == 1 and parch == 0:\n        return 'Old Family'\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [1, 0, 1, 0.5]\n\ngroup_dict = dict(zip(group_names, group_values))\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 1}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.5}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 1.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.5}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 1.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.5}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 1.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.5}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 1.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.5}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 1.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.5}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 1.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.5}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 1.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0.5}\n\ngroup_dict = {k: v for k, v in group_dict.items() if v == 0}\n\ngroup_dict = {k: v for k, v in group_dict.items() if", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\nprint(df)", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\ndf", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\ndf", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "def mean_std(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('a').apply(mean_std)\nresult.columns = ['mean', 'std']\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "def mean_std(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('b').a.apply(mean_std)\nresult = result.reset_index()\nresult.columns = ['b', 'mean', 'std']\nprint(result)\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x, axis=0)\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = df['b'].apply(softmax)\ndf['min-max'] = df['b'].apply(min_max)\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "result = df.loc[~(df == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1)]\nresult = result.loc[~(result == 0).all(axis=1", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "result = df[df.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result[result.sum() != 0]\nresult = result[result.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult = result", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "result = df[df.max(axis=1) != 2]", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "result = df.where(df == 2, 0)\nresult", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=True).sort_index()\nprint(result)\n", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "sorted_s = s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.sort_index()\nsorted_s = sorted_s.", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str) == 's']\nresult", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# Find the maximum count for each group\nmax_count = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "result = df[df['count'] == df.groupby(['Sp','Mt'])['count'].transform('max')]", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\ngrouped = df.groupby(['Sp', 'Mt'])\nresult = grouped['count'].min().reset_index()\nresult = result[result['count'] == grouped['count'].min()]\nresult", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# Find the maximum count for each group\nmax_count = df.groupby(['Sp','Value'])['count'].max()\n\n# Find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(np.nan)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": " ###\n    # map the dict values to the Date column\n    df['Date'] = df['Member'].map(dict)\n    # fill NaN with the value from the dict\n    df['Date'].fillna(dict, inplace=True)\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17-Aug-1926')\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\nprint(df)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_w'] = df.groupby([df['Date'].dt.weekday]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date').apply(lambda x: x.sum() if x.sum() == 0 else x.sum()).reset_index()\nresult2 = df.groupby('Date').apply(lambda x: x.sum() if x.sum() != 0 else x.sum()).reset_index()\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "result1 = df[df['B'] % 2 == 0].groupby('Date').count()\nresult2 = df[df['B'] % 2 == 1].groupby('Date').count()\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult_mean = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nresult_mean['D'] = result_mean['D'].apply(lambda x: x[0])\nresult_mean['E'] = result_mean['E'].apply(lambda x: x[0])\nresult_mean</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.min)\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\ndef split_string_to_rows(df):\n    df = df.reset_index()\n    df['var2'] = df['var2'].str.split(',')\n    df = df.explode('var2')\n    return df\n\nresult = split_string_to_rows(df)\nprint(result)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\nimport dask.dataframe as dd\n\n# Convert the dataframe to a dask dataframe\ndd_df = dd.from_pandas(df, npartitions=1)\n\n# Split the column into multiple rows\ndd_df = dd_df.assign(var2=dd_df.var2.str.split(','))\n\n# Expand the column\ndd_df = dd_df.explode('var2')\n\n# Convert the dask dataframe back to a pandas dataframe\nresult = dd_df.to_pandas()\n\nprint(result)\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\nimport dask.dataframe as dd\n\n# Convert the dataframe to a dask dataframe\ndd_df = dd.from_pandas(df, npartitions=1)\n\n# Split the column into multiple rows\ndd_df = dd_df.explode('var2')\n\n# Convert the dask dataframe back to a pandas dataframe\nresult = dd_df.compute()\n\nprint(result)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf['fips'] = df['fips'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf['row'] = df['row'].str.split(' ', 1, expand=True)[1]\ndf['fips'] = df['fips'].str.split(' ', 1, expand=True)[0]\ndf['medi'] = df['medi'].str.split(' ', 1, expand=True)[0]\ndf", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        df[col] = df[col].cumsum() / (df[col].astype(float) != 0).astype(int)\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        df[col] = df[col].cumsum() / (df[col].sum() + 1)\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    # calculate the cumulative average for each row using pandas\n    # ignore zero values\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean(), axis=1)\n   ", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        df[col] = df[col].cumsum() / (df[col].sum() + 1)\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)\ndf['label'] = df['label'].replace([0, 1], [1, 0])\ndf['DateTime'] = df['DateTime'].str.replace('.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\n# Create a new column 'label'\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n\n# Set label for the first row to 1\ndf.loc[0, 'label'] = 1\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = df.departure_time.apply(lambda x: pd.to_datetime(x) - pd.to_datetime(df.arrival_time[0]))\nprint(df)\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = df.departure_time.apply(lambda x: (x - pd.to_datetime(x)) / 1000)\nprint(df)\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['Duration'] = df.departure_time.apply(lambda x: (x - pd.to_datetime(x)) / 1000)\ndf['arrival_time'] = pd.to_datetime(df.arrival_time)\ndf['departure_time'] = pd.to_datetime(df.departure_time)\ndf['Duration'] = df.departure_time.apply(lambda x: (x - df.arrival_time).seconds)\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df.groupby('key1').apply(lambda x: x.loc[x['key2'].endswith('e'), 'key2'].size()).reset_index(name='count')\nresult", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.index[df.value.argmax()]\nmin_result = df.index[df.value.argmin()]\nprint(max_result, min_result)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.mode(axis=0)[0][0]\nmedian_result = df.median()[0]\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df[~(99 <= df['closing_price'] <= 101)]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "result = df.groupby(\"item\").apply(lambda x: x.loc[x[\"diff\"].idxmin()])\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\nprint(df)\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\nprint(df)\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df['SOURCE_NAME'].values\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# Calculate the number of NaN values in the column\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# Calculate the number of NaN values to be filled\nnan_to_fill = nan_count // 2\n\n# Create a mask for the NaN values to be filled\nmask = np.isnan(df['Column_x'])\n\n# Fill the NaN values with the mode of the column\ndf['Column_x'][mask] = df['Column_x'].mode()[0]\n\n# Fill the first 50% of NaN values with '0'\ndf['Column_x'][mask[:nan_to_fill]] = 0\n\n# Fill the last 50% of NaN values with '1'\ndf['Column_x'][mask[-nan_to_fill:]] = 1\n\nprint(df)\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values in the column\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# Calculate the number of NaN values to be filled\nnan_to_fill = nan_count * 0.3\n\n# Calculate the number of NaN values to be filled in the first 30%\nnan_to_fill_first_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to be filled in the middle 30%\nnan_to_fill_middle_30 = np.floor(nan_to_fill * 0.6)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last_30 = nan_to_fill - nan_to_fill_first_30 - nan_to_fill_middle_30\n\n# Calculate the number of NaN values to be filled in the first 30%\nnan_to_fill_first_30 = np.floor(nan_to_fill_first_30)\n\n# Calculate the number of NaN values to be filled in the middle 30%\nnan_to_fill_middle_30 = np.floor(nan_to_fill_middle_30)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill_last_30)\n\n# Calculate the number of NaN values to be filled in the first 30%\nnan_to_fill_first_30 = np.floor(nan_to_fill_first_30)\n\n# Calculate the number of NaN values to be filled in the middle 30%\nnan_to_fill_middle_30 = np.floor(nan_to_fill_middle_30)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill_last_30)\n\n# Calculate the number of NaN values to be filled in the first 30%\nnan_to_fill_first_30 = np.floor(nan_to_fill_first_30)\n\n# Calculate the number of NaN values to be filled in the middle 30%\nnan_to_fill_middle_30 = np.floor(nan_to_fill_middle_30)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill_last_30)\n\n# Calculate the number of NaN values to be filled in the first 30%\nnan_to_fill_first_30 = np.floor(nan_to_fill_first_30)\n\n# Calculate the number of NaN values to be filled in the middle 30%\nnan_to_fill_middle_30 = np.floor(nan_to_fill_middle_30)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill_last_30)\n\n# Calculate the number of NaN values to be filled in the first 30%\nnan_to_fill_first_30 = np.floor(nan_to_fill_first_30)\n\n# Calculate the number of NaN values to be filled in the middle 30%\nnan_to_fill_middle_30 = np.floor(nan_to_fill_middle_30)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill_last_30)\n\n# Calculate the number of NaN values to be filled in the first 30%\nnan_to_fill_first_30 = np.floor(nan_to_fill_first_30)\n\n# Calculate the number of NaN values to be filled in the middle 30%\nnan_to_fill_middle_30 = np.floor(", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# Create a mask for the NaN values\nmask = np.isnan(df['Column_x'])\n\n# Create a binary array of 0 and 1\nbinary = np.array([0,1])\n\n# Create a random binary array of 0 and 1\nrandom_binary = np.random.choice(binary, size=(len(mask), 2))\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with the same length as the mask\nrandom_binary = np.where(mask, random_binary, binary)\n\n# Create a binary array of 0 and 1 with", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame([(a.iloc[i, 0], b.iloc[i, 0]) for i in range(a.shape[0])], columns=['one', 'two'])\nresult", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0], c.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0], c.iloc[1,0])], columns=['one', 'two'])\nresult", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "result = pd.DataFrame([(a.iloc[i, 0], b.iloc[i, 0]) for i in range(a.shape[0])], columns=['one', 'two'])\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)\nresult = result.replace([np.nan], np.nan)", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(','))).to_frame()\nresult.columns = ['text']\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame()\nresult.columns = ['text']\nresult", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', '))).to_frame()\nresult.columns = ['text']\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].to_series().str.split(', ', expand=True).apply(lambda x: ', '.join(x), axis=1)\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame()\nresult.columns = ['text']\nresult.index = [0]\nresult", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.concat([df1, df2], axis=0)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values('date')).reset_index(drop=True)\nresult = result.groupby", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n# Create a new column 'date_formatted' to format the date\ndf1['date_formatted'] = df1['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n\n# Create a new column 'date_formatted' to format the date\ndf2['date_formatted'] = df2['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n\n# Concatenate the two dataframes on the 'id' column\nresult = pd.concat([df1, df2], axis=0)\n\n# Sort the result by 'id' and 'date'\nresult = result.sort_values(['id', 'date'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "result = pd.concat([df1, df2], axis=0)\nresult = result.groupby(['id', 'city', 'district', 'date']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).apply(lambda x: x.sort_values(['date'])).reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district']).first().reset_index()\nresult = result.sort_values(['id', 'date'])\nresult = result", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.sort_values('A', inplace=True)\nresult.reset_index(drop=True, inplace=True)\nresult.columns = ['A', 'B']\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x', 'B_y']\nresult.set_index('A', inplace=True)\nresult.index = range(len(result))\nresult = result.sort_index()\nresult", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['dulplicated'] = result['A'].apply(lambda x: x in D['A'])\nresult", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist()).to_dict()\nprint(result)\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist()).to_dict()\nresult = {k: [(v[0], v[1]) for v in result[k]] for k in result}\nresult = pd.DataFrame(list(result.items()), index=result.keys())\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist()).to_dict()\nresult = {k: [(v[0], v[1]) for v in result[k]] for k in result}\nresult = pd.DataFrame(list(result.items()), index=result.keys())\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={0:'amount-time-tuple'})\nresult = result.sort_values(['time', 'amount'])\nresult = result.", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\n# Create a new dataframe with the same index as the series\ndf = pd.DataFrame(series.index, index=series.index, columns=series.index)\n\n# Expand the dataframe to the same shape as the series\ndf = df.reindex(series.index, axis=0)\n\n# Expand the dataframe to the same shape as the series\ndf = df.reindex(series.index, axis=1)\n\n# Expand the dataframe to the same shape as the series\ndf = df.reindex(series.index, axis=0)\n\n# Print the dataframe\nprint(df)\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\n# Create a new dataframe with the same index as the series\ndf = pd.DataFrame(series.index, index=series.index, columns=series.index)\n\n# Expand the series to the dataframe\ndf = df.join(series)\n\n# Rename the columns\ndf.columns = series.index\n\n# Print the dataframe\nprint(df)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.columns[~df.columns.str.contains(s)].tolist()[0]\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "result = df.columns[~df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new column for each code in the original dataframe\nfor i in range(len(df)):\n    new_df[f'code_{i}'] = df.loc[i, 'codes']\n\n# Fill the NaN values with 0\nnew_df = new_df.fillna(0)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new column for each code in the original dataframe\nfor i in range(1, len(df) + 1):\n    new_df[f'code_{i}'] = df.loc[i-1, 'codes']\n\n# Fill the NaN values with 0\nnew_df = new_df.fillna(0)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# Create a new dataframe with the first column as the index\ndf_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new = df.set_index(0)\n\n# Create a new dataframe with the first column as the index\ndf", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "result = df['col1'].apply(lambda x: x[0])\nresult = result.tolist()\nprint(result)\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x[::-1]))).values.tolist()\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x))).values.tolist()\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype(int) / 60).astype(int)\n\n# Create bins\nbins = pd.date_range(start=df['Time'].min(), end=df['Time'].max(), freq='2m')\n\n# Create bins and average values\ndf['Time'] = df['Time'].astype(int)\ndf['Time'] = bins[df['Time']]\n\n# Average values\ndf = df.groupby(df['Time']).mean()\n\n# Interpolate values\ndf = df.resample('2m').mean()\n\n# Convert back to datetime\ndf['Time'] = df['Time'].apply(lambda x: pd.to_datetime(x, unit='m'))\n\n# Convert to datetime format\ndf['Time'] = df['Time'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n\n# Set the time as index\ndf.set_index('Time', inplace=True)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype(int) / 60).astype(int)\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\ndf = df.groupby('Bin').sum()\n\n# Create a new column for the binning\ndf['Bin'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Group by the bin and sum the values\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[np.where(np.isnan(df).any(axis=1))]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.columns[np.all(df.isnull(), axis=1)]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[np.where(np.isnan(df).any(axis=1))].tolist()", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(np.nan, np.nan) if np.isnan(df.iloc[0, i]) and np.isnan(df.iloc[8, i]) else (df.iloc[0, i], df.iloc[8, i]) for i in range(10)]\nresult = list(zip(result, df.columns))\nresult", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.transpose().reset_index(drop=True).transpose()\ndf.columns = df.columns.str.replace('_1','')\ndf.columns = df.columns.str.replace('_2','')\ndf.columns = df.columns.str.replace('_3','')\ndf.columns = df.columns.str.replace('_4','')\ndf.columns = df.columns.str.replace('_5','')\ndf.columns = df.columns.str.replace('_6','')\ndf.columns = df.columns.str.replace('_7','')\ndf.columns = df.columns.str.replace('_8','')\ndf.columns = df.columns.str.replace('_9','')\ndf.columns = df.columns.str.replace('_10','')\ndf.columns = df.columns.str.replace('_11','')\ndf.columns = df.columns.str.replace('_12','')\ndf.columns = df.columns.str.replace('_13','')\ndf.columns = df.columns.str.replace('_14','')\ndf.columns = df.columns.str.replace('_15','')\ndf.columns = df.columns.str.replace('_16','')\ndf.columns = df.columns.str.replace('_17','')\ndf.columns = df.columns.str.replace('_18','')\ndf.columns = df.columns.str.replace('_19','')\ndf.columns = df.columns.str.replace('_20','')\ndf.columns = df.columns.str.replace('_21','')\ndf.columns = df.columns.str.replace('_22','')\ndf.columns = df.columns.str.replace('_23','')\ndf.columns = df.columns.str.replace('_24','')\ndf.columns = df.columns.str.replace('_25','')\ndf.columns = df.columns.str.replace('_26','')\ndf.columns = df.columns.str.replace('_27','')\ndf.columns = df.columns.str.replace('_28','')\ndf.columns = df.columns.str.replace('_29','')\ndf.columns = df.columns.str.replace('_30','')\ndf.columns = df.columns.str.replace('_31','')\ndf.columns = df.columns.str.replace('_32','')\ndf.columns = df.columns.str.replace('_33','')\ndf.columns = df.columns.str.replace('_34','')\ndf.columns = df.columns.str.replace('_35','')\ndf.columns = df.columns.str.replace('_36','')\ndf.columns = df.columns.str.replace('_37','')\ndf.columns = df.columns.str.replace('_38','')\ndf.columns = df.columns.str.replace('_39','')\ndf.columns = df.columns.str.replace('_40','')\ndf.columns = df.columns.str.replace('_41','')\ndf.columns = df.columns.str.replace('_42','')\ndf.columns = df.columns.str.replace('_43','')\ndf.columns = df.columns.str.replace('_44','')\ndf.columns = df.columns.str.replace('_45','')\ndf.columns = df.columns.str.replace('_46','')\ndf.columns = df.columns.str.replace('_47','')\ndf.columns = df.columns.str.replace('_48','')\ndf.columns = df.columns.str.replace('_49','')\ndf.columns = df.columns.str.replace('_50','')\ndf.columns = df.columns.str.replace('_51','')\ndf.columns = df.columns.str.replace('_52','')\ndf.columns = df.columns.str.replace('_53','')\ndf.columns = df.columns.str.replace('_54','')\ndf.columns = df.columns.str.replace('_55','')\ndf.columns = df.columns.str.replace('_56','')\ndf.columns = df.", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.transpose().reset_index(drop=True).transpose()\ndf.columns = df.columns.str.replace('_0', '')\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf.columns = df.columns.str.replace('_6', '')\ndf.columns = df.columns.str.replace('_7', '')\ndf.columns = df.columns.str.replace('_8', '')\ndf.columns = df.columns.str.replace('_9', '')\ndf.columns = df.columns.str.replace('_10', '')\ndf.columns = df.columns.str.replace('_11', '')\ndf.columns = df.columns.str.replace('_12', '')\ndf.columns = df.columns.str.replace('_13', '')\ndf.columns = df.columns.str.replace('_14', '')\ndf.columns = df.columns.str.replace('_15', '')\ndf.columns = df.columns.str.replace('_16', '')\ndf.columns = df.columns.str.replace('_17', '')\ndf.columns = df.columns.str.replace('_18', '')\ndf.columns = df.columns.str.replace('_19', '')\ndf.columns = df.columns.str.replace('_20', '')\ndf.columns = df.columns.str.replace('_21', '')\ndf.columns = df.columns.str.replace('_22', '')\ndf.columns = df.columns.str.replace('_23', '')\ndf.columns = df.columns.str.replace('_24', '')\ndf.columns = df.columns.str.replace('_25', '')\ndf.columns = df.columns.str.replace('_26', '')\ndf.columns = df.columns.str.replace('_27', '')\ndf.columns = df.columns.str.replace('_28', '')\ndf.columns = df.columns.str.replace('_29', '')\ndf.columns = df.columns.str.replace('_30', '')\ndf.columns = df.columns.str.replace('_31', '')\ndf.columns = df.columns.str.replace('_32', '')\ndf.columns = df.columns.str.replace('_33', '')\ndf.columns = df.columns.str.replace('_34', '')\ndf.columns = df.columns.str.replace('_35', '')\ndf.columns = df.columns.str.replace('_36', '')\ndf.columns = df.columns.str.replace('_37', '')\ndf.columns = df.columns.str.replace('_38', '')\ndf.columns = df.columns.str.replace('_39', '')\ndf.columns = df.columns.str.replace('_40', '')\ndf.columns = df.columns.str.replace('_41', '')\ndf.columns = df.columns.str.replace('_42', '')\ndf.columns = df.columns.str.replace('_43', '')\ndf.columns = df.columns.str.replace('_44', '')\ndf.columns = df.columns.str.replace('_45', '')\ndf.columns = df.columns.str.replace('_46', '')\ndf.columns = df.columns.str.replace('_47', '')\ndf.columns = df.columns.str.replace('_48', '')\ndf.columns = df.columns.str.replace('_49', '')\ndf.columns = df.columns.str.replace('_50', '')\ndf.columns = df.columns.str.replace('_51', '')\ndf.columns = df.columns.str.replace('_52', '')\ndf.columns = df.columns.", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\nprint(df)", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\nprint(df)", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "for col in list_of_my_columns:\n    df[col] = df[col].mean()\nprint(df)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by='time')\nprint(result)\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(['time', 'VIM'], ascending=[True, True])", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "dates_to_delete = ['2020-02-17', '2020-02-18']\ndates_to_delete = [pd.to_datetime(date) for date in dates_to_delete]\n\n# Create a new column with the date of the first day of each month\ndf['Month'] = df['Date'].dt.month\n\n# Create a new column with the date of the last day of each month\ndf['Month_end'] = df['Month'].shift(-1)\n\n# Create a new column with the date of the first day of each month\ndf['Month_start'] = df['Month'].shift(1)\n\n# Create a new column with the date of the last day of each month\ndf['Month_end_date'] = df['Month_end'].apply(lambda x: pd.to_datetime(str(x) + '-12-31'))\n\n# Create a new column with the date of the first day of each month\ndf['Month_start_date'] = df['Month_start'].apply(lambda x: pd.to_datetime(str(x) + '-01-01'))\n\n# Create a new column with the date of the last day of each month\ndf['Month_end_date'] = df['Month_end'].apply(lambda x: pd.to_datetime(str(x) + '-12-31'))\n\n# Create a new column with the date of the first day of each month\ndf['Month_start_date'] = df['Month_start'].apply(lambda x: pd.to_datetime(str(x) + '-01-01'))\n\n# Create a new column with the date of the last day of each month\ndf['Month_end_date'] = df['Month_end'].apply(lambda x: pd.to_datetime(str(x) + '-12-31'))\n\n# Create a new column with the date of the first day of each month\ndf['Month_start_date'] = df['Month_start'].apply(lambda x: pd.to_datetime(str(x) + '-01-01'))\n\n# Create a new column with the date of the last day of each month\ndf['Month_end_date'] = df['Month_end'].apply(lambda x: pd.to_datetime(str(x) + '-12-31'))\n\n# Create a new column with the date of the first day of each month\ndf['Month_start_date'] = df['Month_start'].apply(lambda x: pd.to_datetime(str(x) + '-01-01'))\n\n# Create a new column with the date of the last day of each month\ndf['Month_end_date'] = df['Month_end'].apply(lambda x: pd.to_datetime(str(x) + '-12-31'))\n\n# Create a new column with the date of the first day of each month\ndf['Month_start_date'] = df['Month_start'].apply(lambda x: pd.to_datetime(str(x) + '-01-01'))\n\n# Create a new column with the date of the last day of each month\ndf['Month_end_date'] = df['Month_end'].apply(lambda x: pd.to_datetime(str(x) + '-12-31'))\n\n# Create a new column with the date of the first day of each month\ndf['Month_start_date'] = df['Month_start'].apply(lambda x: pd.to_datetime(str(x) + '-01-01'))\n\n# Create a new column with the date of the last day of each month\ndf['Month_end_date'] = df['Month_end'].apply(lambda x: pd.to_datetime(str(x) + '-12-31'))\n\n# Create a new column with the date of the first day of each month\ndf['Month_start_date'] = df['Month_start'].apply(lambda x: pd.to_datetime(str(x) + '-01-01'))\n\n# Create a new column with the date of the last day of each month\ndf['Month_end_date'] = df['Month_end'].apply(lambda x: pd.to_datetime(str(x) + '-12-31'))\n\n# Create a new column with the date of the first day of each month\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Create a new column 'Day' to store the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Day_Num' to store the day of the week as a number\ndf['Day_Num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a new column 'Day_Num_Date' to store the day of the week as a date\ndf['Day_Num_Date'] = df['Day_Num'].apply(lambda x: pd.to_datetime(df['Date'].values[x]))\n\n# Create a", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr[np.abs(corr) > 0.3].stack().reset_index(level=1, drop=True).reset_index(level=0, drop=True)\nresult.columns = ['Col1', 'Col2']\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index + 1\nresult.index = result.index", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr.where(corr > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0, 0)\nresult =", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "result = df.rename(columns={df.columns[-1]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "result = df.rename(columns={df.columns[0]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndef find_frequent(row):\n    return row.value_counts().max()\n\ndef find_freq_count(row):\n    return row.value_counts().max()\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndef find_frequent(row):\n    return row.value_counts().index[0]\n\ndef find_freq_count(row):\n    return row.value_counts().values[0]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    return [row[i] for i in range(len(row)) if row[i] == row.max()]\n\ndef find_freq_count(row):\n    return len([row[i] for i in range(len(row)) if row[i] == row.max()])\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "res = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nres = res.reset_index()\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[res['foo'].notna()]\nres = res[res['bar'].notna()]\nres = res[", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "res = df.groupby([\"id1\",\"id2\"]).mean().reset_index()\nres.columns = [\"id1\",\"id2\",\"foo\",\"bar\"]\nres.loc[res.bar.isnull(), \"bar\"] = 0\nres", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'a_col']]\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum','foo','b_col']]\nresult</code!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = np.delete(x, np.where(np.isnan(x)))\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = x.tolist()\nresult = [i for i in result if not np.isnan(i)]\nresult = np.array(result)\nresult = result.tolist()\nresult", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((a.shape[0], 4))\nb[:, 0] = np.where(a == 0, 1, 0)\nb[:, 1:] = np.where(a == 1, 1, 0)\nb[:, 3] = np.where(a == 3, 1, 0)\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((a.shape[0], a.max() + 1))\nb[np.arange(a.shape[0]), a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n        else:\n            b[i, j] = 0\n", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i, j] = a[i, j] + 1\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\nprint(result)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=1)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=0)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape, order='C')\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a), a.shape, order='C')\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    return max_value, max_index\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2, axis=None)[1], a.shape)\nprint(result)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "z = np.any(np.isnan(a), axis=0)\na = np.delete(a, z, axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = np.delete(a, np.where(np.isnan(a)))\nprint(a)\n", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\nprint(result)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravel_index(np.argmin(a), a.shape)\nresult = min_index\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "min_index = np.unravel_index(np.argmin(a, axis=0), a.shape)\nresult = np.array([[0, 1], [1, 0]])\nprint(min_index)\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.degrees(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.deg2rad(degree))", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * 180 / np.pi\nresult = np.degrees(result)\nprint(result)", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)\nprint(a)\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(example_a, 2))\nprint(f(example_a, 3))\nprint(f(example_a, 4))\nprint(f(example_a, 5))\nprint(f(example_a, 6))\nprint(f(example_a, 7))\nprint(f(example_a, 8))\nprint(f(example_a, 9))\nprint(f(example_a, 10))\nprint(f(example_a, 11))\nprint(f(example_a, 12))\nprint(f(example_a, 13))\nprint(f(example_a, 14))\nprint(f(example_a, 15))\nprint(f(example_a, 16))\nprint(f(example_a, 17))\nprint(f(example_a, 18))\nprint(f(example_a, 19))\nprint(f(example_a, 20))\nprint(f(example_a, 21))\nprint(f(example_a, 22))\nprint(f(example_a, 23))\nprint(f(example_a, 24))\nprint(f(example_a, 25))\nprint(f(example_a, 26))\nprint(f(example_a, 27))\nprint(f(example_a, 28))\nprint(f(example_a, 29))\nprint(f(example_a, 30))\nprint(f(example_a, 31))\nprint(f(example_a, 32))\nprint(f(example_a, 33))\nprint(f(example_a, 34))\nprint(f(example_a, 35))\nprint(f(example_a, 36))\nprint(f(example_a, 37))\nprint(f(example_a, 38))\nprint(f(example_a, 39))\nprint(f(example_a, 40))\nprint(f(example_a, 41))\nprint(f(example_a, 42))\nprint(f(example_a, 43))\nprint(f(example_a, 44))\nprint(f(example_a, 45))\nprint(f(example_a, 46))\nprint(f(example_a, 47))\nprint(f(example_a, 48))\nprint(f(example_a, 49))\nprint(f(example_a, 50))\nprint(f(example_a, 51))\nprint(f(example_a, 52))\nprint(f(example_a, 53))\nprint(f(example_a, 54))\nprint(f(example_a, 55))\nprint(f(example_a, 56))\nprint(f(example_a, 57))\nprint(f(example_a, 58))\nprint(f(example_a, 59))\nprint(f(example_a, 60))\nprint(f(example_a, 61))\nprint(f(example_a, 62))\nprint(f(example_a, 63))\nprint(f(example_a, 64))\nprint(f(example_a, 65))\nprint(f(example_a, 66))\nprint(f(example_a, 67))\nprint(f(example_a, 68))\nprint(f(example_a, 69))\nprint(f(example_a, 70))\nprint(f(example_a, 71))\nprint(f(example_a, 72))\nprint(f(example_a, 73))\nprint(f(example_a, 74))\nprint(f(example_a, 75))\nprint(f(example_a, 76))\nprint(f(example_a, 77))\nprint(f(example_a, 78))\nprint(f(example", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), denominator)\nprint(result)", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.divide(numerator, denominator)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "if denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator // denominator, denominator // denominator)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = np.array([(a[i] + b[i] + c[i]) / 3 for i in range(len(a))])", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(a, np.maximum(b, c))", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.flatten())[::-1]\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a.flatten())[::-1]\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=0)\nresult = np.concatenate((result, np.flip(result[1:], 0)), axis=1", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.concatenate((np.diag(a), np.diag(a)[::-1]))\nprint(result)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(np.arange(a.shape[0])), np.diag(np.arange(a.shape[1])-a.shape[0]+1)), axis=1)\nresult = np.transpose(result)\nresult = np.concatenate((result, np.transpose(a)), axis=1)\nresult = np.transpose(result)\nprint(result)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = np.array([x for x in X.flatten()])", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.empty((r,c), dtype=np.object)\nfor i in range(r):\n    for j in range(c):\n        result[i,j] = X[i,j]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = []\n    for i in range(len(X)):\n        for j in range(len(X[i])):\n            result.append(X[i][j])\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty((X.shape[0], X.shape[1]), dtype=np.object)\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result[i, j] = X[i, j]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.fromstring(mystr, dtype=int, sep='')\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.sum(a.T == a, axis=1).max()\n\n# Get the maximum number of linearly independent vectors\nresult = np.where(np.all(a.T == a, axis=1))\nresult = result[0]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "# calculate the t-statistic\nt_statistic = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.ttest_ind(a, b, equal_var=True, alternative='two-sided')[1]\nprint(p_value)", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "# calculate the t-statistic\nt_statistic = np.mean(a) - np.mean(b)\n\n# calculate the p-value\np_value = scipy.stats.ttest_ind(a, b, equal_var=True, alternative='two-sided')[1]\nprint(p_value)", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the weighted mean and variance\nweighted_mean = (anobs * amean + bnobs * bmean) / (anobs + bnobs)\nweighted_var = (anobs * avar + bnobs * bvar) / (anobs + bnobs)\n\n# calculate the t-value\nt_value = (weighted_mean - bmean) / np.sqrt(weighted_var / anobs + bvar / bnobs)\n\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_value, anobs + bnobs)\n\n# print the p-value\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(A, return_index=True)[1])]\noutput = output[np.in1d(output, np.unique(B, return", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\nprint(c.shape)\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\nprint(c.shape)\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)\nresult = np.sort(result, axis=2)\nresult = np.sort(result, axis=0)\nprint(result)\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, 0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\nprint(result)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\nprint(a)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n\nprint(f(example_a, 2, 66))\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.all(a == a[0], axis=1), axis=1)", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.all(a == a[0], axis=1), axis=0)", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.all(a == a[0], axis=1))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\nprint(result)", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    # calculate the 2D integral over (cosx)^4 + (siny)^2\n    # use Simpson's rule\n    # result = np.sum(np.sum(np.cos(x)**4 + np.sin(y)**2))\n    ### END SOLUTION ###\n    return result\n\n# calculate the 2D integral over the function f\nresult = dblquad(f, example_x, example_y, lambda x: 0, lambda y: 0)\nprint(result)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\n# compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow, high = grades[0], grades[-1]\nwhile high - low > threshold:\n  mid = (low + high) / 2\n  if ecdf(grades)[mid] < threshold:\n    high = mid\n  else:\n    low = mid\n\nprint(low, high)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "randomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = np.random.randint(0, 1, size=(one_ratio*size).astype(int))\nrandomLabel[randomLabel == 1] = np.random.randint(0, 1, size=(one_ratio*size).astype(int))\nrandomLabel = np.random.permutation(randomLabel)\nprint(randomLabel)\n", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.tensor(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = np.array(a)\n", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\nprint(a_tf)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a) + 1\nresult = result.tolist()\nresult = [result[i] for i in result]\nresult = [result[i] for i in result]\nprint(result)\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[::-1][:N]\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "patch_size = 2\na_shape = a.shape\npatch_shape = (a_shape[0] // patch_size, a_shape[1] // patch_size)\nresult = np.array([a[i:i+patch_shape[0], j:j+patch_shape[1]] for i in range(patch_shape[0]) for j in range(patch_shape[1])])\nresult = result[:patch_shape[0], :patch_shape[1]]\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w))\nresult = np.reshape(result, (h, w))\nprint(result)\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "patch_size = 2\na_shape = a.shape\npatch_shape = (a_shape[0] // patch_size, a_shape[1] // patch_size)\nresult = np.array([a[i:i+patch_shape[0], j:j+patch_shape[1]] for i in range(patch_shape[0]) for j in range(patch_shape[1])])\nresult = result[:patch_shape[0], :patch_shape[1]]\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.fromstring(string, sep=' ', dtype=float)\na = a.reshape(2, 2)\nprint(a)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.random.loguniform(min, max, n)", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.log(np.random.uniform(min, max, n))", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.log(np.random.uniform(min, max, n))\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))\nprint(result)\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "def sub2ind(dims, index):\n    return np.asscalar(np.prod(dims)) + np.asscalar(np.prod(index))\n\nresult = sub2ind(dims, index)\nprint(result)\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df.values.dtype)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.sum(a[accmap])\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0], axis=0)\nprint(result)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.minimum.reduce(a[index], axis=0)\nprint(result)\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        z[i, j] = elementwise_function(x[i, j], y[i, j])\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, probabilit)\nprint(result)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "a = np.pad(a, ((0, 0), (low_index, high_index)), 'constant')\nresult = a[1:3, 1:3]\nprint(result)\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[x.real != 0]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_max = [np.max(bin_data[i]) for i in range(len(bin_data))]\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (x - x_min) * (x - x_max)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    x_min = np.clip(x_min, x, x_max)\n    x_max = np.clip(x_max, x, x_min)\n    x_min = np.clip(x_min, x, x_max)\n    x_max = np.clip(x_max, x, x_min)\n    return np.clip(x, x_min, x_max)\n\nprint(smoothclamp(x, x_min, x_max, N))", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "result = np.correlate(a, b, mode='full')\nprint(result)", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(4)])\nresult = np.reshape(result, (4, 15, 5))\nprint(result)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(15)])\nprint(result)", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.unpackbits(np.uint8(a))\nresult = np.reshape(result, (a.shape[0], m))\nresult = np.transpose(result)\nprint(result)", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:m].reshape(-1, m)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = np.where(result == ", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "result = np.unpackbits(np.uint8(a)).reshape(-1, m)\nresult = np.exclusive_xor(result, np.ones((1, m)))\nresult = np.concatenate((np.ones((1, m)), result), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=0)\nresult = np.concatenate((result, np.ones((1, m))), axis=", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc -= 3*\u03c3\n\u03bc += 3*\u03c3\nresult = (\u03bc-3*\u03c3, \u03bc+3*\u03c3)\nprint(result)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc -= 2*\u03c3\n\u03bc += 2*\u03c3\nresult = (\u03bc-2*\u03c3, \u03bc+2*\u03c3)\nprint(result)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    third_std_dev = np.mean(a) + 3*std_dev\n    return third_std_dev, third_std_dev\n\nresult = f(example_a)\nprint(result)\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nresult = np.abs(a - mean) > 2*std_dev\nresult", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data, percentile)\nprint(prob)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[0, 1] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.all(a == np.max(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1), axis=1)\nmask = np.where(mask, True, False)\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "result = np.corrcoef(post, distance)[0, 1]", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.matmul(X, np.transpose(X, (1, 0, 2)))", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((Y.shape[0], Y.shape[1]))\nfor i in range(Y.shape[1]):\n    X[:, i] = np.dot(Y[:, i], Y[:, i].T)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = np.any(a == number)", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = A[~np.isin(A, B)]\nC = A[~np.isin(A, B)]\nEND SOLUTION\n<code>\nprint(C)\n<code>\n# Output:\n# array([3, 3, 3, 4, 5, 6, 7])\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses numpy's not operator to invert the result. This is a very efficient way to solve the problem.\n<code>\n# This solution is very efficient because it uses numpy's isin function to check if a value in B is in A, and then uses num", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[np.in1d(A, B)]", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "C = A[np.logical_or(A==1, A==4)]", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.max(rankdata(a)) - np.min(rankdata(a)) + 1 - np.min(rankdata(a))\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult =", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.max(rankdata(a)) - np.min(a) + 1 - np.min(rankdata(a))\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    result = np.max(a) - np.min(a) + 1 - np.min(a)\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_a))\n# Output: array([7, 6, 3, 1, 3, 6, 3, 1])\n# This solution is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not the same as the rankdata function, but it is a solution to the problem at hand.\n# The solution is to find the maximum value in the array and subtract it from the minimum value, then add 1 to the minimum value, and finally subtract the minimum value from the maximum value.\n# This is the same as the rankdata function, but it is not", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.linalg.norm(X, 1)\nresult = X / l1.reshape(5,1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.linalg.norm(X, ord=2, axis=1)\nresult = np.divide(X, x[:, None])\nprint(result)", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = np.divide(X, x[:, np.newaxis])\nprint(result)", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].str.contains(target)\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "result = np.sqrt(np.sum((a[:, None, :] - a[None, :, :])**2, axis=2))", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "result = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if i == j:\n            continue\n        result[i, j] = np.linalg.norm(a[i] - a[j])\nresult = np.square(result)\nresult = np.sum(result, axis=1)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult = np.sum(result)\nresult = np.sqrt(result)\nresult = np.square(result)\nresult", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "result = np.zeros((dim, dim))\nfor i in range(dim):\n    for j in range(i + 1, dim):\n        result[i, j] = np.linalg.norm(a[:, i] - a[:, j])\n        result[j, i] = result[i, j]\nresult = np.triu(result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 0,", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# Remove zero values\na = a[a!=0]\n\n# Remove adjacent duplicates\na = np.concatenate((a[1:], a[0:-1]))\n\n# Remove all duplicates\nresult = np.unique(a)\n\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# Remove adjacent (before removing) duplicate non-zero value and all the zero value\na = np.where(a[:-1] != 0, a, 0)\n\n# Remove all the zero value\na = np.where(a != 0, a, 0)\n\nresult = a[a != 0]\n\nprint(result)\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "lat_lon_val = np.stack([lat, lon, val], axis=1)\ndf = pd.DataFrame(lat_lon_val, columns=['lat', 'lon', 'val'])\nprint(df)\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "lat_lon_val = np.stack((lat, lon, val), axis=1)\nmax_values = np.max(lat_lon_val, axis=1)\nmax_values = np.reshape(max_values, (-1, 1))\nmax_values = np.repeat(max_values, lat_lon_val.shape[1], axis=1)\nmax_values = np.reshape(max_values, (-1, 1))\n\ndf = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val, 'maximum': max_values})\nprint(df)\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, shape[-1] - size[1] + 1)\n    strides = a.strides + a.strides\n    return np.lib.stride_tricks.as_strided(a, shape, strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, shape[-1] - size[1] + 1)\n    strides = a.strides + a.strides\n    return np.lib.stride_tricks.as_strided(a, shape, strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:-1]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = c in CNTS\nprint(result)", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = c in CNTS\nprint(result)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\ndf", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.matrix([[12.22151125, 0, 0, 0],[0,4.92815942, 0, 0],[0,0,2.06380839,0 ],[0,0,0,0.29766152]])\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a = np.where(np.triu(a, 1) == 0, 0, a)", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\n\n# Calculate the number of days between the start and end dates\nnum_days = (end_date - start_date).days\n\n# Create a linear space of dates\ndates = np.linspace(start_date, end_date, num_days)\n\n# Convert the dates to pandas DatetimeIndex\nresult = pd.DatetimeIndex(dates)\n\nprint(result)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No index found\")\nelse:\n    print(result)", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))\nresult = indices[0]\nif len(result) == 0:\n    result = np.array([])\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "# calculate the coefficients a, b and c\na = np.sum((x - np.mean(x)) * y) / np.sum((x - np.mean(x)) ** 2)\nb = np.mean(y) - a * np.mean(x)\nc = np.mean(y)\n\n# calculate the result\nresult = np.array([a, b, c])\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "# calculate the coefficients\ncoeffs = np.polyfit(x, y, degree)\n# reverse the coefficients to get the order from highest to lowest\nresult = np.flip(coeffs)\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = [0, 1, 2, 3]\ntemp_df = df.apply(lambda x: x - temp_arr[x[0]], axis=1)\ntemp_df = temp_df.apply(lambda x: x.tolist())\nprint(temp_df)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\nprint(result)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "mask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = np.logical_not(mask)\narr[mask] = arr[mask] + 5\n\nprint(arr)\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr = np.where(arr < n1, 0, np.where(arr >= n2, 30, arr + 5))", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "result = np.nonzero(s1 != s2)[0].shape[0]\nprint(result)\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "result = np.nonzero(np.not_equal(s1, s2))[0].shape[0]\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a[1]) and np.array_equal(a[0], a[2]) and np.array_equal(a[1], a[2])\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = np.all(np.isnan(a), axis=1)", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_shape_arr = np.ones(max_shape)\n    # find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "# calculate the maximum shape\nmax_shape = np.max(shape)\n# calculate the padding\npadding = np.array([(max_shape - shape[0]) // 2, (max_shape - shape[1]) // 2])\n# zero pad the array\nresult = np.pad(a, padding, mode='constant')\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3,3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b[:, 0]], axis=1)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=1)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b[:, 2]])", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = np.where(df['a'] > 1, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "# Create a mask that is True for all rows except the first and last row, and all columns except the first and last column\nmask = np.logical_and(np.logical_not(np.all(im[1:-1, 1:-1] == 0, axis=1)), np.logical_not(np.all(im[1:-1, 1:-1] == 0, axis=0)))\n\n# Apply the mask to the entire image\nresult = np.where(mask, im, 0)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "# Find the indices of the non-zero elements\nindices = np.where(A != 0)\n# Get the indices of the maximum value in each column\nmax_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each column\nmax_col_indices = np.argmax(A, axis=0)\n# Get the indices of the maximum value in each row\nmax_row_indices = np.argmax(A, axis=1)\n# Get the indices of the maximum value in each", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask that is True for all elements in the array that are not zero\nmask = np.all(im != 0, axis=1)\n# Create a mask", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "# Create a mask that is True for all rows except the first and last row, and all columns except the first and last column\nmask = np.all(im[1:-1, 1:-1] == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == 0, axis=1)\n\n# Create a mask that is True for all rows and", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\nsns.scatterplot(x, y, label='x-y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "line_styles = ['-', '--', '-.', ':']\nfor line_style in line_styles:\n    y = np.random.randn(10)\n    plt.plot(x, y, line_style)\n\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "line_styles = ['-', '--', '-.', ':']\nfor line_style in line_styles:\n    y = np.random.randn(10)\n    plt.plot(x, y, line_style)\n\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, 'd-')\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.plot(x, y, 'd', linewidth=2, markersize=10, marker='d')\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "highlight_range = np.array([2, 4])\nhighlight_color = 'r'\nhighlight_linewidth = 2\nhighlight_label = 'Highlight'\n\nplt.plot(x, color=highlight_color, linewidth=highlight_linewidth, label=highlight_label)\n\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = x * 2 + 2\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1, 2])\ny = np.array([0, 2, 4])\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    style=\"Gender\",\n    alpha=0.6,\n    height=5,\n    aspect=1,\n)\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style('darkgrid')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('ticks')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('white')\ns", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "sns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize=20, fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"-\", lw=5, markersize=30, color=\"black\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), x, 'r-', lw=5, markersize=30, alpha=0.5)\nplt.plot(range(10), y, 'g-', lw=5, markersize=30, alpha=0.5)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "x_rotated = np.rotate(x, 45)\ny_rotated = np.cos(x_rotated)\nplt.plot(x_rotated, y_rotated, label=\"cos\")\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "x_rotated = np.rotate(x, 45)\ny_rotated = np.cos(x_rotated)\nplt.plot(x_rotated, y_rotated, label=\"cos\")\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.arange(0, 2 * np.pi, 2 * np.pi / 10)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "xlabel = \"X\"\nplt.xlabel(xlabel)\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticks(g.get_xticks(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = np.sort(y)[::-1]\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.arange(0, 1.5, 0.1)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "y_ticks = np.arange(-1, 2, 0.5)\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "x_y_z = np.array([x, y, z]).T\n\n# plot x, then y then z\nplt.figure(figsize=(10, 5))\nplt.plot(x_y_z[0], x_y_z[1], label='x')\nplt.plot(x_y_z[0], x_y_z[2], label='y')\nplt.plot(x_y_z[1], x_y_z[2], label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "x = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.ticklabel_format(style='plain', axis='y')\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "# ax = sns.lineplot(x=x, y=y)\n# ax.plot(x, y, '--')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, ax = plt.subplots(2, 1, sharex=True)\nax[0].plot(x, y1, label='sin(x)')\nax[1].plot(x, y2, label='cos(x)')\nax[0].set_ylabel('sin(x)')\nax[1].set_ylabel('cos(x)')\nax[0].legend()\nax[1].legend()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\nax[0].plot(x, y1, 'r')\nax[0].set_title('sin(x)')\nax[1].plot(x, y2, 'b')\nax[1].set_title('cos(x)')\nax[0].set_xlabel('x')\nax[1].set_xlabel('x')\nax[0].set_ylabel('y1')\nax[1].set_ylabel('y2')\nax[0].set_ylim(-1, 1)\nax[1].set_ylim(-1, 1)\nax[0].set_xlim(0, 2 * np.pi)\nax[1].set_xlim(0, 2 * np.pi)\nax[0].set_yticks([-1, 0, 1])\nax[1].set_yticks([-1, 0, 1])\nax[0].set_xticks([0, np.pi, 2 * np.pi])\nax[1].set_xticks([0, np.pi, 2 * np.pi])\nax[0].set_xlim(0, 2 * np.pi)\nax[1].set_xlim(0, 2 * np.pi)\nax[0].set_yticks([-1, 0, 1])\nax[1].set_yticks([-1, 0, 1])\nax[0].set_xticks([0, np.pi, 2 * np.pi])\nax[1].set_xticks([0, np.pi, 2 * np.pi])\nax[0].set_xlim(0, 2 * np.pi)\nax[1].set_xlim(0, 2 * np.pi)\nax[0].set_yticks([-1, 0, 1])\nax[1].set_yticks([-1, 0, 1])\nax[0].set_xticks([0, np.pi, 2 * np.pi])\nax[1].set_xticks([0, np.pi, 2 * np.pi])\nax[0].set_xlim(0, 2 * np.pi)\nax[1].set_xlim(0, 2 * np.pi)\nax[0].set_yticks([-1, 0, 1])\nax[1].set_yticks([-1, 0, 1])\nax[0].set_xticks([0, np.pi, 2 * np.pi])\nax[1].set_xticks([0, np.pi, 2 * np.pi])\nax[0].set_xlim(0, 2 * np.pi)\nax[1].set_xlim(0, 2 * np.pi)\nax[0].set_yticks([-1, 0, 1])\nax[1].set_yticks([-1, 0, 1])\nax[0].set_xticks([0, np.pi, 2 * np.pi])\nax[1].set_xticks([0, np.pi, 2 * np.pi])\nax[0].set_xlim(0, 2 * np.pi)\nax[1].set_xlim(0, 2 * np.pi)\nax[0].set_yticks([-1, 0, 1])\nax[1].set_yticks([-1, 0, 1])\nax[0].set_xticks([0, np.pi, 2 * np.pi])\nax[1].set_xticks([0, np.pi, 2 * np.pi])\nax[0].set_xlim(0, 2 * np.pi)\nax[1].set_xlim(0, 2 * np.pi)\nax[0].set_yticks([-1, 0, 1])\nax[1].set_yticks([-1, 0, 1])\nax[0].set_xticks([0, np.pi, 2 * np.pi])\nax[1].set_xticks([0, np.", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4, 1))\nplt.grid(axis='x', which='major', linestyle='-', linewidth=0.5, color='black')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.grid(True, axis='y')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.xticks(np.arange(1, 2, 0.1))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "# axes = axes.flatten()\n# for ax in axes:\n#     ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n#     ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n# plt.tight_layout()\n# plt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlim(0, 4)\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.plot(x, y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot(x, y)\nplt.plot", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\nplt.xlabel('X')\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "# Create a joint regression plot\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green')\n\n# Create a distribution plot\nsns.distplot(tips['total_bill'], color='blue')\n\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "# Create a new dataframe with the regression line\nregression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill']})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint_joint_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint_joint_joint_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint_joint_joint_joint_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint_joint_joint_joint_joint_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_joint_joint_joint_joint_joint_joint_joint_joint_joint_joint_joint = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 5))\nfor celltype in df[\"celltype\"].unique():\n    data = df[df[\"celltype\"] == celltype]\n    plt.bar(data[\"s1\"], data[\"s2\"], label=celltype)\n\nplt.xlabel(\"s1\")\nplt.ylabel(\"s2\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 5))\nfor celltype in df[\"celltype\"].unique():\n    data = df[df[\"celltype\"] == celltype]\n    plt.bar(data[\"s1\"], data[\"s2\"], label=celltype)\n\nplt.xlabel(\"s1\")\nplt.ylabel(\"s2\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y over X')\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, 'r')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Line of X over Y')\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, cmap='hot', aspect='auto')\nplt.xlabel('X Labels')\nplt.ylabel('Y Labels')\nplt.xticks(range(len(xlabels)), xlabels, rotation=90)\nplt.yticks(range(len(ylabels)), ylabels)\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\n# copy the code of the above plot and edit it to have legend for all three cruves in the two subplots\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\n\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\n\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "# create a data frame from a, b and c\ndf = pd.DataFrame({'a': a, 'b': b, 'c': c})\n\n# create a scatter plot\nplt.scatter(df['a'], df['b'])\n\n# annotate each data point with correspond numbers in c\nfor i, (x, y) in enumerate(zip(df['a'], df['b'])):\n    plt.annotate(str(c[i]), (x, y), text=str(c[i]))\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(labelspacing=0)\nplt.title('y over x')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(labelcolor='black', title='Legend')\nplt.legend(title='Legend', title_fontsize=14, title_fontweight='bold')\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, alpha=0.5, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, ax1 = plt.subplots(3, 1, figsize=(10, 10))\nax2 = ax1.twinx()\n\n# Create the first subplot\nax1.plot(x, y, 'r')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\n\n# Create the second subplot\nax2.plot(x, y, 'g')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\n\n# Create the third subplot\nax3 = ax1.twinx()\nax3.plot(x, y, 'b')\nax3.set_xlabel('x')\nax3.set_ylabel('y')\n\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Set the legend of the chart\nplt.legend()\n\n# Set the title of the chart\nplt.title('Histogram of x and y')\n\n# Set the x-axis label\nplt.xlabel('x')\n\n# Set the y-axis label\nplt.ylabel('y')\n\n# Show the chart\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 5))\nplt.hist([x, y], bins=10, alpha=0.5, label=['x', 'y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "x = np.linspace(0, 5, 100)\ny = a*x + b\n\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n# create two colormaps\ncmap_x = plt.get_cmap('viridis')\ncmap_y = plt.get_cmap('viridis')\n\n# plot x and y\nax[0].imshow(x, cmap=cmap_x)\nax[1].imshow(y, cmap=cmap_y)\n\n# create a colorbar for each subplot\ncbar_x = fig.colorbar(ax[0].imshow(x, cmap=cmap_x), ax=ax[0])\ncbar_y = fig.colorbar(ax[1].imshow(y, cmap=cmap_y), ax=ax[1])\n\n# set the colorbar's position\ncbar_x.set_position([0.9, 0.15])\ncbar_y.set_position([0.9, 0.35])\n\n# set the colorbar's label\ncbar_x.set_label('x')\ncbar_y.set_label('y')\n\n# set the title of the figure\nfig.set_title('Colorbar in two subplots')\n\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, 'r')\naxs[0].set_title('Y and Z')\naxs[1].plot(a, z, 'g')\naxs[1].set_title('Y and Z')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "x = np.array([point[0] for point in points])\ny = np.array([point[1] for point in points])\n\nplt.plot(x, y)\nplt.yscale('log')\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'r')\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "for line, color in zip(lines, c):\n    plt.plot(*line, color=color)\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.loglog(x, y, 'o-')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-log plot of y over x')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "# Create a histogram of the data\nplt.hist(data, bins=np.arange(0, 21000, 1000), edgecolor='black')\n\n# Set the y tick labels as percentage\ny_tick_labels = np.arange(0, 21000, 1000)\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.0f')\nplt.yticks(y_tick_labels)\n\n# Set the y tick labels as percentage\ny_tick_labels = np.format_float_with_positional_formatting(y_tick_labels, pos_format='%.", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, 'r-', alpha=0.5, linewidth=2)\nplt.plot(x, y, 'r-', alpha=0.5, linewidth=2, marker='o')\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\nfig.legend(loc='upper right')\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=False, sharey=False, figsize=(10, 10))\n\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\n\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['second'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "x_ticks = np.arange(0, 10, 2)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.xticks(rotation=-60)\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(x, alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, 'r')\naxs[0].set_title('First Subplot')\naxs[1].plot(x, y, 'g')\naxs[1].set_title('Second Subplot')\n\nfig.suptitle('Figure')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df['Type A'], df['Type B'])\nplt.xlabel('Type A')\nplt.ylabel('Type B')\nplt.title('Line Chart')\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='|')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none')\nplt.plot(x, y, '|', color='red')\nplt.plot(x, y, '|', color='red', hatch='|')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(x, y, '|', color='red', hatch='|', edgecolor='none')\nplt.plot(", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*', s=100, hatch='v')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "xlim = [1, 5]\nylim = [1, 4]\n\nplt.xlim(xlim)\nplt.ylim(ylim)\n\nplt.imshow(data, extent=xlim + ylim, aspect='auto')\n\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.plot([3, 3], [0, 1], '--', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 5))\nplt.pie(height, labels=labels, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedgeprops={'width': 0.4})\n\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b-')\nplt.grid(True, linestyle='-', linewidth=0.5, color='0.6')\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', axis='x', linestyle='-', linewidth=0.5, color='gray')\nplt.grid(which='major', axis='x', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'r-', alpha=0.5, linewidth=2, edgecolor='b')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color='green')\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Specify the positions of the bars\nblue_bar_pos = np.array([1, 2, 3])\norange_bar_pos = np.array([4, 5, 6])\n\n# Specify the width of the bars\nblue_bar_width = np.array([1, 1, 1])\norange_bar_width = np.array([1, 1, 1])\n\n# Specify the heights of the bars\nblue_bar_height = np.array([23, 25, 17])\norange_bar_height = np.array([19, 18, 14])\n\n# Specify the labels of the bars\nblue_bar_label = np.array(['Blue Bar 1', 'Blue Bar 2', 'Blue Bar 3'])\norange_bar_label = np.array(['Orange Bar 1', 'Orange Bar 2', 'Orange Bar 3'])\n\n# Specify the positions of the bars\nblue_bar_pos = np.array([1, 2, 3])\norange_bar_pos = np.array([4, 5, 6])\n\n# Specify the width of the bars\nblue_bar_width = np.array([1, 1, 1])\norange_bar_width = np.array([1, 1, 1])\n\n# Specify the heights of the bars\nblue_bar_height = np.array([23, 25, 17])\norange_bar_height = np.array([19, 18, 14])\n\n# Specify the labels of the bars\nblue_bar_label = np.array(['Blue Bar 1', 'Blue Bar 2', 'Blue Bar 3'])\norange_bar_label = np.array(['Orange Bar 1', 'Orange Bar 2', 'Orange Bar 3'])\n\n# Specify the positions of the bars\nblue_bar_pos = np.array([1, 2, 3])\norange_bar_pos = np.array([4, 5, 6])\n\n# Specify the width of the bars\nblue_bar_width = np.array([1, 1, 1])\norange_bar_width = np.array([1, 1, 1])\n\n# Specify the heights of the bars\nblue_bar_height = np.array([23, 25, 17])\norange_bar_height = np.array([19, 18, 14])\n\n# Specify the labels of the bars\nblue_bar_label = np.array(['Blue Bar 1', 'Blue Bar 2', 'Blue Bar 3'])\norange_bar_label = np.array(['Orange Bar 1', 'Orange Bar 2', 'Orange Bar 3'])\n\n# Specify the positions of the bars\nblue_bar_pos = np.array([1, 2, 3])\norange_bar_pos = np.array([4, 5, 6])\n\n# Specify the width of the bars\nblue_bar_width = np.array([1, 1, 1])\norange_bar_width = np.array([1, 1, 1])\n\n# Specify the heights of the bars\nblue_bar_height = np.array([23, 25, 17])\norange_bar_height = np.array([19, 18, 14])\n\n# Specify the labels of the bars\nblue_bar_label = np.array(['Blue Bar 1', 'Blue Bar 2', 'Blue Bar 3'])\norange_bar_label = np.array(['Orange Bar 1', 'Orange Bar 2', 'Orange Bar 3'])\n\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, ax1 = plt.subplots()\n\nax1.plot(x, y, 'r-')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('y over x')\nax1.legend(['y over x'], loc='upper left')\n\nax2 = ax1.twinx()\nax2.plot(a, z, 'g-')\nax2.set_ylabel('z')\nax2.set_title('z over a')\nax2.legend(['z over a'], loc='upper right')\n\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "x = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.axis([0, 10, 0, 10])\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(data=df, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"species\", sharey=False, col_wrap=3, height=5, aspect=0.8, kind=\"bar\")\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(10, 10))\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0.", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$')\nplt.bold_title()\nplt.show()\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.plot(x, y, label='Line')\nplt.legend(labelspacing=0.1)\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.plot(x, y, label='Line')\nplt.legend(loc='upper right', handlelength=0.3)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.plot(x, y, marker=\"*\", label=\"Line\")\nplt.legend(loc='best')\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.imshow(data, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.plot(x, y)\nplt.title('Figure 1')\nplt.bold('Figure')\nplt.bold('1')\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x[::-1], y[::-1])\nplt.plot(x", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n\n# Create a scatter plot of x over y\nplt.scatter(x, y, s=100)\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='r', edgecolor='k')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax in axs.flat:\n    ax.plot(x, y)\n    ax.set_xlim([0, 10])\n    ax.set_ylim([0, 10])\n\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 10, 2), edgecolor='black')\nplt.xlabel('x')\nplt.ylabel('Count')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 6))\nplt.errorbar(x, y, yerr=error, fmt='o', color='blue', label='y')\nplt.fill_between(x, y-error, y+error, alpha=0.2, color='blue', label='y error')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 0], 'r-')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "for i in range(len(box_position)):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], fmt='o', color=c[i])\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(a, z, label='Z')\naxs[0].set_title('Y')\naxs[1].set_title('Z')\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].set_ylabel('Z')\naxs[0].set_ylabel('Y')\naxs[0].legend()\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs.flat:\n    ax.plot(x, y)\n    ax.set_xticks(x)\n    ax.set_yticks(y)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('y over x')\n\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1)\nplt.matshow(d, fignum=1, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nplt.table(\n    df,\n    col_width=[1, 1, 1, 1],\n    col_names=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    row_names=df.index,\n    cell_contents=df.values,\n    bbox=[0, 0, 1, 1],\n    cell_linewidth=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,\n    edgecolor=\"black\",\n    linewidth=1,\n    align=\"center\",\n    cell_align=\"center\",\n    cell_padding=1,", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "# Create a new dataframe for the group \"Fat\"\ndf_fat = df[df['diet'] == 'Fat']\n# Create a new dataframe for the group \"No Fat\"\ndf_no_fat = df[df['diet'] == 'No Fat']\n\n# Create a scatter plot for the group \"Fat\"\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_fat, x='time', y='pulse', hue='kind', col='diet', style='kind', palette='deep')\nplt.title('Group: Fat')\n\n# Create a scatter plot for the group \"No Fat\"\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_no_fat, x='time', y='pulse', hue='kind', col='diet', style='kind', palette='deep')\nplt.title('Group: No Fat')\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_kind_time = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_pulse_time = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_diet_time_kind = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_time_kind = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_time_pulse = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_time_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_time_kind_diet = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_time_pulse_diet = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_time_diet_kind = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_time_kind_diet_pulse = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_time_pulse_diet_kind = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_time_diet_kind_pulse = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_time_kind_diet_pulse_diet = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_time_pulse_diet_kind_diet = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_time_diet_kind_pulse_diet_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_time_kind_diet_pulse_diet_diet_kind = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df_time_pulse_diet_kind_diet_pulse_diet = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\nnew_df_time_diet_kind_pulse_diet_pulse_diet_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\nnew_df_time_kind_diet_pulse_diet_pulse_diet_diet_kind = df[[\"time\", \"kind\"]]\n\n# Create a new", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.8, kind=\"scatter\")\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Plot of y over x')\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='upper left')\nplt.show()\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 6))\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(x=\"species\", y=\"bill_length_mm\", hue=\"sex\", data=df, jitter=0.1, edgecolor=\"black\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "\n# Create a FaceGrid\nfacegrid = sns.FaceGrid(df, row_names=df[\"b\"], col_names=df[\"a\"], margin_right=0.05, margin_left=0.05, margin_top=0.05, margin_bottom=0.05, height=1.5, width=1.5, aspect=1.5, share_all=True)\n\n# Create a list of xticks\nxticks = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n#", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_zlim([0, 1])\nax.set_view_angle(30, 30)\nax.set_view_angle(100, 50)\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.hidexticks()\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[1, 1], height_ratios=[1, 1])\n\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x[i, j])\n        ax.axis('off')\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nprint(x.numpy()) # prints 1\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nprint(x.numpy()) # prints 114514\n", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    # Create a 2D tensor with the same shape as the labels\n    result = tf.zeros([len(example_labels), 10])\n\n    # Iterate over the labels and set the corresponding value in the result tensor\n    for i, label in enumerate(example_labels):\n        result[i, label] = 1\n\n    return result\n\nprint(f())\n### END SOLUTION ###\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nresult = tf.cast(result, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return [[i, i+1, i+2]]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return tf.concat([input, input + 1, input + 2], axis=0)\n\ndef map_func(input):\n    return tf.reshape(f(input), [-1])\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(map_func)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\nlengths = tf.constant([4, 3, 5, 2])\ntotal_length = 8\n\nmask = tf.ones((total_length, 1))\nmask = tf.concat([mask, tf.zeros((total_length, 1))], axis=1)\n\nmask = tf.where(lengths > 0, tf.ones((total_length, 1)), mask)\n\nmask = tf.reshape(mask, (total_length, 1))\n\nresult = tf.where(lengths > 0, tf.ones((total_length, 1)), mask)\n\nprint(result)\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\nlengths = tf.constant([4, 3, 5, 2])\ntotal_length = 8\n\nmask = tf.ones_like(lengths)\nmask = tf.sequence_length_mask(lengths, total_length)\n\nresult = tf.where(mask, tf.zeros_like(lengths), tf.ones_like(lengths))\n\nprint(result)\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "lengths = tf.constant([4, 3, 5, 2])\nmask = tf.sequence_length_mask(lengths, max_length=8)\nmask = tf.cast(mask, tf.float32)\nresult = tf.where(mask, tf.ones_like(mask), tf.zeros_like(mask))\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths),", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "lengths = tf.constant([4, 3, 5, 2])\ntotal_length = 8\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1s and 0s with the number of 0s equal to the length of the lengths tensor\nmask = tf.cast(tf.range(total_length) < lengths, tf.float32)\n\n# Create a mask of 1", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([a, b], axis=1)\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.reshape(result, [2, -1])\nresult = tf.transpose(result)\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    # Use tf.stack to stack the two tensors along the first axis\n    # Then use tf.reshape to reshape the result to a 2D tensor\n    # Finally use tf.transpose to transpose the result\n    # result = tf.transpose(tf.reshape(tf.stack([a, b]), [2, -1]))\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "a = tf.reshape(a, (50, 100, 512))\nresult = a", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "a = tf.reshape(a, (50, 100, 1, 512))\nresult = a", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "a = tf.reshape(a, (1, 50, 100, 1, 512))\nresult = a", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.math.prod(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.reciprocal(A)\nprint(result)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\n# calculate the L2 distance d(A,B) element-wise\nl2_distance = tf.square(a - b)\n\n# element-wise reduce which returns the sum of the elements in each row\nresult = tf.reduce_sum(l2_distance, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "# calculate the L2 distance column-wise\nl2_distance = tf.square(a - b)\n\n# reduce by column\nresult = tf.reduce_sum(l2_distance, axis=0)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    # calculate the L2 distance d(A,B) element-wise\n    square_diff = tf.square(tf.subtract(A, B))\n    # reduce the square difference by row\n    result = tf.reduce_sum(square_diff, axis=0)\n    # return the result\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = tf.gather_nd(x, tf.stack([y, z], axis=1))", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = x[row, col]\nresult = m\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    return tf.gather_nd(x, tf.stack([y, z], axis=1))\n    ### END SOLUTION ###\n\nresult = f(example_x, example_y, example_z)\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "result = tf.einsum('ijk,ikl->ijl', A, B)", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "result = tf.einsum('ijk,ikl->ijl', A, B)", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "x = tf.constant(x)\nresult = tf.strings.unicode_transpose(x)\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    result = tf.strings.unicode_transpose(x)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "x_nonzero = tf.where(x != 0, x, tf.zeros_like(x))\nresult = tf.reduce_sum(x_nonzero, axis=-1) / tf.reduce_sum(x_nonzero, axis=-1, keepdims=True)\nresult = tf.where(x == 0, tf.zeros_like(x), result)\nprint(result)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "x_nonzero = tf.where(x != 0, x, tf.zeros_like(x))\nresult = tf.reduce_mean(x_nonzero, axis=-1)\nresult = tf.where(x != 0, result, tf.zeros_like(x))\nprint(result)\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": " ###\n    # get the number of non-zero entries in the last dimension\n    num_non_zero = tf.cast(tf.count_nonzero(x, axis=-1), tf.float32)\n    # divide by the number of non-zero entries\n    num_non_zero = tf.where(num_non_zero > 0, num_non_zero, tf.ones_like(num_non_zero))\n    # average the last dimension\n    result = tf.reduce_sum(x, axis=-1) / num_non_zero\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\n# Create two random matrices A and B\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Calculate the sum of the matrix multiplication\nresult = tf.reduce_sum(tf.matmul(A, B))\n\n# Print the result\nprint(result)\n\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "result = tf.argmax(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "result = tf.argmax(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    # Find the maximum value in each row\n    max_values = tf.argmax(a, axis=1)\n    # Convert the maximum value to the index of the maximum value in each row\n    result = tf.where(a == max_values[:, None], max_values, tf.constant(0))\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_a))\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=0) + 1", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n\n# Save the model\nmodel.save('my_model')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "# generate 10 random integers from the uniform distribution\nresult = tf.random.uniform([10], minval=1, maxval=4, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "seed_x = 10\ntf.random.set_seed(seed_x)\n\nrandom_integers = tf.random.uniform([114], minval=2, maxval=5, dtype=tf.int32)\nresult = tf.reshape(random_integers, [114, 1])\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": " ###\n    tf.random.set_seed(seed_x)\n    random_integers = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n    return random_integers\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "print(tf.__version__)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "# Fit the data using a logarithmic polynomial\nx_log = np.log(x)\ny_log = np.log(y)\n\n# Fit the data using a polynomial of order 1\nx_poly = np.polyfit(x_log, y_log, 1)\n\n# Convert the coefficients back to the original scale\nA_log = np.exp(x_poly[0])\nB_log = np.exp(x_poly[1])\n\n# Save the result in a numpy array\nresult = np.array([A_log, B_log])\n\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the data using a polynomial of order 2\nA, B = np.polyfit(x, y, 2)\nresult = np.array([A, B])\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, A, B, C):\n    return A * np.exp(B * x) + C\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array([result[0], result[1], result[2]])\nprint(result)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha\nresult", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c - 2) ** 2 + (3 * a - b - c) ** 2 + sin(b) + cos(b) + 4)\n\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.t.pdf(z_scores, 1)", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = scipy.stats.norm.cdf(z_scores, mu, sigma)\np_values = 1 - p_values", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = scipy.stats.zscore(p_values)\nprint(z_scores)", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(stddev, mu)\nexpected_value = dist.mean()\nmedian = dist.median()\nprint(expected_value, median)", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    result = sA * sB\n    return result\n\nresult = f(example_sA, example_sB)\nprint(result)\n### END SOLUTION ###\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid of points\nx, y, z = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of V values\nv = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of points\npoints = np.array([[27.827, 18.530, -30.417], [24.002, 17.759, -24.782],\n                   [22.145, 13.687, -33.282], [17.627, 18.224, -25.197],\n                   [29.018, 18.841, -38.761], [24.834, 20.538, -33.012],\n                   [26.232, 22.327, -27.735], [23.017, 23.037, -29.23],\n                   [28.761, 21.565, -31.586], [26.263, 23.686, -32.766]])\n\n# Create a 3D grid of V values\nv = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of points\npoints = np.array([[27.827, 18.530, -30.417], [24.002, 17.759, -24.782],\n                   [22.145, 13.687, -33.282], [17.627, 18.224, -25.197],\n                   [29.018, 18.841, -38.761], [24.834, 20.538, -33.012],\n                   [26.232, 22.327, -27.735], [23.017, 23.037, -29.23],\n                   [28.761, 21.565, -31.586], [26.263, 23.686, -32.766]])\n\n# Create a 3D grid of V values\nv = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of points\npoints = np.array([[27.827, 18.530, -30.417], [24.002, 17.759, -24.782],\n                   [22.145, 13.687, -33.282], [17.627, 18.224, -25.197],\n                   [29.018, 18.841, -38.761], [24.834, 20.538, -33.012],\n                   [26.232, 22.327, -27.735], [23.017, 23.037, -29.23],\n                   [28.761, 21.565, -31.586], [26.263, 23.686, -32.766]])\n\n# Create a 3D grid of V values\nv = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of points\npoints = np.array([[27.827, 18.530", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid of points\nx, y, z = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of V values\nv = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of points to interpolate\nx_interp, y_interp, z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request points\nrequest_x, request_y, request_z = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_points = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of V values for the request points\nrequest_v = np.random", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, reshape=False)\nxrot = x0 + np.cos(np.radians(angle))*x0\nyrot = y0 + np.sin(np.radians(angle))*y0\nxrot, yrot = np.round_(xrot), np.round_(yrot)\nprint(xrot, yrot)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = np.diagonal(M.toarray())", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, 'uniform')\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # Use scipy's kstest function to perform the K-S test\n    # The result should be a tuple of two elements: the K-S test statistic and the p-value\n    # The K-S test statistic is the value of the K-S test, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated by a random distribution.\n    # The K-S test statistic is the mean of the observed times and the expected times, and the p-value is the probability that the observed data was generated", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, 'uniform')\nprint(result.pvalue)", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = c1.concatenate(c2)\nprint(Feature)\n", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "# Create a pairwise distance matrix\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# Find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# Find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# Create a mapping from points2 to points1\nmapping = np.zeros((points2.shape[0], points1.shape[0]))\nmapping[np.arange(points2.shape[0]), min_indices] = points2\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(distances[np.arange(points2.shape[0]), min_indices])\n# Find the total distance of the mapping\ntotal_distance = np.sum(", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "# Create a pairwise distance matrix\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# Find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# Find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# Create a mapping from points2 to points1\nmapping = np.zeros((points2.shape[0], points1.shape[0]))\nmapping[np.arange(points2.shape[0]), min_indices] = points2\n# Find the total Manhattan distance\ntotal_distance = np.sum(np.abs(points1 - points2))\n# Find the total Manhattan distance for the mapping\nmapping_distance = np.sum(np.abs(points1 - mapping))\n# Print the result\nprint(mapping)\nprint(total_distance)\nprint(mapping_distance)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b = b.toarray()\nb[0, 0] = 0\nb = sparse.csr_matrix(b)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "result = np.where(img > threshold)\nresult = np.unique(result, axis=0)\nresult = np.unique(result, axis=1)\nresult = np.array([result.shape[0], result.shape[1]])\nprint(result)\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "result = np.where(img < threshold, 1, 0)\nresult = ndimage.measurements.label(result)\nresult = np.sum(result)\nprint(result)\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use numpy's argmax function to find the maximum value in the image\n    # Then use numpy's where function to find the position of the maximum value\n    # Then use numpy's unique function to find the unique positions of the maximum value\n    # Finally, return the number of unique positions\n    ### END SOLUTION ###\n    return np.unique(np.where(img > threshold)).shape[0]\n\nprint(f(example_img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "# Find the regions of cells which value exceeds a given threshold\nregion_values = img[img > threshold]\n\n# Find the center of mass of such regions\ncenter_of_mass = np.mean(region_values, axis=(1, 2))\n\n# Find the distance between the center of mass of such regions and the top left corner\ndistance = np.sqrt((center_of_mass[0] - 0)**2 + (center_of_mass[1] - 0)**2)\n\n# Output the distances as a list\nresult = distance.tolist()\nprint(result)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M.T - np.diag(np.diag(M.toarray()))\nM = M + M", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    # make the matrix symmetric\n    sA = sA + sA.T\n    # make the matrix symmetric\n    sA = (sA + sA.T)/2\n    ### END SOLUTION ###\n    return sA\n\n# call the function\nsA = f(example_sA)\nprint(sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\n# Create a mask of the square array\nmask = np.ones((32, 32))\nmask[10:-10, 10:-10] = 0\n\n# Erase the square array\neroded = scipy.ndimage.erosion(square, mask)\n\n# Dilate the eroded array\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Find the indices of the non-zero elements in the dilated array\nindices = np.where(dilated != 0)\n\n# Set the values at the indices to 0\nsquare[indices] = 0\n\n# Print the result\nprint(square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask of the square\nmask = np.zeros_like(square, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erase the square\nsquare[mask] = 0\n\n# Dilate the square\ndilated_square = scipy.ndimage.binary_dilation(square, iterations=1)\n\n# Create a mask of the dilated square\nmask = np.zeros_like(dilated_square, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erase the dilated square\ndilated_square[mask] = 0\n\n# Print the square\nprint(dilated_square)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_value = np.max(col.toarray())\nmin_value = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Get the median and mode of the column\nmedian = np.median(col.toarray())\nmode = np.mode(col.toarray())\n\nprint(median)\nprint(mode)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *a):\n    return sum(a[i]*np.cos(np.pi*x/(2*tau)*i) for i in range(1, degree+1))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\narray_1d = example_array.flatten()\n# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.pdist(array_1d, 'euclidean')\n# Convert the distances back tometres\ndistances_metres = distances * np.mean(example_array)\n# Convert the distances to a 2D array\ndistances_2d = np.array(distances_metres).reshape(len(array_1d), len(array_1d))\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distance = np.min(distances_2d)\n# Print the result\nprint(min_distance)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Calculate pairwise Manhattan distances between all regions\ndistances = scipy.spatial.distance.cdist(example_array, example_array, metric='cityblock')\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distance = np.min(distances, axis=1)\n# Create a 2D array with the minimum distance separating the nearest edges of each raster patch\nresult = np.stack((np.arange(distances.shape[0]), np.arange(distances.shape[0]), min_distance), axis=1)\nprint(result)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate pairwise Euclidean distances between all regions\n    # to obtain the minimum distance separating the nearest edges of each raster patch\n    # Convert distances from cells tometres\n    # result = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n    #                   [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n    #                   [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n    # # Convert distances from cells tometres\n    # result = np.multiply(result, 1000)\n    # # Return the result\n    # return result\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_array))\n", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(x.shape[1]):\n    tck = interpolate.splrep(x[:, i], y[:, i], s=4)\n    result[i] = interpolate.splev(x_val, tck)\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array([38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0])\nx2 = np.array([39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8])\nx3 = np.array([34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0])\nx4 = np.array([34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8])\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\nprint(statistic)\nprint(critical_values)\nprint(significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\n\nresult = ss.anderson_ksamp(x1, x2)\nprint(result)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(df['A']) # keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = np.roll(df['B'], 3, axis=0)\ndf['AC'] = np.roll(df['C'], 3, axis=0)\ndf['BC'] = np.roll(df['B'], 3, axis=0) * np.roll(df['C'], 3, axis=0)\n\ndf['AB'] = np.roll(df['AB'], 3, axis=0)\ndf['AC'] = np.roll(df['AC'], 3, axis=0)\ndf['BC'] = np.roll(df['BC'], 3, axis=0)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B'] == 2, df['C'], np.nan)\n\ndf['AB'] = np.where(df['B'] == 2, df['B'], np.nan)\ndf['AC'] = np.where(df['C'] == 1, df['C'], np.nan)\ndf['BC'] = np.where(df['B", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nresult = is_csr_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n\nresult = is_lil_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\nprint(result)", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores)[1]", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n    ### END SOLUTION ###\n\nprint(f(example_pre_course_scores, example_during_course_scores))\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "kurtosis_result = np.mean(a)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interpolated = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interpolated(x, y)\nprint(result)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a 2D array with the values of s and t\n    s_t = np.array([s, t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for s, t in s_t]).reshape(-1, 1)\n    # Create a 2D array with the values of the function f\n    f_values = np.array([f(s, t) for", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\nextraPoints = np.array(extraPoints)\n# Get the regions of the voronoi tessallation\nregions = vor.regions\n# Create a list of indices for the regions of the voronoi tessallation\nindices = np.array([i for i in range(len(regions)) if regions[i] not in vor.vertices])\n# Create a list of indices for the extra points\nextraPointsIndices = np.array([i for i in range(len(extraPoints)) if np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are in the voronoi tessallation\nextraPointsInVoronoi = np.array([i for i in range(len(extraPoints)) if np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are not in the voronoi tessallation\nextraPointsNotInVoronoi = np.array([i for i in range(len(extraPoints)) if not np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are in the voronoi tessallation\nextraPointsInVoronoi = np.array([i for i in range(len(extraPoints)) if np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are not in the voronoi tessallation\nextraPointsNotInVoronoi = np.array([i for i in range(len(extraPoints)) if not np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are in the voronoi tessallation\nextraPointsInVoronoi = np.array([i for i in range(len(extraPoints)) if np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are not in the voronoi tessallation\nextraPointsNotInVoronoi = np.array([i for i in range(len(extraPoints)) if not np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are in the voronoi tessallation\nextraPointsInVoronoi = np.array([i for i in range(len(extraPoints)) if np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are not in the voronoi tessallation\nextraPointsNotInVoronoi = np.array([i for i in range(len(extraPoints)) if not np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are in the voronoi tessallation\nextraPointsInVoronoi = np.array([i for i in range(len(extraPoints)) if np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are not in the voronoi tessallation\nextraPointsNotInVoronoi = np.array([i for i in range(len(extraPoints)) if not np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are in the voronoi tessallation\nextraPointsInVoronoi = np.array([i for i in range(len(extraPoints)) if np.all(extraPoints[i] == vor.vertices, axis=1)])\n# Create a list of indices for the extra points that are not in the voronoi tessallation\nextraPointsNotInVoronoi = np.array([i for i in range(len(extraPoints)) if not np.all(extra", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "# Create a list of points that are not in the voronoi cell\nextraPointsNotInCell = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi cell\nextraPointsInCell = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellNotInCell = [point for point in extraPoints if point in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi cell and not in the voronoi cell\nextraPointsNotInCellNotInCell = [point for point in extraPoints if point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are in the voronoi cell and not in the voronoi cell and not in the voronoi cell\nextraPointsInCellNotInCellNotInCell = [point for point in extraPoints if point in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell\nextraPointsNotInCellNotInCellNotInCell = [point for point in extraPoints if point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices and point not in vor.regions and point not in vor.vertices]\n# Create a list of points that are not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the voronoi cell and not in the vor", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "# Find the maximum size of the vectors\nmax_size = max([len(v) for v in vectors])\n\n# Create a sparse matrix with zeros\nresult = sparse.csc_matrix((max_size, max_size), dtype=np.float64)\n\n# Fill the sparse matrix with the vectors\nfor i, v in enumerate(vectors):\n    result[i, :len(v)] = v\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant')\nb = np.roll(b, 1, axis=1)\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[i, j] for i, j in zip(row, column)]\nprint(result)\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, :, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u,o2,x)\nprint(prob)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(x = 2.5, u = 1, o2 = 3))\n# Output: 0.5\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a 2D DCT matrix\ndct_matrix = np.zeros((N, N))\nfor i in range(N):\n    for j in range(N):\n        dct_matrix[i, j] = sf.dct(np.ones(N))[i, j]\n\n# Normalize the DCT matrix\ndct_matrix = dct_matrix / np.sqrt(np.sum(dct_matrix**2))\n\n# Print the DCT matrix\nprint(dct_matrix)\nresult = dct_matrix", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "result = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result)\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "result = np.array([scipy.stats.binom.pmf(i, N, p) for i in range(N+1)]).reshape(N+1, N+1)\nresult = np.triu(result) + np.triu(result, k=1).transpose()\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 1, 1, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 3, 1, result)\nresult = np.where(result == 4, 0, result)\nresult = np.where(result == 5, 1, result)\nresult = np.where(result == 6, 0, result)\nresult = np.where(result == 7, 1, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 9, 1, result)\nresult = np.where(result == 10, 0, result)\nresult = np.where(result == 11, 1, result)\nresult = np.where(result == 12, 0, result)\nresult = np.where(result == 13, 1, result)\nresult = np.where(result == 14, 0, result)\nresult = np.where(result == 15, 1, result)\nresult = np.where(result == 16, 0, result)\nresult = np.where(result == 17, 1, result)\nresult = np.where(result == 18, 0, result)\nresult = np.where(result == 19, 1, result)\nresult = np.where(result == 20, 0, result)\nresult = np.where(result == 21, 1, result)\nresult = np.where(result == 22, 0, result)\nresult = np.where(result == 23, 1, result)\nresult = np.where(result == 24, 0, result)\nresult = np.where(result == 25, 1, result)\nresult = np.where(result == 26, 0, result)\nresult = np.where(result == 27, 1, result)\nresult = np.where(result == 28, 0, result)\nresult = np.where(result == 29, 1, result)\nresult = np.where(result == 30, 0, result)\nresult = np.where(result == 31, 1, result)\nresult = np.where(result == 32, 0, result)\nresult = np.where(result == 33, 1, result)\nresult = np.where(result == 34, 0, result)\nresult = np.where(result == 35, 1, result)\nresult = np.where(result == 36, 0, result)\nresult = np.where(result == 37, 1, result)\nresult = np.where(result == 38, 0, result)\nresult = np.where(result == 39, 1, result)\nresult = np.where(result == 40, 0, result)\nresult = np.where(result == 41, 1, result)\nresult = np.where(result == 42, 0, result)\nresult = np.where(result == 43, 1, result)\nresult = np.where(result == 44, 0, result)\nresult = np.where(result == 45, 1, result)\nresult = np.where(result == 46, 0, result)\nresult = np.where(result == 47, 1, result)\nresult = np.where(result == 48, 0, result)\nresult = np.where(result == 49, 1, result)\nresult = np.where(result == 50, 0, result)\nresult = np.where(result == 51, 1, result)\nresult = np.where(result == 52, 0, result)\nresult = np.where(result == 53, 1, result)\nresult = np.where(result == 54, 0, result)\nresult = np.where(result == 55, 1, result)\nresult = np.where(result == 56, ", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x))\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "result = df.apply(lambda x: (x, stats.zscore(x)), axis=1)\nresult = result.reset_index()\nresult.columns = ['probegenes','data','sample1','sample2','sample3']\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=0)\n\n# round to 3 decimal places\nzscore = np.round(zscore, 3)\n\n# create a new dataframe with zscore and data\nresult = pd.DataFrame(np.c_[df.values, zscore], columns=np.append(df.columns, 'zscore'))\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y, x = np.mgrid[0:6, 0:6]\nmid = np.array([[0, 0], [6, 6]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.mgrid[0:shape[0], 0:shape[1]]\nmid = np.array([[5., 4., 3., 3., 4., 5.],\n               [4., 3., 2., 2., 3., 4.],\n               [3., 2., 1., 1., 2., 3.],\n               [3., 2., 1., 1., 2., 3.],\n               [4., 3., 2., 2., 3., 4.],\n               [5., 4., 3., 3., 4., 5.]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.mgrid[0:shape[0], 0:shape[1]]\n    center = np.array([shape[0] / 2, shape[1] / 2])\n    return distance.cdist(np.dstack((y, x)), center)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\n\nx0 = np.array([2, 3, 1, 4, 20])\nout = scipy.optimize.minimize(residual, x0, args=(a, y))\nprint(out.x)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((y - func(x)) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, bounds=[x_lower_bounds, x_true])\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) if 0 < t < 2*np.pi else 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.hstack([sa, sb])", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\ndef integrand(x, c):\n    return 2*c*x\n\nc = 5\nlow = 0\nhigh = 1\n\nresult, error = scipy.integrate.quad(lambda x: integrand(x, c), low, high)\nprint(result)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    def eqn(x):\n        return 2*x*c\n    result, error = scipy.integrate.quad(eqn, low, high)\n    return result\n\n# define the array of c values\nc = array([5, 10, 15])\n# calculate the array of solutions\nI = array([f(c=c[n]) for n in range(len(c))])\nprint(I)\n<code>\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.reshape(1, 1)\nV_reshaped = V.reshape(1, 1)\nV_reshaped += x\nV = V_reshaped.toarray()\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V += x", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V += x\nV += y", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    column /= length\n    # update the original column of the matrix\n    sa[:, col] = column\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    column /= length\n    # update the original column of the matrix\n    sa[:, col] = column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = np.where(a > 0, 1, 0)", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# calculate the distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest element to each centroid\nresult = np.argmin(distances, axis=1)\nprint(result)", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the minimum distance for each cluster\nclosest_cluster_index = np.argmin(distances, axis=1)\n# find the closest point to each cluster\nclosest_points = data[closest_cluster_index]\n# assign the closest points to the corresponding cluster\nresult = scipy.spatial.distance.cdist(closest_points, centroids)\n", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# calculate the distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the k-closest elements to each centroid\nclosest_indices = np.argpartition(distances, k, axis=0)[:k]\n# find the index of the k-th closest element to each centroid\nresult = np.argpartition(closest_indices, k, axis=0)[:k]\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result = fsolve(lambda x: eqn(x, x, b), x0=x)\n    print(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result = fsolve(eqn, x0=x, args=(a,))\n    print(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Define the distribution function\ndef bekkers_distribution(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Use scipy.stats.kstest to test the distribution function\nresult, pvalue = sp.stats.kstest(sample_data, bekkers_distribution, args=(estimated_a, estimated_m, estimated_d))\nprint(result, pvalue)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Estimate the parameters of the function\nestimated_a, estimated_m, estimated_d = 1,1,1\n\n# Create the sample data\nsample_data = np.array([1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9])\n\n# Define the function\ndef bekkers(x, a, m, d):\n    return a*np.exp(-(x**(1/3) - m)**2/(2*d**2))*x**(-2/3)\n\n# Estimate the parameters of the function\nestimated_a, estimated_m, estimated_d = sp.optimize.curve_fit(bekkers, sample_data, bekkers(sample_data, estimated_a, estimated_m, estimated_d))\n\n# Create the distribution function\ndistribution_function = bekkers(sample_data, estimated_a, estimated_m, estimated_d)\n\n# Perform the K-S test\nresult = stats.kstest(distribution_function, 'norm')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def rolling_integral(x, y):\n    return integrate.trapz(x, y)\n\nrolling_integral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral)\nprint(rolling_integral_df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "x = np.array([(2,2), (1,2), (2,3), (3,2), (2,1)])\ny = np.array([5,7,8,10,3])\neval = np.array([(2.7, 2.3)])\n\n# Create a grid data object\ngrid_data = scipy.interpolate.GridDataPattern(x, y, eval)\n\n# Interpolate the value at the evaluation points\nresult = grid_data.interpolate(eval)\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "weights = sciopt.maximum_likelihood_solver(a.values, np.arange(0, 12), method='maximum likelihood')\nweights = weights.x", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "result = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nprint(result)\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "# find the indices of the elements that are less or equal to the neighbouring n elements\nindices = np.where((arr[:-n] <= arr[n:]) | (arr[:-n] <= arr[-n:]))\n# find the indices of the elements that are less or equal to the neighbouring n elements forwards\nindices_forward = np.where((arr[n:] <= arr[-n:]) | (arr[n:] <= arr[-n-1:-n]))\n# find the indices of the elements that are less or equal to the neighbouring n elements backwards\nindices_backward = np.where((arr[-n:] <= arr[-n-1:-n]) | (arr[-n:] <= arr[-n-1:-n-1]))\n# combine the indices of the elements that are less or equal to the neighbouring n elements forwards and backwards\nindices_combined = np.concatenate((indices_forward[0], indices_backward[0]))\n# sort the indices of the elements that are less or equal to the neighbouring n elements forwards and backwards\nindices_combined = np.sort(indices_combined)\n# sort the indices of the elements that are less or equal to the neighbouring n elements forwards\nindices_forward = np.sort(indices_forward[0])\n# sort the indices of the elements that are less or equal to the neighbouring n elements backwards\nindices_backward = np.sort(indices_backward[0])\n# combine the indices of the elements that are less or equal to the neighbouring n elements forwards and backwards\nindices_combined = np.concatenate((indices_forward, indices_backward))\n# sort the indices of the elements that are less or equal to the neighbouring n elements forwards and backwards\nindices_combined = np.sort(indices_combined)\nresult = indices_combined", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "# find the absolute indices of the elements\nabs_indices = np.where(arr == np.max(arr, axis=1))\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] > abs_indices[0] - n)\nbackward_indices = np.where(abs_indices[0] < abs_indices[0] + n)\n# combine the forward and backward indices\nresult = np.concatenate((forward_indices[0], backward_indices[0]))\n# sort the result\nresult = np.sort(result)\n# remove the duplicates\nresult = np.unique(result)\n# print the result\nprint(result)\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "num_cols = df.select_dtypes(include=np.number).columns\noutlier_cols = [col for col in num_cols if np.abs(stats.zscore(df[col])) > 3]\ndf = df[outlier_cols]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], data['target']))\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], data['target_names']))\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + data['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    data_df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                            columns=np.append(data['feature_names'], data['target']))\n    return data_df\n    ### END SOLUTION ###\n\n# Test the function\nprint(solve(data))\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a list of unique elements in the column\nunique_elements = np.unique(df['Col3'])\n\n# Create a new dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'], values=unique_elements, drop_first=True)\n\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n\n# Add the original column back\ndf_out = pd.concat([df['Col1'], df_out], axis=1)\n\n# Print the output dataframe\nprint(df_out)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# Create a list of unique names\nnames = np.unique(np.concatenate(df['Col3'].values))\n\n# Create a new dataframe with the unique names as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(names))), columns=names)\n\n# Iterate over the rows in the dataframe\nfor i in range(df.shape[0]):\n    # Iterate over the columns in the dataframe\n    for j in range(df.shape[1]):\n        # If the column is not the first one\n        if j != 0:\n            # Iterate over the names\n            for k in range(len(names)):\n                # If the name exists in the column\n                if names[k] in df.values[i, j]:\n                    # Set the value to 1\n                    df_out.values[i, j] = 1\n\n# Print the dataframe\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Create a list of unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a dataframe with one-hot-encoded values for each unique element\ndf_out = pd.get_dummies(df.iloc[:, -1], values=unique_elements, drop_first=True)\n\n# Add the one-hot-encoded columns to the original dataframe\ndf = pd.concat([df, df_out], axis=1)\n\n# Drop the last column\ndf = df.drop(columns=[df.columns[-1]])\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# Create a list of unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with the unique elements as columns\nout_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over the rows of the dataframe\nfor i in range(df.shape[0]):\n    # Iterate over the unique elements\n    for j in range(unique_elements.shape[0]):\n        # If the element exists in the last column, set the value to 1\n        if unique_elements[j] in df.iloc[i, -1]:\n            out_df.iloc[i, j] = 1\n\n# Print the output dataframe\nprint(out_df)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# Create a list of unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with the unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over the last column and one-hot-encode the elements\nfor i in range(new_df.shape[1]):\n    new_df.iloc[:, i] = np.where(new_df.iloc[:, i] == unique_elements[i], 1, 0)\n\n# Remove the last column\nnew_df = new_df.drop(new_df.columns[-1], axis=1)\n\n# Add the original last column back\nnew_df = np.concatenate((new_df, df.iloc[:, -1].values.reshape(-1, 1)), axis=1)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\nproba = np.array([1 / (1 + np.exp(-x)) for x in svmmodel.decision_function(x_test)])\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\nproba = np.array([1 / (1 + np.exp(-x)) for x in proba])\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "# Convert the transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transformed output\nnew_df = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the new dataframe to a numpy array\nnew_array = new_df.to_numpy()\n\n# Convert the numpy array to a sparse matrix\nnew_sparse = csr_matrix(new_array)\n\n# Print the new sparse matrix\nprint(new_sparse)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "# Convert the transform_output to a pandas DataFrame\ntransform_output_df = pd.DataFrame(transform_output)\n\n# Create a new DataFrame with the original data and the transform_output_df\ndf = pd.concat([df_origin, transform_output_df], axis=1)\n\n# Print the new DataFrame\nprint(df)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # transform the transform_output back to a dataframe\n    transform_output_df = pd.DataFrame(transform_output.toarray())\n    # merge the original dataframe and the transform_output_df\n    result = pd.concat([df, transform_output_df], axis=1)\n    ### END SOLUTION ###\n    return result\n\n# call the function\nresult = solve(df_origin, transform_output)\nprint(result)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline\nprint(clf)", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline steps\nprint(clf.steps)", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a new step\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.append(new_step)\n\n# Print the new pipeline\nprint(clf)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps[1] = ('reduce_poly_2', PolynomialFeatures())\nclf.steps[2] = ('dim_svm_2', PCA())\nclf.steps[3] = ('sVm_233_2', SVC())\nprint(clf.steps)", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\n# Insert ('t1919810', PCA()) right before 'svdm'\nclf.steps.insert(1, ('t1919810', PCA()))\n\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# Define the parameters to be searched\nparamGrid = {\n    \"learning_rate\": [0.01, 0.1, 0.001],\n    \"max_depth\": [3, 4, 5],\n    \"n_estimators\": [100, 200, 300]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the GridSearchCV\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, iid=False)\n\n# Fit the GridSearchCV\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the best iteration\nbest_iteration = gridsearch.best_iteration_\n\n# Get the best estimator\nbest_estimator = gridsearch.best_estimator_\n\n# Get the best score on the test set\ntest_score = best_estimator.score(testX, testY)\n\n# Get the best prediction on the test set\nbest_prediction = best_estimator.predict(testX)\n\n# Print the best parameters\nprint(\"Best parameters: \", best_params)\n\n# Print the best score\nprint(\"Best score: \", best_score)\n\n# Print the best iteration\nprint(\"Best iteration: \", best_iteration)\n\n# Print the best estimator\nprint(\"Best estimator: \", best_estimator)\n\n# Print the best score on the test set\nprint(\"Test score: \", test_score)\n\n# Print the best prediction on the test set\nprint(\"Best prediction: \", best_prediction)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\n# Define the grid of parameters to search\nparamGrid = {\n    \"learning_rate\": [0.01, 0.1, 0.001],\n    \"max_depth\": [3, 4, 5],\n    \"n_estimators\": [100, 200, 300]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the grid search parameters\nfit_params = {\"early_stopping_rounds\": 42,\n              \"eval_metric\": \"mae\",\n              \"eval_set\": [[testX, testY]]}\n\n# Perform the grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\ngridsearch.fit(trainX, trainY, **fit_params)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the prediction\nc = gridsearch.predict(testX)\n\n# Print the best parameters, best score and prediction\nprint(\"Best parameters: \", best_params)\nprint(\"Best score: \", best_score)\nprint(\"Prediction: \", c", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\nproba = np.mean(proba, axis=0)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_pred = scaler.inverse_transform(scaled[:, 0])\n    # return t_pred\n    return t_pred\n\n# solve(data, scaler, scaled)\n<!! END SOLUTION !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\nproba = grid_search.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef solve():\n    tfidf = TfidfVectorizer(preprocessor=preprocess)\n    tfidf.fit(['This is a test'])\n    print(tfidf.transform(['This is a test']))\n\nsolve()\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# example variable\ntext = \"This is a Test.\"\n\n# create a data frame\ndf = pd.DataFrame([text], columns=['text'])\n\n# apply tfidf\ntfidf.fit(df['text'])\n\n# print the tfidf vector\nprint(tfidf.transform(df['text']))\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Apply preprocessing.scale to the data\nscaled_data = preprocessing.scale(data)\n\n# Create a new DataFrame with the scaled data\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to the original index\nscaled_data_df.index = data.index\n\n# Set the columns of the scaled DataFrame to the original columns\nscaled_data_df.columns = data.columns\n\n# Set the index of the scaled DataFrame to", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a new DataFrame with the scaled values\nscaled_data = preprocessing.scale(data)\nscaled_data = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n\n# Save the scaled data to a new DataFrame\ndf_out = scaled_data\n\n# Print the scaled data\nprint(df_out)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "column_names = np.array([X.columns[i] for i in np.where(clf.feature_importances_ > 0.05)[0]])\nprint(column_names)", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    center_p = centers[p]\n    samples = X[np.argmin(np.linalg.norm(X - center_p, axis=1))]\n    return samples\n    ### END SOLUTION ###\n\nsamples = get_samples(p, X, km)\nprint(samples)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# print the converted data\nprint(X_train)\n\n# create GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n\n# print the model\nprint(clf)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = np.hstack((X_train, y_train.reshape(-1, 1)))\n\n# create GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\n\n# Create a SVM regression model\nmodel = svm.SVR(kernel='rbf')\n\n# Fit the model\nmodel.fit(X, y)\n\n# Predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"poly\", degree=2)\n# fit the model\nsvm_reg.fit(X, y)\n# predict the values\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarities_of_queries(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    document_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.dot(query_matrix, document_matrix.T) / (np.linalg.norm(query_matrix, axis=1)[:, None] * np.linalg.norm(document_matrix, axis=1))\n    return cosine_similarities_of_queries", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarities_of_queries(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    document_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.dot(query_matrix, document_matrix.T) / (np.linalg.norm(query_matrix, axis=1)[:, None] * np.linalg.norm(document_matrix, axis=1))\n    return cosine_similarities_of_queries", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    # calculate the tf-idf of the documents\n    matrix = tfidf.transform(documents)\n    # calculate the tf-idf of the queries\n    query_matrix = tfidf.transform(queries)\n    # calculate the cosine similarity of the queries with the documents\n    cosine_similarities = np.dot(query_matrix, matrix.T) / (np.linalg.norm(query_matrix, 1) * np.linalg.norm(matrix, 1))\n    # return the cosine similarity of the queries with the documents\n    return cosine_similarities\n\n# test the function\nqueries = [\"query 1\", \"query 2\", \"query 3\", \"query 4\", \"query 5\"]\ndocuments = [\"document 1\", \"document 2\", \"document 3\", \"document 4\", \"document 5\"]\nprint(solve(queries, documents))\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to one hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Create a data frame from the new features\nnew_features_df = pd.DataFrame(new_features, columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Use sklearn's feature selection utilities to select the features\nselected_features = sklearn.feature_selection.select_features(new_features_df, threshold=0.01)\n\n# Print the selected features\nprint(selected_features)\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "new_f = np.array([[1 if feature in sample else 0 for feature in f[i]] for i in range(len(f))])\nnew_f = np.array(new_f, dtype=int)\nnew_f = new_f.reshape(-1, 1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.zeros((new_f.shape[0], 1))), axis=1)\nnew_f = np.concatenate((new_f, np.ones((new_f.shape[0], 1))), axis", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to one hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Create a data frame from the new features\nnew_features_df = pd.DataFrame(new_features, columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Use sklearn's feature selection utilities to select the features\nselected_features = sklearn.feature_selection.select_features(new_features_df, 2)\n\n# Print the selected features\nprint(selected_features)\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[0], 1)), features_array), axis=1)\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    features_array = np.concatenate((np.ones((features_array.shape[", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "new_features = np.array([[1 if feature in sample else 0 for feature in features] for sample in features])\nnew_features = np.array(new_features, dtype=int)\nprint(new_features)", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the data matrix\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model to the data\nclustering.fit(data_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model\nclustering.fit(dist_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the similarity matrix\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model\nclustering.fit(simM)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix using the distance matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# Perform hierarchical clustering with 2 clusters\ncluster_labels = fcluster(linkage_matrix, 2, criterion='max')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a linkage matrix using the distance matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maximum')\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a linkage matrix using the distance matrix\nlinkage_matrix = linkage(simM, 'ward')\n\n# Perform hierarchical clustering with 2 clusters\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maximum')\ncluster_labels = cluster_labels.tolist()\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\nbox_cox_transformer = BoxCoxTransformer()\nbox_cox_data = box_cox_transformer.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\nbox_cox_data = transformer.transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\n\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\nyeo_johnson_data = transformed_data", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\nyeo_johnson_data = transformed_data", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(analyzer='word')\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\ntransformed_text = np.where(transformed_text == 0, '', transformed_text)\ntransformed_text = np.where(transformed_text == 1, '', transformed_text)\ntransformed_text = np.where(transformed_text == 2, '', transformed_text)\ntransformed_text = np.where(transformed_text == 3, '', transformed_text)\ntransformed_text = np.where(transformed_text == 4, '', transformed_text)\ntransformed_text = np.where(transformed_text == 5, '', transformed_text)\ntransformed_text = np.where(transformed_text == 6, '', transformed_text)\ntransformed_text = np.where(transformed_text == 7, '', transformed_text)\ntransformed_text = np.where(transformed_text == 8, '', transformed_text)\ntransformed_text = np.where(transformed_text == 9, '', transformed_text)\ntransformed_text = np.where(transformed_text == 10, '', transformed_text)\ntransformed_text = np.where(transformed_text == 11, '', transformed_text)\ntransformed_text = np.where(transformed_text == 12, '', transformed_text)\ntransformed_text = np.where(transformed_text == 13, '', transformed_text)\ntransformed_text = np.where(transformed_text == 14, '', transformed_text)\ntransformed_text = np.where(transformed_text == 15, '', transformed_text)\ntransformed_text = np.where(transformed_text == 16, '', transformed_text)\ntransformed_text = np.where(transformed_text == 17, '', transformed_text)\ntransformed_text = np.where(transformed_text == 18, '', transformed_text)\ntransformed_text = np.where(transformed_text == 19, '', transformed_text)\ntransformed_text = np.where(transformed_text == 20, '', transformed_text)\ntransformed_text = np.where(transformed_text == 21, '', transformed_text)\ntransformed_text = np.where(transformed_text == 22, '', transformed_text)\ntransformed_text = np.where(transformed_text == 23, '', transformed_text)\ntransformed_text = np.where(transformed_text == 24, '', transformed_text)\ntransformed_text = np.where(transformed_text == 25, '', transformed_text)\ntransformed_text = np.where(transformed_text == 26, '', transformed_text)\ntransformed_text = np.where(transformed_text == 27, '', transformed_text)\ntransformed_text = np.where(transformed_text == 28, '', transformed_text)\ntransformed_text = np.where(transformed_text == 29, '', transformed_text)\ntransformed_text = np.where(transformed_text == 30, '', transformed_text)\ntransformed_text = np.where(transformed_text == 31, '', transformed_text)\ntransformed_text = np.where(transformed_text == 32, '', transformed_text)\ntransformed_text = np.where(transformed_text == 33, '', transformed_text)\ntransformed_text = np.where(transformed_text == 34, '', transformed_text)\ntransformed_text = np.where(transformed_text == 35, '', transformed_text)\ntransformed_text = np.where(transformed_text == 36, '', transformed_text)\ntransformed_text = np.where(transformed_text == 37, '', transformed_text)\ntransformed_text = np.where(transformed_text == 38, '', transformed_text)\ntransformed_text = np.where(transformed_text == 39, '', transformed_text)\ntransformed_text = np.where(transformed_text == 40, '', transformed_text)\ntransformed_text = np.where(transformed_text == 41, '', transformed_text)\ntransformed_text = np.where(transformed_text == 42, '', transformed_text)\ntransformed_", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop([-1], axis=1), dataset[-1], test_size=0.2, random_state=42)\n\n# Define x and y\nx_train_all = x_train\ny_train_all = y_train\nx_test_all = x_test\ny_test_all = y_test\n\n# Define x and y\nx_train = x_train_all\ny_train = y_train_all\nx_test = x_test_all\ny_test = y_test_all\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features and the target\nx = data.drop(columns=['target'])\ny = data['target']\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop([-1], axis=1), dataset[-1], test_size=0.2, random_state=42)\n\n# Define x and y\nx_train_all = x_train\ny_train_all = y_train\nx_test_all = x_test\ny_test_all = y_test\n\n# Define x and y\nx_train = x_train_all\ny_train = y_train_all\nx_test = x_test_all\ny_test = y_test_all\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": " ###\n    # split the data into training and testing sets\n    train_data, test_data = np.split(data, [int(data.shape[0]*0.8)])\n    # define x and y\n    x_train = train_data[:, :-1]\n    y_train = train_data[:, -1]\n    x_test = test_data[:, :-1]\n    y_test = test_data[:, -1]\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION ###\n\n# call the function\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "mse_values = df['mse'].values\nlabels = KMeans(n_clusters=2, n_init=10).fit(mse_values.reshape(-1, 1)).predict(mse_values.reshape(-1, 1))\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# Reshape the data to 2D array\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans object with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit the model to the data\nkmeans.fit(X)\n\n# Get the labels of the clusters\nlabels = kmeans.predict(X)\n\n# Get the centroid of the clusters\ncentroids = kmeans.cluster_centers_\n\n# Print the centroids\nprint(centroids)\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "# Create a LinearSVC object\nsvc = LinearSVC(penalty='l1', loss='l1', dual=False)\n# Fit the model\nsvc.fit(X, y)\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(np.array(svc.coef_).flatten() != 0)]\nprint(selected_feature_names)", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "# Create a LinearSVC object\nsvc = LinearSVC(penalty='l1', loss='l2', max_iter=10000, random_state=42)\n# Fit the model\nsvc.fit(X, y)\n# Get the selected features\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\nprint(selected_feature_names)", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    svm = LinearSVC(penalty='l1', loss='l1', dual=False, tol=1e-5, max_iter=10000)\n    svm.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[svm.get_support()]\n    return selected_feature_names\n    ### END SOLUTION ###\n\n# Test the function\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nfeature_names = vectorizer.get_feature_names()\nfeature_names.sort()\nprint(feature_names)\nBEGIN SOLUTION\n<code>\nfeature_names = vectorizer.get_feature_names()\nfeature_names.sort()\nprint(feature_names)\nEND SOLUTION\nEND SOLUTION\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nBEGIN SOLUTION\n<code>\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\nEND SOLUTION\nEND SOLUTION\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vocabulary = ['Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX']\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nfeature_names = np.sort(feature_names)\nX = np.sort(X.toarray(), axis=1)\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "slopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\nslopes = slopes.reshape(-1,1)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "slopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\nslopes = slopes.reshape(-1,1)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed data\nprint(df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ### END SOLUTION ###\n    return df\n\ndef load_data():\n    df = pd.DataFrame({\n        'Sex': ['male', 'female', 'male', 'female'],\n        'Age': [20, 25, 30, 35],\n        'PassengerId': [1, 2, 3, 4],\n        'Survived': [0, 1, 0, 1]\n    })\n    return df\n\ntransformed_df = Transform(load_data())\nprint(transformed_df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    a_transformed = scaler.transform(a)\n    return a_transformed\n    ### END SOLUTION ###\n\n# Test the function\na = np.array([[1, 2], [3, 4]])\nprint(Transform(a))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close, ma50, ma100, ma200])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array([['dsa', '2'], ['sato', '3']])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:,:-1].astype(float)\ny = dataframe.iloc[:,-1]\nlogReg = LogisticRegression()\nlogReg.fit(X,y)\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the data to numpy array\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:, -1]\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model\nlogReg.fit(X[:None], y)\n# Predict the class of the last row\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "train_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n\ntrain_dataframe = train_dataframe.", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "train_dataframe = features_dataframe[features_dataframe['date'] <= np.mean(features_dataframe['date'])]\ntest_dataframe = features_dataframe[features_dataframe['date'] > np.mean(features_dataframe['date'])]\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=0.2, random_state=42)\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": " ###\n    # sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(by='date')\n    # calculate the number of rows to be in the test set\n    test_size = len(features_dataframe) * 0.8\n    # split the dataframe into train and test sets\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=test_size, shuffle=False)\n    # return the train and test sets\n    return train_dataframe, test_dataframe\n    ### END SOLUTION ###\n\n# call the function\nsolve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "cols = ['X2', 'X3']\nfor month in df['Month'].unique():\n    month_data = df[df['Month'] == month][cols]\n    month_data_scale = scaler.fit_transform(month_data)\n    df.loc[df['Month'] == month, cols + '_scale'] = month_data_scale\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "cols = myData.columns[2:4]\nfor month in myData['Month'].unique():\n    month_data = myData[myData['Month'] == month][cols]\n    month_data = month_data.values.reshape(-1, 1)\n    month_data = scaler.fit_transform(month_data)\n    month_data = month_data.reshape(-1, 1)\n    myData.loc[myData['Month'] == month, cols] = month_data\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = GridSearch_fitted.fit(X, Y)\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = GridSearch_fitted.fit(X, Y)\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_\nfull_results.best_score_\nfull_results.best_params_\nfull_results.grid_scores_", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# Load your data\ndata = np.load('data.npy')\n\n# Fit the model\nmodel = IsolationForest()\nmodel.fit(data)\n\n# Save the model\nnp.save('sklearn_model', model)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "# tokenizing each description\ndescription_tokens = df['description'].apply(lambda x: x.split())\n\n# transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(description_tokens)\n\n# calculate cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = np.ones((len(df), len(df)))\nfor i in range(len(df)):\n    for j in range(len(df)):\n        if i == j:\n            cosine_similarity_matrix[i, j] = 1\n        else:\n            cosine_similarity_matrix[i, j] = 1 - sklearn.metrics.cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])\n\n# print the similarity matrix\nprint(cosine_similarity_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\noptim.param_groups[0]['lr'] = 0.01\noptim.step()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n# Define the learning rate scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define a function to update the learning rate\ndef update_learning_rate(optimizer, epoch):\n    if epoch > 10:\n        scheduler.step()\n        optimizer.param_groups[0]['lr'] = 0.001\n\n# Update the learning rate after each epoch\nfor epoch in range(100):\n    update_learning_rate(optim, epoch)\n    # Perform your training steps\n    # ...\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\noptim.zero_grad()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define the learning rate schedule\nlr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Set the initial learning rate\nlr = 0.05\n\n# Loop over epochs\nfor epoch in range(100):\n    # Update the learning rate\n    lr_schedule.step()\n\n    # Get the current learning rate\n    lr = lr_schedule.get_lr()\n\n    # Print the learning rate\n    print(f\"Epoch {epoch}, Learning rate: {lr}\")\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the word2vec model to a PyTorch embedding layer\nembedding_weights = torch.tensor(word2vec.wv.vectors)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], -1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    embedding_weights = word2vec.wv\n    embedding_weights_np = embedding_weights.toarray()\n    embedding_weights_tensor = torch.tensor(embedding_weights_np)\n    return embedding_weights_tensor\n    ### END SOLUTION ###\n\n# Test the function\nembedding_weights_tensor = get_embedded_input(input_Tensor)\nprint(embedding_weights_tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x)\nprint(px)", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "x = np.random.rand(6,6)\npx = pd.DataFrame(x)\nprint(px)", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "A_logical = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical] # Throws error\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "A_log = torch.ByteTensor([1, 1, 0]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "A_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    ### END SOLUTION ###\n    return C\n\ndef load_data():\n    A_log = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_log, B\n\nA_log, B = load_data()\nC = solve(A_log, B)\nprint(C)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "A_log = torch.ByteTensor([0, 0, 1]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Convert the index to a numpy array\nidx_array = idx.numpy()\n\n# Use numpy index_select to get the required columns\nC = np.array([B[i, idx_array[i]] for i in range(B.shape[0])])\n\n# Convert the numpy array back to a PyTorch tensor\nC = torch.from_numpy(C)\n\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    # Convert the numpy array to a torch tensor\n    t = torch.tensor(a, dtype=torch.float32)\n    ### END SOLUTION ###\n    return t\n\n# Test the function\na = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=np.object)\n\nt = Convert(a)\nprint(t)\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones_like(lens)\nmask[lens == 0] = 0\nmask[lens == 1] = 1\nmask[lens == 2] = 0\nmask[lens == 3] = 1\nmask[lens == 4] = 0\nmask[lens == 5] = 1\nmask[lens == 6] = 0\nmask[lens == 7] = 1\nmask[lens == 8] = 0\nmask[lens == 9] = 1\nmask[lens == 10] = 0\nmask[lens == 11] = 1\nmask[lens == 12] = 0\nmask[lens == 13] = 1\nmask[lens == 14] = 0\nmask[lens == 15] = 1\nmask[lens == 16] = 0\nmask[lens == 17] = 1\nmask[lens == 18] = 0\nmask[lens == 19] = 1\nmask[lens == 20] = 0\nmask[lens == 21] = 1\nmask[lens == 22] = 0\nmask[lens == 23] = 1\nmask[lens == 24] = 0\nmask[lens == 25] = 1\nmask[lens == 26] = 0\nmask[lens == 27] = 1\nmask[lens == 28] = 0\nmask[lens == 29] = 1\nmask[lens == 30] = 0\nmask[lens == 31] = 1\nmask[lens == 32] = 0\nmask[lens == 33] = 1\nmask[lens == 34] = 0\nmask[lens == 35] = 1\nmask[lens == 36] = 0\nmask[lens == 37] = 1\nmask[lens == 38] = 0\nmask[lens == 39] = 1\nmask[lens == 40] = 0\nmask[lens == 41] = 1\nmask[lens == 42] = 0\nmask[lens == 43] = 1\nmask[lens == 44] = 0\nmask[lens == 45] = 1\nmask[lens == 46] = 0\nmask[lens == 47] = 1\nmask[lens == 48] = 0\nmask[lens == 49] = 1\nmask[lens == 50] = 0\nmask[lens == 51] = 1\nmask[lens == 52] = 0\nmask[lens == 53] = 1\nmask[lens == 54] = 0\nmask[lens == 55] = 1\nmask[lens == 56] = 0\nmask[lens == 57] = 1\nmask[lens == 58] = 0\nmask[lens == 59] = 1\nmask[lens == 60] = 0\nmask[lens == 61] = 1\nmask[lens == 62] = 0\nmask[lens == 63] = 1\nmask[lens == 64] = 0\nmask[lens == 65] = 1\nmask[lens == 66] = 0\nmask[lens == 67] = 1\nmask[lens == 68] = 0\nmask[lens == 69] = 1\nmask[lens == 70] = 0\nmask[lens == 71] = 1\nmask[lens == 72] = 0\nmask[lens == 73] = 1\nmask[lens == 74] = 0\nmask[lens == 75] = 1\nmask[lens == 76] = 0\nmask[lens == 77] = 1\nmask[lens == 78] = 0\nmask[lens == 79] = 1\nmask[lens == 80] = 0\nmask[lens == 81] = 1\nmask[lens == 82] = 0\nmask[lens == 83] = 1\nmask[lens == 84] = 0\nmask[lens == 85] = 1\nmask[lens == 86] = 0\nmask[lens == 87] = 1\nmask[lens == 88] = 0\nmask[lens == 89] = 1\nmask[lens == 90] = 0\nmask[l", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "lens = torch.tensor([1, 9, 3, 5])\nmask = torch.ones_like(lens)\nmask[lens - 1] = 0\nmask = torch.where(lens > 0, 1, 0)\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.arange(lens.numel()).unsqueeze(0).expand(lens.numel(), -1) < lens.unsqueeze(1)\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    max_len = np.max(lens)\n    mask = np.zeros((len(lens), max_len))\n    for i in range(len(lens)):\n        mask[i, lens[i]-1:] = 1\n    return mask\n    ### END SOLUTION ###\n\nlens = np.array([3, 5, 4])\nmask = get_mask(lens)\nprint(mask)\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with diagonal elements\ndiag_ele = np.ones((index_in_batch, 1))\n\n# Create a 2D tensor with drag_ele\ndrag_ele = np.ones((index_in_batch, 1))\n\n# Create a 2D tensor with index_in_batch * drag_ele\nindex_in_batch_drag_ele = index_in_batch * drag_ele\n\n# Create a 3D tensor with index_in_batch_drag_ele\nTensor_3D = torch.tensor(index_in_batch_drag_ele)\n\n# Print the 3D tensor\nprint(Tensor_3D)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    # Create a 2D tensor with the same shape as t\n    diag_ele = np.ones(t.shape)\n    # Create a 3D tensor with the same shape as t\n    result = np.tile(diag_ele, (t.shape[0], 1, 1))\n    # Convert the 2D tensor to a 3D tensor\n    result = torch.from_numpy(result)\n    ### END SOLUTION ###\n    return result\n\n# Test the function\nt = torch.randn(5, 5)\nprint(Convert(t))\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = a.reshape(1, 11)\nb = b.reshape(1, 11)\nab = np.vstack((a, b))\nab = torch.tensor(ab)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = a.reshape(1, -1)\nb = b.reshape(1, -1)\nab = np.vstack((a, b))\nab = torch.tensor(ab)\nprint(ab.shape)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    # stack the tensors a and b along the first dimension\n    # then stack the result with b along the first dimension\n    # finally stack the result with a along the first dimension\n    # return the result\n    ### END SOLUTION ###\n    return np.stack([a, b, b], axis=0)\n\n# test the function\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[7, 8, 9], [10, 11, 12]])\nprint(solve(a, b))\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[ : , lengths : , : ] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[ : , lengths : , : ] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\nprint(new_tensors)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.tensor(lt)\n    return tensor_of_tensors\n    ### END SOLUTION ###\n\n# Call the function\nlist_of_tensors = [torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = torch.gather(t, 1, idx.reshape(-1, 1)).squeeze()", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = t[np.arange(t.shape[0]), idx]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "result = x[ids]", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "result = x[ids.reshape(-1,1), :]", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask for the selected indices\nmask = (ids == 1).reshape(-1, 1)\n\n# Gather the selected slices\nresult = x[mask].reshape(-1, 2)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "softmax_output = softmax_output.reshape(-1, 3)\ny = np.argmax(softmax_output, axis=1)\ny = y.reshape(-1, 1)\ny = y.astype(int)\nprint(y)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "softmax_output = softmax_output.reshape(-1, 1)\ny = np.argmax(softmax_output, axis=1)\ny = y.reshape(-1, 1)\nprint(y)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "softmax_output = softmax_output.reshape(-1, 1)\ny = np.argmax(softmax_output, axis=1)\ny = y.reshape(-1, 1)\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    # Find the maximum value in each row\n    max_values = np.argmax(softmax_output, axis=1)\n    # Create a tensor with the maximum value\n    max_tensor = torch.tensor(max_values, dtype=torch.int64)\n    # Return the tensor\n    return max_tensor\n    ### END SOLUTION ###\n\n# Test the function\nsoftmax_output = np.array([[0.2, 0.1, 0.7], [0.6, 0.2, 0.2], [0.1, 0.8, 0.1]])\nprint(solve(softmax_output))\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "y = torch.argmax(softmax_output, 1)\ny = y.view(-1, 1)\nreturn y", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "images = Variable(torch.from_numpy(images))\nlabels = Variable(torch.from_numpy(labels))\nloss = cross_entropy2d(images, labels)\nprint(loss.item())\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = np.sum(A == B)\ncnt_equal = cnt_equal.item()\nprint(cnt_equal)\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = np.sum(A == B)\ncnt_equal = cnt_equal.item()\nprint(cnt_equal)\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = np.sum(A != B)\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = np.sum(A == B)\n    ### END SOLUTION ###\n    return cnt_equal\n\nA = np.random.rand(1000, 1)\nB = np.random.rand(1000, 1)\n\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "cnt_equal = np.sum(A[-x:] == B[-x:])", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = np.sum(A[-x:] != B[-x:])", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensors_31.append(a[:, :, i:i+10, :, :])\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (31, 1, 10, 40, 1)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensors_31.append(a[:, :, i:i+1, :, :])\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 31, 10, 1)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output[mask == 1] = clean_input_spectrogram[mask == 1]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask == 0] = clean_input_spectrogram[mask == 0]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x), torch.abs(y))\nmin_y = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.where(x < y, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\nsigned_max = torch.where(x > y, max_x, max_y)\nsigned_max = torch.where(x > y, sign_x, sign_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x), torch.abs(y))\n    min_y = torch.min(torch.abs(x), torch.abs(y))\n    min_x_sign = torch.where(x < 0, -1, 1)\n    min_y_sign = torch.where(y < 0, -1, 1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.repeat(1, x.shape[1])\n    min_y_sign = min_y_sign.repeat(1, x.shape[1])\n    min_x = min_x.repeat(x.shape[0], 1)\n    min_y = min_y.repeat(y.shape[0], 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min_y_sign.reshape(-1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1)\n    min_y_sign = min", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\nprint(f\"Confidence score: {confidence_score}, Class name: {class_name}\")\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# Get the last column of a and the first column of b\na_last_col = a[:, -1:]\nb_first_col = b[:, 0]\n\n# Calculate the average of the last column of a and the first column of b\nresult = a_last_col + b_first_col\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a[:, :-1]\n    b = b[:, 1:]\n    result = a + b\n    return result\n    ### END SOLUTION ###\n\n# Test the function\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\nprint(solve(a, b))\n# Output: [[1 2 4 6 7]\n#          [1 2 4 6 7]]\n#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "t = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "t = torch.tensor([[1, 2], [3, 4]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
