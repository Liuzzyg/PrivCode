{"id": 0, "code": "\nresult = df.reindex(List)\nresult = result.reset_index(drop=True)\nprint(result)\n", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "\ndf['Type_Order'] = df['Type'].rank(method='first')\ndf = df.sort_values('Type_Order')\ndf = df.reset_index(drop=True)\ndf = df.loc[List]\ndf = df.reset_index(drop=True)\nresult = df\nresult", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a mask for the rows where the value in column Qu1 is not in the value_counts() of column Qu1\nmask = df['Qu1'].map(df['Qu1'].value_counts()) < 2\n\n# Apply the mask to the column Qu1\ndf.loc[mask, 'Qu1'] = 'other'\n\n# Create a mask for the rows where the value in column Qu2 is not in the value_counts() of column Qu2\nmask = df['Qu2'].map(df['Qu2'].value_counts()) < 2\n\n# Apply the mask to the column Qu2\ndf.loc[mask, 'Qu2'] = 'other'\n\n# Create a mask for the rows where the value in column Qu3 is not in the value_counts() of column Qu3\nmask = df['Qu3'].map(df['Qu3'].value_counts()) < 2\n\n# Apply the mask to the column Qu3\ndf.loc[mask, 'Qu3'] = 'other'\n\nresult = df\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a mask for the rows where the value in column Qu1 is not in the list of values with count >= 3\nmask = df['Qu1'].isin(df['Qu1'].value_counts()[df['Qu1'].value_counts() >= 3].index)\n\n# Apply the mask to the DataFrame\ndf.loc[~mask, 'Qu1'] = 'other'\n\n# Repeat the process for Qu2 and Qu3\nmask = df['Qu2'].isin(df['Qu2'].value_counts()[df['Qu2'].value_counts() >= 3].index)\ndf.loc[~mask, 'Qu2'] = 'other'\n\nmask = df['Qu3'].isin(df['Qu3'].value_counts()[df['Qu3'].value_counts() >= 3].index)\ndf.loc[~mask, 'Qu3'] = 'other'\n\nresult = df\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "\n    for col in df.columns:\n        counts = df[col].value_counts()\n        for index, value in counts.items():\n            if value >= 2:\n                df[col] = df[col].replace(index, 'other')\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a function to replace values\ndef replace_values(x):\n    if x.nunique() >= 3:\n        return 'other'\n    else:\n        return x\n\n# Apply the function to each column\ndf['Qu1'] = df['Qu1'].apply(replace_values)\ndf['Qu2'] = df['Qu2'].apply(replace_values)\ndf['Qu3'] = df['Qu3'].apply(replace_values)\n\nresult = df\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a function to replace values\ndef replace_values(x):\n    if x in ['apple', 'potato', 'egg']:\n        return x\n    elif x in ['sausage', 'banana']:\n        return 'other'\n    else:\n        return x\n\n# Apply the function to each column\ndf['Qu1'] = df['Qu1'].apply(replace_values)\ndf['Qu2'] = df['Qu2'].apply(replace_values)\ndf['Qu3'] = df['Qu3'].apply(replace_values)\n\n# Create a dictionary to map the values\nmap_dict = {'apple': 'apple', 'potato': 'other', 'egg': 'egg', 'sausage': 'other', 'banana': 'other'}\n\n# Apply the dictionary to each column\ndf['Qu1'] = df['Qu1'].map(map_dict)\ndf['Qu2'] = df['Qu2'].map(map_dict)\ndf['Qu3'] = df['Qu3'].map(map_dict)\n\nresult = df\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "\ndf['keep_if_dup'] = df['keep_if_dup'].map({'Yes': True, 'No': False})\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result[['id', 'url', 'keep_if_dup']]\n", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "\ndf['drop_if_dup'] = df['drop_if_dup'].map({'Yes': True, 'No': False})\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result[['id', 'url', 'drop_if_dup']]\nresult['drop_if_dup'] = result['drop_if_dup'].map({True: 'Yes', False: 'No'})\nresult\n", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\ndf['keep_if_dup'] = df['keep_if_dup'].map({'Yes': True, 'No': False})\nresult = df.drop_duplicates(subset='url', keep='last')\nresult = result[result['keep_if_dup']]\nresult = result[['id', 'url', 'keep_if_dup']]\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\nresult = df.groupby('name').apply(lambda x: {k: {y: z} for k, y, z in zip(x['v1'], x['v2'], x['v3'])}).to_dict()\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": "\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# First, we need to parse the message to extract the key-value pairs\ndf['job'] = df['message'].apply(lambda x: x.split(':')[1].strip())\ndf['money'] = df['message'].apply(lambda x: x.split(',')[0].split(':')[1].strip())\ndf['wife'] = df['message'].apply(lambda x: x.split(',')[1].split(':')[1].strip())\ndf['group'] = df['message'].apply(lambda x: x.split(',')[2].split(':')[1].strip())\ndf['kids'] = df['message'].apply(lambda x: x.split(',')[3].split(':')[1].strip())\n\n# Then, we drop the message column\ndf = df.drop(['message'], axis=1)\n\n# Finally, we convert the dataframe to a wide format\nresult = df.pivot_table(index=['name', 'status', 'number'], columns=['job', 'wife', 'group', 'kids'], aggfunc='first').reset_index()\n\n# Rename the columns\nresult.columns = ['name', 'status', 'number', 'job', 'money', 'wife', 'group', 'kids']\n\n# Fill the NaN values with 'none'\nresult.fillna('none', inplace=True)\n\nresult\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df['score'] * 10\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * df['product'].apply(lambda x: 1 if x not in products else 10)", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "\ndf['score'] = df['product'].apply(lambda x: 10 if x in products else x)\ndf['score'] = df['score'] * df['score']\ndf", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "df['score'] = df['product'].apply(lambda x: 1 if x in products else x)\ndf['score'] = (df['score'] - df['score'].min()) / (df['score'].max() - df['score'].min())\ndf", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.idxmax(axis=1)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = df.idxmax(axis=1)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col]], axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf = df[df['Date'].between('17-Jan-2019', '20-Feb-2019')]\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "df['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf = df.iloc[1:]\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "df['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf = df.iloc[:-1]\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf = df.iloc[:-1, :]\ndf = df.reset_index(drop=True)\nprint(df)\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf = df.loc[df.index[-1]]\ndf['#2'] = df['#2'].shift(1)\ndf = df.loc[df.index[0]]\ndf", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'X' for col in df.columns]", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col if i == 0 else col for i, col in enumerate(df.columns)]", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "\n# Create a list of all column names that end with 'X'\nx_columns = [col for col in df.columns if col.endswith('X')]\n\n# Create a list of all column names that don't end with 'X'\nnon_x_columns = [col for col in df.columns if not col.endswith('X')]\n\n# Create a dictionary to map the new column names\nrename_dict = {**{col+'X': col for col in x_columns}, **{col: col+'AX' for col in non_x_columns}}\n\n# Rename the columns\ndf = df.rename(columns=rename_dict)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "\nresult = df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\nresult['group_color'] = df.groupby('group')['group_color'].first()\nresult.columns = ['_'.join(col).strip() for col in result.columns.values]\nresult.reset_index(inplace=True)\nresult\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' in col})\nresult['group_color'] = df.groupby('group')['group_color'].first()\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "df['val32'] = df.groupby('group')['val1', 'val2', 'val42'].transform(lambda x: x.mean())\ndf['val3'] = df.groupby('group')['val1', 'val2', 'val42'].transform(lambda x: x.sum())\nresult = df.groupby('group')['group_color', 'val1', 'val2', 'val32', 'val3'].mean()", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "\nresult = df.loc[row_list, column_list].mean()", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "\nresult = df.loc[row_list, column_list].sum()", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "\nresult = df.loc[row_list, column_list].sum()\nresult = result.drop(result.idxmax())\n", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "\nresult = df.apply(lambda x: pd.Series(x.value_counts()), axis=0)\nresult = result.fillna(0)\nresult = result.astype(float)\nresult = result.loc[:, (result != 0).any(axis=0)]\nresult = result.loc[(result != 0).any(axis=1)]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]\nresult = result.loc[result.index.repeat(result.columns.repeat())]", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "df.isnull().sum()", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "\nresult = df.apply(lambda x: pd.Series([x.value_counts()], index=['id', 'temp', 'name']))", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "df = df.iloc[[0,1]]\ndf.columns = df.loc[0].astype(str) + '_' + df.loc[1].astype(str)\ndf = df.iloc[2:]\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.replace(np.nan, '', regex=True)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Unnamed: 1': 'Concentration'})\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'Sample_type'})\ndf = df", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "df = df.iloc[[0,1]]\ndf.columns = df.loc[0].astype(str) + '_' + df.loc[1].astype(str)\ndf = df.iloc[2:]\ndf = df.reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "\ndf['0'] = df['0'].where(df['0'].notnull(), df['0'])\ndf['1'] = df['1'].where(df['1'].notnull(), df['1'])\ndf['2'] = df['2'].where(df['2'].isnull(), df['2'])\n\nresult = df\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "\ndf['0'] = df['0'].replace({np.nan: df['0'].mean()})\ndf['1'] = df['1'].replace({np.nan: df['1'].mean()})\ndf['2'] = df['2'].replace({np.nan: df['2'].mean()})\n\nresult = df\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "\ndf['0'] = df['0'].mask(df['0'].isnull(), df['1'].notnull().astype(int))\ndf['1'] = df['1'].mask(df['1'].isnull(), df['2'].notnull().astype(int))\ndf['2'] = df['2'].mask(df['2'].isnull(), df['0'].notnull().astype(int))\ndf = df.where(df['0'].notnull(), df['0'])\ndf = df.where(df['1'].notnull(), df['1'])\ndf = df.where(df['2'].notnull(), df['2'])\ndf = df.where(df['0'].isnull(), 0)\ndf = df.where(df['1'].isnull(), 0)\ndf = df.where(df['2'].isnull(), 0)\ndf = df.fillna(0)\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df.replace([np.nan], [0])\ndf = df", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# Create a mask for rows where value is less than threshold\nmask = df['value'] < thresh\n\n# Apply the mask to the dataframe\ndf.loc[mask] = df.loc[mask].sum()\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the index back to 'lab'\ndf.set_index('lab', inplace=True)\n\n# Set the index back to 'lab'\ndf.reset_index(inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\n# First, create a mask for rows where value is less than threshold\nmask = df['value'] < thresh\n\n# Then, create a new group by object\ngroup = df.groupby(mask)\n\n# Calculate the average of each group\nresult = group.mean()\n\n# Reset the index\nresult = result.reset_index()\n\n# Rename the column\nresult.columns = ['lab', 'value']\n\n# If there are still rows with value less than threshold, add them to the result\nresult = pd.concat([result, df.loc[df['value'] < thresh]])\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Rename the column\nresult.columns = ['lab', 'value']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# Create a mask for the rows to be replaced\nmask = (df['value'] >= section_left) & (df['value'] <= section_right)\n\n# Replace the rows with the average of the values in the masked rows\ndf.loc[mask, 'value'] = df[mask].mean().values[0]\n\n# Reset the index\ndf = df.reset_index()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\nresult = df.assign(**{f'inv_{col}': 1 / df[col] for col in df.columns})\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndf = df.add_prefix('exp_')\nresult = df.assign(**{f'exp_{col}': lambda x: x[col] for col in df.columns})\nresult", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\ndf['inv_A'] = 1 / df['A']\ndf['inv_B'] = 1 / df['B']\n\nresult = df\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndf = df.add_prefix('sigmoid_')\ndf = df.assign(**{f'sigmoid_{col}': sigmoid(df[col]) for col in df.columns})\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the maximum value in each column\nmax_idx = df.idxmax()\n\n# Create a mask where the maximum value is not in the minimum index\nmask = np.invert(np.isin(max_idx, min_idx))\n\n# Get the index of the maximum value in the masked column\nresult = max_idx[mask]\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the maximum value in each column\nmax_idx = df.idxmax()\n\n# Get the index of the first occurrence of the column-wise maximum\nresult = max_idx[min_idx]\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "\n# Create a pivot table with 'user' as the index and 'dt' as the columns\npivot_table = df.pivot_table(index='user', columns='dt', aggfunc='first')\n\n# Fill the missing values with 0\nresult = pivot_table.fillna(0)\n\n# Reverse the index to get the original order\nresult = result.reindex(pd.date_range(start=min(df['dt']), end=max(df['dt'])), level='dt')\n\n# Reset the index to get the original order\nresult = result.reset_index()\n\n# Rename the columns\nresult.columns = ['user', 'dt', 'val']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "df = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf['val'] = df['val'].replace({np.nan: 0})\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.groupby(['user',", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "df['val'] = df.groupby('user')['val'].apply(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['dt'] = df['dt'].map(lambda x: x.strftime('%Y-%m-%d'))\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\ndf['val'] = df['val'].map(lambda x: x.fillna(x.iloc[0]).values)\ndf = df.sort_values(['user', 'dt'])\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "df = df.groupby(['user', 'dt']).val.max().reset_index()\ndf['val'] = df.groupby('user')['val'].transform('max')\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "df['dt'] = df.groupby('user')['dt'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('max')\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.sort_values(['user', 'dt'])\ndf =", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "df['name'] = df['name'].astype('category').cat.codes\ndf['name'] += 1\nresult = df\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "df['a'] = df.groupby('name')['a'].transform('first') + 1\nresult = df\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "\n    df['name'] = df['name'].astype('category').cat.codes\n    df['name'] += 1\n    return df\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "df['ID'] = df.groupby('name')['a'].transform('first')\ndf = df.drop('a', axis=1)\ndf = df.rename(columns={'ID': 'a'})\ndf = df.sort_values('name')\ndf = df.reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.sort_values(['user', 'date'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.sort_values(['user', 'date'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.sort_values(['user', 'date'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "\ndf_sub = df.loc[df['c'] > 0.5, columns]\nresult = df_sub.values", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "\ndf_sub = df.loc[df['c'] > 0.45, columns]\nresult = df_sub.values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    df_sub = df.loc[df['c'] > 0.5, columns]\n    df_sub['sum'] = df_sub[columns[0]] + df_sub[columns[1]]\n    return df_sub\n    ### END SOLUTION\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the dataframe by date\ndf = df.sort_values('date')\n\n# Create a new column 'date_range' which is the date of the row plus the number of days\ndf['date_range'] = df['date'] + pd.to_timedelta(X, unit='d')\n\n# Create a new column 'previous_date' which is the date of the previous row\ndf['previous_date'] = df['date'].shift(1)\n\n# Filter the rows where the date_range is greater than the previous_date\ndf = df[df['date_range'] > df['previous_date']]\n\n# Reset the index of the filtered dataframe\ndf = df.reset_index(drop=True)\n\n# Drop the 'date_range' and 'previous_date' columns\ndf = df.drop(['date_range', 'previous_date'], axis=1)\n\nresult = df\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the dataframe by date\ndf = df.sort_values('date')\n\n# Create a new column 'week' which represents the week number of the date\ndf['week'] = df['date'].dt.week\n\n# Create a new column 'week_end' which represents the end of the week\ndf['week_end'] = df['date'].dt.weekday\n\n# Create a new column 'week_start' which represents the start of the week\ndf['week_start'] = df['date'].shift(1).dt.weekday\n\n# Create a new column 'week_start_end' which represents the start and end of the week\ndf['week_start_end'] = df['week_start'] + df['week_end']\n\n# Create a new column 'week_start_end_plus_one' which represents the start and end of the week plus one\ndf['week_start_end_plus_one'] = df['week_start_end'] + 1\n\n# Create a new column 'week_start_end_minus_one' which represents the start and end of the week minus one\ndf['week_start_end_minus_one'] = df['week_start_end'] - 1\n\n# Create a new column 'week_start_end_minus_one_plus_one' which represents the start and end of the week minus one plus one\ndf['week_start_end_minus_one_plus_one'] = df['week_start_end_minus_one'] + 1\n\n# Create a new column 'week_start_end_minus_one_minus_one' which represents the start and end of the week minus two\ndf['week_start_end_minus_one_minus_one'] = df['week_start_end_minus_one'] - 2\n\n# Create a new column 'week_start_end_minus_one_minus_one_plus_one' which represents the start and end of the week minus two plus one\ndf['week_start_end_minus_one_minus_one_plus_one'] = df['week_start_end_minus_one_minus_one'] + 1\n\n# Create a new column 'week_start_end_minus_one_minus_one_minus_one' which represents the start and end of the week minus three\ndf['week_start_end_minus_one_minus_one_minus_one'] = df['week_start_end_minus_one_minus_one'] - 3\n\n# Create a new column 'week_start_end_minus_one_minus_one_minus_one_plus_one' which represents the start and end of the week minus three plus one\ndf['week_start_end_minus_one_minus_one_minus_one_plus_one'] = df['week_start_end_minus_one_minus_one_minus_one'] + 1\n\n# Create a new column 'week_start_end_minus_one_minus_one_minus_one_minus_one' which represents the start and end of the week minus four\ndf['week_start_end_minus_one_minus_one_minus_one_minus_one'] = df['week_start_end_minus_one_minus_one_minus_one'] - 4\n\n# Create a new column 'week_start_end_minus_one_minus_one_minus_one_minus_one_plus_one' which represents the start and end of the week minus four plus one\ndf['week_start_end_minus_one_minus_one_minus_one_minus_one_plus_one'] = df['week_start_end_minus_one_minus_one_minus_one_minus_one'] + 1\n\n# Create a new column 'week_start_end_minus_one_minus_one_minus_one_minus_one_minus_one' which represents the start and end of the week minus five\ndf['week_start_end_minus_one_minus_one_minus_one_minus_one_minus_one'] = df['week_start_end_minus_one_minus_one_minus_one_minus_one'] - 5\n\n# Create a new column 'week_start_end_minus_one_minus_one_minus_one_minus_one_minus_one_plus_one' which represents the start and end", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\n# Convert date to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the dataframe by date\ndf = df.sort_values('date')\n\n# Create a new column 'week' which represents the week number of the date\ndf['week'] = df['date'].dt.week\n\n# Create a new column 'week_end' which represents the end of the week\ndf['week_end'] = df['date'].dt.weekday\n\n# Create a new column 'week_start' which represents the start of the week\ndf['week_start'] = df['date'].shift(1).dt.weekday\n\n# Create a new column 'week_start_end' which represents the start and end of the week\ndf['week_start_end'] = df['week_start'] + df['week_end']\n\n# Create a new column 'week_start_end_minus_1' which represents the start and end of the week minus 1\ndf['week_start_end_minus_1'] = df['week_start_end'] - 1\n\n# Create a new column 'week_start_end_minus_1_plus_1' which represents the start and end of the week minus 1 plus 1\ndf['week_start_end_minus_1_plus_1'] = df['week_start_end_minus_1'] + 1\n\n# Create a new column 'week_start_end_minus_1_plus_1_minus_1' which represents the start and end of the week minus 1 plus 1 minus 1\ndf['week_start_end_minus_1_plus_1_minus_1'] = df['week_start_end_minus_1_plus_1'] - 1\n\n# Create a new column 'week_start_end_minus_1_plus_1_minus_1_minus_1' which represents the start and end of the week minus 1 plus 1 minus 1 minus 1\ndf['week_start_end_minus_1_plus_1_minus_1_minus_1'] = df['week_start_end_minus_1_plus_1_minus_1'] - 1\n\n# Create a new column 'week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1' which represents the start and end of the week minus 1 plus 1 minus 1 minus 1 minus 1\ndf['week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1'] = df['week_start_end_minus_1_plus_1_minus_1_minus_1'] - 1\n\n# Create a new column 'week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1_minus_1' which represents the start and end of the week minus 1 plus 1 minus 1 minus 1 minus 1 minus 1\ndf['week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1_minus_1'] = df['week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1'] - 1\n\n# Create a new column 'week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1_minus_1_minus_1' which represents the start and end of the week minus 1 plus 1 minus 1 minus 1 minus 1 minus 1 minus 1\ndf['week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1_minus_1_minus_1'] = df['week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1_minus_1'] - 1\n\n# Create a new column 'week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1_minus_1_minus_1_minus_1' which represents the start and end of the week minus 1 plus 1 minus 1 minus 1 minus 1 minus 1 minus 1 minus 1\ndf['week_start_end_minus_1_plus_1_minus_1_minus_1_minus_1_minus_1_minus_1_minus_1'] = df['week_start_end_minus_", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean().reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\ndf['col1'] = df.groupby(df.index // 4).col1.transform('sum')\nresult = df\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df\nresult", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# First, we need to create a new column that indicates the group number\ndf['group'] = list(itertools.chain(*[[i]*j for i, j in enumerate([3, 2, 3, 2] + [3, 2]*((len(df)-1)//6))]))\n\n# Then, we group by the new column and calculate the sum and average\nresult = df.groupby('group').agg({'col1': ['sum', 'mean']})['col1'].reset_index()\n\n# Finally, we rename the columns and transpose the dataframe\nresult.columns = result.columns.dtype(str)\nresult = result.transpose()\nresult.columns = ['sum', 'avg']\nresult.index += 1\n\nprint(result)\n", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# First, we need to create a new column that indicates the group number\ndf['group'] = list(range(1, len(df) + 1))\n\n# Then, we group by the group number and calculate the sum and average for each group\nresult = df.groupby('group').agg({'col1': ['sum', 'mean']})\n\n# Finally, we reset the index and transpose the dataframe\nresult = result.reset_index().transpose()\n\n# The result is a dataframe with the desired structure\nprint(result)\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df['A'] = df['A'].mask(df['A'] == 0).ffill(downcast='infer')\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df['A'] = df['A'].mask(df['A'] == 0).ne(0).ffill(downcast='infer')\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "df['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill').combine(df['A'].fillna(method='bfill'), max)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['number'] = df['duration'].str.extract('(\\d+)').astype(int)\ndf['time'] = df['duration'].str.extract('([a-zA-Z]+)').str.replace(' ', '').str.replace('.', '').str.replace('year', 'day').str.replace('week', 'week').str.replace('month', 'month')\ndf['time_days'] = df['time'].map({'day': 1, 'week': 7, 'month': 30})\ndf", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].astype(int)\ndf", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "\n    df['time'] = df['duration'].str.split(' ').str[0]\n    df['number'] = df['duration'].str.split(' ').str[1]\n    df['time_days'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n    df['number'] = df['number'].astype(int)\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1].astype(int)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number']\ndf\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = [all(getattr(df1.loc[i], col) == getattr(df2.loc[i], col) for col in columns_check_list) for i in range(len(df1))]\nprint(result)\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "\nresult = [all(getattr(df1, column) == getattr(df2, column) for column in columns_check_list) for _ in range(len(df1))]\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='date')\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='datetime')\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "\n    df['date'] = pd.to_datetime(df['date'])\n    return df[['date', 'x', 'y']].values\n    ### END SOLUTION\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "\n    df.index = pd.to_datetime(df.index)\n    df = df.swaplevel(0,1)\n    return df\n    ### END SOLUTION\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df_melt = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf_pivot = df_melt.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf_pivot.columns.name = None\ndf_pivot.columns = ['Country', 'year', 'var1', 'var2']\ndf_pivot", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df_melt = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf_pivot = df_melt.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf_pivot.columns.name = None\ndf_pivot.sort_values(['Country', 'year'], inplace=True)\ndf_pivot.reset_index(drop=True, inplace=True)\ndf_pivot", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "df = df.loc[df.filter(like='Value').abs().max(axis=1) < 1]", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "df = df.loc[abs(df.filter(like='Value').abs().max(axis=1) > 1).index.tolist()]", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\ndf = df.loc[(df.abs().max(axis=1) > 1).all(axis=1)]\ndf = df.rename(columns=lambda x: x.replace('Value_', ''))\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df['A'] = df['A'].str.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "\n    df.replace({'&AMP;': '&'}, regex=True, inplace=True)\n    return df\n    ### END SOLUTION\n\nf()\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True, inplace=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df.replace({'&AMP;': '&'}, regex=True, inplace=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "\ndf['first_name'] = df['name'].apply(lambda x: x.split(' ')[0] if ' ' in x else x)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[1] if ' ' in x else None)\ndf\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "\ndf['1_name'] = df['name'].apply(lambda x: x.split(' ')[0])\ndf['2_name'] = df['name'].apply(lambda x: x.split(' ')[1])\ndf\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "\ndef split_name(name):\n    if ' ' in name:\n        first_name, last_name = name.split(' ', 1)\n        if '.' in last_name:\n            middle_name, last_name = last_name.split('. ', 1)\n            last_name = last_name.strip()\n        else:\n            middle_name = None\n    else:\n        first_name, last_name = name, None\n    return first_name, middle_name, last_name\n\ndf['first name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(split_name))\ndf = df[['first name', 'middle_name', 'last_name']]\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = df2.merge(df1, on='Timestamp', how='left')\nresult['data'] = result['data'].fillna(method='ffill').astype(int)\nresult = result.sort_values('Timestamp')\nresult\n", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1['Timestamp'], df2['Timestamp'], direction='nearest')\nresult['data'] = df2['stuff']\nresult = result.sort_values('Timestamp')\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where((df['col2'] <=50) & (df['col3'] <=50), df['col1'], df['col1'].max())\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.to_numeric(x, errors='coerce') == x else 'and')\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.to_numeric(x, errors='ignore') == x else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.to_numeric(x, errors='raise') == x else x)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: [int(i) for i in str(x).split() if str(i).isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else x)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "\n    result = []\n    for i in df['Field1']:\n        if not isinstance(i, int):\n            result.append(i)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum() * 100\ndf['val2'] = df['val2'] / df['val2'].sum() * 100\ndf['val3'] = df['val3'] / df['val3'].sum() * 100\ndf['val4'] = df['val4'] / df['val4'].sum() * 100\n\ndf", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "df = df.drop(test)\nresult = df\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\n# Calculate the pairwise distances between cars\ndf['distance'] = ((df['x'] - df['x'].shift())**2 + (df['y'] - df['y'].shift())**2).apply(lambda x: x**0.5)\n\n# Find the nearest neighbour for each car\ndf['nearest_neighbour'] = df.groupby('time')['distance'].transform('idxmin')\n\n# Calculate the average of the distances for each frame\ndf['average_distance'] = df.groupby('time')['distance'].mean()\n\ndf\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\n# First, we need to calculate the Euclidean distance for each pair of cars\ndf['euclidean_distance'] = ((df['x'].values[:, None, None] - df['x'].values[None, :, None])**2 + (df['y'].values[:, None, None] - df['y'].values[None, :, None])**2).sqrt()\n\n# Then, we group by 'time' and 'car', and find the car with the smallest Euclidean distance\ndf['farmost_neighbour'] = df.groupby('time')['euclidean_distance'].transform('min')\n\n# Finally, we calculate the average of the distances for each time point\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].transform('mean')\n\ndf\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df.apply(lambda row: ','.join(row.dropna().astype(str).values), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().astype(str)), axis=1)", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().astype(str)), axis=1)\ndf", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values[::-1]), axis=1)\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index[~df.index.isin(sample_df.index)]\n\ndf = df.drop(df.index[~df.index.isin(sample_df.index)])\n\ndf = pd.concat([df, sample_df])\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change the value of the ProductId column of these rows to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.reset_index(drop=True, inplace=True)\n\ndf = df.drop(sample_df.index)\n\ndf = pd.concat([df, sample_df])\ndf = df.sort_index()\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# First, we calculate the number of rows to select for each user\nnum_rows = df.groupby('UserId').apply(lambda x: int(x.shape[0] * 0.2))\n\n# Then, we select the rows for each user\nselected_rows = df.groupby('UserId').apply(lambda x: x.sample(num_rows[x.index], random_state=0))\n\n# We set the Quantity to zero\nselected_rows['Quantity'] = 0\n\n# We keep the indexes of the altered rows\nselected_rows.index = df.index[~df.index.isin(selected_rows.index)]\n\n# Finally, we concatenate the original dataframe with the selected rows\ndf = pd.concat([df, selected_rows])\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\nresult = df.loc[df['index_original'] == True]\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\nresult = df.loc[df['index_original'] == True]\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "\n    df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\n    duplicate = df.loc[df['index_original'] == True]\n    return duplicate\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "df['index_original'] = df.duplicated(subset=['col1','col2', '3col'], keep=False)\nresult = df.loc[df['index_original'] == True]\nresult['index_original'] = df.duplicated(subset=['col1','col2', '3col'], keep=False)\nresult\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\nresult = df.loc[df['index_original'] == True]\nresult", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt']).loc[lambda x: x['count'] == x['count'].max()]\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "\n# First, find the maximum count in each group\nmax_counts = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp','Mt'])['count'], on=['Sp','Mt'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count in the group\nresult = result[result['count'] == result.groupby(['Sp','Mt'])['count'].transform('max')]\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\n# First, find the minimum count for each group\nmin_counts = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\n\n# Then, join the original dataframe with the minimum counts\nresult = df.join(min_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt']).drop(columns=['count'])\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp','Value'])['count'], on=['Sp','Value'])\n\n# Finally, filter the result to only include rows where count equals the maximum count\nresult = result[result['count'] == result['count'].max()]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df[df['Category'].isin(filter_list)]", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df[~df['Category'].isin(filter_list)]", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "\n# Create a list of tuples where each tuple contains the first, second and third column level\nvalue_vars = [('A', 'B', 'E'),\n              ('A', 'B', 'F'),\n              ('A', 'C', 'G'),\n              ('A', 'C', 'H'),\n              ('A', 'D', 'I'),\n              ('A', 'D', 'J')]\n\n# Use the list of tuples to melt the data frame\nresult = pd.melt(df, value_vars=value_vars)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "\nresult = df.reset_index().melt(id_vars='index').drop(columns='index').rename(columns={'variable': 'variable_0', 'value': 'value_0'})\n\nfor i in range(1, 7):\n    df['variable_' + str(i)] = df['variable'].apply(lambda x: x[i])\n    df['value_' + str(i)] = df['value'].apply(lambda x: x[i])\n    df = df.drop(columns=['variable', 'value'])\n\nresult = pd.concat([result, df])\nresult = result.sort_values(['variable_0', 'variable_1', 'variable_2', 'value_0', 'value_1', 'value_2'])\nresult = result[['variable_0', 'variable_1', 'variable_2', 'value_0', 'value_1', 'value_2']]\nresult = result.reset_index(drop=True)\nresult\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax().reset_index(drop=True)\ndf", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] >= 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index(drop=True)\nresult.name = 'v'\n", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "\nresult = df.groupby('r')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index(drop=True)\n", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index(drop=True)\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "\ndef relationship(df):\n    result = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                if df[col1].nunique() == 1:\n                    result.append(f'{col1} {col2} one-to-many')\n                elif df[col2].nunique() == 1:\n                    result.append(f'{col1} {col2} many-to-one')\n                else:\n                    result.append(f'{col1} {col2} many-to-many')\n    return result\n\nresult = relationship(df)\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "\ndef relationship(column1, column2):\n    if len(column1.unique()) == 1:\n        return 'one-2-many'\n    elif len(column2.unique()) == 1:\n        return 'one-2-one'\n    else:\n        return 'many-2-many'\n\nresult = [f'{column1.name} {column2.name} {relationship(column1, column2)}' for column1, column2 in df.columns[:2].T] + [f'{column1.name} {column2.name} {relationship(column1, column2)}' for column1, column2 in df.columns[2:].T]\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "\ndef relationship(x, y):\n    if x.nunique() == 1:\n        if y.nunique() > 1:\n            return 'one-to-many'\n        else:\n            return 'one-to-one'\n    elif y.nunique() == 1:\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\n\nresult = df.applymap(relationship)\nresult", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "\ndef relationship(x, y):\n    if len(x) == len(y):\n        return 'one-2-many'\n    elif len(x) == 1 or len(y) == 1:\n        return 'one-2-one'\n    else:\n        return 'many-2-many'\n\nresult = df.applymap(relationship)\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "\n# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n\n# sort by bank account first\ndfiban_uniq.sort_values(by=['bank'], inplace=True)\n\nresult = dfiban_uniq\nresult\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\nprint(\"Hello, World!\")\n!pip install torch\n!pip install transformers\n!pip install torchvision\n!pip install matplotlib\n!pip install scikit-learn\n!pip install pandas\n!pip install numpy\n!pip install seaborn\n!pip install torchtext\n!pip install torch-optimizer\n!pip install torch-scatter\n!pip install torch-sparse\n!pip install torch-geometric\n!pip install torch-cuda\n!pip install torch-segmentation\n!pip install torch-hub\n!pip install torch-quantum\n!pip install torch-music\n!pip install torch-music-loss\n!pip install torch-music-transformer\n!pip install torch-music-datasets\n!pip install torch-music-utils\n!pip install torch-music-audio\n!pip install torch-music-spectrogram\n!pip install torch-music-image\n!pip install torch-music-video\n!pip install torch-music-text\n!pip install torch-music-tabular\n!pip install torch-music-tabular-audio\n!pip install torch-music-tabular-spectrogram\n!pip install torch-music-tabular-image\n!pip install torch-music-tabular-video\n!pip install torch-music-tabular-text\n!pip install torch-music-tabular-tabular\n", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\ndf.groupby('Family')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndf['Family'] = ['Has Family', 'New Family', 'No Family', 'Old Family'][(df['SibSp'] == 1) & (df['Parch'] == 1)]\ndf['Family'] = df['Family'].replace({'Has Family': 1, 'New Family': 0, 'No Family': 1, 'Old Family': 0.5})\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "\ndf = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\nresult = df", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "\ndf['cokey'] = df.groupby('cokey')['A'].transform(lambda x: x.sort_values())\ndf = df.sort_values('cokey')\nresult = df", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "df = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult = df\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "\ndef std_mean(x):\n    return pd.Series({'mean': x.mean(), 'std': x.std()})\n\nresult = df.groupby('a').apply(std_mean)\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "\ndef std_mean(x):\n    return pd.Series({'mean': x.mean(), 'std': x.std()})\n\nresult = df.groupby('b')['a'].apply(std_mean)\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "\nimport numpy as np\n\n# calculate softmax\ndf['softmax'] = np.exp(df['b']) / np.exp(df['b']).sum()\n\n# calculate min-max\ndf['min-max'] = (df['b'] - df['b'].min()) / (df['b'].max() - df['b'].min())\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "\n# First, we need to find the rows and columns that only contain zeros.\n# Then, we can drop these rows and columns.\n\n# Find rows and columns that only contain zeros.\nrows_with_zeros = df.loc[:, df.eq(0).all()].index\ncols_with_zeros = df.loc[df.eq(0).all(), :].columns\n\n# Drop rows and columns that only contain zeros.\nresult = df.drop(rows_with_zeros, axis=0).drop(cols_with_zeros, axis=1)\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "df = df.loc[:, (df.sum() != 0)]\ndf = df.loc[(df.sum(axis=1) != 0)]\nresult = df\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "df = df.loc[df.max(axis=1) == 2]\ndf = df.loc[df.max(axis=0) == 2]\nresult = df\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "\nresult = df.applymap(lambda x: 0 if x > 1 else x)\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=[True, False])\nresult", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.reset_index().rename(columns={'index':'value'}).sort_values(['value','index'])\ndf['index'] = df['index'].astype(str)\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('tf', '')\ndf['index'] = df['index'].str.replace('p', '')\ndf['index'] = df['index'].str.replace('1', '')\ndf['index'] = df['index'].str.replace('0', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('-', '')\ndf['index'] = df['index'].str.replace('n', '')\ndf['index'] = df['index'].str.replace('e', '')\ndf['index'] = df['index'].str.replace('r', '')\ndf['index'] = df['index'].str.replace('d', '')\ndf['index'] = df['index'].str.replace('u', '')\ndf['index'] = df['index'].str.replace('y', '')\ndf['index'] = df['index'].str.replace('o', '')\ndf['index'] = df['index'].str.replace('a', '')\ndf['index'] = df['index'].str.replace('i', '')\ndf['index'] = df['index'].str.replace('t', '')\ndf['index'] = df['index'].str.replace('v', '')\ndf['index'] = df['index'].str.replace('s', '')\ndf['index'] = df['index'].str.replace('m', '')\ndf['index'] = df['index'].str.replace('g', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('?', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "\ndf = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]\nresult = df\n", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str).str.isalpha()]\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt']).loc[lambda x: x['count'] == x['count'].max()]\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "\n# First, find the maximum count in each group\nmax_counts = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp','Mt'])['count'], on=['Sp','Mt'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count in the group\nresult = result[result['count'] == result.groupby(['Sp','Mt'])['count'].transform('max')]\n\nprint(result)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\n# First, find the minimum count for each group\nmin_counts = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\n\n# Then, join the original dataframe with the minimum counts\nresult = df.join(min_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt']).drop(columns=['count'])\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp','Value'])['count'], on=['Sp','Value'])\n\n# Finally, filter the result to only include rows where count equals the maximum count\nresult = result[result['count'] == result['count'].max()]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\ndf", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'].fillna('17/8/1926', inplace=True)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    df['Date'] = df['Member'].map(dict)\n    ### END SOLUTION\n    return df\n\nresult = f(example_dict, example_df)\nprint(result)\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict).fillna('17/8/1926')\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).Date.transform('count')\ndf", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month'),'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year'),'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby('Val').Val.transform('count')\ndf", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month'),'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year'),'Val']).Date.transform('count')\ndf['Count_w'] = df.groupby([df['Date'].dt.week.rename('week'),'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Val']).Date.transform('count')\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "\n# First, create a new column that indicates whether the value is zero or not\ndf['Zero'] = df['B'] == 0\n\n# Then, group by 'Date' and 'Zero', and count the number of rows in each group\nresult1 = df.groupby('Date')['Zero'].sum().reset_index()\n\n# Similarly, create a new column that indicates whether the value is non-zero or not\ndf['Non-zero'] = df['B'] != 0\n\n# Then, group by 'Date' and 'Non-zero', and count the number of rows in each group\nresult2 = df.groupby('Date')['Non-zero'].sum().reset_index()\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['B'] = df['B'] % 2\ndf['C'] = df['C'] % 2\n\nresult1 = df.groupby('Date').sum()\nresult2 = df.groupby('Date').sum()\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc={'D':np.sum, 'E':np.mean})\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D':np.sum, 'E':np.mean})", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc={'D':np.max, 'E':np.min})", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\ndf['var2'] = df['var2'].str.split(',').apply(lambda x: pd.Series(x).append(pd.Series(x).dropna()))\ndf = df.reset_index(drop=True)\ndf\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda x: x['var2'].str.split(',').str[0].repeat(x['var2'].str.len())).compute()\n\nresult\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\ndf['var2'] = df['var2'].str.split('-').apply(lambda x: pd.Series(x).apply(lambda y: [y, '']))\ndf = df.explode('var2').reset_index(drop=True)\ndf = df.rename(columns={'var2': 'value'})\ndf = df.drop(columns=['value'])\ndf = df.groupby(['var1', 'value']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fillna('').reset_index()\ndf = df.rename(columns={'index': 'var2'})\ndf = df.groupby(['var1', 'var2']).size().unstack().reset_index()\ndf.columns.name = None\ndf = df.fill", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf = df.drop(columns=['row'])\ndf = df.rename(columns={'fips': 'FIPS'})\ndf", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf = df[['fips', 'row']]\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf['row'] = df['row'].str.split(' ', 2, expand=True)[0]\ndf = df[['fips', 'medi', 'row']]\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndf = df.set_index('Name').replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndf = df.set_index('Name').replace(0, np.nan)\ndf = df.iloc[:, ::-1].expanding(axis=1).mean().iloc[:, ::-1]\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.replace(np.", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "\n    df = df.set_index('Name')\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1)[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndf = df.set_index('Name').replace(0, np.nan)\ndf = df.iloc[:, ::-1].expanding(axis=1).mean().iloc[:, ::-1]\ndf = df.replace(np.nan, 0)\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf = df.fillna(method='pad')\ndf = df.fillna(method='ffill')\ndf = df", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'].diff().gt(0)).astype(int)\ndf.loc[0, 'label'] = 1\ndf", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf.loc[0, 'label'] = 1\ndf['label'] = df['label'].replace({-1: -1, 0: 0, 1: 1})\ndf\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: -1 if x < 0 else (1 if x > 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_timedelta(df['departure_time'] - df['arrival_time'])\ndf['Duration'] = df['Duration'].apply('{:02}:{:02}:{:02}'.format(*pd.to_timedelta(df['Duration']).seconds//86400))\ndf", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.groupby('id')['departure_time'].shift() - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.groupby('id')['departure_time'].shift() - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf['Duration'] = df['Duration'].fillna(0)\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df[df['key2'].str.endswith('e')].groupby('key1').size()", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.idxmax()\nmin_result = df.idxmin()\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.mode(axis=0)[0][0]\nmedian_result = df.median()[0]\nprint(mode_result,median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "df = df[(df['closing_price'].between(99, 101))]\n", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "df = df[(df['closing_price'] < 100) | (df['closing_price'] > 101)]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "df1 = df.groupby(\"item\", as_index=False)[\"diff\"].min()\nresult = df[df.groupby(\"item\")[\"diff\"].transform(\"min\") == df[\"diff\"]]", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# First, calculate the number of NaN values in the column\nnum_nan = df['Column_x'].isna().sum()\n\n# Then, calculate the number of values to replace\nnum_replace = num_nan // 2\n\n# Create a mask for the first half of the NaN values\nmask = df['Column_x'].isna()\n\n# Create a mask for the second half of the NaN values\nmask2 = df['Column_x'].isna() & (df['Column_x'].isna().shift(1) == False)\n\n# Replace the first half of the NaN values with 0\ndf.loc[mask, 'Column_x'] = 0\n\n# Replace the second half of the NaN values with 1\ndf.loc[mask2, 'Column_x'] = 1\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n\n# Calculate the number of values to replace\nreplace_count = int(nan_count * 0.3)\n\n# Create a mask for the first 30% of NaN values\nmask = df['Column_x'].isna()\nmask[:replace_count] = False\n\n# Create a mask for the middle 30% of NaN values\nmask[replace_count:replace_count*2] = False\n\n# Create a mask for the last 30% of NaN values\nmask[replace_count*2:] = False\n\n# Replace the NaN values with the corresponding values\ndf['Column_x'] = df['Column_x'].mask(mask)\n\n# Fill the NaN values with the mode of the column\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0])\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# First, we need to calculate the number of 0s and 1s in the column\nnum_zeros = df['Column_x'].sum()\nnum_ones = len(df) - num_zeros\n\n# Then, we create a new column with the desired distribution of 0s and 1s\ndf['Column_x'] = df['Column_x'].apply(lambda x: 0 if x==1 else 1)\n\n# Now, we replace the NaN values with the desired distribution\ndf['Column_x'] = df['Column_x'].apply(lambda x: 0 if x==1 else 1)\n\n# Finally, we replace the NaN values with the desired distribution\ndf['Column_x'] = df['Column_x'].apply(lambda x: 0 if x==1 else 1)\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame(list(zip(a['one'], b['one'])), columns=['one', 'two'])\nresult['two'] = list(zip(a['two'], b['two']))\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.concat([a, b, c], axis=1).groupby(level=0, axis=1).apply(lambda x: list(x.values)).reset_index()\nresult.columns = ['one', 'two']\nresult = result[['one', 'two']]\nresult = result.apply(lambda x: tuple(x), axis=1)\nresult = pd.DataFrame(list(result), columns=['one', 'two'])\nresult", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\n# Create a new dataframe with the same columns as a and b\nresult = pd.DataFrame(columns=['one', 'two'])\n\n# Iterate over the rows of a and b\nfor i in range(len(a)):\n    # Create a tuple from the corresponding elements in a and b\n    tuple_elements = (a.iloc[i, 0], b.iloc[i, 0])\n    # Append the tuple to the result dataframe\n    result = result.append(pd.Series(tuple_elements, index=['one', 'two']), ignore_index=True)\n\n# Fill the vacancy with np.nan\nresult = result.fillna(np.nan)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult.fillna(0, inplace=True)\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult.fillna(0, inplace=True)\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult.fillna(0, inplace=True)\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "\ndf['text'] = df['text'].apply(lambda x: ', '.join(x.split(', ')))\nresult = df.groupby(df.index).apply(lambda x: ', '.join(x['text']))\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "\nresult = df['text'].apply(lambda x: '-'.join(x.split('-')[1:])).str.cat(df['text'].str.split('-').str[1:], sep='-')\nresult = pd.DataFrame({'text': [result]})\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "\ndf['text'] = df['text'].apply(lambda x: ','.join(x.split(',')))\nresult = df.groupby(df.index)['text'].apply(lambda x: ','.join(x)).reset_index(drop=True)\nresult = result.rename(index={0: 'text'})\nresult['text'] = result['text'].str.strip(',')\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = pd.Series(df['text'].values.flatten(), index=df['text'])\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "\nresult = df['text'].str.cat(sep='-')\nprint(result)\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "df2['city'] = df2['id'].map(df1.set_index('id')['city'])\ndf2['district'] = df2['id'].map(df1.set_index('id')['district'])\ndf2 = df2.drop(['id'], axis=1)\nresult = pd.concat([df1, df2], axis=1)\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "df2['date'] = df2['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\ndf1 = df1.merge(df2, on=['id', 'city', 'district'], how='left')\nresult = pd.concat([df1, df2]).sort_values(['id', 'date'])\nresult", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "df2['city'] = df2['id'].map(df1.set_index('id')['city'])\ndf2['district'] = df2['id'].map(df1.set_index('id')['district'])\ndf2['date'] = df2['id'].map(df1.set_index('id')['date'])\ndf2 = df2.sort_values(['id', 'date'])\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date'])\nresult\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer').fillna(D['B'].values)", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer').fillna(D['B'].values)", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "\n# First, merge C and D on the 'A' column\nmerged = pd.merge(C, D, on='A', how='outer')\n\n# Then, create a new column 'duplicated' that is True if the 'A' column in C is present in D, and False otherwise\nmerged['duplicated'] = merged['A'].apply(lambda x: x in D['A'].values)\n\n# Finally, sort the DataFrame by the 'A' column in C\nresult = merged.sort_values('A').reset_index(drop=True)\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "\ndf['time_amount'] = list(zip(df['time'], df['amount']))\nresult = df.groupby('user')['time_amount'].apply(list)\nresult\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "df['amount-time'] = list(zip(df['time'], df['amount']))\nresult = df.groupby('user')['amount-time'].apply(list).reset_index()\nresult.columns = ['user', 'amount-time']\nresult['amount-time'] = result['amount-time'].apply(lambda x: [tuple(i) for i in x])\nresult", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "\ndf['amount-time'] = list(zip(df['time'], df['amount']))\nresult = df.groupby('user')['amount-time'].apply(list).reset_index()\nresult.columns = ['user', 'amount-time']\nresult['amount-time'] = result['amount-time'].apply(lambda x: [tuple(i) for i in x])\nresult\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\ndf = pd.concat([series.apply(pd.Series).stack()], axis=1)\ndf.index = series.index\ndf.columns = ['column']\ndf = df.reset_index()\ndf = df.rename(columns={'column': 'value'})\ndf = df.pivot(index='index', columns='file1', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'file1', 'column': 'value'})\ndf = df.pivot(index='file1', columns='file2', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file1': 'file2', 'file2': 'file3', 'index': 'file1', 'column': 'value'})\ndf = df.pivot(index='file1', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file1': 'file3', 'index': 'file2', 'column': 'value'})\ndf = df.pivot(index='file2', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file2': 'file3', 'index': 'file1', 'column': 'value'})\ndf = df.pivot(index='file1', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file1': 'file3', 'index': 'file2', 'column': 'value'})\ndf = df.pivot(index='file2', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file2': 'file3', 'index': 'file1', 'column': 'value'})\ndf = df.pivot(index='file1', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file1': 'file3', 'index': 'file2', 'column': 'value'})\ndf = df.pivot(index='file2', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file2': 'file3', 'index': 'file1', 'column': 'value'})\ndf = df.pivot(index='file1', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file1': 'file3', 'index': 'file2', 'column': 'value'})\ndf = df.pivot(index='file2', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file2': 'file3', 'index': 'file1', 'column': 'value'})\ndf = df.pivot(index='file1', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file1': 'file3', 'index': 'file2', 'column': 'value'})\ndf = df.pivot(index='file2', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df.rename(columns={'file2': 'file3', 'index': 'file1', 'column': 'value'})\ndf = df.pivot(index='file1', columns='file3', values='value')\ndf = df.fillna(0)\ndf = df.reset_index()\ndf = df", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\ndf = pd.DataFrame(series.tolist(), index=series.index).reset_index().rename(columns={'index':'name'})\ndf['name'] = df['name'].astype(str)\ndf = df.set_index(['name'])\ndf_concatenated = df.transpose()\ndf_concatenated.columns = ['0', '1', '2', '3']\ndf_concatenated\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "\nresult = [col for col in df.columns if s in col]\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "\nresult = df.columns[df.columns.str.contains(s)].tolist()[0]\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "\n# Find the column names that contain the string 'spike'\nspike_cols = [col for col in df.columns if s in col]\n\n# If there are any columns that contain 'spike', select the first one\nif spike_cols:\n    result = df[spike_cols[0]]\nelse:\n    result = None\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# First, we need to flatten the list of codes into a single series\nflat_codes = df['codes'].apply(pd.Series).stack()\n\n# Then, we create a new dataframe with the flattened codes, and fill NaNs with 0\ndf_flat = pd.DataFrame(flat_codes.values.reshape(-1, 1), columns=['code']).fillna(0)\n\n# Finally, we merge the original dataframe with the flattened dataframe\nresult = pd.merge(df, df_flat, left_index=True, right_index=True)\n\n# We drop the original codes column\nresult.drop('codes', axis=1, inplace=True)\n\n# We rename the column names to 'code_{}'\nresult.columns = ['code_{}'.format(i) for i in result.columns]\n\nprint(result)\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# First, we need to flatten the list of codes into a single series\nflat_codes = df['codes'].apply(pd.Series).stack().squeeze()\n\n# Then, we create a new dataframe with the flattened codes as columns\nresult = flat_codes.to_frame().reset_index(drop=True)\n\n# If the original dataframe has more columns, we add NaNs for the missing codes\nif len(df.columns) > len(result.columns):\n    missing_cols = set(df.columns) - set(result.columns)\n    result = result.append(pd.Series([float('NaN')]*len(missing_cols), index=missing_cols), ignore_index=True)\n\nresult", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# First, we need to flatten the list of lists into a single list\nflat_codes = [item for sublist in df['codes'] for item in sublist]\n\n# Then, we create a new dataframe with the flattened list\ndf_flat = pd.DataFrame(flat_codes, columns=['codes'])\n\n# Now, we can use the pivot function to reshape the dataframe\nresult = df_flat.pivot(index='codes', columns='codes', values='codes').fillna(0)\n\n# We need to convert the dataframe to a list of lists\nresult = result.values.tolist()\n\n# Finally, we need to convert the list of lists to a dataframe\nresult = pd.DataFrame(result, columns=['code_'+str(i) for i in range(1, len(result[0])+1)])\n\nprint(result)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\nimport ast\n\ndf['col1'] = df['col1'].apply(lambda x: [ast.literal_eval(i) for i in x])\nresult = [i for sublist in df['col1'].values.tolist() for i in sublist]\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\nimport ast\n\ndf['col1'] = df['col1'].apply(lambda x: ', '.join(map(str, x[::-1])))\nresult = df['col1'].str.cat(sep=', ')\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\nimport ast\n\ndf['col1'] = df['col1'].apply(lambda x: ', '.join(map(str, ast.literal_eval(x))))\nresult = df['col1'].str.cat(sep=', ')\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "df['Time'] = df['Time'].apply(lambda x: x.floor('2min'))\ndf.groupby('Time').mean()\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "df['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf.groupby('Time').mean()\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\")\ndf", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\")\ndf", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME']).dt.strftime('%d-%b-%Y %a %H:%M:%S')\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\ndf", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[~filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "\nresult = df.columns[df.isnull().all(axis=1)]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "\nresult = df.columns[df.iloc[0].eq(df.iloc[8])]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "\nresult = df.columns[df.iloc[0] != df.iloc[8]].tolist()", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "\nresult = [(x, y) for x, y in zip(df.iloc[0], df.iloc[8]) if x != y or (x == np.nan and y != np.nan) or (x != np.nan and y == np.nan)]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.stack().reset_index()\ndf.columns = [f'{j}_{i}' for i, j in df['level_1'].values]\ndf = df.drop('level_1', 1).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze()\ndf = df.reset_index().rename(columns={'index':'A_'+str(df.index+1)})\ndf = df.set_index('A_'+str(df.index+1)).T.squeeze", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.T.reset_index().T\ndf.columns = [f'{c}_{i}' for i, c in enumerate(df.columns)]\ndf = df.iloc[[0]]\n", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "\ndf.sort_index(level=['treatment','time'], inplace=True)\nresult = df\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "\ndf.sort_values(by=['treatment','time','VIM'], inplace=True)\nresult = df\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "\n# Convert the date to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create a mask for the dates to be removed\nmask = (df['Date'].dt.date != pd.to_datetime('2020-02-17').date()) & (df['Date'].dt.date != pd.to_datetime('2020-02-18').date())\n\n# Apply the mask to the dataframe\nresult = df[mask]\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "\n# Create a new column 'Day' to store the day of the week\ndf['Day'] = df.index.strftime('%A')\n\n# Filter the dataframe to keep only the rows where the day is not 'Friday'\nresult = df[df['Day'] != 'Friday']\n\n# Reset the index\nresult.reset_index(inplace=True)\n\n# Convert the 'Date' column to datetime format\nresult['Date'] = pd.to_datetime(result['Date'])\n\n# Set the 'Date' column as the index\nresult.set_index('Date', inplace=True)\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "\n# Create a mask for the correlation matrix where the correlation is above 0.3\nmask = (corr > 0.3).stack()\n\n# Create a new DataFrame where the mask is True\nresult = corr.mask(mask)\n\n# Set the masked values to NaN\nresult.mask(mask, np.nan)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "\nresult = corr.stack().reset_index()\nresult.columns = ['col1', 'col2', 'value']\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.drop(['col1', 'col2'], 1)\nresult = result.groupby(['col1', 'col2']).first().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[result['value'] > 0.3]\nresult = result.drop(['value'], 1)\nresult = result.set_index(['col1', 'col2'])\nresult = result.unstack().reset_index()\nresult = result.loc[", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.rename(columns={df.columns[0]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndf['frequent'] = df.apply(lambda row: 1 if row.max() == 1 else 0, axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndf['frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    values, counts = np.unique(row, return_counts=True)\n    return list(values[counts == np.max(counts)])\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(lambda row: len(row['frequent']), axis=1)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "df['bar'] = df['bar'].replace('NULL', np.nan)\ndf['bar'] = df['bar'].astype(float)\ndf['bar'] = df.groupby('id1')['bar'].transform('mean')\ndf = df.groupby(['id1', 'id2'])['foo', 'bar'].mean().reset_index()\nresult = df\n", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "df['bar'] = df['bar'].replace('NULL', 0)\ndf['bar'] = df['bar'].astype(float)\ndf['foo'] = df['foo'].astype(float)\n\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nresult['bar'] = df.groupby(['id1', 'id2'])['bar'].transform('mean')\nresult['bar'] = result['bar'].replace('NULL', 0)\nresult", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='outer')\ndf_c = df_c.drop(['b_col'], axis=1)\nresult = df_c\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='outer')\ndf_c = df_c.drop(['a_col','b_col'], axis=1)\nresult = df_c\n", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[np.logical_not(np.isnan(x))]", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = x.tolist()\nresult = [i for i in result if i is not None]\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.eye(max(a), dtype=int)[a-1]", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((len(a), 4))\nb[:, a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((len(a), max(a) + 1))\nb[:, a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((len(a), max(a)))\nb[np.arange(len(a)), a.astype(int)-1] = 1", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "\n# First, find the smallest and largest values in a\nmin_val = a.min()\nmax_val = a.max()\n\n# Then, create a one-hot encoding array\nb = np.zeros((a.shape[0], a.shape[1], max_val - min_val + 1))\n\n# Set the appropriate values in b to 1\nb[np.arange(a.shape[0]), np.arange(a.shape[1]), a - min_val] = 1", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=1)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=0)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": "\n    max_value = np.max(a)\n    max_indices = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    return max_value, max_indices\n    ### END SOLUTION\nresult = f(example_a)\nprint(result)\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2)[-1], a.shape)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "import numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\n\nmask = np.any(np.isnan(a), axis=0)\na = a[:, ~mask]\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "import numpy as np\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \nresult = np.array(a)", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "result = np.unravel_index(a.argmin(), a.shape)\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape)\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.deg2rad(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "cos_value = np.cos(np.radians(degree))\nresult = cos_value\nprint(result)\n", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > 0.9:\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.degrees(np.arctan(value))\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "padding_length = length - len(A) % length\nresult = np.pad(A, (0, padding_length), 'constant')\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "padding_length = length - len(A) % length\nresult = np.pad(A, (0, padding_length), 'constant')\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "print(np.power(a, power))\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nresult = f(example_a, 2)\nprint(result)\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), 1)\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.array([numerator, denominator]).astype(int) / np.gcd.reduce(np.array([numerator, denominator]))\n    return tuple(result)\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = (numerator / denominator, denominator / numerator) if denominator != 0 else (np.nan, np.nan)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.where(a > b, a, b)\nresult = np.where(result > c, result, c)\nprint(result)\n", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.flatten()[-1::-1])", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.fliplr(a).diagonal()\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.diag(np.fliplr(a).diagonal())\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.diag(np.fliplr(a).diagonal())\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": "\n    result = [i for sublist in X for i in sublist]\n    ### END SOLUTION\n    return result\nprint(f())\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\nresult = list(result)\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "import numpy as np\n\ndef get_maximal_independent_vectors(matrix):\n    # Convert the matrix to a list of lists\n    matrix_list = matrix.tolist()\n\n    # Get the transpose of the matrix\n    transposed_matrix = matrix_list[0][::-1]\n\n    # Get the list of independent vectors\n    independent_vectors = [list(x) for x in np.array(transposed_matrix).T if np.all(np.array(transposed_matrix) == np.array(x), axis=1)]\n\n    # Get the list of independent vectors with the maximum length\n    maximal_independent_vectors = max(independent_vectors, key=len)\n\n    return maximal_independent_vectors\n\nresult = get_maximal_independent_vectors(a)\nprint(result)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the standard deviation\nstd_dev = np.sqrt(np.sum([anobs, bnobs]*avar))\n\n# calculate the t-statistic\nt_statistic = (amean - bmean) / (np.sqrt((anobs - bnobs)*avar))\n\n# calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_statistic), anobs + bnobs))\n\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A.view(np.int64), B.view(np.int64))]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.setdiff1d(np.unique(A.flatten()), np.unique(B.flatten()))\noutput = output.reshape((-1, 3))\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.zeros_like(b)\nfor i in range(3):\n    for j in range(3):\n        for k in range(3):\n            result[i, j, k] = b[np.unravel_index(np.argsort(a[i, j, k])[::-1], (3, 3, 3))]", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": "\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION\n    print(a)\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = array_of_arrays.copy()", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.diff(a, axis=1) == 0, axis=1)", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.diff(a, axis=1) == 0, axis=1)", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.array_equal(a[0], a[1]))\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "from scipy.integrate import simps\n\ndef f(x, y):\n    return (np.cos(x))**4 + (np.sin(y))**2\n\nresult = simps(f(x, y), x, y)\nprint(result)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": "\n    return (np.cos(x))**4 + (np.sin(y))**2\n    ### END SOLUTION\n\nresult = dblquad(f, 0, 1, 0, 1)\nprint(result)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(data):\n    x = np.sort(data)\n    n = x.shape[0]\n    y = np.arange(1, n+1) / n\n    return x, y", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(data):\n    # normalize data\n    x = np.sort(data)\n    y = np.arange(1, len(x) + 1) / len(x)\n    return x, y\n\nx, y = ecdf(grades)\nresult = np.interp(eval, x, y)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(data):\n    # normalize data\n    x = np.sort(data)\n    y = np.arange(1, len(x) + 1) / len(x)\n    return x, y\n\ndef longest_interval(data, threshold):\n    x, y = ecdf(data)\n    for i in range(len(y)):\n        if y[i] < threshold:\n            low = x[i]\n            high = x[i + 1]\n            return low, high\n\nlow, high = longest_interval(grades, threshold)\nprint(f\"The longest interval [{low}, {high}) that satisfies ECDF(x) < {threshold} is [{low}, {high}).\")\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.choice([0, 1], size=size, p=[one_ratio, 1-one_ratio])", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = np.array(a)\n", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\nresult = result.tolist()\nprint(result)\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[-N:][::-1]\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "\ndef extract_patches(a, patch_size):\n    h, w = a.shape\n    patches = []\n    for i in range(0, h - patch_size[0] + 1, patch_size[0]):\n        for j in range(0, w - patch_size[1] + 1, patch_size[1]):\n            patches.append(a[i:i + patch_size[0], j:j + patch_size[1]])\n    return patches\n\npatch_size = (2, 2)\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "\ndef sliding_window(a, window_size):\n    shape = a.shape[:-1] + (a.shape[-1] - window_size[0] + 1, a.shape[-1] - window_size[1] + 1)\n    strides = a.strides[:-1] + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\nwindow_size = (2, 2)\nresult = sliding_window(a, window_size)\nprint(result)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "\ndef extract_patches(a, patch_size):\n    h, w = a.shape\n    patches = []\n    for i in range(0, h - patch_size[0] + 1, patch_size[0]):\n        for j in range(0, w - patch_size[1] + 1, patch_size[1]):\n            patches.append(a[i:i + patch_size[0], j:j + patch_size[1]])\n    return patches\n\npatch_size = (2, 2)\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "\n# Calculate the number of patches\nnum_patches_x = a.shape[0] // patch_size\nnum_patches_y = a.shape[1] // patch_size\n\n# Create the result array\nresult = np.empty((num_patches_x, num_patches_y, patch_size, patch_size), dtype=a.dtype)\n\n# Fill the result array\nfor i in range(num_patches_x):\n    for j in range(num_patches_y):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n\n# Remove the extra rows and columns\nresult = result[:num_patches_x, :num_patches_y, :patch_size, :patch_size]\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.pad(a, ((0, h - a.shape[0]), (0, w - a.shape[1])), mode='constant')\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "\n# Calculate the number of patches\nnum_patches_x = a.shape[0] // patch_size\nnum_patches_y = a.shape[1] // patch_size\n\n# Create the result array\nresult = np.empty((num_patches_x, num_patches_y, patch_size, patch_size), dtype=a.dtype)\n\n# Fill the result array\nfor i in range(num_patches_x):\n    for j in range(num_patches_y):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n\n# Remove the extra rows and columns\nresult = result[:num_patches_x, :num_patches_y, :patch_size, :patch_size]\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high+1]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "import numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(eval(string))\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.random.uniform(min, max, n)\nresult = np.log(result)\nresult = np.exp(result)\nresult", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.random.exponential(scale=np.log(max), size=n) + np.log(min)\n", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.power(np.random.uniform(min, max, n), 1.0/np.log(max/min))\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.array([])", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, index)\n", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = np.sub2ind(a.shape, index)\nprint(result)", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype=[('int', 'int32'), ('float', 'float32')])\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.array([np.sum(a[accmap==i]) for i in range(a.size)])", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.maximum.at(a, index, a)\nprint(result)\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.cumsum(a[accmap >= 0])", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = a[index]\nresult = np.min(result)\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.vectorize(elementwise_function)(x, y)\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, p=probabilit)", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "\n# Create a larger array with zeros\npad_array = np.zeros((a.shape[0] + high_index + low_index, a.shape[1] + high_index + low_index))\n\n# Copy the original array into the larger array\npad_array[high_index:-high_index, high_index:-high_index] = a\n\n# Now you can slice the larger array\nresult = pad_array[low_index:-low_index, low_index:-low_index]\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.iscomplex(x)]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, len(data)//bin_size)\nbin_data_mean = [np.mean(bins) for bins in bin_data]\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.lib.split(data, np.arange(bin_size, len(data) + 1, bin_size))\nbin_data_max = [max(bin) for bin in bin_data]\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.lib.arraysetops.partition(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, data.shape[1]//bin_size)\nbin_data_mean = [np.mean(bin_data[i], axis=0) for i in range(len(bin_data))]\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    t = (x - x_min) / (x_max - x_min)\n    t = max(0, min(1, t))\n    t = t * t * (3 - 2 * t)\n    return t\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0.0, 1.0)\n    t = t * t * (3. - 2. * t)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (t - 1.)\n    t = t * t * (t + 1.)\n    t = t * t * (", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\n# First, we need to create a circular version of b\nb_circular = np.roll(b, 1)\n\n# Then, we can perform the circular correlation\nresult = np.correlate(a, b_circular, mode='full')\n\n# Finally, we need to remove the extra elements from the result\nresult = result[:len(a)]\n\nprint(result)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.groupby('major').apply(lambda x: x.values.reshape(1, -1, 5)).values", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = df.groupby('major').apply(lambda x: x.values.reshape(1,4,5)).values", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.array([list(np.binary_repr(num, width=m)) for num in a])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = result.astype(int)\nprint(result)\n", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.array([list(np.binary_repr(i, width=m)) for i in a])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "\ndef convert_to_binary(num, m):\n    binary_array = np.unpackbits(np.uint8(num))\n    binary_array = np.pad(binary_array, (0, m - len(binary_array)), 'constant', constant_values=(0))\n    return binary_array\n\nresult = np.array([convert_to_binary(num, m) for num in a])\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\nresult = (\u03bc - 3*\u03c3, \u03bc + 3*\u03c3)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nresult = (mean - 2*std_dev, mean + 2*std_dev)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": "\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    start = mean - 3*std_dev\n    end = mean + 3*std_dev\n    return (start, end)\n    ### END SOLUTION\nf()\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\nresult = (a >= lower_bound) & (a <= upper_bound)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.compressed(), percentile)\nprint(prob)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[:, 0] = 0", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.argmax(a, axis=1)\nmask = mask.reshape(-1, 1)\nmask = mask == np.arange(mask.shape[0])[:, None]\nmask", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "\n# Convert the lists to numpy arrays\npost = np.array(post)\ndistance = np.array(distance)\n\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(post, distance)[0, 1]\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ij,ik->ijk', X, X)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.linalg.solve(Y.reshape(-1, Y.shape[-1]), np.ones(Y.shape[-1]))\nX = X.reshape(Y.shape[:-1])\n", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = A[~np.isin(A, B)]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[np.in1d(A, B)]", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "mask = np.isin(A, B)\nC = A[mask]", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = len(a) - rankdata(a) + 1", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = len(a) - rankdata(a).astype(int) + 1", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    return rankdata(a, method='max').tolist()\n    ### END SOLUTION ###\n\nresult = f()\nprint(result)\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))[0]", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.linalg.norm(X, 1, axis=1)\nresult = X / l1[:, np.newaxis]", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "\nresult = LA.norm(X, axis=1, ord=2)", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = X / x[:, np.newaxis]", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].str.contains(target)\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "\ndistances = np.sqrt(np.sum((a - a.T)**2, axis=1))\nresult = distances\nprint(result)\n", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "from scipy.spatial.distance import pdist, squareform\nresult = squareform(pdist(a, 'euclidean'))", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "\ndef euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b) ** 2))\n\nresult = np.array([[euclidean_distance(a[i], a[j]) for j in range(len(a))] for i in range(len(a))])\nresult = np.triu(result)\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# Remove the adjacent (before removing) duplicate non-zero value and all the zero value\nresult = np.unique(a[a != 0])\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# Create a mask to exclude adjacent duplicates\nmask = np.concatenate(([True], (a[:-1] != a[1:]) & (a[:-1] != 0) & (a[1:] != 0), [True]))\n\n# Apply the mask to the array\nresult = a[mask]", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf = df.reset_index(drop=True)\ndf = df.reindex(columns=['lat', 'lon', 'val'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ### END SOLUTION\n    return df\ndf = f(lat, lon, val)\nprint(df)\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df.max(axis=1)\ndf = df.reset_index(drop=True)\ndf = df.reindex(columns=['lat', 'lon', 'val', 'maximum'])\ndf", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "\ndef rolling_window(a, size):\n    shape = a.shape[:-1] + (a.shape[-1] - size[0] + 1, size[1])\n    strides = a.strides[:-1] + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "\ndef rolling_window(a, size):\n    shape = a.shape[:-1] + (a.shape[-1] - size[0] + 1, size[1])\n    strides = a.strides[:-1] + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:,:-1]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = any(np.array_equal(c, cnt) for cnt in CNTS)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = any(np.array_equal(c, cnt) for cnt in CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\ndf\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.eye(i.shape[0])\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[1:-1, 1:-1] = 0", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\ndelta = end_date - start_date\nresult = pd.date_range(start=start_date, periods=n, freq=delta)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No such index\")\nelse:\n    print(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))[0]\nif indices.size > 0:\n    result = indices\nelse:\n    result = np.array([])\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "\n# calculate the coefficients a, b and c using the numpy.polyfit function\ncoeff = np.polyfit(x, y, 3)\nresult = coeff[::-1]\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "\n# Create a matrix with the x values as columns\nX = np.column_stack((np.ones(len(x)), np.array(x)))\n\n# Use numpy's linalg.lstsq function to solve for the coefficients\ncoeffs, residuals, rank, s = np.linalg.lstsq(X, np.array(y), rcond=None)\n\n# Reverse the coefficients to get the coefficients in the order of highest to lowest order\nresult = coeffs[::-1]\n\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = [0,1,2,3]\ndf = df.sub(temp_arr, axis=1)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "\nmask = arr < -10\narr[mask] = 0\n\nmask2 = (arr >= -10) & (arr < 15)\narr[mask2] = 30\n\nmask3 = arr >= 15\narr[mask3] = arr[mask3] + 5\n\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "\narry_temp = arr.copy()\nmask = arry_temp < n1\nmask2 = arry_temp < n2\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arry[mask3] + 5\narry[~mask2] = 30 \n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "print np.nonzero(np.logical_or(np.isnan(s1), np.isnan(s2)))[0].shape[0]\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a[1]) and np.array_equal(a[0], a[2]) and np.array_equal(a[1], a[2])\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all() for arr in a)", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "\n# Calculate the number of rows and columns needed to reach the shape\nrows_needed = np.ceil(a.shape[0] / shape[0])\ncols_needed = np.ceil(a.shape[1] / shape[1])\n\n# Calculate the number of rows and columns to pad to reach the shape\nrows_to_pad = shape[0] - a.shape[0] // rows_needed\ncols_to_pad = shape[1] - a.shape[1] // cols_needed\n\n# Zero pad the array\nresult = np.pad(a, ((0, rows_to_pad), (0, cols_to_pad)))\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "\n# Calculate the number of rows and columns needed to match the shape\nrows_needed = np.ceil(a.shape[0] / shape[0])\ncols_needed = np.ceil(a.shape[1] / shape[1])\n\n# Calculate the number of rows and columns to pad to match the shape\nrows_to_pad = shape[0] - a.shape[0] // rows_needed\ncols_to_pad = shape[1] - a.shape[1] // cols_needed\n\n# Zero pad the array\nresult = np.pad(a, ((0, rows_to_pad), (0, cols_to_pad)))\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "\n# Calculate the number of rows and columns needed to pad the array\npad_rows = [(0, max(0, shape[0] - a.shape[0])) for i in range(a.shape[0])]\npad_cols = [(0, max(0, shape[1] - a.shape[1])) for i in range(a.shape[1])]\n\n# Pad the array\nresult = np.pad(a, pad_rows + pad_cols, mode='constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    diff_row = shape[0] - arr.shape[0]\n    diff_col = shape[1] - arr.shape[1]\n    result = np.pad(arr, ((0, diff_row), (0, diff_col)))\n    ### END SOLUTION ###\n\nresult = f(example_arr, shape=(93,13))\nprint(result)\nprint(result.shape)\n", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "\n# calculate the padding needed\npad_top = (shape[0] - a.shape[0]) // 2\npad_bottom = shape[0] - a.shape[0] - pad_top\npad_left = (shape[1] - a.shape[1]) // 2\npad_right = shape[1] - a.shape[1] - pad_left\n\n# zero pad the array\nresult = np.pad(a, ((pad_top, pad_bottom), (pad_left, pad_right)))\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[:, :, None]]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[:, :, None]])", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[:, :, None]])", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "\nmask = (df['a'] > 1) & (df['a'] <= 4)\nresult = df.loc[mask, 'b'].values\nresult = np.where(mask, df.loc[mask, 'b'].values, np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "\n# Create a mask where the image is not all zeros\nmask = im.any(axis=1)\n\n# Create a new array with the same shape as the original image, but with the mask applied\nresult = im[mask]\n\n# Reshape the result to have the same shape as the original image\nresult = result.reshape(-1, result.shape[-1])\n\n# Remove the rows with all zeros\nresult = result[result.any(axis=1)]\n\n# Remove the columns with all zeros\nresult = result[:, result.any(axis=0)]\n\nprint(result)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "mask = np.any(A, axis=0)\nresult = A[mask]\nmask = np.any(A, axis=1)\nresult = result[:, mask]\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\n# First, we need to identify the non-zero elements in the image.\n# Then, we need to identify the rows and columns that contain these non-zero elements.\n# Finally, we need to create a new image with the non-zero elements, but with the rows and columns that contain these non-zero elements.\n\n# Identify the non-zero elements in the image.\nnon_zero_elements = im[np.nonzero(im)]\n\n# Identify the rows and columns that contain these non-zero elements.\nrows = np.unique(np.where(im)[0])\ncolumns = np.unique(np.where(im)[1])\n\n# Create a new image with the non-zero elements, but with the rows and columns that contain these non-zero elements.\nresult = np.zeros((len(rows), len(columns)))\nfor i in range(len(rows)):\n    for j in range(len(columns)):\n        result[i, j] = non_zero_elements[np.where(np.logical_and(im == rows[i], im == columns[j]))]\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "\n# First, create a mask where the image is not entirely zero\nmask = im.any(axis=1)\n\n# Then, create a new array where the mask is True\nresult = im[mask]\n\n# Finally, transpose the result to get the columns as rows\nresult = result.T\n\n# Now, the result is a 2D array with the rows of the original image, but with the peripheral zeros removed\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(0.1))\nplt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.figure(figsize=(10, 6))\nfor i in range(10):\n    plt.plot(x, np.random.rand(10), linestyle='dashed', alpha=0.5, label=f'line style {i}')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.figure(figsize=(10, 6))\nfor i in range(10):\n    plt.plot(x, np.random.rand(10), linestyle='dashed', alpha=0.5, label=f'line style {i}')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, marker='d', linestyle='-', color='blue')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, marker='d', linestyle='-', linewidth=2, markersize=10)\nplt.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, 'r--')\nplt.fill_between(range(2, 4), x[2:4], color='red', alpha=0.5)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    height=5,\n    aspect=1.5,\n)\nplt.show()\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style(\"whitegrid\")\nsns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, marker='+', markersize=7, linestyle='-')\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.legend(title='xyz', title_fontsize=20)\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.setp(l, color='black')\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), \"r-\", lw=5, markersize=30)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "plt.xticks(np.arange(0, 2 * np.pi, 2))\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.imshow(H, cmap='hot', interpolation='nearest')\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# split the title into words\nwords = myTitle.split(' ')\n# get the longest word\nlongest_word = max(words, key=len)\n# get the number of characters in the longest word\nnum_characters = len(longest_word)\n# get the number of lines needed\nnum_lines = len(words)\n# get the number of characters per line\ncharacters_per_line = num_characters / num_lines\n# get the number of characters in the title\ntotal_characters = len(myTitle)\n# get the number of lines needed\nnum_lines = total_characters / characters_per_line\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the number of lines needed\nnum_lines = int(num_lines)\n# get the", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = -y\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5])\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.plot(x, y)\nplt.xticks(np.arange(min(x), max(x)+1, 1))\nplt.yticks(np.arange(min(y), max(y)+1, 1))\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().get_yaxis().get_major_formatter().labelOnlyBase = False\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax.plot(x, y, linestyle='--')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_title('sin(x)')\naxs[1].plot(x, y2, label='cos(x)', axes=axs[1])\naxs[1].set_title('cos(x)')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y1, color='blue')\naxs[0].set_title('Sine Wave')\naxs[1].plot(x, y2, color='red')\naxs[1].set_title('Cosine Wave')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks(rotation=90)\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks([3, 4])\nplt.grid(axis='x')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(min(y), max(y), 1))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5)\nplt.axvline(x=3, color='r', linestyle='-', linewidth=0.5)\nplt.axvline(x=4, color='r', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(min(y), max(y), 1))\nplt.xticks(np.arange(min(x), max(x), 1))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5)\nplt.grid(axis='x', which='both', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), gridspec_kw={'hspace': 0.3, 'wspace': 0.3})\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.xaxis.tick_top()\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.xticks(np.arange(0, 10, 2))\nplt.ylabel(\"Y\")\nplt.yticks(np.arange(0, 10, 2))\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks([])\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().tick_params(axis='y')\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\nplt.xticks([])\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green', scatter_kws={'color':'blue'})\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='g')\nplt.show()\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips)\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df[\"celltype\"], df[\"s1\"], color=\"blue\")\nplt.bar(df[\"celltype\"], df[\"s2\"], color=\"red\")\nplt.xlabel(\"celltype\", rotation=45)\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.ylabel(\"Y\", color=\"red\")\nplt.xticks(color=\"red\")\nplt.yticks(color=\"red\")\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='Y over X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y over X')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.xticks(x, rotation='vertical')\nplt.tick_params(axis='both', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "\n# import numpy as np\n# x = np.array([0.22058956, 0.33088437, 2.20589566])\n# y = np.array([0, 0, 1])\n# plt.plot(x, y, 'o-')\n# plt.show()\n\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.imshow(rand_mat, cmap='hot', interpolation='nearest')\nplt.xticks(range(len(xlabels)), xlabels, rotation=90)\nplt.yticks(range(len(ylabels)), ylabels)\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[1].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].set_title('Y')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n                palette=\"deep\", size=\"body_mass_g\", sizes=(30, 300), legend=False)\nplt.show()\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.figure(figsize=(10, 6))\nplt.scatter(b, a, c=c, cmap='viridis')\nplt.xlabel('b')\nplt.ylabel('a')\nplt.title('Scatter plot of a over b')\nplt.colorbar()\nfor i, j, k in zip(b, a, c):\n    plt.annotate(k, (j, i))\nplt.show()\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend', fancybox=True)\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, alpha=0.6, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y=x')\naxs[0].set_title('First subplot')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[0].legend()\n\naxs[1].plot(x, y, label='y=x')\naxs[1].set_title('Second subplot')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].hist(x, bins=5, alpha=0.5, label='x')\naxs[0].hist(y, bins=5, alpha=0.5, label='y')\naxs[0].legend()\naxs[0].set_title('Grouped Histogram of x and y')\n\naxs[1].hist(x, bins=5, alpha=0.5, label='x')\naxs[1].hist(y, bins=5, alpha=0.5, label='y')\naxs[1].legend()\naxs[1].set_title('Grouped Histogram of x and y')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.plot([a, c], [b, d])\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, axs = plt.subplots(2, 1)\n\n# create a colormap\ncmap = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_x = cmap(x)\ncmap_y = cmap(y)\n\n# plot x and y on the first subplot\naxs[0].imshow(x, cmap=cmap_x)\naxs[0].set_title('x')\n\n# plot x and y on the second subplot\naxs[1].imshow(y, cmap=cmap_y)\naxs[1].set_title('y')\n\n# create a colorbar for the first subplot\ncbar = fig.colorbar(axs[0].imshow(x, cmap=cmap_x), ax=axs[0])\n\n# create a colorbar for the second subplot\ncbar = fig.colorbar(axs[1].imshow(y, cmap=cmap_y), ax=axs[1])\n\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[0].plot(a, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].plot(x, z, label='Y')\naxs[1].plot(a, y, label='Z')\naxs[1].set_title('Y and Z')\n\n# Add a legend to the main title\naxs[0].legend()\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.figure(figsize=(10, 6))\nplt.plot([x[0] for x in points], [x[1] for x in points])\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-scale y-axis')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "plt.figure()\nfor line, color in zip(lines, c):\n    plt.plot(*zip(*line), color=color)\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.figure(figsize=(8, 6))\nplt.loglog(x, y, basex=10, basey=10)\nplt.xlabel('x (base 10)')\nplt.ylabel('y (base 10)')\nplt.title('y over x on a log-log plot')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "\nfig, ax = plt.subplots(4, 1)\n\nfor i, column in enumerate(df.columns):\n    ax[i].plot(df.index, df[column])\n    ax[i].scatter(df.index, df[column])\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "\n# Calculate the cumulative sum of the data\ncumulative_sum = np.cumsum(data)\n\n# Normalize the cumulative sum to get the probabilities\nprobabilities = cumulative_sum / cumulative_sum[-1]\n\n# Create a bar chart of the data\nplt.bar(range(len(data)), data, alpha=0.5, color='blue')\nplt.plot(range(len(data)), probabilities, color='red')\n\n# Set y tick labels as percentage\nplt.yticks(np.arange(0, max(data) + 1, 1000))\n\n# Set y tick labels as 10%, 20%, etc.\nplt.ylabel('Percentage')\nplt.xticks(range(len(data)), ['1000', '2000', '3000', '4000', '5000', '6000', '7000'])\n\n# Set the x tick labels as the data points\nplt.xticks(rotation=90)\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=5, markeredgecolor='red', markerfacecolor='blue', markeredgewidth=1.5)\nplt.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, y, label='y')\naxs[0].legend(loc='upper right')\n\naxs[1].plot(x, z, label='z')\naxs[1].plot(a, z, label='z')\naxs[1].legend(loc='upper right')\n\nfig.legend(loc='upper right')\n\nplt.show()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=False, sharey=False)\n\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['second'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label='$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(range(0, 10, 2), [2.1, 3, 7.6])\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\nplt.tight_layout()\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.gca().set_xticks(rotation=-60)\nplt.tick_params(axis='y', which='both', direction='out')\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(x, minor=True)\nplt.gca().get_xticklabels()\nplt.gca().get_xticklabels()[1].set_alpha(0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0.05)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0.0, y=0.0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 1)\naxs.plot(x, y, label='y over x')\naxs.set_title('Figure')\naxs.legend()\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "df.plot(kind='line', x='Type A', y='Type B', ax=plt.gca())\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Line Chart')\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='||')\nplt.show()\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolors='none', hatch='|')\nplt.show()\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*', hatch='//')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*^', s=100, hatch='|')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.xlim(0, 5)\nplt.ylim(1, 4)\nplt.imshow(data, extent=(0, 5, 1, 4))\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(y, x, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.axvline(x=3, color='r', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 10))\nplt.pie(height, labels=labels, autopct='%1.1f%%', startangle=140)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedgeprops={'width': 0.4})\nplt.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\nplt.grid(which='major', linestyle='-', linewidth='1')\nplt.show()\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.title(\"Activity Distribution\")\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.title(\"Activity Distribution\")\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='none', markeredgecolor='blue', markeredgewidth=2)\nplt.show()\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "plt.bar(np.arange(3), blue_bar, color='b', align='center')\nplt.bar(np.arange(3) + 0.2, orange_bar, color='orange', align='center')\n\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.xticks(np.arange(0, max(x)+1, 1))\nplt.yticks(np.arange(0, max(y)+1, 1))\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "\nsns.catplot(x=\"bill_length_mm\", y=\"species\", hue=\"sex\", data=df, height=5, aspect=0.9, kind=\"bar\", sharey=False)\n\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(5, 5))\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 0], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [1, 0], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r\"$\\phi$\", fontsize=16, fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc='center left', bbox_to_anchor=(0.3, 0.5), title=\"Line\")\nplt.show()\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05), fancybox=True, shadow=True)\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.show()\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "fig, ax = plt.subplots()\n\n# \u7ed8\u5236\u6570\u636e\nim = ax.imshow(data, cmap='viridis')\n\n# \u6dfb\u52a0\u989c\u8272\u6761\ncbar = plt.colorbar(im)\n\n# \u663e\u793a\u56fe\u5f62\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\nplt.title('Figure 1', fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 1", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolors='black')\nplt.show()\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_title('Subplot')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 6))\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 1], 'r--')\nplt.plot([-1, 1], [0, 0], 'r--')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color='r')\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(a, z, label='Z')\naxs[0].set_title('Y')\naxs[1].set_title('Z')\naxs[1].set_position_above()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1)\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nsns.heatmap(df, cbar=False, cmap=\"viridis\", linewidths=0.5, annot=True, fmt=\".2f\",\n            xticklabels=df.columns, yticklabels=df.columns)\nplt.title(\"Penguins Data\")\nplt.show()\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Line Chart of Y over X')\nplt.xticks(x)\nplt.yticks(y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(x)\nplt.yticks(y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xticks(x, rotation=90)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "\nplt.figure(figsize=(10, 6))\n\n# Create a scatter plot for \"time\" vs \"pulse\" with \"kind\" as hue and \"diet\" as col\nsns.scatterplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\n\n# Set the title of the plot\nplt.title(\"Group: Fat vs No Fat\")\n\n# Show the plot\nplt.show()\n\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "\nplt.figure(figsize=(10, 6))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.9, kind=\"point\")\nplt.xticks(rotation=90)\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Exercise Time vs Pulse\")\nplt.show()\n\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "plt.figure(figsize=(10, 5))\n\n# First subplot\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Time vs Pulse\")\nplt.grid(False)\n\n# Second subplot\nplt.subplot(1, 2, 2)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Time vs Pulse\")\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='upper right', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "fig, ax = plt.subplots()\nax.plot(t, a, label='sin(t)')\nax.plot(t, b, label='cos(t)')\nax.plot(t, c, label='sin(t) + cos(t)')\nax.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=0.3, palette=\"deep\")\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\ng = sns.FaceGrid(axes, marginals=[axes], fig=fig)\n\nfor ax, c in zip(g.axes.flat, df[\"c\"].values):\n    ax.plot(df[\"a\"], c, \"o\")\n    ax.set_xticks(np.arange(min(df[\"a\"]), max(df[\"a\"]), 2))\n    ax.set_xticklabels(np.arange(min(df[\"a\"]), max(df[\"a\"]), 2))\n\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azim=100, elev=50)\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[0, 1], height_ratios=[1, 0])\naxes = []\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.imshow(x[i])\n    ax.axis('off')\n    axes.append(ax)\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\nnum_classes = 10\nnum_labels = len(labels)\n\n# Create a tensor of zeros with the same shape as the labels\nresult = tf.zeros([num_classes, num_labels], dtype=tf.int32)\n\n# Set the values of the result tensor to the corresponding labels\nfor i, label in enumerate(labels):\n    result[label][i] = 1\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\nnum_classes = 10\nnum_samples = len(labels)\n\n# Create a tensor of ones with the same shape as the labels\nones_tensor = tf.ones((num_samples, num_classes), dtype=tf.int32)\n\n# Create a tensor of zeros with the same shape as the labels\nzeros_tensor = tf.zeros((num_samples, num_classes), dtype=tf.int32)\n\n# Create a tensor of ones where the labels are in the range of the tensor shape\nresult = tf.tensor_scatter_nd_update(ones_tensor, tf.constant(labels), 1)\n\n# Create a tensor of zeros where the labels are not in the range of the tensor shape\nresult = tf.tensor_scatter_nd_update(zeros_tensor, tf.constant(labels), 1)\n\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "\n# Convert the list to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a tensor of zeros with the same shape as the labels tensor\nresult = tf.zeros_like(labels_tensor)\n\n# Set the values of the result tensor to the corresponding values in the labels tensor\nresult = tf.scatter_nd(tf.expand_dims(labels_tensor, 1), tf.range(10), result.shape)\n\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": "\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ### END SOLUTION\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\n# Convert the list of integers to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a tensor of ones with the same shape as the labels tensor\nones_tensor = tf.ones_like(labels_tensor)\n\n# Create a tensor of zeros with the same shape as the labels tensor\nzeros_tensor = tf.zeros_like(labels_tensor)\n\n# Create a tensor of ones with the same shape as the labels tensor\nresult = tf.where(labels_tensor == 0, ones_tensor, zeros_tensor)\n\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.concat([[i], [i+1], [i+2]], axis=0)\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return [i for sublist in input for i in sublist]\n    ### END SOLUTION ###\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(f)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding = tf.sequence_padding(ones, lengths, padding_value=0)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.sequence_padding(zeros, lengths, padding_value=1)\n\n# Combine the padding tensor with the original ones tensor\nresult = tf.where(padding == 0, ones, result)\n\n# Combine the padding tensor with the original zeros tensor\nresult = tf.where(padding == 0, zeros, result)\n\n# Combine the padding tensor with the original ones tensor\nresult = tf.where(padding == 1, ones, result)\n\n# Combine the padding tensor with the original zeros tensor\nresult = tf.where(padding == 1, zeros, result)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones_tensor = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros_tensor = tf.zeros_like(lengths)\n\n# Create a tensor of indices from 0 to the maximum length\nindices = tf.range(tf.reduce_max(lengths) + 1)\n\n# Create a tensor of indices padded with ones\npadded_indices = tf.pad(indices, [[0, 8 - tf.reduce_max(lengths)], [0]])\n\n# Create a mask by selecting the elements in the padded indices tensor that are less than or equal to the corresponding elements in the lengths tensor\nmask = tf.scatter_nd(padded_indices, ones_tensor)\n\n# Create a mask by selecting the elements in the zeros tensor that are greater than the corresponding elements in the lengths tensor\nmask = tf.scatter_nd(padded_indices, zeros_tensor)\n\n# Print the result\nprint(mask)\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n\n# Convert the mask to a tensor of integers\nmask = tf.cast(mask, tf.float32)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(mask)\n\n# Subtract the mask from the ones tensor\nresult = ones - mask\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": "\n    max_length = tf.constant(8)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\n    result = tf.cast(mask, tf.int32)\n    return result\n\nresult = f(example_lengths)\nprint(result)\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of indices from 0 to the maximum length\nindices = tf.range(tf.reduce_max(lengths) + 1)\n\n# Create a mask by selecting the ones and zeros at the indices specified by the lengths tensor\nmask = tf.tensor_scatter_nd_update(ones, tf.expand_dims(indices, 1), axis=0)\nmask = tf.tensor_scatter_nd_update(mask, tf.expand_dims(lengths, 1), axis=0)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nfull_mask = tf.ones_like(mask)\n\n# Create a mask by selecting the ones and zeros at the indices specified by the lengths tensor\nresult = tf.tensor_scatter_nd_update(full_mask, tf.expand_dims(indices, 1), axis=0)\nresult = tf.tensor_scatter_nd_update(result, tf.expand_dims(lengths, 1), axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "cartesian_product = tf.expand_dims(a, 1) * tf.expand_dims(b, 0)\nresult = tf.reshape(cartesian_product, [-1, 2])\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "\n    cartesian_product = tf.expand_dims(a, 1) * tf.expand_dims(b, 0)\n    return tf.reshape(cartesian_product, [-1, 2])\n\nresult = f(example_a, example_b)\nprint(result)\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "result = tf.squeeze(a, [2])", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.expand_dims(a, axis=2)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "result = tf.expand_dims(tf.expand_dims(a, axis=1), axis=1)", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\n# Calculate the L2 distance element-wise\ndiff = tf.square(tf.subtract(a, b))\n\n# Calculate the sum of the squares\nsum_of_squares = tf.reduce_sum(diff, axis=0)\n\n# Calculate the L2 distance\nresult = tf.sqrt(sum_of_squares)\n\nprint(result)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "distances = tf.square(tf.subtract(a, b))\nresult = tf.reduce_sum(distances, axis=0)\nprint(result)", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": "\n    return tf.sqrt(tf.reduce_sum(tf.square(tf.sub(A, B))))\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "\nresult = tf.gather_nd(x, tf.stack([y, z], axis=1))\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = tf.gather_nd(x, tf.stack([row, col], axis=1))\nresult = m.numpy()\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "\n    return tf.gather_nd(x, tf.stack([y, z], axis=1))\n\nresult = f(example_x, example_y, example_z)\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\nresult = tf.einsum('ijk,ilk->ijl', A, B)\nresult = tf.reduce_sum(result, axis=-1)\n\nprint(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "\nresult = tf.einsum('ijk,ilk->ijl', A, B)\nresult = tf.reduce_sum(result, axis=-1)\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\nx = tf.constant(x)\nresult = tf.strings.unicode_transcode(x, \"UTF-8\")\nresult = tf.strings.unicode_transcode(result, \"UTF-8\")\nresult = tf.strings.unicode_transcode(result, \"UTF-8\")\nresult = tf.strings.unicode_transcode(result, \"UTF-8\")\nresult = tf.strings.unicode_transcode(result, \"UTF-8\")\n\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": "\n    return [i.decode('utf-8') for i in x]\n    ### END SOLUTION\n\nresult = f(example_x)\nprint(result)\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "mask = tf.cast(tf.math.reduce_any(x != 0, axis=-1), tf.float32)\nresult = tf.reduce_sum(x, axis=-1) / mask\nresult = tf.where(tf.math.not_equal(mask, 0), result, tf.zeros_like(result))\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = tf.not_equal(x, 0)\nresult = tf.reduce_sum(x * mask, axis=-2) / tf.reduce_sum(mask, axis=-2)\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    mask = tf.math.reduce_any(x != 0, axis=-1)\n    y = tf.reduce_sum(x, axis=-1) / tf.cast(tf.reduce_sum(mask, axis=-1), tf.float32)\n    return y\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\ntry:\n    Session = tf.Session\nexcept AttributeError:\n    Session = tf.compat.v1.Session\n\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\n\nwith Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\n# Get the indices of the maximum values in each row\nindices = tf.argmax(a, axis=1)\n\n# Convert the indices to a tensor of type tf.int32\nindices = tf.cast(indices, tf.int32)\n\n# Convert the indices to a tensor of type tf.int64\nindices = tf.cast(indices, tf.int64)\n\n# Convert the indices to a tensor of type tf.float32\nindices = tf.cast(indices, tf.float32)\n\n# Print the result\nprint(indices)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\nresult = tf.argmax(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": "\n    return tf.argmax(a, axis=1)\n    ### END SOLUTION\n\nprint(f(example_a))\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "\nresult = tf.argmin(a, axis=1) + 1", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save(\"my_model\")\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=4, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[114], minval=2, maxval=5, dtype=tf.int32)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform(shape=[10], minval=1, maxval=4, dtype=tf.int32)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "import tensorflow as tf\n\nprint(tf.__version__)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "coeff = np.polyfit(np.log(x), y, 1)\nresult = np.array([np.exp(coeff[1]), coeff[0]])\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "coeff = np.polyfit(x, y, 2)\nresult = coeff\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "test_statistic, p_value = stats.ks_2samp(x, y)\n", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + ((3 * a - b - c)**2 + sin(b) + cos(b) + 4)\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.norm.sf(z_scores)\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [2 * (1 - scipy.stats.norm.cdf(z, mu, sigma)) for z in z_scores]\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = np.array([scipy.stats.norm.ppf(1 - p) for p in p_values])", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm([stddev],loc=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa.dot(sb)", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": "\n    return sA.dot(sB)\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "\n# Create a grid from the points\nx, y, z = np.mgrid[0:30:100j, 0:30:100j, 0:-30:100j]\n\n# Interpolate the moisture V values onto the grid\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "\n# Create a grid from the points\nx, y, z = np.mgrid[points[:, 0].min():points[:, 0].max():100j,\n                   points[:, 1].min():points[:, 1].max():100j,\n                   points[:, 2].min():points[:, 2].max():100j]\n\n# Interpolate the moisture V at the points specified in the request\ninterp = scipy.interpolate.LinearNDInterpolator((x, y, z), V)\nresult = interp(request)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, mode='reflect')\nxrot = x0 + np.cos(np.radians(angle))*data_rot.shape[1]\nyrot = y0 + np.sin(np.radians(angle))*data_rot.shape[0]\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "\n# Extract the main diagonal of a sparse matrix\ndiagonal = M.diagonal()\n\n# Convert the diagonal to a numpy array\nresult = diagonal.toarray()\n\nprint(result)\n", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, 'uniform')\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    ks_result = stats.kstest(times, 'uniform')\n    return ks_result\n    ### END SOLUTION ###\nresult = f(example_times, example_rate, example_T)\nprint(result)\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "# Convert the list of times to numpy array\ntimes = np.array(times)\n\n# Perform the K-S test\nresult = stats.kstest(times, 'uniform')\n\n# Print the result\nprint(result)", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "dist_matrix = scipy.spatial.distance.cdist(points1, points2)\nresult = scipy.optimize.linear_sum_assignment(dist_matrix)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "dist_matrix = scipy.spatial.distance.cdist(points1, points2)\nresult = scipy.optimize.linear_sum_assignment(dist_matrix)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.data[:] = 0\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "labels, num_regions = ndimage.label(img > threshold)\nresult = num_regions\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "labels, num_labels = ndimage.label(img < threshold)\nresult = num_labels\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    labels, num_labels = ndimage.label(img > threshold)\n    return num_labels\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\n# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the coordinates of the regions\ncoords = np.argwhere(mask)\n\n# Calculate the center of mass of the regions\ncenters = np.array([(np.mean(coord[0]), np.mean(coord[1])) for coord in coords])\n\n# Calculate the distance between the center of mass of the regions and the top left corner\ndistances = np.sqrt(np.square(centers[:, 0]) + np.square(centers[:, 1]))\n\nresult = list(distances)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T - np.diag(M.diagonal())\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": "\n    sA = (sA + sA.T)/2\n    sA = sA + sA.T - np.diag(sA.diagonal())\n    ### END SOLUTION\n    return sA\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\n# Create a mask of the square\nmask = np.zeros(square.shape, dtype=bool)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask)\n\n# Invert the mask\nmask = np.logical_not(mask)\n\n# Apply the mask to the square\nsquare = np.where(mask, 0, square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask where the image is not zero\nmask = square != 0\n\n# Use scipy.ndimage.label to get the connected components\nlabels, n_components = scipy.ndimage.label(mask)\n\n# Create a new array with the same shape as the original image, but filled with zeros\nnew_square = np.zeros_like(square)\n\n# For each component, if it is not isolated (i.e., it has more than one connected neighbour),\n# set the corresponding pixels in the new image to the maximum value in the original image\nfor component in range(1, n_components):\n    mask_component = labels == component\n    if np.any(mask_component[:-1, :] + mask_component[1:, :] + mask_component[:, :-1] + mask_component[:, 1:]):\n        new_square[mask_component] = np.max(square[mask_component])\n\n# Set the isolated components to zero\nnew_square[labels == 0] = 0\n\n# Print the new image\nprint(new_square)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_val = np.max(col.toarray())\nmin_val = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\nimport scipy.stats as stats\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\nmedian = np.median(col.toarray())\nmode = stats.mode(col.toarray())[0][0]\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *coeffs):\n    return sum(c * np.cos(n * np.pi * x / tau) for n, c in enumerate(coeffs))\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\nflat_array = example_array.flatten()\n\n# Create a boolean mask to exclude the zero values\nmask = flat_array != 0\n\n# Apply the mask to the flattened array\nflat_array = flat_array[mask]\n\n# Calculate the pairwise Euclidean distances\ndistances = scipy.spatial.distance.pdist(flat_array, metric='euclidean')\n\n# Convert the pairwise distances to a 2D array\ndistances_array = np.array(distances).reshape(len(flat_array), len(flat_array))\n\n# Convert the distances to meters\ndistances_array = distances_array * example_array.mean()\n\n# Create a dataframe to store the results\nresult = pd.DataFrame(distances_array, index=flat_array, columns=flat_array)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Convert the array to a 1D array\nflat_array = example_array.flatten()\n# Create a boolean mask to exclude the zero values\nmask = flat_array != 0\n# Apply the mask to the flattened array\nflat_array = flat_array[mask]\n# Create a pairwise distance matrix\ndistances = scipy.spatial.distance.pdist(flat_array.reshape(-1, 1), 'cityblock')\n# Convert the pairwise distances to a 2D array\ndistances_array = np.array(distances).reshape(len(flat_array), len(flat_array))\n# Convert the 2D array to a list of tuples\nresult = [(flat_array[i], flat_array[j], distances_array[i][j]) for i in range(len(flat_array)) for j in range(len(flat_array))]\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Convert the raster to a 1D array\n    raster_array = example_array.flatten()\n    # Calculate pairwise Euclidean distances\n    distances = scipy.spatial.distance.pdist(raster_array, metric='euclidean')\n    # Convert the distances to a 2D array\n    distances_array = np.array(distances).reshape(len(raster_array), len(raster_array))\n    # Find the minimum distance between each pair of regions\n    min_distances = np.min(distances_array, axis=1)\n    # Convert the minimum distances to the original units\n    min_distances_in_meters = min_distances * np.max(raster_array)\n    # Return the result\n    return min_distances_in_meters\n    ### END SOLUTION ###\n\nresult = f(example_array)\nprint(result)\n", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "result = np.empty((5, 100))\nfor i in range(5):\n    tck, u = interpolate.splprep([x[i], y[i]], s=0)\n    result[i] = interpolate.splev(x_val, tck)\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x = np.array([x1, x2, x3, x4])\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x)\nprint(statistic, critical_values, significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nresult = ss.anderson_ksamp([x1, x2])\nprint(result)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "\ndef tau1(x):\n    y = np.array(df['A']) # keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = df['A'].rolling(window=3).apply(lambda x: tau1(x))\ndf['AC'] = df['A'].rolling(window=3).apply(lambda x: tau1(x))\ndf['BC'] = df['B'].rolling(window=3).apply(lambda x: tau1(x))\n\ndf", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "result = bool(sa.data.count(0))", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "result = bool(sa.count_nonzero()) == 0\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores)[1]", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n    ### END SOLUTION ###\n    return p_value\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "\nfrom scipy.stats import kurtosis\n\nkurtosis_result = kurtosis(a)[0]\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "z_interp = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = z_interp(x, y)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    interpolated = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n    result = interpolated(s, t)\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "import numpy as np\n\n# Create a list of the indices of the regions occupied by the extra points\nresult = np.array([vor.point_region[i] for i in extraPoints])\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "import numpy as np\n\n# Create a list of indices for the extra points\nextra_indices = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the Voronoi diagram\nvor_points = [vor.vertices[i] for i in range(len(vor.vertices))]\n\n# Create a list of the regions in the Voronoi diagram\nvor_regions = [vor.point_region[i] for i in range(len(vor.point_region))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions in the extra points\nextra_regions = [vor.point_region[", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "max_size = max([v.size for v in vectors])\nresult = sparse.csc_matrix(np.pad(vectors, ((0, 0), (0, max_size - v.size)), 'constant', constant_values=0))\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=[0, 1])", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[i, j] for i, j in zip(row, column)]", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "f = scipy.interpolate.interp1d(x, array, kind='cubic')\nnew_array = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nprob = NormalDistro(u,o2,x)\nprint(prob)", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n!pip install scipy\n!pip install numpy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "\n# Create a 2D array with random values\nA = np.random.rand(N, N)\n\n# Compute the DCT matrix\ndct_matrix = sf.dctn(A, norm='ortho')\n\n# Store the result in the variable 'result'\nresult = dct_matrix", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "offset = [-1, 0, 1]\nresult = sparse.diags(matrix, offset, shape=(5, 5)).toarray()\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "\ndef binomial_distribution(n, p):\n    return scipy.stats.binom.pmf(range(n+1), n, p)\n\nresult = binomial_distribution(N, p)\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x), axis=0)\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "df['data'] = 'data'\ndf['zscore'] = stats.zscore(df.drop('data', axis=1), axis=1)\nresult = df\nresult\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "df['data'] = df.apply(lambda row: pd.Series({'sample1': row['sample1'], 'sample2': row['sample2'], 'sample3': row['sample3']}), axis=1)\ndf['zscore'] = df.apply(lambda row: pd.Series({'sample1': stats.zscore(row['sample1']), 'sample2': stats.zscore(row['sample2']), 'sample3': stats.zscore(row['sample3'])}), axis=1)\ndf = df.drop(['sample1', 'sample2', 'sample3'], axis=1)\ndf = df.rename(columns={'sample1': 'data', 'sample2': 'data', 'sample3': 'data'})\ndf = df.rename(columns={'sample1': 'zscore', 'sample2': 'zscore', 'sample3': 'zscore'})\ndf = df.round({'data': 3, 'zscore': 3})\nresult = df\nresult\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "center = np.array([3, 3])\ny = np.array(range(shape[0]))\nx = np.array(range(shape[1]))\nresult = distance.cdist(np.dstack((y, x)), center, 'euclidean')\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.ogrid[:shape[0], :shape[1]]\nresult = distance.cdist(np.dstack([x, y]), mid)", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    center = np.array([(shape[0] - 1) / 2, (shape[1] - 1) / 2])\n    return distance.cdist(np.dstack((x, y)), center, 'euclidean')\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((func(x) - y) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, method='Nelder-Mead')\nprint(out.x)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((func(x) - y) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, method='L-BFGS-B', bounds=[(-b / 2.0) for b in x_lower_bounds])['x']\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) if 0 < t < 2*np.pi else 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.hstack([sa, sb])", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\n# define the function\ndef integrand(x, c):\n    return 2*c*x\n\n# define the integral\nresult, error = scipy.integrate.quad(integrand, low, high, args=(c,))\n\n# append the result to the array\nI.append(result)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n\n# create an array to store the solutions\nI = np.zeros(len(c))\n\n# loop over the values of c\nfor n, c_value in enumerate(c):\n    I[n] = f(c=c_value)\n\n# print the solutions\nprint(I)\n", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x_mask = V.keys()\nV.data[x_mask] += x\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V.data += x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V.data += x\nV.data += y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    normalized_column = column / length\n    # update the original column in the sparse matrix\n    sa[:, col] = normalized_column\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].toarray()\n    # get the column length\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    normalized_column = column / length\n    # update the original column in the sparse matrix\n    sa[:, col] = normalized_column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "\na = (a > 0).astype(int)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "\na = (a == 0) | (a == 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argmin(distances, axis=0)\nresult = closest_indices", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_points = np.argmin(distances, axis=1)\nresult = data[closest_points]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argpartition(distances, k, axis=0)[:k, :]\nresult = closest_indices", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result.append(fsolve(eqn, x0=x, args=(a, b)))\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result.append(fsolve(eqn, (x, a), args=(a, b)))\nresult = np.array(result)\nresult = result[result[:, 0].argsort()]\nresult = result[:, 1:]\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Fit the data to the bekkers function\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the goodness of fit using K-S test\nkstest_result = sp.stats.kstest(sample_data, bekkers(range_start, *popt))\n\n# Print the result\nprint(kstest_result)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Fit the data to the bekkers function\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nkstest_result = sp.stats.kstest(sample_data, bekkers(range_start, *popt))\n\n# Check if the p-value is less than 0.05 to reject the null hypothesis\nresult = kstest_result.pvalue < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "df['Time'] = pd.to_datetime(df['Time'])\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(lambda x: integrate.cumtrapz(x['A'], x['Time']))\nintegral_df = integral_df.reset_index(drop=True)\nintegral_df.columns = ['A']\nintegral_df['Time'] = integral_df.index\nintegral_df['Time'] = integral_df['Time'].apply(lambda x: x.strftime('%Y-%m-%d-%H:%M:%S'))\nintegral_df['Time'] = pd.to_datetime(integral_df['Time'])\nintegral_df.set_index('Time', inplace=True)\nintegral_df.index.name = 'Time'\nintegral_df.index = pd.to_datetime(integral_df.index)\nintegral_df.index.name = 'Time'\nintegral_df.head()\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "f = scipy.interpolate.griddata(x, y, eval, method='linear')\nresult = f[0]", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "\n# calculate the number of categories\nn_categories = len(a.columns)\n\n# calculate the total number of observations\nn_observations = a.shape[0]\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flatten())\n\n# calculate the total number of unique values in the data\nn_unique_values = len(a.values.flat", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "\n# find the indices of the elements that are less or equal to the neighbouring n elements\nindices = np.where((np.roll(arr, -n) <= arr) & (np.roll(arr, n) <= arr))[0]\n\n# add the first and last indices of the array to the result\nresult = np.concatenate((indices[0], indices[-1]))\n\nprint(result)\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "\n# Create a 2D array with the same shape as the input array, filled with False values\nmask = np.zeros_like(arr, dtype=bool)\n\n# Create a 2D array with the same shape as the input array, filled with False values\nresult = []\n\n# Loop over the rows of the array\nfor i in range(arr.shape[0]):\n    # Loop over the columns of the array\n    for j in range(arr.shape[1]):\n        # Get the 2D neighborhood around the current element\n        neighborhood = arr[max(0, i-n):i+n+1, max(0, j-n):j+n+1]\n        # If the current element is less or equal to the neighborhood, set the mask at the current element to True\n        mask[i, j] = np.all(arr[i, j] <= neighborhood)\n        # If the mask at the current element is True, add the current element to the result\n        if mask[i, j]:\n            result.append([i, j])\n\n# Convert the result to a numpy array\nresult = np.array(result)\nprint(result)\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "df = df[(np.abs(stats.zscore(df.select_dtypes(include=np.number))) < 3).all(axis=1)]\n", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": "\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n    return df\n    ### END SOLUTION\n\nprint(solve(data))\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# First, we need to flatten the list of fruits\ndf['Col3'] = df['Col3'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a dictionary where the keys are the unique fruits and the values are 1s\nfruit_dict = {fruit: 1 for sublist in df['Col3'].values for fruit in sublist}\n\n# Now, we create a new dataframe with the one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'], prefix='Apple', prefix_sep=' ', drop_first=True)\n\n# We replace the original column with the one-hot-encoded column\ndf['Col3'] = df['Col3'].replace(fruit_dict)\n\n# We add the original column to the dataframe\ndf_out['Col3'] = df['Col3']\n\n# We drop the original column\ndf.drop('Col3', axis=1, inplace=True)\n\n# We print the dataframe\nprint(df_out)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# Convert the list of strings into a one-hot encoded dataframe\ndf_out = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\n\n# Remove the original column\ndf_out.drop('Col3', axis=1, inplace=True)\n\n# Rename the columns\ndf_out.columns = ['Apple', 'Orange', 'Banana', 'Grape']\n\n# Add the original column back\ndf_out = pd.concat([df['Col1'], df['Col2'], df_out], axis=1)\n\n# Reset the index\ndf_out.reset_index(drop=True, inplace=True)\n\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# First, we need to convert the list of fruits into a binary matrix\n# We will use the get_dummies function from pandas\ndf_out = pd.get_dummies(df['Col4'].apply(pd.Series).stack()).sum(level=0)\n\n# Then, we need to join the original dataframe with the one-hot-encoded dataframe\ndf_out = df.drop('Col4', axis=1).join(df_out)\n\n# Finally, we need to reset the index and rename the columns\ndf_out = df_out.reset_index().rename(columns={'level_1': 'Col4'})\n\nprint(df_out)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# First, we need to flatten the list of fruits into a single series\nfruits = df['Col3'].apply(pd.Series).stack()\n\n# Then, we create a dataframe with the unique fruits as columns\ndf_out = pd.get_dummies(fruits)\n\n# Finally, we join the original dataframe with the one-hot-encoded dataframe\ndf_out = df.drop('Col3', axis=1).join(df_out)\n\nprint(df_out)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# First, we need to flatten the list of fruits into a single series\nfruits = pd.Series(np.concatenate(df['Col3'].values))\n\n# Then, we create a new dataframe with the unique fruits as columns\ndf_out = pd.get_dummies(fruits)\n\n# Finally, we add the original dataframe as a new column\ndf_out['Col3'] = df['Col3']\n\n# Now, df_out is a dataframe where each unique fruit is a column, with a 1 in the column if the fruit is present in the original dataframe, and a 0 otherwise\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "\n# Convert the sparse matrix to a DataFrame\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Concatenate the original DataFrame and the transformed DataFrame\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "\n# Convert the sparse matrix to a DataFrame\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Merge the original DataFrame and the transformed DataFrame\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # convert the sparse matrix to a dense matrix\n    transform_output_dense = transform_output.todense()\n    # create a dataframe from the dense matrix\n    df_transformed = pd.DataFrame(transform_output_dense)\n    # concatenate the original dataframe and the transformed dataframe along the columns axis\n    result = pd.concat([df, df_transformed], axis=1)\n    return result\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\nclf.steps.pop(1) # remove step 'poly'\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.pop(1) # remove the second step\nclf.steps.insert(1, ('reduce_poly', PolynomialFeatures())) # insert a new step\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\nclf.steps.append(('new_step', SomeTransformer()))\nclf.fit(X_train, y_train)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps.append(('new_step', LinearSVC()))\nclf.fit(X_train, y_train)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PCA()))\nclf.steps\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'n_estimators': [50, 100, 200],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [2, 4, 6]\n}\nb = np.zeros(len(paramGrid))\nc = np.zeros(len(paramGrid))\nfor i, params in enumerate(paramGrid):\n    gs = GridSearchCV(model, params, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX,trainY)\n    b[i] = gs.best_score_\n    c[i] = gs.predict(testX)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n    'max_depth': [3, 4, 5, 6, 7],\n    'n_estimators': [100, 200, 300, 400, 500]\n}\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\nb, c = gridsearch.fit(trainX, trainY, fit_params=fit_params, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX, trainY)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    from sklearn.linear_model import LinearRegression\n    model = LinearRegression()\n    model.fit(data, scaled)\n    predicted = model.predict(data)\n    inversed = scaler.inverse_transform(predicted)\n    return inversed\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = str(model)\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data)\ntf_idf_out = pipe[\"tf_idf\"].transform(data)\n", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nclf = grid_search.best_estimator_\nproba = clf.predict_proba(X_test)", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Convert the DataFrame to a numpy array\ndata_array = data.values\n\n# Scale the data\nscaled_data = preprocessing.scale(data_array)\n\n# Convert the scaled data back to a DataFrame\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Assign the scaled DataFrame to the output variable\ndf_out = scaled_data_df\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# First, you need to convert your DataFrame to a numpy array\ndata_array = data.values\n\n# Then, apply the preprocessing.scale function to the numpy array\nscaled_data = preprocessing.scale(data_array)\n\n# Finally, convert the scaled numpy array back to a DataFrame\ndf_out = pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "\n# Get the indices of the selected features\nindices = np.where(model.get_support())[0]\n\n# Get the column names of the selected features\ncolumn_names = X.columns[indices]\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "\n# Get the feature importances\nimportances = clf.feature_importances_\n\n# Get the indices of the sorted feature importances\nindices = np.argsort(importances)[::-1]\n\n# Rename the features\ncolumn_names = [X.columns[i] for i in indices]\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "import numpy as np\ncolumn_names = np.array(X.columns)[model.get_support()]\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = [X.columns[i] for i in np.where(model.get_support())[0]]\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\ndistances = km.transform(X)\nclosest_50_samples = X[np.argpartition(distances, 50)[:, :50]]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\ndistances = km.transform(X)[:, 0]\nclosest_50_samples = X[np.argsort(distances)[:50]]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\ndistances = km.transform(X)\nclosest_100_samples = X[np.argpartition(distances, 100)[:, :100]]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    distances = np.linalg.norm(centers - X, axis=1)\n    closest_samples = np.argpartition(distances, p)[:p]\n    return X[closest_samples]\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# define the model\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Gaussian Kernel\ngaussian_kernel = svm.SVC(kernel='rbf')\n\n# Fit the model\ngaussian_kernel.fit(X_train, y_train)\n\n# Predict the response\npredict = gaussian_kernel.predict(X_test)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# define the model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# define the model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict\npredict = model.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "\ntfidf_query = tfidf.transform(queries)\ncosine_similarities_of_queries = cosine_similarity(tfidf_query, tfidf.transform(documents))\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "\ntfidf_query = tfidf.transform(queries)\ncosine_similarities_of_queries = cosine_similarity(tfidf_query, tfidf.transform(documents))\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    query_tfidf = tfidf.transform(queries)\n    cosine_similarities_of_queries = cosine_similarity(query_tfidf, tfidf.transform(documents))\n    return cosine_similarities_of_queries\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# Convert the features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Create a binary matrix from the DataFrame\nnew_features = df.to_numpy()\n\n# Convert the binary matrix to a 2D array\nnew_features = np.array(new_features).reshape(-1, 1)\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(f)\n\n# Convert the DataFrame to a numpy array\nnew_f = df.values\n\n# Convert the numpy array to a 2D array\nnew_f = np.array(new_f)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# Convert the features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Create a binary matrix from the DataFrame\nnew_features = df.to_numpy()\n\n# Convert the binary matrix to a 2D array\nnew_features = np.array(new_features, dtype=int)\n\n# Print the new features\nprint(new_features)\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a DataFrame\n    df = pd.DataFrame(features)\n\n    # Create a dictionary to store the one-hot encoding\n    one_hot_dict = {}\n\n    # Iterate over the columns in the DataFrame\n    for col in df.columns:\n        # Create a new column with the one-hot encoding\n        df[col] = df[col].astype('category').cat.codes\n\n        # Add the new column to the dictionary\n        one_hot_dict[col] = df[col]\n\n    # Convert the dictionary to a 2D array\n    new_features = np.array(list(one_hot_dict.values()))\n\n    return new_features\n\nfeatures = solve(features)\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D array\nnew_features = np.array(new_features)\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from the data_matrix\ndistances = pd.DataFrame(data_matrix)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data_matrix\ncluster_labels = cluster.fit_predict(distances)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = pd.DataFrame(data_matrix)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data\ncluster_labels = cluster.fit_predict(dist_matrix)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Convert the similarity matrix to a distance matrix\ndistM = 1 - np.array(simM)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the distance matrix and get the cluster labels\ncluster_labels = cluster.fit_predict(distM)\ncluster_labels", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Convert the data to a numpy array\ndata_matrix = np.array(data_matrix)\n\n# Compute the linkage matrix\nZ = linkage(data_matrix, 'ward')\n\n# Perform hierarchical clustering\ncluster_labels = fcluster(Z, t=2, criterion='maxclust')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\n# compute the linkage matrix\nZ = linkage(data_matrix, 'ward')\n\n# perform clustering\ncluster_labels = fcluster(Z, t=2, criterion='maxclust')\ncluster_labels", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Convert the similarity matrix to a distance matrix\ndistM = 1 - np.array(simM)\n\n# Perform hierarchical clustering\nZ = linkage(distM, 'ward')\ncluster_labels = fcluster(Z, t=2, criterion='maxclust')\ncluster_labels", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from scipy.stats import boxcox\nbox_cox_data, fitted_lambda = boxcox(data)", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(punctuation='!\"?\"\\'')\ntransformed_text = vectorizer.fit_transform(text)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataframe into features and target\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    from sklearn.model_selection import train_test_split\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\n<code>\n!pip install sklearn\n!pip install pandas\n!pip install numpy\n!pip install scikit-learn\n!pip install seaborn\n!pip install matplotlib\n!pip install nltk\n!pip install tensorflow\n!pip install keras\n!pip install pytorch\n!pip install pytorch-lightning\n!pip install transformers\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install pytorch-lightning\n!pip install", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "\n# Reshape the data to 2D array\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans instance with 2 clusters\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n\n# Get the labels\nlabels = kmeans.labels_\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# Reshape the data to fit the KMeans model\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans model with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit the model to the data\nkmeans.fit(X)\n\n# Get the labels of the data\nlabels = kmeans.predict(X)\n\n# Get the centroids of the clusters\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "selector = LinearSVC(C=1.0, penalty='l1', dual=False).fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selector.coef_ != 0]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "selector = LinearSVC(C=1.0, penalty='l1', dual=False).fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selector.coef_ != 0]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    svc = LinearSVC(penalty='l1', dual=False)\n    svc.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\n    return selected_feature_names\n    ### END SOLUTION ###\n\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n\n# Reverse the order of the features\nfeature_names = feature_names[::-1]\nX = X[:, ::-1]\n\n# Convert to binary matrix\nX = np.where(X > 0, 1, 0)\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nvectorizer.fit(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = vectorizer.transform(corpus).toarray()\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[['Time', col]].dropna()\n    X, Y = df2['Time'], df2[col]\n    slope = LinearRegression().fit(X.values.reshape(-1, 1), Y.values)\n    slopes.append(slope.coef_[0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[['Time', col]].dropna()\n    X, Y = df2['Time'], df2[col]\n    slope = LinearRegression().fit(X.values.reshape(-1, 1), Y.values)\n    slopes.append(slope.coef_[0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n    ### END SOLUTION\n\ndf = Transform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "ElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\nprint (\"R^2 for training set:\")\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\")\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\nnp_array_transformed = scaler.fit_transform(np_array)\ntransformed = np_array_transformed", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\nnp_array_transformed = scaler.fit_transform(np_array)\ntransformed = np_array_transformed", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": "\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION\n\nnp_array = Transform(np_array)\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([[close_buy1, m5, m10, ma20]])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = []\nfor i in X:\n    new_X.append([ord(i[0][0]), i[0][1]])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = []\nfor i in X:\n    new_X.append([float(i[0]), float(i[1])])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = []\nfor i in X:\n    new_X.append([ord(i[0][0]), int(i[0][1])])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# Separating the data into dependent and independent variables\nX = dataframe.iloc[:,:-1].astype(float)\ny = dataframe.iloc[:,-1]\n\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n\n# Predict\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Separate the features and the target variable\nX = dataframe.iloc[:,:-1].astype(float)\ny = dataframe.iloc[:,-1]\n# Create an instance of the Logistic Regression model\nlogReg = LogisticRegression()\n# Fit the model to the data\nlogReg.fit(X, y)\n# Predict the target variable for the new data\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Calculate the number of days in the dataframe\nnum_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the test set\ntest_days = int(num_days * 0.8)\n\n# Get the indices for the train and test sets\ntrain_indices = np.arange(0, test_days)\ntest_indices = np.arange(test_days, num_days)\n\n# Get the train and test dataframes\ntrain_dataframe = features_dataframe.loc[train_indices]\ntest_dataframe = features_dataframe.loc[test_indices]\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Calculate the number of days in the dataframe\nnum_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the test set\ntest_days = int(num_days * 0.2)\n\n# Calculate the number of days in the train set\ntrain_days = num_days - test_days\n\n# Create the train and test dataframes\ntrain_dataframe = features_dataframe.iloc[:train_days]\ntest_dataframe = features_dataframe.iloc[train_days:]\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    features_dataframe['date'] = pd.to_datetime(features_dataframe['date'])\n    features_dataframe = features_dataframe.sort_values(by='date')\n    train_dataframe = features_dataframe.iloc[:int(len(features_dataframe)*0.2)]\n    test_dataframe = features_dataframe.iloc[int(len(features_dataframe)*0.2):]\n    return train_dataframe, test_dataframe\n    ### END SOLUTION\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2', 'X3']] = scaler.fit_transform(df[['X2', 'X3']])", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "\nmyData['new_A2'] = scaler.fit_transform(myData[['Month', 'A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['Month', 'A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False, token_pattern='[^\\s]+')\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results.sort_values(by='rank_test_score')\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values('mean_fit_time')\nfull_results", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport joblib\njoblib.dump(fitted_model, 'sklearn_model')\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix, tfidf_matrix)\ncosine_similarity_matrix", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptimizer = optim.param_groups[0]['lr']\nprint(optimizer)\noptim.param_groups[0]['lr'] = 0.001\nprint(optim.param_groups[0]['lr'])\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\noptimizer = optim.state_dict()\noptimizer['param_groups'][0]['lr'] = 0.001\noptim.load_state_dict(optimizer)\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptimizer = optim.param_groups[0]['lr']\nprint(optimizer)\noptim.param_groups[0]['lr'] = 0.0005\nprint(optim.param_groups[0]['lr'])\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n\n# Define a function to update the learning rate\ndef update_learning_rate(optimizer, epoch):\n    if epoch > 10:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.0005\n    else:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.05\n\n# Apply the function to the optimizer\noptim.step()\noptim.zero_grad()\nupdate_learning_rate(optim, epoch)\noptim.step()\noptim.zero_grad()\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the gensim word2vec model to PyTorch tensor\nweight = torch.tensor(word2vec.wv.vectors, dtype=torch.float32)\n\n# Create an embedding layer with the weights\nembedding = torch.nn.Embedding.from_pretrained(weight)\n\n# Use the embedding layer to embed the input data\nembedded_input = embedding(input_Tensor)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # Convert the input tensor to a list of words\n    words = input_Tensor.tolist()\n    # Get the word2vec model\n    word2vec = Word2Vec.load_word2vec_format('path_to_your_word2vec_model', binary=True)\n    # Create a dictionary to store the embeddings\n    embeddings = {}\n    # Loop over the words in the input tensor\n    for word in words:\n        # If the word is in the word2vec model, get the embedding\n        if word in word2vec.wv:\n            embeddings[word] = word2vec.wv[word]\n    # Convert the embeddings to a PyTorch tensor\n    embeddings_tensor = torch.tensor(list(embeddings.values()))\n    ### END SOLUTION ###\n    return embeddings_tensor\nembedded_input = get_embedded_input(input_Tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.numpy())", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(np.array(x))\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "\nC = B[:, A_log.bool()]\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "\n# Convert the ByteTensor to LongTensor\nA_logical = A_logical.long()\n\n# Use the logical index to slice the tensor\nC = B[:, A_logical]\n\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "\nC = B[:, A_log.bool()]\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "\nC = B[:, A_log.bool()]\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log.bool()]\n    return C\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "\nC = B[:, A_log.bool()]\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Convert the LongTensor to a numpy array\nidx_np = idx.numpy()\nB_np = B.numpy()\n\n# Use numpy's indexing to select the columns\nC_np = B_np[:, idx_np]\n\n# Convert the numpy array back to a PyTorch tensor\nC = torch.from_numpy(C_np)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "\nx_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "\n# Convert numpy array to pandas dataframe\ndf = pd.DataFrame(x_array)\n\n# Convert pandas dataframe to torch tensor\nx_tensor = torch.tensor(df.values, dtype=torch.float32)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    return torch.tensor(a, dtype=torch.float16)\n    ### END SOLUTION ###\n\n# Test the function\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\nmax_len = max(lens)\nmask = torch.arange(max_len).expand(len(lens), max_len).t()\nmask = mask.long()\nmask = (mask < lens.unsqueeze(-1)).long()\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask.long()\n    ### END SOLUTION ###\n\nmask = get_mask(lens)\nprint(mask)\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with the same shape as your 2D tensor\nTensor_2D_same_shape = torch.zeros_like(Tensor_2D)\n\n# Fill the 2D tensor with the diagonal elements of your 2D tensor\ntorch.diag_embed(Tensor_2D, 0, Tensor_2D_same_shape)\n\n# Now you can multiply it with a matrix to get your 3D tensor\nMatrix = ... # put solution in this variable\n\nTensor_3D = torch.mm(Tensor_2D_same_shape, Matrix)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    return torch.diag(t)\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor(a, dtype=torch.float32)\nb = torch.tensor(b, dtype=torch.float32)\nab = torch.stack([a, b], dim=0)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "ab = torch.cat([a, b], 0)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    c = torch.stack((a, b), 0)\n    return c\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[torch.arange(10), lengths, :] = 0\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "\nmask = torch.arange(1000)[None, :] < lengths[:, None]\na[mask] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[torch.arange(10), lengths - 1, :] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "\nmask = torch.arange(1000)[None, :] < lengths[:, None]\na[mask] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.stack(lt)\n    return tensor_of_tensors\n    ### END SOLUTION ###\n\ntensor_of_tensors = Convert(list_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "\nresult = torch.index_select(t, 0, torch.tensor(idx))\nprint(result)\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "\nresult = torch.index_select(t, 0, torch.tensor(idx, dtype=torch.long))\n", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "\nresult = torch.index_select(t, 0, torch.tensor(idx))\nprint(result)\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\n# Flatten the tensor\nx = x.view(-1, 3)\n\n# Find the maximum scores\nmax_scores, max_indices = torch.max(x, 1)\n\n# Gather the selected slices\nresult = torch.gather(x, 1, max_indices.unsqueeze(-1).expand(-1, -1, 2))\nresult = result.squeeze(1)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\n# Flatten the tensor\nx = x.view(-1, 114)\n\n# Gather the slices\nresult = torch.gather(x, 1, ids.unsqueeze(-1).unsqueeze(-1))\n\n# Reshape the result\nresult = result.squeeze(1)\n\nEND SOLUTION\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask where the ids tensor is 1\nmask = ids == 1\n\n# Gather the scores where the mask is True\nscores = x[mask].view(-1, 2)\n\n# Find the maximum score\nmax_score = scores.max(dim=0)[0]\n\n# Create a result tensor with the maximum score\nresult = torch.cat([max_score, ids], dim=1)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "\n_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1,1)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "\n_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1,1)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "\n# Get the maximum value in each row\nmax_values, y = torch.max(softmax_output, 1)\n\n# Create a tensor of zeros with the same shape as softmax_output\ny = torch.zeros_like(softmax_output)\n\n# Fill the tensor with the indices of the maximum values\ny.scatter_(0, y, torch.arange(softmax_output.size(0)).unsqueeze(1).to(softmax_output.device))\n\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    _, predicted = torch.max(softmax_output, 1)\n    return predicted.view(-1,1)\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "\n# Get the maximum value in each row\nmax_values, y = torch.max(softmax_output, 1)\n\n# Create a tensor of the same shape as softmax_output but filled with zeroes\ny = y.unsqueeze(1).expand_as(softmax_output)\n\n# Set the values in y to be the maximum value\ny = (softmax_output - y).squeeze()\n\n# Convert y to long tensor\ny = y.long()\n\n# Print the result\nprint(y)\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "\n# Convert labels to one-hot encoding\nlabels = F.one_hot(labels, num_classes=7).float()\n\n# Calculate cross entropy loss\nloss = F.cross_entropy(input=images, target=labels, reduction='none')\n\n# Average loss over batch size\nloss = torch.mean(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = (A == B).sum().item()\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "\ncnt_equal = torch.sum((A == B).float())\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "\ncnt_not_equal = torch.sum((A != B).bool())\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = (A == B).sum().item()\n    ### END SOLUTION ###\n    return cnt_equal\n\nA, B = load_data()\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "\ncnt_equal = torch.sum((A[-x:] == B[-x:]).float())\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "\ncnt_not_equal = torch.sum((A[-x:] != B[-x:]).bool())", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = torch.split(a, chunk_dim, dim=3)\nassert len(tensors_31) == 31\nfor i in range(31):\n    print(tensors_31[i].shape)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = torch.split(a, chunk_dim, dim=2)\nassert len(tensors_31) == 31\nassert tensors_31[0].shape == (1, 3, 10, 10, 1)\nassert tensors_31[1].shape == (1, 3, 10, 10, 1)\nassert tensors_31[2].shape == (1, 3, 10, 10, 1)\n#... and so on\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output = torch.where(mask.bool(), clean_input_spectrogram, output)", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask == 0] = clean_input_spectrogram[mask == 0]", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x, min_y = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.where(x >= 0, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.where(x >= 0, sign_x, -sign_y) * max\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_xy = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = sign_x * min_xy + sign_y * min_xy\n    return signed_min\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf.item(), class_names[classes.item()]\n\nconfidence_score, class_predicted = predict_allCharacters(input)\nprint(f\"Confidence score: {confidence_score}, Predicted class: {class_predicted}\")\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# Get the last column of a and the first column of b\nlast_a, first_b = a[:, -1:], b[:, 0]\n\n# Compute the average of last_a and first_b\nresult = torch.cat([last_a, (last_a + first_b) / 2], dim=1)\n\n# Remove the last column of a\nresult = torch.cat([result, a[:, :-1]], dim=1)\n\n# Remove the first column of b\nresult = torch.cat([result, b[:, 1:]], dim=1)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a.squeeze()\n    b = b.squeeze()\n    c = torch.cat([a[:, :-1], b[:, 1:]], 1)\n    d = c.mean(1, keepdim=True)\n    e = torch.cat([d, d], 1)\n    return e.view(-1, 1).expand(-1, -1, 2).contiguous()\n    ### END SOLUTION ###\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torchvision\n!pip install matplotlib\n!pip install scikit-learn\n!pip install seaborn\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install torch\n!pip install numpy\n!pip install pandas\n!", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "\n# reshape tensor to 2D\nt = t.reshape(1, -1)\n\n# create a new tensor with zeros\nnew = torch.zeros(t.shape)\n\n# stack tensors\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "\n# reshape tensor to 4x4\nt = t.reshape(1, 2, 2)\n\n# create a new tensor with zeros\nnew = torch.zeros(1, 4, 4)\n\n# stack the original tensor and the new tensor\nresult = torch.cat([t, new], dim=1)\n\nprint(result)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "\n# reshape tensor to 2D\nt = t.reshape(1, -1, 2)\n\n# create a new tensor with -1\nnew = torch.full((1, 4, 2), -1)\n\n# stack tensors\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data.unsqueeze(1), W.unsqueeze(0)).squeeze()\nresult = result.view(10, 2, 3)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
