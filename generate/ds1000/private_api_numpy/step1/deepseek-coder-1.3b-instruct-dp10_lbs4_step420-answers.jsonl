{"id": 0, "code": "result = df.iloc[List]", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.iloc[List]\nprint(result)\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x).loc[x.values])\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create a new column with the values that have at least two occurrences\nresult = counts.loc[counts.loc[0] >= 2]\n\n# Create", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\ncounts = df.apply(lambda x: pd.value_counts(x) >= 3", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    # create a new column with the value of the column 'Qu1' if the value count is greater or equal to 2\n    df['Qu1'] = np.where(np.array([np.sum(df[col] == val) for col, val in zip(df.columns[1:], ['cheese', 'potato', 'banana', 'apple', 'egg'])] >= 2), 'other', df['Qu1'])\n    # create a new column with the value of the column 'Qu2' if the value count is greater or equal to 2\n    df['Qu2'] = np.where(np.array([np.sum(df[col] == val) for col, val in zip(df.columns[1:], ['banana', 'apple', 'sausage'])] >= 2), 'other', df['Qu2'])\n    # create a new column with the value of the column 'Qu3' if the value count is greater or equal to 2\n    df['Qu3'] = np.where(np.array([np.sum(df[col] == val) for col, val in zip(df.columns[1:], ['cheese', 'potato', 'other'])] >= 2), 'other', df['Qu3'])\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a function to change values in column Qu1\ndef change_values_in_column_Qu1(data):\n    value_counts = data.Qu1.value_counts()\n    data.Qu1[value_counts >= 3] = 'other'\n    return data\n\n# Create a function to change values in column Qu2 and Qu3\ndef change_values_in_column_Qu2_and_Qu3(data):\n    value_counts = data.Qu2.value_counts()\n    data.Qu2[value_counts >= 2] = 'other'\n    value_counts = data.Qu3.value_counts()\n    data.Qu3[value_counts >= 2] = 'other'\n    return data\n\n# Apply the functions to the data\ndata = change_values_in_column_Qu1(data)\ndata = change_values_in_column_Qu2_and_Qu3(data)\n\n# Print the result\nprint(data)\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a function to replace values in column Qu1\ndef replace_values_in_column(column):\n    counts = pd.value_counts(column)\n    counts = counts[counts >= 3]\n    counts = counts.index\n    column[column.isin(counts)] = 'other'\n    return column\n\n# Apply the function to column Qu1\ndf['Qu1'] = replace_values_in_column(df['Qu1'])\n\n# Create a function to replace values in column Qu2 and Qu3\ndef replace_values_in_columns(column1, column2):\n    counts = pd.value_counts(column1)\n    counts = counts[counts >= 2]\n    counts = counts.index\n    column1[column1.isin(counts)] = 'other'\n    column2[column2.isin(counts)] = 'other'\n    return column1, column2\n\n# Apply the function to column Qu2 and Qu3\ndf['Qu2'], df['Qu3'] = replace_values_in_columns(df['Qu2'], df['Qu3'])\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\n# Create a new column 'keep_if_dup_bool' to convert 'keep_if_dup' to boolean\ndf['keep_if_dup_bool'] = df['keep_if_dup'] == 'Yes'\n\n# Drop duplicates\nresult = df.drop_duplicates(subset='url', keep='last')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndef df_to_dict(df):\n    result = {}\n    for name in df['name'].unique():\n        result[name] = {}\n        for v1 in df['v1'].unique():\n            result[name][v1] = {}\n            for v2 in df['v2'].unique():\n                result[name][v1][v2] = {}\n                for v3 in df['v3'].unique():\n                    result[name][v1][v2][v3] = {}\n                    for i in range(len(df)):\n                        if df.loc[i, 'name'] == name and df.loc[i, 'v1'] == v1 and df.loc[i, 'v2'] == v2 and df.loc[i, 'v3'] == v3:\n                            result[name][v1][v2][v3][df.loc[i, 'v1']] = df.loc[i, 'v3']\n    return result\n\nresult = df_to_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    # Use the tz_convert function to convert the timezone to UTC\n    df['datetime'] = df['datetime'].apply(lambda x: x.tz_convert('UTC'))\n    # Use the str() function to get the characters you want\n    df['datetime'] = df['datetime'].apply(lambda x: str(x).split('-')[0])\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\ndef extract_key_values(message):\n    keys = []\n    values = []\n    for line in message:\n        if line.startswith('[') and line.endswith(']'):\n            line = line[1:-1]\n            key, value = line.split(': ')\n            keys.append(key)\n            values.append(value)\n    return keys, values\n\ndef expand_dataframe(df):\n    for index, row in df.iterrows():\n        message = row['message']\n        keys, values = extract_key_values(message)\n        df.loc[index, 'job'] = values[0]\n        df.loc[index, 'money'] = values[1]\n        df.loc[index, 'wife'] = values[2]\n        df.loc[index, 'group'] = values[3]\n        df.loc[index, 'kids'] = values[4]\n    return df\n\nresult = expand_dataframe(df)\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df['score'] * 10\ndf = df[df['product'].isin(products)]\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10\ndf['score'] = df['score'].where(df['product'].isin(products), 0)\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "for product in products:\n    df.loc[df['product'].between(product[0], product[1]), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\nfor product in products:\n    df.loc[df['product'] == product, 'score'] = (df.loc[df['product'] == product, 'score'] - df.loc[df['product'] == product, 'score'].min()) / (df.loc[df['product'] == product, 'score'].max() - df.loc[df['product'] == product, 'score'].min())\n\nprint(df)\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = ''\nfor i in range(len(df)):\n    if df.iloc[i, 0] == 1:\n        df.iloc[i, 0] = 0\n    if df.iloc[i, 1] == 1:\n        df.iloc[i, 1] = 0\n    if df.iloc[i, 2] == 1:\n        df.iloc[i, 2] = 0\n    if df.iloc[i, 3] == 1:\n        df.iloc[i, 3] = 0\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 0] = 'A'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 1] = 'B'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 2] = 'C'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0:\n        df.iloc[i, 3] = 'D'\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = 'A'\ndf.loc[df['A'] == 1, 'category'] = 'B'\ndf.loc[df['B'] == 1, 'category'] = 'C'\ndf.loc[df['C'] == 1, 'category'] = 'D'\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndef convert_to_list(df):\n    return df.apply(lambda x: [i for i in df.columns if x[i]], axis=1)\n\ndf['category'] = convert_to_list(df)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\ndf = df.set_index('Date')\ndf = df.reindex(List, fill_value=0)\ndf = df.reset_index()\ndf = df.rename(columns={'Date':'Date'})\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x + '!')\nshifted_df = shifted_df.drop(shifted_df.index[0])\nshifted_df = shifted_df.reset_index(drop=True)\nprint(shifted_df)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.index = shifted_df.index.map(lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df.rename(index=lambda x: x + '!')\nshifted_df = shifted_df.reset_index(drop=True)\nshifted_df = shifted_df", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\ndf['#1'] = df['#1'].shift(-1)\n\n# shift the last row of the first column up 1 row\ndf['#2'] = df['#2'].shift(1)\n\n# remove the first row of the first column\ndf = df[1:]\n\n# print the dataframe\nprint(df)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df = df.rename(columns=lambda x: x + 'X')", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df = df.rename(columns=lambda x: x + 'X')\nprint(df)\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "# Create a list of column names\ncolumn_names = ['HeaderA', 'HeaderB', 'HeaderC', 'HeaderX']\n\n# Create a list of column names that end with 'X'\ncolumn_names_with_x = [col for col in column_names if col.endswith('X')]\n\n# Create a list of column names that don't end with 'X'\ncolumn_names_without_x = [col for col in column_names if not col.endswith('X')]\n\n# Create a list of column names that are not in the list of column names with 'X'\ncolumn_names_other = [col for col in column_names if col not in column_names_with_x]\n\n# Create a list of column names that are not in the list of column names without 'X'\ncolumn_names_other_without_x = [col for col in column_names if col not in column_names_without_x]\n\n# Create a list of column names that are not in the list of column names other\ncolumn_names_other_other = [col for col in column_names if col not in column_names_other]\n\n# Create a list of column names that are not in the list of column names other without 'X'\ncolumn_names_other_other_without_x = [col for col in column_names if col not in column_names_other_other]\n\n# Create a list of column names that are not in the list of column names other other without 'X'\ncolumn_names_other_other_other_without_x = [col for col in column_names if col not in column_names_other_other_other]\n\n# Create a list of column names that are not in the list of column names other other other without 'X'\ncolumn_names_other_other_other_other_without_x = [col for col in column_names if col not in column_names_other_other_other_other]\n\n# Create a list of column names that are not in the list of column names other other other other without 'X'\ncolumn_names_other_other_other_other_other_without_x = [col for col in column_names if col not in column_names_other_other_other_other_other]\n\n# Create a list of column names that are not in the list of column names other other other other other without 'X'\ncolumn_names_other_other_other_other_other_other_without_x = [col for col in column_names if col not in column_names_other_other_other_other_other_other]\n\n# Create a list of column names that are not in the list of column names other other other other other other without 'X'\ncolumn_names_other_other_other_other_other_other_other_without_x = [col for col in column_names if col not in column_names_other_other_other_other_other_other_other]\n\n# Create a list of column names that are not in the list of column names other other other other other other other without 'X'\ncolumn_names_other_other_other_other_other_other_other_other_without_x = [col for col in column_names if col not in column_names_other_other_other_other_other_other_other_other]\n\n# Create a list of column names that are not in the list of column names other other other other other other other other without 'X'\ncolumn_names_other_other_other_other_other_other_other_other_other_without_x = [col for col in column_names if col not in column_names_other_other_other_other_other_other_other_other_other]\n\n# Create a list of column names that are not in the list of column names other other other other other other other other other other without 'X'\ncolumn_names_other_other_other_other_other_other_other_other_other_other_other_without_x = [col for col in column_names if col not in column_names_other_other_other_other_other_other_other_other_other_other_other]\n\n# Create a list of column names that are not in the list of column names other other other other other other other other other other other other other without 'X'\ncolumn_names_other_other_other_other_other_other_other_other_other_other_other_other_other_without_x = [", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "def mean_columns(df):\n    return df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\n\nresult = mean_columns(df)\nprint(result)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' not in col})\nprint(result)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "def aggregate_columns(df):\n    aggregated_columns = {}\n    for col in df.columns:\n        if col.endswith('2'):\n            aggregated_columns[col] = 'mean'\n        else:\n            aggregated_columns[col] = 'sum'\n    return aggregated_columns\n\naggregated_columns = aggregate_columns(df)\nresult = df.groupby('group').agg(aggregated_columns)\nprint(result)\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "def meanAdvance(row_list, column_list, axis=0):\n    return df.loc[row_list, column_list].mean(axis=axis)\n\nresult = meanAdvance(row_list, column_list)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "def sumAdvance(row_list, column_list, axis=0):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list)\nprint(result)\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "def sumAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list, axis=0)\nresult = result.sort_values(ascending=False).drop_duplicates()\nresult", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.apply(lambda x: x.value_counts())\nresult = result.fillna(0)\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "result = df.apply(lambda x: x.value_counts())\nresult", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0,1]]\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0,1]]\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "result = df.where(df.notnull(), 1).where(df.isnull(), 2)\nresult", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "result = df.where(df.isnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 1)\nresult = result.where(result.notnull(), 2)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result.where(result.notnull(), np.nan)\nresult = result", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "result = df.where(df.isnull(), np.nan)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result.where(result.notnull(), 0)\nresult = result", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "# Find rows where value is less than the threshold\nrows_to_replace = df[df['value'] < thresh]\n# Sum the values of these rows\nresult = rows_to_replace.sum()\n# Replace the original rows with the sum of these rows\ndf = df.replace(rows_to_replace, result)\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "# find rows with value less than threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# find rows with value greater than threshold\nrows_greater_than_thresh = df[df['value'] >= thresh]\n# calculate average of values in rows_greater_than_thresh\navg_value = rows_greater_than_thresh['value'].mean()\n# replace rows with value less than threshold with average value\nresult = df.loc[rows_less_than_thresh.index]\nresult.loc['X'] = avg_value\nprint(result)\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "# find the rows in the section\nrows_in_section = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\n# calculate the average of the rows in the section\naverage = rows_in_section['value'].mean()\n# replace the rows in the section with the average\nresult = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\nresult['value'] = average\nresult = result.reset_index()\nresult", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "result = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndef exp_column(df, column):\n    return df[column]**(1/df[column].count())\n\nfor column in df.columns:\n    df[f'exp_{column}'] = exp_column(df, column)\n\nresult = df\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "inv_A = df['A'].map(lambda x: 1/x if x != 0 else 0)\ninv_B = df['B'].map(lambda x: 1/x if x != 0 else 0)\n\nresult = pd.DataFrame({\"A\": df['A'], \"B\": df['B'], \"inv_A\": inv_A, \"inv_B\": inv_B})\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = df[col].apply(sigmoid)\n\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = df.apply(lambda x: x[x == x.max()].index[-1], axis=1)\n\n# Create a mask to ignore the last occurrence of the column-wise maximum\nmask = np.arange(len(last_max_index)) < last_max_index\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[mask]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast_max_index = last_max_index[-1]\n\n# Get the index of the last occurrence of the column-wise maximum\nlast", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "# Get the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Create a mask to ignore the rows before the minimum occurrence\nmask = np.arange(min_index.shape[0]) >= np.arange(min_index.shape[0])\n\n# Get the index of the first occurrence of the column-wise maximum\nmax_index = df.apply(lambda x: x[mask].argmax(), axis=1)\n\n# Create a new DataFrame with the maximum index\nresult = pd.DataFrame(max_index, index=min_index, columns=list('abc'))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum date to the maximum date\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with all dates and fill in 0 for the val column\nresult = pd.DataFrame(date_range, columns=['dt'])\nresult['val'] = 0\n\n# Join the original data frame with the new data frame\nresult = pd.merge(result, df, on='dt', how='left')\n\n# Fill in the val column with the corresponding value from the original data frame\nresult['val'] = result['val'].fillna(df['val'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum to the maximum\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with all dates and fill with 0\nnew_df = pd.DataFrame(date_range, columns=['dt'])\nnew_df['val'] = 0\n\n# Join the new data frame with the original data frame\nresult = pd.merge(new_df, df, on='dt', how='left')\nresult", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = p", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum date to the maximum date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Expand the date column to have all the dates there while simultaneously filling in the maximum val of the user for the val column\nresult = new_df.merge(df, on='dt', how='left').fillna(df['val'].max())\nresult", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new dataframe with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Join the new dataframe with the original dataframe on the date column\nresult = new_df.join(df.set_index('dt'), on='dt')\n\n# Fill the maximum val of the user for the val column\nresult['val'] = result.groupby('user')['val'].transform('max')\n\n# Convert the date column to the desired format\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "result = df.groupby('name').apply(lambda x: x.assign(name=x.index + 1)).reset_index(drop=True)\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "\n# Create a unique ID for each value in column a\nunique_id = df['a'].drop_duplicates().reset_index(drop=True)\n\n# Create a new column with unique ID for each value in column a\ndf['a'] = df['a'].apply(lambda x: unique_id[unique_id == x].index[0])\n\n# Create a unique ID for each value in column b\nunique_id = df['b'].drop_duplicates().reset_index(drop=True)\n\n# Create a new column with unique ID for each value in column b\ndf['b'] = df['b'].apply(lambda x: unique_id[unique_id == x].index[0])\n\n# Create a unique ID for each value in column c\nunique_id = df['c'].drop_duplicates().reset_index(drop=True)\n\n# Create a new column with unique ID for each value in column c\ndf['c'] = df['c'].apply(lambda x: unique_id[unique_id == x].index[0])\n\nresult = df\nprint(result)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', ''))\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x.replace(' ', '", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\n# Create a unique ID for each name\nunique_id = df['name'].apply(lambda x: x + str(df['a'].values[df['name'] == x].sum()))\n\n# Create a new column 'ID' with unique ID\ndf['ID'] = unique_id\n\n# Replace each name with a unique ID\ndf['ID'] = df['ID'].apply(lambda x: x.replace('Aaron', '1'))\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = pd.pivot_table(df, values='value', index='user', columns='date', aggfunc='first')\ndf = df.reset_index()\ndf = df.rename(columns={'date': 'value'})\ndf = df.sort_values('date')\nprint(df)\n", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = pd.pivot_table(df, values='value', index='user', columns='01/12/15', aggfunc='first')\ndf = pd.pivot_table(df, values='value', index='user', columns='02/12/15', aggfunc='first')\ndf = pd.pivot_table(df, values='value', index='user', columns='someBool', aggfunc='first')\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = pd.pivot_table(df, values='value', index='user', columns='date', aggfunc='first')\ndf = df.reset_index()\ndf = df.rename(columns={'date': 'value'})\ndf = df.sort_values('user')\nprint(df)\n", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df.c > 0.5][columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df.c > 0.45][columns].values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": " ###\n    # Use the loc function to select the rows where the value in column 'c' is greater than 0.5\n    # Then select the columns you want\n    # Finally, convert the result to a numpy array\n    ### END SOLUTION ###\n    return df.loc[df.c > 0.5, columns].to_numpy()\n\n# Create a DataFrame\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# Call the function\nprint(f(df, columns=['b', 'e']))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": " ###\n    # select the rows where column 'c' is greater than 0.5\n    # then select the columns 'b' and 'e'\n    # then compute the sum of these columns\n    # return the result\n    ### END SOLUTION ###\n    return df[df.c > 0.5][columns].sum()\n\n# create a dataframe\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# call the function\nprint(f(df, columns=['b', 'e']))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df[df[columns[0]] > 0.5][columns]\n    ### END SOLUTION\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndef find_overlapping_dates(df, X):\n    dates = df['date'].values\n    dates_list = dates.tolist()\n    dates_list = dates_list[1:]\n    dates_list.sort()\n    overlapping_dates = []\n    for i in range(len(dates_list)):\n        if i < len(dates_list)-1 and dates_list[i] + X < dates_list[i+1]:\n            overlapping_dates.append(dates_list[i])\n            overlapping_dates.append(dates_list[i+1])\n        elif i == len(dates_list)-1 and dates_list[i] + X <= dates_list[i]:\n            overlapping_dates.append(dates_list[i])\n            overlapping_dates.append(dates_list[i])\n    return overlapping_dates\n\noverlapping_dates = find_overlapping_dates(df, X)\nresult = df[~df['date'].isin(overlapping_dates)]\nprint(result)\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef find_overlapping_dates(df, X):\n    dates = df['date'].values\n    dates_list = dates.tolist()\n    dates_list = dates_list[1:]\n    dates_list = dates_list + dates_list[:X]\n    return dates_list\n\ndef remove_overlapping_dates(df, X):\n    dates = find_overlapping_dates(df, X)\n    return df[~df['date'].isin(dates)]\n\nresult = remove_overlapping_dates(df, X)\nprint(result)\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef find_overlapping_dates(df, X):\n    dates = df['date'].values\n    dates_list = dates.tolist()\n    dates_list = dates_list[1:]\n    dates_list.sort()\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list[1:]\n    dates_list = dates_list", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['col1'] = df['col1'].apply(lambda x: x / 3)\n\n# Create a new column with the binning of the data\ndf['", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\n# Create a new column 'bin' which will hold the bin number\ndf['bin'] = (df['col1'].astype(int) // 3).astype(int)\n\n# Create a new column 'bin_start' which will hold the start of each bin\ndf['bin_start'] = df['bin'].shift(1)\n\n# Create a new column 'bin_end' which will hold the end of each bin\ndf['bin_end'] = df['bin'].shift(-1)\n\n# Create a new column 'bin_count' which will hold the count of each bin\ndf['bin_count'] = (df['bin'] == df['bin_start']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which will hold the count of each bin\ndf['bin_count_end'] = (df['bin'] == df['bin_end']).astype(int)\n\n# Create a new column 'bin_count_end' which", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n# Create a new column with the bin number\ndf['bin'] = (df['col1'] // 4).astype(int)\n\n# Create a new column with the bin number\ndf['bin'] = (df['col1'] // 4).astype(int)\n\n# Group by bin and sum the values in each bin\nresult = df.groupby('bin').sum()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\n# Create a new column with the number of rows in each group of 3 rows\ndf['group'] = (df.index // 3).astype(int)\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows in each group\ndf['group_num'] = (df.group.values[:-1] + 1) * 3\n\n# Create a new column with the number of rows", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column for the sum of the first 3 rows\ndf['sum'] = df.iloc[:3, 0].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['avg'] = df.iloc[:2, 0].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:, 0].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:, 0].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:, 0].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:, 0].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:, 0].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:, 0].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:, 0].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:, 0].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:, 0].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:, 0].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:, 0].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:, 0].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:, 0].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:, 0].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:, 0].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:, 0].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:, 0].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:, 0].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:, 0].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:, 0].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:, 0].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['avg_last'] = df.iloc[-2:, 0].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['sum_first'] = df.iloc[3:, 0].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['avg_first'] = df.iloc[2:, 0].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['sum_last'] = df.iloc[-3:, 0].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 2 rows of the dataframe\nlast_2_rows = df.tail(2)\n\n# Create a new column with the first 3 rows of the dataframe\nfirst_3_rows = df.head(3)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 2 rows of the dataframe\nlast_2_rows = df.tail(2)\n\n# Create a new column with the first 3 rows of the dataframe\nfirst_3_rows = df.head(3)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 2 rows of the dataframe\nlast_2_rows = df.tail(2)\n\n# Create a new column with the first 3 rows of the dataframe\nfirst_3_rows = df.head(3)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 2 rows of the dataframe\nlast_2_rows = df.tail(2)\n\n# Create a new column with the first 3 rows of the dataframe\nfirst_3_rows = df.head(3)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 2 rows of the dataframe\nlast_2_rows = df.tail(2)\n\n# Create a new column with the first 3 rows of the dataframe\nfirst_3_rows = df.head(3)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 2 rows of the dataframe\nlast_2_rows = df.tail(2)\n\n# Create a new column with the first 3 rows of the dataframe\nfirst_3_rows = df.head(3)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 2 rows of the dataframe\nlast_2_rows = df.tail(2)\n\n# Create a new column with the first 3 rows of the dataframe\nfirst_3_rows = df.head(3)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a new column with the last 2 rows of the dataframe\nlast_2_rows = df.tail(2)\n\n# Create a new column with the first 3 rows of the dataframe\nfirst_3_rows = df.head(3)\n\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df.tail(3)\n\n# Create a new column with the first 2 rows of the dataframe\nfirst_2_rows = df.head(2)\n\n# Create a", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(method='ffill')\ndf = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(df.transform(lambda x: x.where(x.ne(0), x.ne(0)).max()))\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['duration'] = df['duration'].str.split(' ', 1).str[0]\ndf['time'] = df['duration'].str.split(' ', 1).str[1]\ndf['number'] = df['duration'].str.split(' ', 1).str[0]\ndf['time_days'] = df['duration'].str.split(' ', 1).str[1]\n\ndf['time_days'] = df['time_days'].apply(lambda x: x.replace('day', '1').replace('week', '7').replace('month', '30').replace('year', '365'))\n\nprint(df)\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    df['duration'] = df['duration'].str.split(' ', 1).str[0]\n    df['number'] = df['duration'].str.split(' ', 1).str[1]\n    df['time'] = df['duration'].str.split(' ', 1).str[0]\n    df['time_days'] = df['time'].apply(lambda x: x.replace('year', 365).replace('day', 1).replace('week', 7).replace('month', 30))\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['number'] = df['number'].apply(lambda x: x.replace('.', ''))\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number'].astype(int)\nprint(df)\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.where(np.all(np.array([df1[column] == df2[column] for column in columns_check_list]), axis=1))", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = np.all(np.where((df1[columns_check_list] == df2[columns_check_list]), True, False), axis=1)", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\n    # Create a new column with the date and x, y values\n    df['date'] = df['date'].apply(lambda x: pd.to", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    # parse date index using pd.to_datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # swap the two levels\n    df = df.set_index(['id', 'date'])\n    # return the solution in this function\n    return df\n    ### END SOLUTION ###\n\n# test the function\ndf = pd.DataFrame({'x': ['abc'], 'y': [100], 'date': ['3/1/1994']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot_table(index=['Country', 'Variable'], columns='year', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns=['Variable'])\ndf = df.sort_values(['Country', 'year'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.sort_values(['Country', 'Variable', 'year'], ascending=[True, True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='value', fill_value=0)\ndf = df.reset_index()\ndf = df.rename(columns={'value': 'var1'})\ndf = df.drop(columns=['Variable'])\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'year': 'var1'})\ndf = df.sort_values(['Country', 'var1'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var2'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var3'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var4'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var5'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var6'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var7'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var8'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var9'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var10'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var11'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var12'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var13'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var14'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var15'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var16'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var17'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var18'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var19'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var20'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var21'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var22'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var23'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var24'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var25'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var26'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var27'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var28'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var29'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var30'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var31'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var32'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var33'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var34'], ascending=[True, False])\ndf = df.sort_values(['Country', 'var35'], ascending", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\n# Create a list of column names\ncolumn_names = [f'Value_{i}' for i in range(1, 11)]\n\n# Create a new dataframe with only the columns that have a value less than 1\nnew_df = df[column_names]\n\n# Filter out rows where all values are less than 1\nnew_df = new_df[new_df.max(axis=1) < 1]\n\nprint(new_df)\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\n# Create a list of column names\ncolumn_names = [f'Value_{i}' for i in range(1, 11)]\n\n# Create a new dataframe with only the columns that have a value greater than 1\nnew_df = df[column_names]\n\n# Create a new dataframe with only the rows where any column has a value greater than 1\nnew_df = new_df[new_df.any(axis=1)]\n\nprint(new_df)\n", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a list of column names that start with 'Value'\nvalue_columns = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new dataframe with only the columns that have a value more than 1\nnew_df = df", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df = df.replace({'&LT;': '<'}, regex=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    # replace &AMP; with '&'\n    df = df.replace({'&AMP;': '&'})\n    # return the result\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: x.split(' ')[1] if ' ' in x else None)\n\nprint(df)\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x.split(' ')[1] if ' ' in x else x)\n\nprint(df)\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['name'] = df['name'].apply(validate_single_space_name)\n\n# Create a new DataFrame with the first and last names\nfirst_names = df[df['name'].notna()]['name'].apply(lambda x: x.split(' ')[0])\nlast_names = df[df['name'].notna()]['name'].apply(lambda x: x.split(' ')[1])\n\n# Create a new DataFrame with the first and last names\nnew_df = pd.DataFrame({'first_name': first_names, 'last_name': last_names})\n\n# Print the new DataFrame\nprint(new_df)\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge_asof(df2, df1, on='Timestamp', direction='nearest')\nresult['data'] = df1['data'].values[result['Timestamp'].values]\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1, df2, on='Timestamp', direction='forward')\nresult['data'] = df2['stuff']\nresult = result.sort_values('Timestamp')\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= 50, df['col1'], np.max(df[['col1', 'col2', 'col3']], axis=1))\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = np.where(df['col2'] > 50, df['col1'], np.sum(df[['col1', 'col2', 'col3']], axis=1))\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndef check_integer(row):\n    if not pd.to_numeric(row[\"Field1\"], errors='coerce') .is_integer():\n        return row[\"Field1\"]\n    return None\n\ndf[\"Field1\"] = df.apply(check_integer, axis=1)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndef get_integer_values(row):\n    return [i for i in row if isinstance(i, int)]\n\ndf['Field1'] = df['Field1'].apply(get_integer_values)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    # create a new column with the result of the isnumeric function\n    df['Field1'] = df['Field1'].apply(lambda x: x if pd.to_numeric(x, errors='coerce') is not None else \"and\")\n    # return the new column\n    return df['Field1'].tolist()\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndef compute_percentage(df):\n    total = df.sum()\n    return df / total\n\ndf = compute_percentage(df)\nprint(df)\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)\nprint(result)\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "# calculate the pairwise distances between cars\ndistances = pd.DataFrame(np.sqrt((df['x'].values[:, None] - df['x'].values[None, :])**2 + (df['y'].values[:, None] - df['y'].values[None, :])**2))\n\n# find the nearest neighbour for each car\nnearest_neighbour = np.argmin(distances, axis=1)\n\n# calculate the average of the distances for each frame\nmean_distances = distances.groupby('time').mean()\n\n# create a new dataframe with the nearest neighbour and the mean distance for each time point\ndf2 = pd.DataFrame({'car': df['car'].values, 'nearest_neighbour': nearest_neighbour, 'mean_distance': mean_distances})\n\n# print the result\nprint(df2)\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "# Create a new column 'car_neighbour' that contains the nearest neighbour for each car\ndf['car_neighbour'] = df.groupby('car')['car'].transform(lambda x: x.iloc[x.argmin()])\n\n# Create a new column 'farmost_neighbour' that contains the farthest neighbour for each car\ndf['farmost_neighbour'] = df.groupby('car')['car'].transform(lambda x: x.iloc[x.argmax()])\n\n# Create a new column 'euclidean_distance' that contains the euclidean distance between each car and its neighbour\ndf['euclidean_distance'] = df.groupby('car')['x'].transform(lambda x: x.apply(lambda y: ((x - y)**2).sum()))\n\n# Calculate the average of the distances for each frame using groupby\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf['keywords_all'] = df[cols].apply(lambda x: ', '.join(x.dropna().values.tolist()), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "def concat_columns(df):\n    return \"-\".join(df.columns)\n\ndf[\"keywords_all\"] = df.apply(concat_columns, axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "\ndef concat_keywords(df):\n    keywords_all = []\n    for col in df.columns[1:]:\n        keywords_all.append(df[col].dropna().to_list())\n    return \"-\".join(keywords_all)\n\ndf['keywords_all'] = df.apply(concat_keywords, axis=1)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndef concat_last_rows(df):\n    last_rows = df.iloc[-1:]\n    last_rows_cols = last_rows.columns\n    last_rows_values = last_rows.values\n    last_rows_values = last_rows_values.tolist()\n    last_rows_values = last_rows_values[0]\n    last_rows_values = list(last_rows_values)\n    last_rows_values = [str(i) for i in last_rows_values]\n    last_rows_values = \"-\".join(last_rows_values)\n    last_rows_values = last_rows_values.replace(\"nan\", \"\")\n    last_rows_values = last_rows_values.replace(\" \", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_rows_values = last_rows_values.replace(\"-\", \"\")\n    last_", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_rows = df.sample(n=int(0.20 * len(df)), random_state=0)\n\n# Change Quantity to zero\nsample_rows['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_rows.index = range(len(sample_rows))\n\n# Print the resulting DataFrame\nprint(sample_rows)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_rows = df.sample(n=int(0.20 * len(df)), random_state=0)\n\n# Change value of ProductId to zero\nsample_rows['ProductId'] = 0\n\n# Keep indexes of the altered rows\nsample_rows.index = range(len(sample_rows))\n\n# Print the result\nprint(sample_rows)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# Create a new DataFrame with the 20% of rows from each user\ndf_sample = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n\n# Set the Quantity column of these rows to zero\ndf_sample.loc[df_sample.index, 'Quantity'] = 0\n\n# Keep the indexes of the altered rows\ndf_sample.index = range(len(df_sample))\n\n# Print the resulting DataFrame\nprint(df_sample)\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    # find the duplicates\n    duplicates = df.duplicated(subset=['col1','col2'], keep='first')\n    # find the index of the first duplicate\n    index_original = duplicates.index[duplicates]\n    # add a new column with the index of the first duplicate\n    df['index_original'] = index_original\n    # return the result\n    return df\n\n# call the function\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# Find the maximum value in each group\nmax_values = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Find the rows where count equals the maximum value in each group\nresult = df[df['count'] == max_values]\n\nprint(result)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "min_count = df['count'].min()\nresult = df[df['count'] == min_count]", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")\nprint(result)\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")\nprint(result)\n", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "result = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                  ('A', 'B', 'F'),\n                                  ('A', 'C', 'G'),\n                                  ('A', 'C', 'H'),\n                                  ('A', 'D', 'I'),\n                                  ('A', 'D', 'J')])\nresult", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "result = pd.melt(df, value_vars=[list('BBCCDD'), list('EFGHIJ')], var_names=['variable_0', 'variable_1', 'variable_2'])\nresult = result.sort_values(['variable_0', 'variable_1', 'variable_2'])\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id').val.cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] < 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != np.nan else np.nan)\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship_type(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-to-many'\n    elif column1.min() == column2.max():\n        return 'many-to-one'\n    elif column1.min() == column2.min():\n        return 'one-to-one'\n    elif column1.max() == column2.max():\n        return 'many-to-many'\n    else:\n        return 'none'\n\nresult = []\nfor i in range(df.shape[1]):\n    for j in range(i + 1, df.shape[1]):\n        result.append(f'{df.columns[i]} {df.columns[j]} {relationship_type(df[df.columns[i]], df[df.columns[j]])}')\n\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-2-many'\n    elif column1.min() == column2.max():\n        return 'one-2-one'\n    elif column1.min() == column2.min():\n        return 'many-2-one'\n    elif column1.max() == column2.max():\n        return 'many-2-many'\n    else:\n        return 'unknown'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {relationship(column1, column2)}')\n\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def relationship(column1, column2, column3, column4, column5):\n    if column1.equals(column2) and column1.equals(column3) and column1.equals(column4) and column1.equals(column5):\n        return 'one-to-one'\n    elif column2.equals(column3) and column2.equals(column4) and column2.equals(column5):\n        return 'one-to-many'\n    elif column3.equals(column4) and column3.equals(column5):\n        return 'many-to-one'\n    elif column4.equals(column5):\n        return 'many-to-many'\n    else:\n        return 'none'\n\nresult = pd.DataFrame({\n    'Column1': relationship(df['Column1'], df['Column2'], df['Column3'], df['Column4'], df['Column5']),\n    'Column2': relationship(df['Column2'], df['Column3'], df['Column4'], df['Column5'], df['Column1']),\n    'Column3': relationship(df['Column3'], df['Column4'], df['Column5'], df['Column1'], df['Column2']),\n    'Column4': relationship(df['Column4'], df['Column5'], df['Column1'], df['Column2'], df['Column3']),\n    'Column5': relationship(df['Column5'], df['Column1'], df['Column2'], df['Column3'], df['Column4'])\n})\nprint(result)\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship(x, y):\n    if x.shape[0] == y.shape[0]:\n        return 'one-2-one'\n    elif x.shape[0] == y.shape[0] + 1:\n        return 'one-2-many'\n    elif x.shape[0] + 1 == y.shape[0]:\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\nresult = pd.DataFrame(columns=df.columns, index=df.columns)\nfor c1 in df.columns:\n    for c2 in df.columns:\n        result.loc[c1, c2] = relationship(df[c1], df[c2])\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\nresult = dfiban_uniq\nprint(result)\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\n# Create a new column 'Family'\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\n\n# Group by 'Family' and calculate the mean of 'Survived'\nresult = df.groupby('Family')['Survived'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\n# Create a new column 'Family'\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\n\n# Group by 'Family' and calculate the mean of 'SibSp'\nresult = df.groupby('Family')['SibSp'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndef group_condition(sibsp, parch):\n    if sibsp == 1 and parch == 1:\n        return 'Has Family'\n    elif sibsp == 0 and parch == 0:\n        return 'No Family'\n    elif sibsp == 0 and parch == 1:\n        return 'New Family'\n    elif sibsp == 1 and parch == 0:\n        return 'Old Family'\n    else:\n        return 'Unknown'\n\ngroup_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\n\ngroup_dict = dict(zip(group_names, group_condition))\n\ngrouped = df.groupby(group_dict)\n\nresult = grouped.mean()\nprint(result)\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "def mean_std(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('a').apply(mean_std)\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "def mean_std(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('b').a.apply(mean_std)\nresult", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x, axis=0)\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = df['b'].apply(softmax)\ndf['min-max'] = df['b'].apply(min_max)\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "result = df.loc[~(df.sum(axis=1) == 0)]\nresult = result.loc[~(result.sum(axis=0) == 0)]\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "result = df.loc[df.sum(axis=1) == 0]\nresult = result.loc[result.sum(axis=0) == 0]\nresult", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "max_value = df.max().max()\nresult = df[df.max().max() != max_value]\nresult", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "result = df.where(df == 2, 0)\nresult", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "sorted_s = s.sort_values(ascending=True)\nsorted_s = sorted_s.sort_index()\nresult = sorted_s", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "sorted_s = s.sort_values(ascending=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)\nsorted_s = sorted_s.reset_index(drop=True)", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].apply(lambda x: isinstance(x, str))]\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# Find the maximum value in each group\nmax_values = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Find the rows where count equals the maximum value in each group\nresult = df[df['count'] == max_values]\n\nprint(result)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "min_count = df['count'].min()\nresult = df[df['count'] == min_count]", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": " ###\n    # map the dict values to the Date column\n    df['Date'] = df['Member'].map(dict)\n    # fill NaN with the value from the Date column\n    df['Date'] = df['Date'].fillna(df['Date'])\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17-Aug-1926')\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')).size()\ndf['Count_m'] = df.groupby(df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')).size()\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year')).size()\nprint(df)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_w'] = df.groupby([df['Date'].dt.weekday]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date')['B'].apply(lambda x: x.sum() if x.sum() == 0 else x.sum()).reset_index()\nresult2 = df.groupby('Date')['B'].apply(lambda x: x.sum() if x.sum() == 0 else x.sum()).reset_index()\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "even_values = df[df['B'] % 2 == 0]\nodd_values = df[df['B'] % 2 != 0]\n\nresult1 = even_values.groupby('Date').count()\nresult2 = odd_values.groupby('Date').count()\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nprint(result)\n\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.mean)\nprint(result)\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult_mean = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nresult_mean['D'] = result_mean['D'] * 2\nresult_mean['E'] = result_mean['E'] * 2\nresult_mean", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nprint(result)\n\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nprint(result)\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\ndef split_string_to_rows(df):\n    df = df.reset_index()\n    df['var1'] = df['var1'].str.split(',')\n    df = df.explode('var1')\n    return df\n\nresult = split_string_to_rows(df)\nprint(result)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\ndef split_string_to_rows(df):\n    df = df.replace(',', ', ', regex=True)\n    df = df.replace(' ', ', ', regex=True)\n    df = df.replace('.', ', ', regex=True)\n    return df\n\nresult = split_string_to_rows(df)\nprint(result)\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\ndef split_into_rows(df):\n    return df.assign(var1=df['var1'].str.split('-'))\n\nresult = split_into_rows(df)\nprint(result)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df['fips'] = df['row'].str[:-1]\ndf['row'] = df['row'].str[-1]\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf['row'] = df['row'].str.split(' ', 1, expand=True)[0]\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        if col != 'Name':\n            df[col] = df[col].apply(lambda x: x / (1 if x != 0 else 1))\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        if col != 'Name':\n            df[col] = df[col].rolling(window=len(df[col])).mean()\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    # calculate the cumulative average for each row using pandas\n    # ignore zero values\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x: x.mean() if x.mean() != 0 else x.mean(), axis=1)\n    # result = df.apply(lambda x:", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndef calculate_cumulative_average(df):\n    for col in df.columns:\n        if col != 'Name':\n            df[col] = df[col].rolling(window=len(df[col])).mean()\n    return df\n\ndf = calculate_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)) > 0\ndf['label'] = df['label'].apply(lambda x: 1 if x else 0)\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\n# Create a new column 'label'\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n\n# Set label for the first row to 1\ndf.loc[0, 'label'] = 1\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_datetime(df.departure_time) - pd.to_datetime(df.arrival_time)\ndf['Duration'] = df.Duration.apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = pd.to_datetime(df.Duration)\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds)\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply(lambda x: x.seconds if x.seconds > 0 else 0)\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time.diff().apply", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df.groupby('key1').apply(lambda x: x.loc[x['key2'].endswith('e'), 'key2'].size()).reset_index(drop=True)\nresult", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.index.max()\nmin_result = df.index.min()\nprint(max_result, min_result)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.mode(axis=0)[0][0]\nmedian_result = df.median()[0]\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df[~(99 <= df['closing_price'] <= 101)]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "result = df.groupby(\"item\")[\"diff\"].min()\nresult = result.reset_index()\nresult", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "# calculate the number of NaN values in Column_x\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# calculate the number of NaN values to be filled\nnan_to_fill = nan_count // 2\n\n# fill the NaN values with the mode of Column_x\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0])\n\n# fill the first nan_to_fill values with 0 and the last nan_to_fill values with 1\ndf['Column_x'] = np.where(np.isnan(df['Column_x']), 0, 1)\n\n# print the dataframe\nprint(df)\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "# Calculate the number of NaN values in the column\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# Calculate the number of NaN values to be filled\nnan_to_fill = nan_count * 0.3\n\n# Calculate the number of NaN values to be filled in the first 30%\nnan_to_fill_first = np.floor(nan_to_fill)\n\n# Calculate the number of NaN values to be filled in the middle 30%\nnan_to_fill_middle = np.floor(nan_to_fill * 0.7)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = nan_to_fill - nan_to_fill_first - nan_to_fill_middle\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_last = np.floor(nan_to_fill_last)\n\n# Calculate the number of NaN values to be filled in the last 30%\nnan_to_fill_", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# Create a mask for the NaN values\nmask = np.isnan(df['Column_x'])\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create a mask for the first 50% of the NaN values\nmask_first_50 = np.isnan(df['Column_x'])[:50]\n\n# Create a mask for the last 50% of the NaN values\nmask_last_50 = np.isnan(df['Column_x'])[50:]\n\n# Create", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "a_b = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0])], columns=['one', 'two'])\nresult = a_b", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "a_b = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0], c.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0], c.iloc[1,0])], columns=['one', 'two'])\nresult = a_b", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "# Create a list of tuples from the dataframes a and b\nresult = [(a.iloc[i], b.iloc[i]) for i in range(a.shape[0])]\n# Convert the list to a DataFrame\nresult = pd.DataFrame(result, columns=['a', 'b'])\n# Fill the nan values with np.nan\nresult = result.fillna(np.nan)\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins, labels=[1, 1, 1, 1]).value_counts()).reset_index()\nresult = groups.set_index('username')\nresult = result.sort_values(by='views')\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins).value_counts()).reset_index()\nresult = groups.set_index('username')\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins, labels=[1, 1, 1, 1]).value_counts()).reset_index()\nresult = groups.set_index('username')\nresult = result.sort_values(by='views')\nresult = result.sort_values(by='views', ascending=False)\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', ')))\nresult = result.to_frame('text')\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).values.tolist()\nresult = pd.DataFrame({'text': result})\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df['text'].tolist()\nresult = ', '.join(result)\nresult = result.replace(' ', ',')\nresult = result.replace('.', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\nresult = result.replace(',,', ',')\n", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].to_series().str.split(', ', expand=True).apply(lambda x: x.str.strip())\nresult = result.apply(lambda x: ', '.join(x))\nresult = result.to_frame('text')\nresult", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame()\nresult.columns = ['text']\nresult", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.concat([df1, df2], axis=0)\nresult['city'] = result['city'].fillna(result['district'])\nresult['district'] = result['district'].fillna(result['city'])\nresult = result.sort_values('id')\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n# merge df1 and df2 on id\nmerged_df = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with city and district from df2\nmerged_df = merged_df.fillna({'city': '', 'district': ''})\n\n# convert date to date format\nmerged_df['date'] = pd.to_datetime(merged_df['date']).dt.strftime('%d-%b-%Y')\n\n# sort by id and date\nmerged_df = merged_df.sort_values(['id', 'date'])\n\n# reset index\nmerged_df = merged_df.reset_index(drop=True)\n\n# print result\nprint(merged_df)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\n# merge df1 and df2 on id\nmerged_df = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with NaN\nmerged_df = merged_df.fillna(value=np.nan)\n\n# sort by id and date\nmerged_df = merged_df.sort_values(['id', 'date'])\n\n# group by id and date, then fill NaN with NaN\nmerged_df = merged_df.groupby(['id', 'date']).apply(lambda x: x.fillna(np.nan))\n\n# sort by id and date\nmerged_df = merged_df.sort_values(['id', 'date'])\n\n# reset index\nmerged_df = merged_df.reset_index(drop=True)\n\n# print result\nprint(merged_df)\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x']\nresult['B_y'] = D['B']\nresult = result.sort_values('A')\nresult = result.drop(columns='B_x')\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x']\nresult['B_y'] = D['B']\nresult = result.sort_values('A')\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['dulplicated'] = result['A'].apply(lambda x: x in D['A'])\nresult", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).tolist())\nprint(result)\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).tolist()\nresult = [list(x) for x in result]\nresult = [(time, amount) for time, amount in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x[1]) for x in result]\nresult = [(sorted(x), x", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).tolist()).to_dict()\nresult = {k: v for k, v in result.items() if v}\nprint(result)\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "df_concatenated = pd.DataFrame(series.values.tolist(), index=series.index, columns=series.index)\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "df_concatenated = pd.DataFrame(series.values.tolist(), index=series.index, columns=series.index)\ndf_concatenated = df_concatenated.transpose()\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.columns[~df.columns.str.contains(s)].tolist()[0]\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "# Find the column names that contain the string 'spike'\ncolumns = df.columns[df.columns.str.contains(s)]\n\n# Create a new dataframe with the columns that contain 'spike'\nresult = df[columns]\n\n# Rename the columns\nresult.columns = [f'spike{i}' for i in range(1, len(result.columns) + 1)]\n\nprint(result)\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index, columns=['code_0', 'code_1', 'code_2'])\n\n# Iterate over the original dataframe and fill the new dataframe with the values from the original dataframe\nfor i in range(len(df)):\n    new_df.loc[i, 'code_0'] = df.loc[i, 'codes'][0]\n    new_df.loc[i, 'code_1'] = df.loc[i, 'codes'][1] if len(df.loc[i, 'codes']) > 1 else None\n    new_df.loc[i, 'code_2'] = df.loc[i, 'codes'][2] if len(df.loc[i, 'codes']) > 2 else None\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()\n\n# Create a new dataframe with the same index as the original one\nnew_df = df.copy()", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# Split the lists into columns\ndf['codes'] = df['codes'].apply(lambda x: pd.Series(x))\n\n# Sort the lists\ndf['codes'] = df['codes'].apply(lambda x: x.sort_values())\n\n# Fill NaN with NaN\ndf['codes'] = df['codes'].apply(lambda x: x.fillna(np.nan))\n\n# Create new columns\nfor i in range(1, 4):\n    df[f'code_{i}'] = df['codes'].apply(lambda x: x[i-1])\n\n# Drop the original codes column\ndf = df.drop(columns='codes')\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "result = df['col1'].apply(lambda x: x[0])\nresult = result.tolist()\nresult = [item for sublist in result for item in sublist]\nprint(result)\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "result = df.loc[0, 'col1'].tolist()[0][::-1].tolist()\nresult = ', '.join(map(str, result))\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x)))\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "# Convert Time to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Create a new column for the bin\ndf['bin'] = df['Time'].apply(lambda x: x.hour * 60 + x.minute)\n\n# Group by bin and mean value\nmean_values = df.groupby('bin')['Value'].mean()\n\n# Create a new DataFrame with bin and mean value\nmean_values_df = pd.DataFrame({'Time': mean_values.index, 'Value': mean_values.values})\n\n# Interpolate values\nmean_values_df['Time'] = mean_values_df['Time'].apply(lambda x: x.replace(minute=x.minute // 2 * 2))\n\n# Create a new column for the bin\nmean_values_df['bin'] = mean_values_df['Time'].apply(lambda x: x.hour * 60 + x.minute)\n\n# Group by bin and mean value\nmean_values_df = mean_values_df.groupby('bin')['Value'].mean()\n\n# Create a new DataFrame with bin and mean value\nmean_values_df = pd.DataFrame({'Time': mean_values_df.index, 'Value': mean_values_df.values})\n\n# Print the result\nprint(mean_values_df)\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "# Convert the time to minutes\ndf['Time'] = (df['Time'].values - df['Time'].values.min()) / 60\n# Bin the time into 3-minute intervals\nbins = pd.cut(df['Time'], bins=np.arange(0, 24, 3))\n# Group by bins and sum the values\ndf = df.groupby(bins).sum()\n# Convert the bins back to time\ndf['Time'] = bins.values * 3\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate(method='linear')\n# Convert the time to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n# Set the time as index\ndf = df.set_index('Time')\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[np.where(np.isnan(df).any(axis=1))]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.columns[np.all(df.isnull(), axis=1)]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[np.where(np.isnan(df).any(axis=1))].tolist()", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(np.nan, np.nan) if np.isnan(df.iloc[0, i]) and np.isnan(df.iloc[8, i]) else (df.iloc[0, i], df.iloc[8, i]) for i in range(10)]\nprint(result)\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.transpose().reset_index(drop=True).transpose()\ndf.columns = df.columns.str.split('_').str[0]\ndf = df.transpose().reset_index(drop=True).transpose()\n", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.transpose().reset_index(drop=True).transpose()\ndf.columns = df.columns.str.replace('_0', '')\ndf.columns = df.columns.str.replace('_1', '')\ndf.columns = df.columns.str.replace('_2', '')\ndf.columns = df.columns.str.replace('_3', '')\ndf.columns = df.columns.str.replace('_4', '')\ndf.columns = df.columns.str.replace('_5', '')\ndf.columns = df.columns.str.replace('_6', '')\ndf.columns = df.columns.str.replace('_7', '')\ndf.columns = df.columns.str.replace('_8', '')\ndf.columns = df.columns.str.replace('_9', '')\ndf.columns = df.columns.str.replace('_10', '')\ndf.columns = df.columns.str.replace('_11', '')\ndf.columns = df.columns.str.replace('_12', '')\ndf.columns = df.columns.str.replace('_13', '')\ndf.columns = df.columns.str.replace('_14', '')\ndf.columns = df.columns.str.replace('_15', '')\ndf.columns = df.columns.str.replace('_16', '')\ndf.columns = df.columns.str.replace('_17', '')\ndf.columns = df.columns.str.replace('_18', '')\ndf.columns = df.columns.str.replace('_19', '')\ndf.columns = df.columns.str.replace('_20', '')\ndf.columns = df.columns.str.replace('_21', '')\ndf.columns = df.columns.str.replace('_22', '')\ndf.columns = df.columns.str.replace('_23', '')\ndf.columns = df.columns.str.replace('_24', '')\ndf.columns = df.columns.str.replace('_25', '')\ndf.columns = df.columns.str.replace('_26', '')\ndf.columns = df.columns.str.replace('_27', '')\ndf.columns = df.columns.str.replace('_28', '')\ndf.columns = df.columns.str.replace('_29', '')\ndf.columns = df.columns.str.replace('_30', '')\ndf.columns = df.columns.str.replace('_31', '')\ndf.columns = df.columns.str.replace('_32', '')\ndf.columns = df.columns.str.replace('_33', '')\ndf.columns = df.columns.str.replace('_34', '')\ndf.columns = df.columns.str.replace('_35', '')\ndf.columns = df.columns.str.replace('_36', '')\ndf.columns = df.columns.str.replace('_37', '')\ndf.columns = df.columns.str.replace('_38', '')\ndf.columns = df.columns.str.replace('_39', '')\ndf.columns = df.columns.str.replace('_40', '')\ndf.columns = df.columns.str.replace('_41', '')\ndf.columns = df.columns.str.replace('_42', '')\ndf.columns = df.columns.str.replace('_43', '')\ndf.columns = df.columns.str.replace('_44', '')\ndf.columns = df.columns.str.replace('_45', '')\ndf.columns = df.columns.str.replace('_46', '')\ndf.columns = df.columns.str.replace('_47', '')\ndf.columns = df.columns.str.replace('_48', '')\ndf.columns = df.columns.str.replace('_49', '')\ndf.columns = df.columns.str.replace('_50', '')\ndf.columns = df.columns.str.replace('_51', '')\ndf.columns = df.columns.str.replace('_52', '')\ndf.columns = df.columns.", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\nprint(df)", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "for column in list_of_my_columns:\n    df[column] = df[column].mean()\nprint(df)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by='time')", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(by=['treatment','dose','time'])\nprint(result)\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "# Create a list of dates to delete\ndates_to_delete = ['2020-02-17', '2020-02-18']\n\n# Create a new DataFrame with the dates to delete\ndates_to_delete = pd.to_datetime(dates_to_delete)\ndates_to_delete = dates_to_delete.to_frame()\n\n# Delete the dates from the original DataFrame\nresult = df[~dates_to_delete.index.isin(df.index)]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Create a new column 'Day' to store the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Day_of_week' to store the day of the week as a number\ndf['Day_of_week'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n\n# Create a new column 'Day_of_week_num' to store the day of the week as a number\ndf['Day_of_week_num'] = df['Day_of_week'].map({'Friday': 1, 'Saturday': 2, 'Sunday': 3, 'Monday': 4, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 7, 'Friday': 8})\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr[np.abs(corr) > 0.3].stack().reset_index(drop=True)\nresult.columns = ['Col1', 'Col2']\nresult.index = result.index + 1\nresult", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr.where(corr > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "last_column = df.columns[-1]\nlast_column_renamed = last_column + '_last'\n\ndf.rename(columns={last_column: last_column_renamed}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "result = df.rename(columns={df.columns[0]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndef find_frequent(row):\n    return row.value_counts().max()\n\ndef find_freq_count(row):\n    return row.value_counts().values.max()\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndef find_frequent(row):\n    return row.value_counts().index[0]\n\ndef find_freq_count(row):\n    return row.value_counts().values[0]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    return [row[i] for i in range(len(row)) if row[i] == row.max()]\n\ndef find_freq_count(row):\n    return len([row[i] for i in range(len(row)) if row[i] == row.max()])\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "res = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nres = res.reset_index()\nres", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "res = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean()\nres = res.reset_index()\nres.columns = [\"id1\",\"id2\",\"foo\",\"bar\"]\nres.loc[res[\"bar\"].isnull(), \"bar\"] = 0\nres", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo']]\nresult", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'b_col']]\nresult", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = np.nan_to_num(x, nan=np.nan)\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "x = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nx = x[~np.isnan(x)]\nresult = x.tolist()\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((a.shape[0], 4))\nb[:, 0] = a == 0\nb[:, 1] = a == 1\nb[:, 2] = a == 2\nb[:, 3] = a == 3\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((a.size, a.size))\nb[a, a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i, j] = a[i, j]\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (len(A), ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "def vec2matrix(A, nrow):\n    return np.reshape(A, (nrow, -1))\n\nB = vec2matrix(A, nrow)\nprint(B)\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "def shift(a, shift):\n    return np.roll(a, shift, axis=1)\n\nresult = shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape, order='C')\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a), a.shape, order='C')\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a), a.shape)\n    return max_index\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2, axis=-1)[:-2, -2], a.shape)\nprint(result)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "z = np.any(np.isnan(a), axis=0)\na = np.delete(a, z, axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = np.delete(a, np.where(np.isnan(a)))\n", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravel_index(np.argmin(a), a.shape)\nresult = min_index\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "min_index = np.unravel_index(np.argmin(a, axis=0), a.shape)\nresult = np.array([[0, 1], [1, 0]])\nprint(min_index)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.degrees(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.degrees(degree))\nprint(result)", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * 180 / np.pi\nresult = np.rad2deg(result)\nprint(result)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)\nprint(a)\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(a, 2))\nprint(f(a, 3))\nprint(f(a, 4))\nprint(f(a, 5))\nprint(f(a, 6))\nprint(f(a, 7))\nprint(f(a, 8))\nprint(f(a, 9))\nprint(f(a, 10))\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), 1)\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    # Use numpy's floor_divide function to divide the numerator and denominator by 100\n    # Then return the result as a tuple\n    return np.floor_divide(numerator, 100), np.floor_divide(denominator, 100)\n    ### END SOLUTION ###\n\n# Test the function\nprint(f())\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), result.astype(int))\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = np.array([(a + b + c) / 3, (b + c) / 2, (a + c) / 3])", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(a, np.maximum(b, c))", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.flatten())[::-1]\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a.flatten())[::-1]\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.concatenate((np.diag(a), np.diag(a.T)[::-1]))", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(a), np.diag(a.T)[::-1]))\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = np.empty((X.shape[0],), dtype=object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.empty((X.shape[0], X.shape[1]), dtype=np.object)\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result[i, j] = X[i, j]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i, j])\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty(X.shape[0], dtype=np.object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.fromstring(mystr, dtype=int, sep='')\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.multiply(a[:, col], multiply_number)\nresult = np.cumsum(result)\n", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum number of linearly independent vectors", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "# calculate the t-statistic\nt_statistic = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_statistic, a.shape[0] + b.shape[0])\nprint(p_value)", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "# calculate the t-statistic\nt_statistic = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_statistic, a.shape[0] + b.shape[0])\nprint(p_value)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the weighted mean and variance\nweight_a = 1 / (anobs * (anobs - 1))\nweight_b = 1 / (bnobs * (bnobs - 1))\n\n# calculate the weighted mean and variance\nweighted_mean_a = amean * weight_a\nweighted_mean_b = bmean * weight_b\n\nweighted_var_a = avar * weight_a\nweighted_var_b = bvar * weight_b\n\n# calculate the t-test\nt_statistic = (weighted_mean_a - weighted_mean_b) / np.sqrt((weighted_var_a + weighted_var_b) / 2)\n\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_statistic, anobs + bnobs - 2)\n\n# print the p-value\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.asarray([A[~np.in1d(A, B)]])", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c)\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)\nprint(result)\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)\n", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, 0)\n", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "del_col = np.sort(del_col)[::-1]\nresult = np.delete(a, del_col, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\nprint(a)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a_l = a.tolist()\n    a_l.insert(pos, element)\n    a = np.asarray(a_l)\n    return a\n    ### END SOLUTION ###\n\nprint(f(a=example_a, pos=2, element=66))\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.all(a == a[0], axis=1))", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.all(a == a[0], axis=1))", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.all(a == a[0], axis=1))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\nprint(result)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    # calculate the 2D integral over (cosx)^4 + (siny)^2\n    # using Simpson's rule\n    # result = np.sum(np.sum(np.cos(x)**4 + np.sin(y)**2, axis=0))\n    ### END SOLUTION ###\n    return result\n\n# call the function\nf(x, y)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\n# compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow = np.min(grades)\nhigh = np.max(grades)\nwhile high - low > threshold:\n  high = np.max(grades[grades < high])\n  low = np.min(grades[grades > low])\n  if high - low < threshold:\n    break\n\nprint(low, high)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.randint(2, size=size)\nnums[nums == 0] = one_ratio\nnums[nums == 1] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 0] = one_ratio\nnums[nums == 1] = 1 - one_ratio\nnums", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.tensor(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = np.array(a)", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a) + 1\nresult = result.tolist()\nresult", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[::-1][:N]\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "patch_size = 2\npatch_count = a.shape[0] // patch_size\nresult = np.zeros((patch_count, patch_count, patch_size, patch_size))\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j, :, :] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "patch_size = 2\na_shape = a.shape\npatch_shape = (a_shape[0] // patch_size, a_shape[1] // patch_size)\nresult = np.array([a[i:i+patch_shape[0], j:j+patch_shape[1]] for i in range(patch_shape[0]) for j in range(patch_shape[1])])\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\nresult = np.transpose(result)\nresult = np.reshape(result, (h*w, 1))\nresult = np.transpose(result)\nresult = np.reshape(result, (h, w))\nprint(result)\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "patch_size = 2\na_shape = a.shape\npatch_shape = (a_shape[0] // patch_size, a_shape[1] // patch_size)\nresult = np.array([a[i:i+patch_shape[0], j:j+patch_shape[1]] for i in range(patch_shape[0]) for j in range(patch_shape[1])])\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.fromstring(string, sep=' ', dtype=np.float64)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.log(np.random.uniform(min, max, n))", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.log(np.random.uniform(min, max, n))", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.log(np.random.uniform(min, max, n))\n    ### END SOLUTION ###\n\nprint(f(min=1, max=np.e, n=10000))\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))\nprint(result)\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))\nprint(result)\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "def sub2ind(dims, indices):\n    return np.ravel_index(indices, dims)\n\nresult = sub2ind(dims, index)\nprint(result)\n", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "def sub2ind(dims, index):\n    return np.asscalar(np.prod(dims)) + np.asscalar(np.prod(index))\n\nresult = sub2ind(dims, index)\nprint(result)\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df.values.dtype)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.sum(a[accmap])\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0])\nprint(result)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.min(a[index])", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    z[i,:] = elementwise_function(x[i,:], y[i,:])", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, probabilit)\nprint(result)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "a = np.pad(a, ((0, 0), (low_index, high_index)), 'constant')\nresult = a[1:3, 1:3]\nprint(result)\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "x = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.abs(x) <= 1.1]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_max = np.max(bin_data, axis=1)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nbin_data_mean = [list(x) for x in bin_data_mean]\nprint(bin_data_mean)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (x - x_min) * (x - x_max)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    x_min = np.clip(x_min, x, x_max)\n    x_max = np.clip(x_max, x, x_min)\n    x_min = np.clip(x_min, x, x_max)\n    x_max = np.clip(x_max, x, x_min)\n    return x_min if x < x_min else x_max if x > x_max else x\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.correlate(a, b, mode='same')\n\n# Print the correlation matrix\nprint(corr_matrix)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(4)])\nprint(result)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(15)])\nprint(result)\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.unpackbits(np.uint8(a))\nresult = np.reshape(result, (a.shape[0], m))\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:m].reshape(-1, m)\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "# Convert the integer array to binary and then unpack the bits\nbinary = np.unpackbits(np.uint8(a))\n# Create a matrix of zeros with the same shape as the binary array\nresult = np.zeros((1, m), dtype=np.uint8)\n# Compute the exclusive OR of all rows\nresult[0, :] = np.bitwise_xor.reduce(binary, axis=0)\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc -= 3*\u03c3\n\u03bc += 3*\u03c3\nresult = (\u03bc-3*\u03c3, \u03bc+3*\u03c3)\nprint(result)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc_minus_2\u03c3 = \u03bc - 2*\u03c3\n\u03bc_plus_2\u03c3 = \u03bc + 2*\u03c3\nresult = (\u03bc_minus_2\u03c3, \u03bc_plus_2\u03c3)\nprint(result)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std = np.std(a)\n    third_std_dev = np.mean(a) + 3*std\n    return third_std_dev\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nupper_bound = mean + 2*std_dev\nlower_bound = mean - 2*std_dev\noutlier = a > upper_bound\noutlier = outlier | a < lower_bound\nresult = outlier", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data, percentile)\nprint(prob)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[0, 1] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.all(a == np.amax(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1), axis=1)\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "result = np.corrcoef(post, distance)[0, 1]", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.matmul(X, np.transpose(X, (1, 0)))", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((Y.shape[0], Y.shape[1]))\nfor i in range(Y.shape[1]):\n    X[:, i] = np.dot(Y[:, i], Y[:, i].T)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = np.any(a == number)", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\n# Create a mask of elements in B that are not in A\nmask = np.in1d(B, A)\n\n# Use the mask to index A and get the unique elements in A\nunique_elements = np.unique(A[mask])\n\n# Create a new array C that contains the unique elements in A\nC = np.array([x for x in unique_elements if x not in B])\nprint(C)\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = np.array([x for x in A if x in B])\n", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "# Create a boolean array that indicates whether each element in A is in B\nbool_array = np.in1d(A, B)\n# Use the boolean array to index A\nC = A[bool_array]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.max(rankdata(a)) - rankdata(a) + 1", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.max(rankdata(a)) - rankdata(a) + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np.max(result) - result + 1\nresult = np", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    return np.max(a) - np.min(a) + 1 - np.min(a)\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.sum(np.abs(X), axis=1)\nresult = X / l1.reshape(-1, 1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.array([LA.norm(v,ord=2) for v in X])\nresult = np.divide(X, x[:, None])\nprint(result)", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = np.divide(X, x[:, None])\nprint(result)", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = np.where(df['a'].str.contains(target), choices, np.nan)\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "result = np.sqrt(np.sum((a[:, None, :] - a[None, :, :])**2, axis=2))", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "result = np.zeros((dim, dim))\nfor i in range(dim):\n    for j in range(i + 1, dim):\n        result[i, j] = np.sqrt(np.sum((a[:, i] - a[:, j]) ** 2))\n        result[j, i] = result[i, j]\nresult = np.triu(result, k=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concatenate((result, np.tile(result[:, 0], (dim, 1))), axis=1)\nresult = np.concaten", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "result = np.triu(np.sqrt(np.sum((a[:, None, :] - a[None, :, :])**2, axis=-1)), k=1)", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "a = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\na = np.unique(a)\na = a[a!=0]\nresult = a", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# Remove adjacent (before removing) duplicate non-zero value and all the zero value\na = np.where((a[:-1] != 0) & (a[1:] != 0), a, 0)\n\n# Remove all zero value\na = np.where(a != 0, a, 0)\n\n# Remove all zero value after removing adjacent (before removing) duplicate non-zero value\na = np.where(a != 0, a, 0)\n\nresult = a\nprint(result)\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "lat_val = np.concatenate((lat, val), axis=1)\nlon_val = np.concatenate((lon, val), axis=1)\n\ndf = pd.DataFrame({'lat': lat_val[0], 'lon': lon_val[0], 'val': val[0]})\n\nfor i in range(1, len(lat)):\n    df = df.append(pd.DataFrame({'lat': lat[i], 'lon': lon[i], 'val': val[i]}, index=[i]))\n\ndf = df.sort_values(['lat', 'lon'])\nprint(df)\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "lat_max = np.max(lat, axis=1)\nlon_max = np.max(lon, axis=1)\nval_max = np.max(val, axis=1)\n\ndf = pd.DataFrame({'lat': lat_max, 'lon': lon_max, 'val': val_max})\n\n# Add a column to its right, consisting of maximum value of each row\ndf['maximum'] = np.max(df, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, size[0])\n    strides = a.strides + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape, strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, size[0])\n    strides = a.strides + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape, strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:,-1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = c in CNTS\nprint(result)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "print(c in CNTS) # prints True\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.matrix([[12.22151125, 0, 0, 0],[0,4.92815942, 0, 0],[0,0,2.06380839,0 ],[0,0,0,0.29766152]])\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a = np.where(np.triu(a, k=1, suppress_errors=True) == 0, 0, a)", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\n\n# Calculate the number of days between the start and end dates\ndays = (end_date - start_date).days\n\n# Create a linear space of dates\ndates = np.linspace(start_date, end_date, n)\n\n# Convert the dates to pandas DatetimeIndex\nresult = pd.DatetimeIndex(dates)\n\nprint(result)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No index found\")\nelse:\n    print(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "# find the indices of a and b in x and y\nindices = np.where((x == a) & (y == b))\n# if the indices are not empty, then the indices are not empty, otherwise the indices are empty\nresult = indices[0] if indices[0].size > 0 else []\nprint(result)\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "# calculate the coefficients a, b and c\na, b, c = np.linalg.solve(np.vstack([x, np.ones(len(x))]), y)\nresult = np.array([a, b, c])\nresult = result[np.argsort(np.abs(result))]\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "# calculate the coefficients\ncoeffs = np.polyfit(x, y, degree)\n# sort the coefficients in descending order\nresult = np.sort(coeffs)[::-1]\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = np.array([0, 1, 2, 3])\ntemp_df = pd.DataFrame(np.repeat([1, 2, 3, 4], 4).reshape(4, -1))\n\nresult = np.subtract(temp_df, temp_arr)\nprint(result)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\nprint(result)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "mask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr >= -10\narr[mask] = arr[mask] + 5\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr = np.where(arr < n1, 0, np.where(arr >= n2, 30, arr + 5))", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "# print the number of times s1 is not equal to s2 (should be 0)\nprint np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "# compare the two arrays and find the number of different elements\nresult = np.nonzero(np.in1d(s1, s2))[0].shape[0]\nprint(result)\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.all(np.array(a) == np.array(a[0]))", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all() for arr in a)\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, 0), (0, shape[1]-a.shape[1])), 'constant')\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "# calculate the maximum shape\nmax_shape = np.array(shape) + np.array([1, 1])\n# calculate the padding\npadding = np.array(shape) - np.array([1, 1])\n# pad the array\nresult = np.pad(a, padding, mode='constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the shape of the array\n    shape_arr = np.shape(arr)\n    # find the shape of the array that matches the shape of the maximum shape\n    match_shape = np.array([max_shape]*shape_arr.shape[0])\n    # zero pad the array to match the shape of the maximum shape\n    result = np.zeros(match_shape)\n    # zero pad the array to match the shape of the maximum shape\n    result[0:shape_arr[0], 0:shape_arr[1]] = arr\n    ### END SOLUTION ###\n    return result\n\n# test the function\nprint(f())\n", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "# calculate the maximum shape\nmax_shape = np.array(shape).reshape(-1, 1)\n# calculate the padding\npadding = np.array(shape) - np.array(a.shape)\n# calculate the padding for each dimension\npadding = np.array([padding[0], padding[1], padding[2]])\n# zero pad the array\nresult = np.pad(a, padding, mode='constant')\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3,3)\nprint(a)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=2)", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=2)", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = np.where(df['a'] > 1, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "# Create a mask that is True for all rows except the first and last row, and all columns except the first and last column\nmask = np.all(im[1:-1, 1:-1] == im[1:-1, 1:-1], axis=1)\nmask = np.all(mask[:, 1:-1], axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0)\n\n# Create a mask that is True for all rows and all columns\nmask = np.all(im == im, axis=1)\nmask = np.all(mask, axis=0", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "# Find the indices of the non-zero elements\nindices = np.where(A != 0)\n# Find the minimum and maximum indices for each column\nmin_indices = np.min(indices, axis=1)\nmax_indices = np.max(indices, axis=1)\n# Slice the array to the bounding box of the non-zero elements\nresult = A[min_indices[0]:max_indices[0]+1, min_indices[1]:max_indices[1]+1]\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "def remove_peripheral_zeros(im):\n    rows, cols = im.shape\n    mask = np.ones((rows, cols), dtype=bool)\n    for i in range(rows):\n        if np.any(im[i, :] == 0):\n            mask[i, :] = False\n    for j in range(cols):\n        if np.any(im[:, j] == 0):\n            mask[:, j] = False\n    return mask\n\nresult = remove_peripheral_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all columns and False for all rows\nmask = np.all(im, axis=0)\n# Create a mask that is True for all rows and False for all columns\nmask = np.all(im, axis=1)\n# Create a mask that is True for all", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 5))\nsns.lineplot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "line_styles = ['-', '--', '-.', ':']\nfor line_style in line_styles:\n    y = np.random.randn(10)\n    plt.plot(x, y, line_style)\n\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "line_styles = ['-', '--', '-.', ':']\nfor line_style in line_styles:\n    y = np.random.randn(10)\n    plt.plot(x, y, line_style)\n\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, 'd')\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'd', linewidth=2, markersize=10, marker='d')\nplt.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "highlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nplt.plot(x, highlight_range, highlight_color)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = x * 2 + 2\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line from (0,0) to (1,2)')\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1, 2])\ny = np.array([0, 2, 4])\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    style=\"Gender\",\n    height=5,\n    aspect=1,\n)\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('darkgrid')\nsns.set_", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "sns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', fontsize=20)\nplt.title('cos(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"k\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), x, \"r-\", lw=5, markersize=30, alpha=0.5)\nplt.plot(range(10), y, \"r-\", lw=5, markersize=30, alpha=0.5)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "x_rotated = np.rot90(x, 1, 1)\ny_rotated = np.cos(x_rotated)\nplt.plot(x_rotated, y_rotated, label=\"cos\")\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "x_rotated = np.rot90(x, 1, 1)\ny_rotated = np.cos(x_rotated)\nplt.plot(x_rotated, y_rotated, label=\"cos\")\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.linspace(0, 2 * np.pi, 11)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "xlabel = \"X\"\nplt.xlabel(xlabel)\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticks(g.get_xticks() + 90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# fit a very long title myTitle into multiple lines\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = np.sort(y)[::-1]\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.arange(0, 1.5, 0.1)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "y_ticks = np.linspace(-1, 1, 11)\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "x_y_z = np.array([x, y, z]).T\n\n# plot x, then y then z\nplt.figure(figsize=(10, 5))\nplt.plot(x_y_z[0], x_y_z[1], label='x')\nplt.plot(x_y_z[0], x_y_z[2], label='y')\nplt.plot(x_y_z[1], x_y_z[2], label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "x = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.yticks(np.arange(0, 700000000, 100000000))\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "# Create a dataframe with the x and y values\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create a dashed line on the lineplot\nsns.lineplot(data=df, x='x', y='y', linestyle='-', linewidth=1, color='black')\n\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\nax[0].plot(x, y1, label='sin(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y1')\nax[0].set_title('sin(x)')\nax[0].legend()\n\nax[1].plot(x, y2, label='cos(x)')\nax[1].set_xlabel('x')\nax[1].set_ylabel('y2')\nax[1].set_title('cos(x)')\nax[1].legend()\n\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\nax[0].plot(x, y1, label='sin(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y1')\nax[0].set_title('sin(x)')\nax[0].legend()\n\nax[1].plot(x, y2, label='cos(x)')\nax[1].set_xlabel('x')\nax[1].set_ylabel('y2')\nax[1].set_title('cos(x)')\nax[1].legend()\n\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel(\"\")\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4, 1))\nplt.grid(axis='x', which='major', linestyle='-', linewidth=1, color='black')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.grid(True, which='y', axis='x', linestyle='-', linewidth=0.5, color='black')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.xticks(np.arange(1, 2, 0.1))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "# axes = axes.flatten()\n# for ax in axes:\n#     ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n#     ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n# plt.tight_layout()\n# plt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlabel(\"Column Labels\")\nax.set_ylabel(\"Row Labels\")\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Y')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y over X')\nplt.legend()\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().tick_params(axis='x', which='both', direction='out')\nplt.gca().tick_params(axis='y', which='both', direction='out')\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y, label='Y')\nplt.ylabel('Y')\nplt.xlabel('X')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "# Create a joint regression plot\nsns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', color='green')\n\n# Create a distribution plot\nsns.distplot(tips['total_bill'], kde=False, color='blue')\n\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "# Create a new dataframe with the regression line\nregression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the joint regression line and histogram\njoint_regression = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                'tip': np.linspace(tips['tip'].", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "# Create a joint regression plot\nsns.jointplot(data=tips, x='total_bill', y='tip', kind='reg')\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 5))\nfor celltype in df[\"celltype\"].unique():\n    data = df[df[\"celltype\"] == celltype]\n    plt.bar(data[\"s1\"], data[\"s2\"], label=celltype)\n\nplt.xlabel(\"s1\")\nplt.ylabel(\"s2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 5))\nfor celltype, s1, s2 in zip(df[\"celltype\"], df[\"s1\"], df[\"s2\"]):\n    plt.bar(celltype, s1, label=f\"s1 = {s1}\")\n    plt.bar(celltype, s2, label=f\"s2 = {s2}\")\n\nplt.xlabel(\"Celltype\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Y over X\")\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\nx = [0.22058956, 0.33088437, 2.20589566]\ny = [0, 0, 0]\nplt.plot(x, y, '--')\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, aspect='auto', cmap='hot', interpolation='nearest')\nplt.xlabel(xlabels, rotation=90)\nplt.ylabel(ylabels, rotation=0)\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\n# copy the code of the above plot and edit it to have legend for all three cruves in the two subplots\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\n\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "# make a scatter plot of a over b\nplt.scatter(a, b, s=100)\n\n# annotate each data point with correspond numbers in c\nfor i, (x, y) in enumerate(zip(a, b)):\n    plt.annotate(c[i], (x, y), text=str(c[i]))\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label=\"y over x\")\nplt.legend(labelcolor=\"blue\", title=\"Legend\")\nplt.legend(title=\"Legend\", title_fontsize=14, title_fontweight=\"bold\")\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, bins=10, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, ax1 = plt.subplots(3, 1, figsize=(10, 10))\n\n# Make the first subplot.\nax1.plot(x, y, 'r')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\n\n# Make the second subplot.\nax2 = ax1.twin()\nax2.plot(x, y, 'g')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\n\n# Make the third subplot.\nax3 = ax1.twin()\nax3.plot(x, y, 'b')\nax3.set_xlabel('x')\nax3.set_ylabel('y')\n\n# Make the first subplot three times wider than the second subplot but they should have the same height.\nfig.set_size_inches(10, 10)\n\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Set the labels of the x and y axis\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Set the title of the chart\nplt.title('Histogram of x and y')\n\n# Set the legend of the chart\nplt.legend()\n\n# Show the chart\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 5))\nplt.hist([x, y], bins=10, alpha=0.5, label=['x', 'y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "x = np.linspace(0, 5, 100)\ny = a * x + b\n\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n# create two colormaps\ncmap_x = plt.get_cmap('viridis')\ncmap_y = plt.get_cmap('viridis')\n\n# create two subplots\nax[0], ax[1] = plt.subplots(1, 2, figsize=(10, 10))\n\n# plot x and y on the first subplot\nax[0].imshow(x, cmap=cmap_x)\nax[0].set_title('x')\n\n# plot x and y on the second subplot\nax[1].imshow(y, cmap=cmap_y)\nax[1].set_title('y')\n\n# create a colorbar for the first subplot\ncbar_x = fig.colorbar(ax[0].imshow(x, cmap=cmap_x), ax=ax[0])\ncbar_x.set_label('x')\n\n# create a colorbar for the second subplot\ncbar_y = fig.colorbar(ax[1].imshow(y, cmap=cmap_y), ax=ax[1])\ncbar_y.set_label('y')\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(a, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].set_title('Y and Z')\naxs[0].legend()\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "# Create a DataFrame from points\ndf = pd.DataFrame(points, columns=['x', 'y'])\n\n# Create a log-scale y-axis\ny_log = np.log10(df['y'])\n\n# Create a line plot\nplt.plot(df['x'], y_log, '-')\n\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'r')\nplt.title('y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "x_log = np.log10(x)\ny_log = np.log10(y)\n\nplt.loglog(x_log, y_log, 'o')\nplt.xlabel('log(x)')\nplt.ylabel('log(y)')\nplt.title('Log-log plot of y over x')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\nplt.plot(df.index, df['A'], label='A')\nplt.plot(df.index, df['B'], label='B')\nplt.plot(df.index, df['C'], label='C')\nplt.plot(df.index, df['D'], label='D')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "# Make a histogram of data\nplt.hist(data, bins=10, edgecolor='black')\n\n# Calculate the total sum of data\ntotal_sum = np.sum(data)\n\n# Calculate the percentage of each data point\npercentage = data / total_sum\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(percentage * 100, 1)\n\n# Set y tick labels as 10%, 20%, etc.\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as 10%, 20%, etc.\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100) / 100\n\n# Set y tick labels as percentage\ny_tick_labels = np.round(y_tick_labels * 100)", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, alpha=0.5, linestyle='-')\nplt.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\nfig.legend(figlegend)\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 10))\n\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\n\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n# Set the title of the first subplot\naxs[0].set_title(\"Bill Depth vs Bill Length\")\n\n# Set the title of the second subplot\naxs[1].set_title(\"Flipper Length vs Bill Length\")\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "x_extra = np.arange(2.1, 7.6, 0.1)\nplt.xticks(x_extra)\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.yticks(np.arange(0, 10, 1))\nplt.xticks(np.arange(2010, 2020, 1))\nplt.plot(x, y)\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(x, alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, 'r')\naxs[0].set_title('First Subplot')\naxs[1].plot(x, y, 'g')\naxs[1].set_title('Second Subplot')\n\nfig.suptitle('Figure')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df[\"Type A\"], df[\"Type B\"], label=\"X\")\nplt.xlabel(\"Type A\")\nplt.ylabel(\"Type B\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='|')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none')\nplt.plot(x, y, '|', color='k')\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*', hatch='v')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "xlim = (0, 5)\nylim = (1, 4)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(data, extent=(xlim[0], xlim[1], ylim[0], ylim[1]))\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.figure(figsize=(10, 5))\nplt.plot([3], [0], 'k-', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 5))\nplt.bar(labels, height, width=0.5, color='blue')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedge_width=0.4)\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b-')\nplt.grid(True, linestyle='-', linewidth=0.5, color='0.6')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(axis='y', which='minor', color='gray', linestyle='-', linewidth=0.5)\nplt.grid(axis='x', which='minor', color='gray', linestyle='-', linewidth=0.5)\nplt.grid(axis='both', which='major', color='gray', linestyle='-', linewidth=0.5)\nplt.grid(axis='both', which='minor', color='gray', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, '--', alpha=0.5, edgecolor='black')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Create a figure and a set of axes\nfig, ax = plt.subplots()\n\n# Create a bar plot for the blue bars\nax.bar(np.arange(3), blue_bar, color='b')\n\n# Create a bar plot for the orange bars\nax.bar(np.arange(3) + 0.5, orange_bar, color='orange')\n\n# Set the x-axis labels\nax.set_xlabel('Index')\n# Set the y-axis labels\nax.set_ylabel('Height')\n\n# Set the title of the plot\nax.set_title('Bar plot')\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[0].legend()\n\naxs[1].plot(a, z, label='z')\naxs[1].set_xlabel('a')\naxs[1].set_ylabel('z')\naxs[1].legend()\n\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y over x')\nplt.grid(True)\nplt.tick_params(axis='x', which='both', tick_interval=1)\nplt.tick_params(axis='y', which='both', tick_interval=1)\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(data=df, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"species\", sharey=False, col_wrap=3, height=5, aspect=0.8, kind=\"bar\")\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(10, 10))\nplt.plot([0.5], [0.5], 'ro')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5,", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.3)\nplt.show()\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.plot(x, y, marker=\"*\", label=\"Line\")\nplt.legend(loc='best')\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, cmap='hot')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(1)\nplt.plot(x, y)\nplt.title(\"Figure 1\")\nplt.bold(\"Figure\")\nplt.bold(\"1\")\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.plot(x[::-1], y[::-1])\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.show()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n\n# Create a scatter plot\nplt.scatter(x, y, s=100)\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, color='red', edgecolor='black')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax, x_val, y_val in zip(axs.flat, x, y):\n    ax.plot(x_val, y_val)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('y over x')\n\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('x')\nplt.ylabel('frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 5))\nplt.errorbar(x, y, yerr=error, fmt='o', color='blue', label='y')\nplt.fill_between(x, y-error, y+error, alpha=0.2, color='blue', label='y error')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 0], 'k-', linewidth=2)\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True)\nax1.plot(x, y, label='Y')\nax1.set_title('Y')\nax2.plot(z, a, label='Z')\nax2.set_title('Z')\nax2.set_ylabel('A')\nax2.set_xlabel('Z')\nax2.set_yticks(np.arange(10))\nax2.set_xlim([0, 10])\nax2.set_ylim([0, 10])\nax2.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xlabel('x')\n        axs[i, j].set_ylabel('y')\n        axs[i, j].set_title(f'Subplot {i},{j}')\n\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1, figsize=(8, 8))\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nplt.table(\n    df,\n    collabels=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    rowlabels=[\"penguin1\", \"penguin2\", \"penguin3\", \"penguin4\", \"penguin5\", \"penguin6\", \"penguin7\", \"penguin8\", \"penguin9\", \"penguin10\"],\n    celltext=[\n        [\"penguin1\", \"penguin2\", \"penguin3\", \"penguin4\"],\n        [\"penguin5\", \"penguin6\", \"penguin7\", \"penguin8\"],\n        [\"penguin9\", \"penguin10\", \"penguin11\", \"penguin12\"],\n        [\"penguin13\", \"penguin14\", \"penguin15\", \"penguin16\"],\n        [\"penguin17\", \"penguin18\", \"penguin19\", \"penguin20\"],\n        [\"penguin21\", \"penguin22\", \"penguin23\", \"penguin24\"],\n        [\"penguin25\", \"penguin26\", \"penguin27\", \"penguin28\"],\n        [\"penguin29\", \"penguin30\", \"penguin31\", \"penguin32\"],\n        [\"penguin33\", \"penguin34\", \"penguin35\", \"penguin36\"],\n        [\"penguin37\", \"penguin38\", \"penguin39\", \"penguin40\"],\n    ],\n    cellcolours=[\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"white\",\n        \"", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"diet\"] == \"No Fat\"]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"kind\"] == \"Fat\"]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] < 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] < 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] > 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] > 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] < 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] > 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] < 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] > 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] < 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] > 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] < 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] > 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] < 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] > 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] < 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] > 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"time\"] < 10]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] > 100]\n\n# Create a new dataframe with the columns \"time\", \"pulse\", \"kind\", and \"diet\"\nnew_df = new_df[new_df[\"pulse\"] < 100]\n\n# Create a new dataframe with the", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"pulse\"\nnew_df = df[[\"time\", \"pulse\"]]\n\n# Create a new", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "# Create a scatter plot of the data\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Plot of y over x')\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(borderless=True)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 6))\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(data=df, x=\"sex\", y=\"bill_length_mm\", hue=\"species\")\nstripplot.set_axis_labels(\"sex\", \"bill_length_mm\")\nstripplot.set_axis_ticks(np.arange(0, 10, 1))\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth(1)\nstripplot.set_axis_linewidth", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_names=[\"a\"], col_names=[\"b\"], share_all=True)\n\n# Create a pointplot of \"c\" over \"a\"\nfacegrid.plot(sns.pointplot, x=\"a\", y=\"c\", hue=\"b\", sharex=False, sharey=False,\n              err_style=\"bars\", err_width=0.5, err_color=\"black\")\n\n# Set xticks of intervals of 1\nfacegrid.ax.set_xticks(np.arange(1, 31, 2))\n\n# Set xtick labels with intervals of 2\nfacegrid.ax.set_xticklabels(np.arange(1, 31, 2))\n\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_zlim([0, 1])\nax.set_view_angle(30, 50)\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tick_params(axis='x', which='both', labelbottom=False, labeltop=False, labelleft=False, labelright=False, bottom=False, top=False, labelbottom=False)\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_per_axis=0.01, height_per_axis=0.01)\n\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.imshow(x[i])\n    ax.axis('off')\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nprint(x.numpy()) # prints 1\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nx = tf.cast(x, tf.int32)\nprint(x.numpy())\n", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    # Create a 2D tensor with the same shape as the input labels\n    result = tf.zeros([len(example_labels), 10])\n\n    # Iterate over the labels and set the corresponding value in the result tensor\n    for i, label in enumerate(example_labels):\n        result[i, label] = 1\n\n    # Return the result tensor\n    return result\n\n# Call the function\nprint(f())\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.compat.v1.py_func(\n    func=lambda i: [i, i+1, i+2],\n    inp=[i],\n    Tout=[tf.int64]\n  )\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=my_map_func)\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\n\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return tf.concat([input, input + 1, input + 2], axis=0)\n\ndef map_func(input):\n    return tf.reshape(f(input), [-1])\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(map_func)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\n\nmask = tf.cast(lengths_padded > 0, tf.int32)\n\nmask = tf.reshape(mask, (4, 8))\n\nprint(mask)\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.where(lengths == 0, 1, 0)\n\n# Create a mask of 1s and 0s where the number of 0s corresponds to the length of the lengths tensor\nmask = tf.", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "lengths = tf.constant([4, 3, 5, 2])\nmask = tf.sequence_length_mask(lengths, 8)\nmask = tf.cast(mask, tf.float32)\nmask = tf.reshape(mask, (4, 8))\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.reshape(result, (4, 8))\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths),", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\nmask = tf.cast(lengths_padded > 0, tf.float32)\nmask = tf.reshape(mask, (4, 8))\nresult = tf.where(mask == 0, tf.ones_like(mask), mask)\nprint(result)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([a, b], axis=1)\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.reshape(result, [2, -1])\nresult = tf.transpose(result)\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    # Use tf.stack to create a 3D tensor with shape (a.shape[0], a.shape[0], b.shape[0])\n    # Then use tf.transpose to transpose the 3D tensor\n    # Finally use tf.reshape to reshape the 3D tensor to 2D tensor\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    ### END SOLUTION ###\n\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.stack([a,a,a],axis=1),perm=[1,0,2]),[a.shape[0],a.shape[0]*b.shape[0]])\n    # return tf.reshape(tf.transpose(tf.", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "a = tf.reshape(a, (50, 100, 512))\nresult = a", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "a = tf.reshape(a, (50, 100, 1, 512))\nresult = a", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "a = tf.reshape(a, (1, 50, 100, 1, 512))\nresult = a", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "# calculate the square difference\ndiff = tf.square(a - b)\n# reduce the square difference by row\nresult = tf.reduce_sum(diff, axis=1)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "# calculate the square difference\ndiff = tf.square(a - b)\n# reduce the square difference by column\nresult = tf.reduce_sum(diff, axis=0)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    # calculate the square difference between A and B\n    square_diff = tf.square(A - B)\n    # reduce the square difference by row\n    result = tf.reduce_sum(square_diff, axis=1)\n    # return the result\n    return result\n\nprint(f(A=example_a,B=example_b))\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = x[y,z]\nresult = tf.gather(x, y, axis=1)\nresult = tf.gather(result, z, axis=0)\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = x[row, col]\nresult = tf.gather(x, col, axis=1)\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    return tf.gather_nd(x, tf.stack([y, z], axis=1))\n    ### END SOLUTION ###\n\nresult = f(example_x, example_y, example_z)\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "result = tf.constant(np.dot(A, B))\nresult = tf.reshape(result, (A.shape[0], A.shape[0], A.shape[1]))\nprint(result)", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "result = tf.constant(np.dot(A, B))\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "x = tf.constant(x)\nx = tf.strings.unicode_transpose(x)\nx = tf.strings.unicode_transpose(x)\nx = tf.strings.unicode_transpose(x)\nx = tf.strings.unicode_transpose(x)\nx = tf.strings.unicode_transpose(x)\nresult = tf.strings.unicode_transpose(x)\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    # Convert the byte list to string list\n    result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result = tf.strings.unicode_transpose(x)\n    # # Convert the string list to string\n    # result =", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "x_nonzero = tf.where(x != 0, x, tf.zeros_like(x))\nx_nonzero_sum = tf.reduce_sum(x_nonzero, axis=-1, keepdims=True)\nresult = tf.where(x_nonzero_sum != 0, x_nonzero / x_nonzero_sum, tf.zeros_like(x))\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "x = tf.where(x != 0, x, tf.zeros_like(x))\nx = tf.reduce_mean(x, axis=-1)\nresult = tf.where(x != 0, x, tf.zeros_like(x))\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": " ###\n    # get the number of non-zero entries in the last dimension\n    num_non_zero = tf.cast(tf.count_nonzero(x, axis=-1, dtype=tf.int32), tf.float32)\n    # compute the average of the last dimension\n    average = tf.reduce_sum(x, axis=-1) / num_non_zero\n    # return the result\n    return average\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\n# Create two random 2D tensors\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Calculate the sum of the matrix multiplication\nresult = tf.reduce_sum(tf.matmul(A, B))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "# Find the index of the maximum value in each row\nmax_index = tf.argmax(a, axis=1)\n\n# Convert the index to the same shape as y\ny = tf.constant([5, 4, 0])\n\n# Compare the maximum index with y and count the same\nsame = tf.equal(max_index, y)\n\n# Calculate the accuracy\naccuracy = tf.reduce_mean(tf.cast(same, tf.float32))\n\n# Print the accuracy\nprint(accuracy)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "# Convert the scores tensor to the index of the maximum value in each column\nmax_indices = tf.argmax(a, axis=1)\n\n# Convert the maximum index to the index of the highest value in each column\nresult = tf.where(a == tf.transpose(max_indices), max_indices, tf.transpose(max_indices))\n\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    # find the maximum value in each row\n    max_values = tf.argmax(a, axis=1)\n    # count the number of times the maximum value in each row is equal to the actual value\n    count = tf.reduce_sum(tf.cast(max_values == example_a, tf.int32))\n    # return the count\n    return count\n\nprint(f(example_a))\n### END SOLUTION ###\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1) + 1", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n\n# Save the model\nmodel.save('my_model')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "# generate 10 random integers from the uniform distribution\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "seed_x = 10\ntf.random.set_seed(seed_x)\n\n# generate 114 random integers from the uniform distribution\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=5, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": " ###\n    tf.random.set_seed(seed_x)\n    random_integers = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n    return random_integers\n\n# call the function\nprint(f())\n### END SOLUTION ###\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "import tensorflow as tf\n\n# get the version of tensorflow\nversion = tf.__version__\nprint(version)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "# Fit the data using a logarithmic polynomial\nx_log = np.log(x)\ny_log = np.log(y)\n\n# Fit the data using a polynomial of order 1\nx_poly = np.polyfit(x_log, y_log, 1)\n\n# Convert the coefficients back to the original form\nA_log = np.exp(x_poly[0])\nB_log = x_poly[1]\n\n# Fit the data using a polynomial of order 2\nx_poly = np.polyfit(x_log, y_log, 2)\n\n# Convert the coefficients back to the original form\nA_poly = x_poly[0]\nB_poly = x_poly[1]\n\n# Fit the data using a polynomial of order 3\nx_poly = np.polyfit(x_log, y_log, 3)\n\n# Convert the coefficients back to the original form\nA_poly3 = x_poly[0]\nB_poly3 = x_poly[1]\n\n# Store the results in a numpy array\nresult = np.array([A_log, B_log, A_poly, B_poly, A_poly3, B_poly3])\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the data using a logarithmic polynomial\nA, B = scipy.optimize.polyfit(x, y, 1, method='log-linear')\nresult = np.array([A, B])\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, A, B, C):\n    return A * np.exp(B * x) + C\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array([result[0], result[1], result[2]])\nprint(result)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.kstest(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + (3*a - b - c)**2 + sin(b) + cos(b) + 4)\n\ndef grad_f(x):\n    a, b, c = x\n    return [2*(a + b - c) - 2, 2*(3*a - b - c) - 2, sin(b) + cos(b) + 4]\n\ndef hess_f(x):\n    a, b, c = x\n    return [[-2, -2, -2], [-2, -2, -2], [-1, -1, -1]]\n\nresult = optimize.minimize(f, initial_guess, jac=grad_f, hess=hess_f)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.t.pdf(z_scores, 0, 1)", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores, scipy.stats.t.pdf(z_scores,", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = scipy.stats.zscore(p_values)\nprint(z_scores)", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)\nprint(result)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(total,mu,stddev)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    return sA * sB\n    ### END SOLUTION ###\n\nresult = f(example_sA, example_sB)\nprint(result)\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a LinearNDInterpolator from the points and values\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\n\n# Get the interpolated value for the request point\nresult = interpolator(request)\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid of points\nx, y, z = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of V values\nv = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of points to interpolate\nx_interp, y_interp, z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request points\nrequest_x, request_y, request_z = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request V values\nrequest_v = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_x_interp, request_y_interp, request_z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request V values\nrequest_v_interp = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_x_interp, request_y_interp, request_z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request V values\nrequest_v_interp = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_x_interp, request_y_interp, request_z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request V values\nrequest_v_interp = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_x_interp, request_y_interp, request_z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request V values\nrequest_v_interp = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_x_interp, request_y_interp, request_z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request V values\nrequest_v_interp = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_x_interp, request_y_interp, request_z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request V values\nrequest_v_interp = np.random.rand(100, 100, 100)\n\n# Create a 3D grid of request points\nrequest_x_interp, request_y_interp, request_z_interp = np.mgrid[0:30:100j, 0:30:100j, 0:30:100j]\n\n# Create a 3D grid of request V values\nrequest_v_interp = np.random.rand", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, reshape=False)\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\nxrot, yrot = np.round(xrot), np.round(yrot)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = np.diagonal(M.toarray())", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # Use scipy's kstest function to perform the K-S test\n    # The result should be a tuple of two elements: the K-S test statistic and the p-value\n    # The K-S test statistic is the value of the K-S test statistic, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing the data if the null hypothesis is true\n    # The K-S test statistic is the mean of the times, and the p-value is the probability of observing", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = c1.concatenate(c2)\nprint(Feature)\n", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "# calculate the distance between points\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# assign points2 to points1\nresult = points2[min_indices]\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "# calculate the distance between points\ndistances = scipy.spatial.distance.cdist(points1, points2, 'euclidean')\n# find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# assign points2 to points1\nresult = points2[min_indices]\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b = b.toarray()\nb[0, 0] = 0\nb = sparse.csr_matrix(b)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "region_count = np.sum(img > threshold)\nprint(region_count)\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "region_count = np.sum(img < threshold)\nprint(region_count)\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use numpy's where function to find the values greater than the threshold\n    # Then use numpy's unique function to find the unique values and their counts\n    # Finally, return the unique values and their counts\n    ### END SOLUTION ###\n    return np.unique(np.where(img > threshold)[0], return_counts=True)\n\nprint(f(example_img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the center of mass of such regions\ncenter_of_mass = np.mean(np.where(mask), axis=1)\n\n# Find the distance between the center of mass of such regions and the top left corner\ndistance = np.sqrt((center_of_mass[0]-0)**2 + (center_of_mass[1]-0)**2)\n\n# Output the distances as a list\nresult = distance.tolist()\nprint(result)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = lil_matrix(M)\nM = M + M.T\nM = M.tocsr()\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    # make the matrix symmetric\n    sA = lil_matrix(sA)\n    # make the matrix symmetric\n    sA = sA.transpose()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    # make the matrix symmetric\n    sA = sA.tocsc()\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    #", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "# erode the square by 1 pixel\neroded = scipy.ndimage.erosion(square, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# dilate the eroded square by 1 pixel\ndilated = scipy.ndimage.dilation(eroded, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# remove the isolated single cells\nsquare[dilated == 0] = 0\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask, 1)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Create a mask of the square array\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask,", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_val = np.max(col.toarray())\nmin_val = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "# Convert the sparse matrix to a dense array\ndarr = np.array(sA.toarray())\n\n# Get the median and mode of the dense array\nmedian = np.median(darr)\nmode = np.mode(darr)\n\n# Print the median and mode\nprint(\"Median: \", median)\nprint(\"Mode: \", mode)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *a):\n    return sum(a[i]*np.cos(np.pi*x/(2*tau)) for i in range(degree))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\narray_1d = example_array.flatten()\n# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.cdist(array_1d, array_1d, metric='euclidean')\n# Convert distances tometres\ndistances_metres = distances * np.mean(example_array)\n# Convert distances to a 2D array\nresult = np.stack((array_1d, distances_metres), axis=1)\n# Print result\nprint(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Calculate pairwise Manhattan distances between all regions\ndistances = scipy.spatial.distance.cdist(np.arange(example_array.shape[0]), np.arange(example_array.shape[1]), metric='cityblock')\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distance = np.min(distances, axis=1)\n# Convert the minimum distance to a 2D array with the from ID, to ID, distance format\nresult = np.stack((np.arange(example_array.shape[0]), np.arange(example_array.shape[1]), min_distance), axis=1)\nprint(result)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate pairwise Euclidean distances between all regions\n    # to obtain the minimum distance separating the nearest edges of each raster patch\n    # Convert distances from cells tometres\n    # result = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n    #                   [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n    #                   [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    #                   [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n    #                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n    # # Convert distances from cells tometres\n    # result = np.multiply(result, 1000)\n    # # Return the result\n    # return result\n    ### END SOLUTION ###\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(x.shape[1]):\n    tck = interpolate.splrep(x[:, i], y[:, i], s=4)\n    result[i] = interpolate.splev(x_val, tck)\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array([38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0])\nx2 = np.array([39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8])\nx3 = np.array([34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0])\nx4 = np.array([34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8])\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\nprint(statistic)\nprint(critical_values)\nprint(significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\n\n# Compare the two datasets\nresult = ss.anderson_ksamp(x1, x2)\n\n# Interpret the result\nif result.statistic < 0.05:\n    print(\"The two datasets are drawn from the same population.\")\nelse:\n    print(\"The two datasets are not drawn from the same population.\")", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(df['A']) # keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\ndf['AC'] = pd.rolling_apply(df['C'], 3, lambda x: tau1(x))\ndf['BC'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\n\nprint(df)\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nresult = is_csr_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n\nresult = is_lil_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores)[1]", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    # Use the ranksums function from scipy to calculate the pvalue\n    p_value = stats.ranksums(pre_course_scores, during_course_scores)\n    # END SOLUTION ###\n    return p_value\n\nprint(f())\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "kurtosis_result = np.mean(a**3) / np.mean(a**3)**2\nprint(kurtosis_result)", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=True)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "def interpolated(s, t):\n    z = (s + t)*np.exp(-6.0 * (s * s + t * t))\n    return z\n\nresult = interpolated(s, t)\nprint(result)\n", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a 2D array with the values of s and t\n    s_t = np.array([s, t]).reshape(-1, 2)\n    # Create a 2D array with the values of z\n    z_s_t = np.array([z, z]).reshape(-1, 2)\n    # Create a 2D array with the values of x and y\n    x_y = np.array([x, y]).reshape(-1, 2)\n    # Create a 2D array with the values of x and y\n    x_y_z = np.array([x, y, z]).reshape(-1, 3)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t = np.array([x, y, z, s, t]).reshape(-1, 5)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t = np.array([x, y, z, s, t, s, t]).reshape(-1, 7)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t_s_t = np.array([x, y, z, s, t, s, t, s, t]).reshape(-1, 9)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t_s_t_s_t = np.array([x, y, z, s, t, s, t, s, t, s, t]).reshape(-1, 11)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t_s_t_s_t_s_t = np.array([x, y, z, s, t, s, t, s, t, s, t, s, t]).reshape(-1, 13)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t_s_t_s_t_s_t_s_t = np.array([x, y, z, s, t, s, t, s, t, s, t, s, t, s, t]).reshape(-1, 15)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t_s_t_s_t_s_t_s_t_s_t = np.array([x, y, z, s, t, s, t, s, t, s, t, s, t, s, t, s, t]).reshape(-1, 17)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t_s_t_s_t_s_t_s_t_s_t_s_t = np.array([x, y, z, s, t, s, t, s, t, s, t, s, t, s, t, s, t, s, t]).reshape(-1, 19)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t_s_t_s_t_s_t_s_t_s_t_s_t_s_t = np.array([x, y, z, s, t, s, t, s, t, s, t, s, t, s, t, s, t, s, t, s, t]).reshape(-1, 21)\n    # Create a 2D array with the values of x and y\n    x_y_z_s_t_s_t_s_t_s_t_s_t_s_t_s_t_s_t_s_t_s_t = np.array([x, y, z, s, t, s, t, s, t, s, t, s, t, s, t, s, t, s, t, s, t", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n# Create a list of points that are in the voronoi tessallation\nvorPoints = [[1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n# Create a list of points that are not in the voronoi tessallation\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "# Create a list of points that are not in the voronoi cell\nextraPointsNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions:\n        extraPointsNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell\nextraPointsInCell = []\nfor point in extraPoints:\n    if point in vor.regions:\n        extraPointsInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellNotInCell = []\nfor point in extraPoints:\n    if point in vor.regions and point not in vor.vertices:\n        extraPointsInCellNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellAndNotInCell = []\nfor point in extraPoints:\n    if point in vor.regions and point in vor.vertices:\n        extraPointsInCellAndNotInCell.append(point)\n\n# Create a list of points that are not in the voronoi cell\nextraPointsNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions:\n        extraPointsNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell\nextraPointsInCell = []\nfor point in extraPoints:\n    if point in vor.regions:\n        extraPointsInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellNotInCell = []\nfor point in extraPoints:\n    if point in vor.regions and point not in vor.vertices:\n        extraPointsInCellNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellAndNotInCell = []\nfor point in extraPoints:\n    if point in vor.regions and point in vor.vertices:\n        extraPointsInCellAndNotInCell.append(point)\n\n# Create a list of points that are not in the voronoi cell\nextraPointsNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions:\n        extraPointsNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell\nextraPointsInCell = []\nfor point in extraPoints:\n    if point in vor.regions:\n        extraPointsInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellNotInCell = []\nfor point in extraPoints:\n    if point in vor.regions and point not in vor.vertices:\n        extraPointsInCellNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellAndNotInCell = []\nfor point in extraPoints:\n    if point in vor.regions and point in vor.vertices:\n        extraPointsInCellAndNotInCell.append(point)\n\n# Create a list of points that are not in the voronoi cell\nextraPointsNotInCell = []\nfor point in extraPoints:\n    if point not in vor.regions:\n        extraPointsNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell\nextraPointsInCell = []\nfor point in extraPoints:\n    if point in vor.regions:\n        extraPointsInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellNotInCell = []\nfor point in extraPoints:\n    if point in vor.regions and point not in vor.vertices:\n        extraPointsInCellNotInCell.append(point)\n\n# Create a list of points that are in the voronoi cell and not in the voronoi cell\nextraPointsInCellAndNotInCell = []\nfor point in extraPoints:", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "# Find the maximum size of the vectors\nmax_size = max([len(v) for v in vectors])\n\n# Create a sparse matrix with zeros\nresult = sparse.csc_matrix((max_size, max_size), dtype=np.float64)\n\n# Fill the sparse matrix with the vectors\nfor i, v in enumerate(vectors):\n    result[i, :len(v)] = v\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scipy.ndimage.median_filter(b, size=(3, 3), mode='constant')\nb = scip", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = np.array([M[row[i], column[i]] for i in range(len(row))])", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, :, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u,o2,x)\nprint(prob)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(x = 2.5, u = 1, o2 = 3))\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a 2D DCT matrix\ndct_matrix = np.zeros((N, N))\nfor i in range(N):\n    for j in range(N):\n        dct_matrix[i, j] = sf.dctn(np.ones(N), 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "result = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result)\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "result = scipy.stats.binom.pmf(np.arange(N+1), N, p)", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x))\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=1)\n\n# create a new dataframe with zscore and data\nresult = pd.DataFrame(data=np.c_[df.values, zscore], index=df.index, columns=np.append(df.columns, 'zscore'))\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=0)\n# round zscore to 3 decimal places\nzscore = np.round(zscore, 3)\n# create a new dataframe with zscore and data\nresult = pd.DataFrame(np.c_[df.values, zscore], columns=np.concatenate([df.columns, ['zscore']]))\nresult", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y, x = np.mgrid[0:6, 0:6]\nmid = np.array([[0, 0], [6, 6], [6, 6]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\nmid = np.array([[5., 4., 3., 3., 4., 5.],\n               [4., 3., 2., 2., 3., 4.],\n               [3., 2., 1., 1., 2., 3.],\n               [3., 2., 1., 1., 2., 3.],\n               [4., 3., 2., 2., 3., 4.],\n               [5., 4., 3., 3., 4., 5.]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.mgrid[0:shape[0], 0:shape[1]]\n    center = np.array([shape[0] / 2, shape[1] / 2])\n    return distance.cdist(np.dstack((y, x)), center)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\n\nx0 = np.array([2, 3, 1, 4, 20])\nout = scipy.optimize.minimize(residual, x0, args=(a, y))\nprint(out.x)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return (y - func(x)) ** 2\n\ndef main():\n    out = scipy.optimize.minimize(residual, x0, bounds=[x_lower_bounds, x_lower_bounds + x_true])\n    print(out.x)\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t-np.sin(t)) if 0 < t < 2*np.pi else 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "def fn(x, c):\n    return 2*x*c\n\nresult = scipy.integrate.quad(lambda x: fn(x, c), low, high)\nprint(result)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    def integrand(x):\n        return 2*c*x\n    result, error = scipy.integrate.quad(integrand, low, high)\n    return result\n\n# call the function\nresult = f(c=5, low=0, high=1)\nprint(result)\n<code>\n\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\nprint(result)\n# output the result\n", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.to_dense()\nV += x", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "x = x.reshape(1, 1)\nV = V + x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "x = sparse.dok_matrix(V.shape, dtype=V.dtype)\nx.data[V.data != 0] = x.data[V.data != 0] + x.data[V.data != 0] * x.data[V.data != 0] * x.data[V.data != 0]\nx.data[V.data == 0] = 0\nx = x.toarray()\nx = x + y\nx = x.toarray()\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    column /= length\n    # update the original column of the matrix\n    sa[:, col] = column\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    column /= length\n    # update the original column of the matrix\n    sa[:, col] = column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = np.where(a > 0, 1, 0)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = np.where(a == 0, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest centroid for each cluster\nresult = np.argmin(distances, axis=1)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest centroid for each data point\nclosest_centroid_index = np.argmin(distances, axis=1)\n# extract the closest centroid\nresult = centroids[closest_centroid_index]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# calculate the distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the k-th closest element to centroid of cluster 0\nclosest_elements = np.argpartition(distances, k, axis=0)[:k, :]\n# find the index of the k-th closest element in original data for each cluster\nresult = np.argwhere(closest_elements == np.argmin(distances, axis=0))\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result = fsolve(lambda x: eqn(x, a, b), x0=x)\n    print(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result = fsolve(lambda x, a: eqn(x, a, x), x0=x, args=(a,))\n    print(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Define the function to calculate the K-S test\ndef kstest(x, y, a, m, d):\n    return stats.kstest(y, bekkers_func(x, a, m, d))\n\n# Fit the function to the data\na, m, d = sp.optimize.curve_fit(bekkers_func, range_start, range_end, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nresult = kstest(range_start, range_end, a, m, d)\nprint(result)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Define the function to calculate the K-S test\ndef ks_test(x, y, alpha=0.05):\n    _, p_value = stats.kstest(y, bekkers_func(x, estimated_a, estimated_m, estimated_d))\n    return p_value < alpha\n\n# Test the function\nx = np.linspace(range_start, range_end, 100)\ny = bekkers(x, estimated_a, estimated_m, estimated_d)\nresult = ks_test(x, y)\nprint(result)", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def rolling_integral(x):\n    return integrate.trapz(x, x)\n\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral)\nprint(integral_df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "x = np.array([(2,2), (1,2), (2,3), (3,2), (2,1)])\ny = np.array([5,7,8,10,3])\neval = np.array([(2.7, 2.3)])\n\n# Create a grid of points\nx_grid = np.linspace(x.min(), x.max(), 100)\ny_grid = np.linspace(y.min(), y.max(), 100)\n\n# Create a grid of points for the evaluation\nx_eval = np.linspace(x.min(), x.max(), 100)\ny_eval = np.linspace(y.min(), y.max(), 100)\n\n# Use griddata to find the value at the sub-pixel value\nresult = scipy.interpolate.griddata(x, y, eval, x_grid, y_grid, method='linear')\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "weights = sciopt.maximum_likelihood_solver(a.values, np.arange(0, 12), method='max')\nweights = weights.x\nweights = weights / np.sum(weights)\nweights = weights.tolist()\nweights = [i / np.sum(weights) for i in weights]\nweights = weights.tolist()\nweights = np.array(weights)\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tolist()\nweights = weights.tol", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\nprint(result)\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "# find the absolute values of the array\nabs_arr = np.abs(arr)\n# find the relative extrema\nrelative_extrema = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))\n# find the original indices of the relative extrema\noriginal_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the relative extrema\noriginal_relative_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) & np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis=0) + n))[0]\n# find the original indices of the absolute extrema\noriginal_abs_extrema_indices = np.where(np.greater_equal(abs_arr, np.mean(abs_arr, axis=0)) | np.less_equal(abs_arr, np.mean(abs_arr, axis", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "# find the absolute indices of the elements\nabs_indices = np.where(arr == np.max(arr, axis=1))\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices[0] > abs_indices[1])\n# find the absolute indices of the elements forwards and backwards in the row\nforward_indices = np.where(abs_indices[0] < abs_indices[1])\nbackward_indices = np.where(abs_indices", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "num_cols = df.columns[df.columns.str.startswith('NUM')]\noutlier_cols = num_cols[np.abs(stats.zscore(df[num_cols])) < 3].tolist()\ndf = df[outlier_cols]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data.data, data.target],\n                     columns=data.feature_names + ['target'])\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + data['target_names'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data.data, data.target],\n                     columns=data.feature_names + ['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    # Convert the data from a Bunch object to a DataFrame\n    data_df = pd.DataFrame(data=np.c_[data.data, data.target],\n                           columns=data.feature_names + ['target'])\n    return data_df\n    ### END SOLUTION ###\n\n# Test the function\nprint(solve(data))\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a list of unique elements in the column\nunique_elements = np.unique(df['Col3'])\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over each row in the dataframe\nfor i in range(df.shape[0]):\n    # Iterate over each element in the list\n    for j in range(unique_elements.shape[0]):\n        # If the element is in the list, set the value to 1\n        if unique_elements[j] in df.iloc[i, 2]:\n            df_out.iloc[i, j] = 1\n\n# Print the output dataframe\nprint(df_out)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column for the original column\ndf_out['Col3'] = df['Col3']\n# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['Col3'].apply(lambda x: x.split(',')))\n# Drop the original column\ndf_out = df_out.drop(columns='Col3')\n# Add a new column", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.DataFrame(np.zeros((len(unique_elements), len(df.columns))), columns=df.columns)\n\n# Iterate over unique elements and set the value to 1\nfor i, element in enumerate(unique_elements):\n    df_out.loc[df.iloc[:, -1] == element, i] = 1\n\n# Print the output dataframe\nprint(df_out)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# One-hot-encode the last column\nnew_df.loc[df.iloc[:, -1] == unique_elements, unique_elements] = 1\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over the last column and one-hot-encode the elements\nfor i in range(unique_elements.shape[0]):\n    new_df.iloc[:, i] = np.where(new_df.iloc[:, i] == unique_elements[i], 1, 0)\n\n# Drop the last column\nnew_df = new_df.drop(columns=[unique_elements[-1]])\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\nproba = np.max(proba, axis=1)\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\nproba = np.max(proba, axis=1)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "# Convert the transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n# Convert the transform_output back to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\ndf = pd.concat([df_origin, transform_output], axis=1)\n\n#", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "# Convert the transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a DataFrame from the transform_output\ntransform_output_df = pd.DataFrame(transform_output.toarray())\n\n# Merge the original data with the transform_output_df\nmerged_df = pd.concat([df_origin, transform_output_df], axis=1)\n\n# Print the merged DataFrame\nprint(merged_df)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # transform the transform_output back to a dataframe\n    transform_output = pd.DataFrame(transform_output, columns=transform_output.columns)\n    # merge the original dataframe and the transform_output\n    result = pd.concat([df, transform_output], axis=1)\n    ### END SOLUTION ###\n    return result\n\ndef load_data():\n    # create a dataframe\n    df = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCD'))\n    # create a sparse matrix\n    transform_output = csr_matrix(np.random.rand(10, 5))\n    return df, transform_output\n\n# call the function\nresult = solve(load_data())\nprint(result)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline\nprint(clf)", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline\nprint(clf)", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline\nprint(clf)", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a new step\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.append(new_step)\n\n# Insert a new step at a specific position\nposition = 1\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.insert(position, new_step)\n\n# Delete a step\ndel clf.steps[1]\n\n# Print the steps\nprint(clf.steps)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\n# Insert a new step\nnew_step = ('new_step', SVC())\nclf.steps.append(new_step)\n\n# Insert a new step at a specific position\nposition = 1\nnew_step = ('new_step', SVC())\nclf.steps.insert(position, new_step)\n\n# Delete a step\nposition = 0\ndel clf.steps[position]\n\n# Print the steps\nprint(clf.steps)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\n# Insert ('t1919810', PCA()) right before 'svdm'\nclf.steps.insert(1, ('t1919810', PCA()))\n\n# Print the new pipeline\nprint(clf)\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"early_stopping_rounds\": [10, 20, 30, 40],\n    \"learning_rate\": [0.01, 0.1, 0.2, 0.3],\n    \"max_depth\": [2, 3, 4, 5],\n    \"n_estimators\": [100, 200, 300, 400],\n    \"gamma\": [0.1, 0.2, 0.3, 0.4],\n    \"alpha\": [0.1, 0.2, 0.3, 0.4],\n    \"max_iter\": [100, 200, 300, 400]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the grid search\ngrid_search = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n\n# Fit the grid search\ngrid_search.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n# Get the best score\nbest_score = grid_search.best_score_\n\n# Get the best iteration\nbest_iteration = grid_search.best_iteration_\n\n# Print the best parameters\nprint(\"Best parameters: \", best_params)\n\n# Print the best score\nprint(\"Best score: \", best_score)\n\n# Print the best iteration\nprint(\"Best iteration: \", best_iteration)\n\n# Print the best model\nprint(\"Best model: \", grid_search.best_estimator_)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"learning_rate\" : [0.01, 0.1, 0.001],\n    \"max_depth\" : [3, 4, 5],\n    \"n_estimators\" : [100, 200, 300],\n    \"gamma\" : [0.1, 0.01, 0.001],\n    \"alpha\" : [0.01, 0.1, 0.001],\n    \"max_iter\" : [100, 200, 300]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n\n# Fit the grid search\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the prediction\nc = gridsearch.predict(testX)\n\n# Get the score on the validation set\nb = gridsearch.score(testX, testY)\n\n# Print the best parameters, best score and prediction\nprint(\"Best parameters: \", best_params)\nprint(\"Best score: \", best_score)\nprint(\"Prediction: \", c)\nprint(\"Score on the validation set: \", b)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\nproba = np.mean(proba, axis=0)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\nproba = np.mean(proba, axis=0)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_predicted = scaler.inverse_transform(scaled[:, 0])\n    # check score\n    score = np.mean(scaled[:, 0] - t_predicted)\n    return score\n\n# solve(data, scaler, scaled)\n<!! END SOLUTION !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = np.mean(scores)\nprint(f'Name model: {model_name}, Mean score: {mean_score}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\n# Define the GridSearchCV object\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV object\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n# Create a new DecisionTreeClassifier with the best parameters\ndt_best = DecisionTreeClassifier(max_depth=best_params['base_estimator__max_depth'], max_samples=best_params['max_samples'])\n\n# Create a new BaggingClassifier with the best parameters\nbc_best = BaggingClassifier(dt_best, n_estimators=20, max_samples=best_params['max_samples'], max_features=best_params['max_features'])\n\n# Fit the new BaggingClassifier\nbc_best.fit(X_train, y_train)\n\n# Get the probabilities\nproba = bc_best.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef solve():\n    tfidf = TfidfVectorizer(preprocessor=preprocess)\n    tfidf.fit([\"This is a test\"])\n    print(tfidf.transform([\"This is a test\"]))\n\nsolve()\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# example variable\ntext = \"This is a test.\"\n\n# apply preprocessor\ntext_transformed = tfidf.transform([text])\n\nprint(text_transformed.toarray())\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Apply preprocessing.scale to the data\nscaled_data = preprocessing.scale(data)\n\n# Create a new DataFrame from the scaled data\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Save the scaled data to a new CSV file\nscaled_data_df.to_csv('scaled_data.csv', index=False)\n\n# Load the scaled data from the new CSV file\nscaled_data_new = pd.read_csv('scaled_data.csv')\n\n# Apply preprocessing.scale to the new data\nscaled_data_new = preprocessing.scale(scaled_data_new)\n\n# Create a new DataFrame from the scaled new data\nscaled_data_new_df = pd.DataFrame(scaled_data_new, columns=scaled_data_new.columns)\n\n# Save the scaled new data to a new CSV file\nscaled_data_new_df.to_csv('scaled_data_new.csv', index=False)\n\n# Load the scaled new data from the new CSV file\nscaled_data_new_new = pd.read_csv('scaled_data_new.csv')\n\n# Print the scaled new data\nprint(scaled_data_new_new)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a new DataFrame with the scaled values\nscaled_data = preprocessing.scale(data)\nscaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Save the scaled data to a new DataFrame\nscaled_data.to_csv('scaled_data.csv')\n\n# Load the scaled data from the new DataFrame\nscaled_data = pd.read_csv('scaled_data.csv')\n\n# Print the scaled data\nprint(scaled_data)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = np.array([X.columns[i] for i in np.where(clf.feature_importances_ > 0.05)[0]])", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "column_names = np.array([f'feature_{i}' for i in range(X.shape[1])])\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    centers = centers[p]\n    return centers\n    ### END SOLUTION ###\n\n# call the function\nsamples = get_samples(p, X, km)\nprint(samples)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = np.concatenate((X_train, X_train[0]), axis=1)\n\n# remove categorical variable\nX_train = X_train.drop([0], axis=1)\n\n# convert to numpy array\nX_train = np.array(X_train)\n\n# create GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"rbf\")\n\n# fit the model\nsvm_reg.fit(X, y)\n\n# predict X\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"poly\", degree=2)\n# fit the model\nsvm_reg.fit(X, y)\n# predict X\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarities(query, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    matrix = np.zeros((len(documents), len(documents)))\n    for i, doc in enumerate(documents):\n        matrix[i, :] = np.dot(query_tfidf, tfidf.transform([doc]))\n    return matrix\n\ncosine_similarities_of_queries = get_cosine_similarities(queries, documents)\nprint(cosine_similarities_of_queries)", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarities(query, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = np.dot(query_tfidf, tfidf.transform(documents).T)\n    return cosine_similarities\n\ncosine_similarities_of_queries = np.array([get_cosine_similarities(query, documents) for query in queries])\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_matrix = tfidf.transform(documents)\n    query_matrix = tfidf.transform(queries)\n    cosine_similarities_of_queries = np.dot(query_matrix, tfidf_matrix.T)\n    return cosine_similarities_of_queries\n    ### END SOLUTION ###\n\n# Test the function\nsolve(queries, documents)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the feature names to a 2D array\nfeature_names = np.array(features[0]).reshape(-1, 1)\n\n# Create a DataFrame from the feature names and the new features\ndf = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the DataFrame to a 2D array\nnew_features = df.values\n\n# Convert the feature names to a 2D array\nfeature_names = df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "# Convert the list of features to a 2D array\nf = np.array(f)\n\n# Create a 2D array with zeros\nnew_f = np.zeros((f.shape[0], f.shape[1]))\n\n# Convert the 2D array to a 2D array with one-hot encoding\nfor i in range(f.shape[0]):\n    for j in range(f.shape[1]):\n        new_f[i, j] = np.where(f[i, j] == 't1')\n\nprint(new_f)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the feature names to a 2D array\nfeature_names = np.array(features[0]).reshape(-1, 1)\n\n# Create a DataFrame from the feature names and the new features\ndf = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the DataFrame to a 2D array\nnew_features = df.values\n\n# Convert the feature names to a 2D array\nfeature_names = df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features\nnew_df = pd.DataFrame(np.concatenate((feature_names, new_features), axis=1), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the new DataFrame to a 2D array\nnew_features = new_df.values\n\n# Convert the feature names to a 2D array\nfeature_names = new_df.columns.values\n\n# Create a DataFrame from the feature names and the new features", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one hot encoding\n    one_hot_encoded_features = np.zeros((features_array.shape[0], features_array.shape[1], features_array.shape[1]))\n    for i in range(features_array.shape[0]):\n        for j in range(features_array.shape[1]):\n            one_hot_encoded_features[i, j, j] = 1\n    # Return the one hot encoded features\n    return one_hot_encoded_features\n    ### END SOLUTION ###\n\n# Test the function\nfeatures = [\n    ['f1', 'f2', 'f3'],\n    ['f2', 'f4', 'f5', 'f6'],\n    ['f1', 'f2']\n]\nprint(solve(features))\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "# Convert the list of features to a 2D numpy array\nnew_features = np.array(features).astype(int)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D numpy array to a 2D numpy array with one-hot encoding\nnew_features = np.concatenate((np.ones((new_features.shape[0], 1)), new_features), axis=1)\n\n# Convert the 2D", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the data matrix\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2)\n\n# Fit the clustering model to the data\nclustering.fit(data_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model\nclustering.fit(dist_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the similarity matrix\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2)\n\n# Fit the clustering model to the similarity matrix\nclustering.fit(simM)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a 2-d array from the data matrix\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.linkage(data_matrix, method='average')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.linkage(dist_matrix, method='average')\n\n# Get the labels of the clusters\ncluster_labels = cluster_labels[:, 2]\n\n# Print the labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a 2-d array from the similarity matrix\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.linkage(simM, method='average')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformer.transform(data)\nbox_cox_data = transformer.transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\n\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\n\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformer.transform(data)", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(analyzer='word')\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=[-1]), dataset[-1], test_size=0.2, random_state=42)\n\n# define x and y\nx_train_all = x_train.values\nx_test_all = x_test.values\ny_train_all = y_train.values\ny_test_all = y_test.values\n\n# split each of those sets so that we can define x and y\nx_train_all_x = x_train_all[:, :-1]\nx_test_all_x = x_test_all[:, :-1]\nx_train_all_y = x_train_all[:, -1]\nx_test_all_y = x_test_all[:, -1]\n\n# print the shape of x_train_all_x, x_test_all_x, x_train_all_y, x_test_all_y\nprint(x_train_all_x.shape)\nprint(x_test_all_x.shape)\nprint(x_train_all_y.shape)\nprint(x_test_all_y.shape)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features and the target\nx = data.drop(columns=['target'])\ny = data['target']\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=[-1]), dataset[-1], test_size=0.3, random_state=42)\n\n# define x and y\nx_train_all = x_train.values\ny_train_all = y_train.values\nx_test_all = x_test.values\ny_test_all = y_test.values\n\n# split each of those sets so that we can define x and y\nx_train_all_x = x_train_all[:, :-1]\nx_train_all_y = x_train_all[:, -1]\nx_test_all_x = x_test_all[:, :-1]\nx_test_all_y = x_test_all[:, -1]\n\n# define x and y\nx_train_all_x = x_train_all_x.reshape(-1, 1)\nx_test_all_x = x_test_all_x.reshape(-1, 1)\n\n# define x and y\nx_train_all_y = x_train_all_y.reshape(-1, 1)\nx_test_all_y = x_test_all_y.reshape(-1, 1)\n\n# define x and y\nx_train_all_x = x_train_all_x.reshape(-1, 1)\nx_test_all_x = x_test_all_x.reshape(-1, 1)\n\n# define x and y\nx_train_all_y = x_train_all_y.reshape(-1, 1)\nx_test_all_y = x_test_all_y.reshape(-1, 1)\n\n# define x and y\nx_train_all_x = x_train_all_x.reshape(-1, 1)\nx_test_all_x = x_test_all_x.reshape(-1, 1)\n\n# define x and y\nx_train_all_y = x_train_all_y.reshape(-1, 1)\nx_test_all_y = x_test_all_y.reshape(-1, 1)\n\n# define x and y\nx_train_all_x = x_train_all_x.reshape(-1, 1)\nx_test_all_x = x_test_all_x.reshape(-1, 1)\n\n# define x and y\nx_train_all_y = x_train_all_y.reshape(-1, 1)\nx_test_all_y = x_test_all_y.reshape(-1, 1)\n\n# define x and y\nx_train_all_x = x_train_all_x.reshape(-1, 1)\nx_test_all_x = x_test_all_x.reshape(-1, 1)\n\n# define x and y\nx_train_all_y = x_train_all_y.reshape(-1, 1)\nx_test_all_y = x_test_all_y.reshape(-1, 1)\n\n# define x and y\nx_train_all_x = x_train_all_x.reshape(-1, 1)\nx_test_all_x = x_test_all_x.reshape(-1, 1)\n\n# define x and y\nx_train_all_y = x_train_all_y.reshape(-1, 1)\nx_test_all_y = x_test_all_y.reshape(-1, 1)\n\n# define x and y\nx_train_all_x = x_train_all_x.reshape(-1, 1)\nx_test_all_x = x_test_all_x.reshape(-1, 1)\n\n# define x and y\nx_train_all_y = x_train_all_y.reshape(-1, 1)\nx_test_all_y = x_test_all_y.reshape(-1, 1)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": " ###\n    # split the data into training and testing sets\n    train_data, test_data = np.split(data, [int(data.shape[0]*0.8)])\n    # define x and y\n    x_train = train_data[:, :-1]\n    y_train = train_data[:, -1]\n    x_test = test_data[:, :-1]\n    y_test = test_data[:, -1]\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION ###\n\n# call the function\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "mse = df['mse'].values\nlabels = KMeans(n_clusters=2, n_init=10).fit(mse.reshape(-1, 1)).predict(mse.reshape(-1, 1))\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "f1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2, n_init=10).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "# Create a LinearSVC object\nsvc = LinearSVC(penalty='l1', loss='l1', dual=False)\n# Fit the model\nsvc.fit(X, y)\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(np.array(svc.coef_).flatten() != 0)]\nprint(selected_feature_names)", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "# Create a LinearSVC object\nsvc = LinearSVC(penalty='l1', loss='l2', max_iter=10000)\n\n# Fit the model\nsvc.fit(X, y)\n\n# Get the selected feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\nprint(selected_feature_names)\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    # Use LinearSVC with penalty='l1'\n    svc = LinearSVC(penalty='l1', loss='l1', max_iter=10000, random_state=42)\n    # Fit the model\n    svc.fit(X, y)\n    # Get the feature names\n    feature_names = np.asarray(vectorizer.get_feature_names())[svc.get_support()]\n    return feature_names\n    ### END SOLUTION ###\n\n# Test the function\nsolve(corpus, y, vectorizer, X)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "slopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\nslopes = slopes.tolist()\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "slopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\nslopes = slopes.flatten()\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ntransformed_df = df['Sex'].apply(le.fit_transform)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column into a new column 'Sex_encoded'\ndf['Sex_encoded'] = le.transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ### END SOLUTION ###\n    return df\n\ndef load_data():\n    df = pd.DataFrame({\n        'Sex': ['male', 'female', 'male', 'female'],\n        'Age': [20, 25, 30, 35],\n        'PassengerId': [1, 2, 3, 4],\n        'PassengerClass': [1, 2, 3, 4]\n    })\n    return df\n\ntransformed_df = Transform(load_data())\nprint(transformed_df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION ###\n\n# Test the function\nnp_array = np.array([[1, 2], [3, 4]])\nprint(Transform(np_array))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close, ma50, ma100, ma200])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array([['dsa', '2'], ['sato', '3']])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n# print(X)\n# print(y)\n\nlogReg = LogisticRegression()\nlogReg.fit(X[:None],y)\n# logReg.fit(dataframe.iloc[-1:],dataframe.iloc[:,-1])\n\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the dataframe to numpy array\nX = dataframe.astype(float).values\n# Convert the class to numpy array\ny = dataframe.iloc[:, -1].values\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model\nlogReg.fit(X, y)\n# Predict the class of the new animal\nnew_animal = np.array([[3.6, 600, 7, 3.35, 40, 20000, 975, 0, 0, 0, 0, 0]])\npredict = logReg.predict(new_animal)\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "train_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.sort_values([\"date\"], ascending=True)\ntest_dataframe = test_dataframe.sort_values([\"date\"], ascending=True)\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "train_dataframe = features_dataframe[features_dataframe['date'] <= np.max(features_dataframe['date'])]\ntest_dataframe = features_dataframe[features_dataframe['date'] > np.max(features_dataframe['date'])]\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": " ###\n    # sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(by=\"date\")\n    # calculate the number of rows in the dataframe\n    n_rows = features_dataframe.shape[0]\n    # calculate the number of rows to be used for the test set\n    test_size = n_rows * 0.8\n    # calculate the number of rows to be used for the train set\n    train_size = n_rows - test_size\n    # split the dataframe into train and test set\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=test_size, train_size=train_size)\n    return train_dataframe, test_dataframe\n    ### END SOLUTION ###\n\n# call the function\nsolve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "cols = df.columns[2:4]\ndf[cols + '_scale'] = scaler.fit_transform(df[cols])\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "myData['new_A2'] = scaler.fit_transform(myData[['A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = GridSearch_fitted.fit(X, y)\nprint(full_results.best_score_)\nprint(full_results.best_params_)\nprint(full_results.grid_scores_)\nprint(full_results.best_score_)\nprint(full_results.best_params_)\nprint(full_results.grid_scores_)\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = GridSearch_fitted.fit(X, y)\nfull_results.best_score_\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\ndef save_model(model):\n    # Save the model weights\n    weights = model.weights_\n    # Save the model means\n    means = model.means_\n    # Save the model covariance\n    covs = model.covs_\n    # Save the model components\n    components = model.components_\n    # Save all the attributes\n    attributes = np.concatenate((weights, means, covs, components))\n    # Save the model to a file\n    np.save('sklearn_model', attributes)\n\n# Save the model\nsave_model(fitted_model)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "# tokenizing each description\ndf['description'] = df['description'].apply(lambda x: x.split())\n\n# transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(df['description'])\n\n# calculate cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = np.ones((len(df), len(df)))\nfor i in range(len(df)):\n    for j in range(i + 1, len(df)):\n        cosine_similarity_matrix[i, j] = 1 - sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])\n\n# print the result\nprint(cosine_similarity_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n# Define the learning rate scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define the function to update the learning rate\ndef update_learning_rate(optimizer, epoch):\n    if epoch > 10:\n        optimizer.param_groups[0]['lr'] = 0.001\n    return optimizer\n\n# Update the learning rate after each epoch\nfor epoch in range(100):\n    optimizer = update_learning_rate(optim, epoch)\n    optimizer.step()\n    scheduler.step()\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define the learning rate schedule\nlr_schedule = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.005 if epoch < 100 else 0.0005)\n\n# Update the learning rate\nfor epoch in range(100):\n    lr_schedule.step()\n    print(f\"Epoch {epoch}, Learning rate: {lr_schedule.get_lr()}\")\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# convert the word2vec model to a numpy array\nembedding_weights = np.array(word2vec.wv.vectors)\n\n# create a PyTorch embedding layer with the weights from the word2vec model\nembedding_layer = torch.nn.Embedding.from_pretrained(torch.tensor(embedding_weights))\n\n# embed the input data using the embedding layer\nembedded_input = embedding_layer(input_Tensor)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # get the embedding weights from gensim\n    embedding_weights = word2vec.wv.vectors\n    # convert the embedding weights to PyTorch tensor\n    embedding_weights_tensor = torch.tensor(embedding_weights, dtype=torch.float32)\n    # return the embedding weights tensor\n    return embedding_weights_tensor\n    ### END SOLUTION ###\n\n# call the function\nembedding_weights_tensor = get_embedded_input(input_Tensor)\nprint(embedding_weights_tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "x = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\nprint(px)\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "x = torch.rand(6,6)\npx = pd.DataFrame(x.numpy())\nprint(px)\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "A_logical = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical]\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "A_log = torch.ByteTensor([1, 1, 0]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "A_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    ### END SOLUTION ###\n    return C\n\ndef load_data():\n    A_log = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_log, B\n\nsolve(A_log, B)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "A_log = torch.ByteTensor([0, 0, 1]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Convert the index to a numpy array\nidx = idx.numpy()\n\n# Use numpy indexing to select the columns\nC = B[np.arange(B.shape[0])[:, None], idx]\n\n# Convert the numpy array back to a PyTorch tensor\nC = torch.from_numpy(C)\n\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    # Convert the numpy array to a torch tensor\n    t = torch.tensor(a, dtype=torch.float32)\n    return t\n    ### END SOLUTION ###\n\n# Test the function\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=np.object)\n\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones_like(lens)\nmask[lens == 0] = 0\nmask[lens == 1] = 1\nmask[lens == 2] = 0\nmask[lens == 3] = 1\nmask[lens == 4] = 0\nmask[lens == 5] = 1\nmask[lens == 6] = 0\nmask[lens == 7] = 1\nmask[lens == 8] = 0\nmask[lens == 9] = 1\nmask[lens == 10] = 0\nmask[lens == 11] = 1\nmask[lens == 12] = 0\nmask[lens == 13] = 1\nmask[lens == 14] = 0\nmask[lens == 15] = 1\nmask[lens == 16] = 0\nmask[lens == 17] = 1\nmask[lens == 18] = 0\nmask[lens == 19] = 1\nmask[lens == 20] = 0\nmask[lens == 21] = 1\nmask[lens == 22] = 0\nmask[lens == 23] = 1\nmask[lens == 24] = 0\nmask[lens == 25] = 1\nmask[lens == 26] = 0\nmask[lens == 27] = 1\nmask[lens == 28] = 0\nmask[lens == 29] = 1\nmask[lens == 30] = 0\nmask[lens == 31] = 1\nmask[lens == 32] = 0\nmask[lens == 33] = 1\nmask[lens == 34] = 0\nmask[lens == 35] = 1\nmask[lens == 36] = 0\nmask[lens == 37] = 1\nmask[lens == 38] = 0\nmask[lens == 39] = 1\nmask[lens == 40] = 0\nmask[lens == 41] = 1\nmask[lens == 42] = 0\nmask[lens == 43] = 1\nmask[lens == 44] = 0\nmask[lens == 45] = 1\nmask[lens == 46] = 0\nmask[lens == 47] = 1\nmask[lens == 48] = 0\nmask[lens == 49] = 1\nmask[lens == 50] = 0\nmask[lens == 51] = 1\nmask[lens == 52] = 0\nmask[lens == 53] = 1\nmask[lens == 54] = 0\nmask[lens == 55] = 1\nmask[lens == 56] = 0\nmask[lens == 57] = 1\nmask[lens == 58] = 0\nmask[lens == 59] = 1\nmask[lens == 60] = 0\nmask[lens == 61] = 1\nmask[lens == 62] = 0\nmask[lens == 63] = 1\nmask[lens == 64] = 0\nmask[lens == 65] = 1\nmask[lens == 66] = 0\nmask[lens == 67] = 1\nmask[lens == 68] = 0\nmask[lens == 69] = 1\nmask[lens == 70] = 0\nmask[lens == 71] = 1\nmask[lens == 72] = 0\nmask[lens == 73] = 1\nmask[lens == 74] = 0\nmask[lens == 75] = 1\nmask[lens == 76] = 0\nmask[lens == 77] = 1\nmask[lens == 78] = 0\nmask[lens == 79] = 1\nmask[lens == 80] = 0\nmask[lens == 81] = 1\nmask[lens == 82] = 0\nmask[lens == 83] = 1\nmask[lens == 84] = 0\nmask[lens == 85] = 1\nmask[lens == 86] = 0\nmask[lens == 87] = 1\nmask[lens == 88] = 0\nmask[lens == 89] = 1\nmask[lens == 90] = 0\nmask[l", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "lens = torch.tensor([1, 9, 3, 5])\nmask = torch.ones_like(lens)\nmask[lens] = 0\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones_like(lens)\nmask[lens == 0] = 0\nmask[lens > 0] = 1\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    mask = torch.zeros(lens.shape[0], lens.shape[1])\n    for i in range(lens.shape[0]):\n        mask[i, :lens[i]] = 1\n    return mask\n    ### END SOLUTION ###\n\nlens = torch.LongTensor([3, 5, 4])\nprint(get_mask(lens))\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with diagonal elements\ndiag_ele = torch.ones(Tensor_2D.shape[0])\n\n# Create a 2D tensor with drag_ele\ndrag_ele = torch.ones(Tensor_2D.shape[0])\n\n# Create a 3D tensor with index_in_batch * Matrix (who is a diagonal matrix, construct by drag_ele)\nindex_in_batch = torch.arange(Tensor_2D.shape[0])\n\n# Create a 3D tensor with index_in_batch * Matrix (who is a diagonal matrix, construct by drag_ele)\nindex_in_batch_matrix = index_in_batch.unsqueeze(1).unsqueeze(2).unsqueeze(3).unsqueeze(4).unsqueeze(5).unsqueeze(6).unsqueeze(7).unsqueeze(8).unsqueeze(9).unsqueeze(10).unsqueeze(11).unsqueeze(12).unsqueeze(13).unsqueeze(14).unsqueeze(15).unsqueeze(16).unsqueeze(17).unsqueeze(18).unsqueeze(19).unsqueeze(20).unsqueeze(21).unsqueeze(22).unsqueeze(23).unsqueeze(24).unsqueeze(25).unsqueeze(26).unsqueeze(27).unsqueeze(28).unsqueeze(29).unsqueeze(30).unsqueeze(31).unsqueeze(32).unsqueeze(33).unsqueeze(34).unsqueeze(35).unsqueeze(36).unsqueeze(37).unsqueeze(38).unsqueeze(39).unsqueeze(40).unsqueeze(41).unsqueeze(42).unsqueeze(43).unsqueeze(44).unsqueeze(45).unsqueeze(46).unsqueeze(47).unsqueeze(48).unsqueeze(49).unsqueeze(50).unsqueeze(51).unsqueeze(52).unsqueeze(53).unsqueeze(54).unsqueeze(55).unsqueeze(56).unsqueeze(57).unsqueeze(58).unsqueeze(59).unsqueeze(60).unsqueeze(61).unsqueeze(62).unsqueeze(63).unsqueeze(64).unsqueeze(65).unsqueeze(66).unsqueeze(67).unsqueeze(68).unsqueeze(69).unsqueeze(70).unsqueeze(71).unsqueeze(72).unsqueeze(73).unsqueeze(74).unsqueeze(75).unsqueeze(76).unsqueeze(77).unsqueeze(78).unsqueeze(79).unsqueeze(80).unsqueeze(81).unsqueeze(82).unsqueeze(83).unsqueeze(84).unsqueeze(85).unsqueeze(86).unsqueeze(87).unsqueeze(88).unsqueeze(89).unsqueeze(90).unsqueeze(91).unsqueeze(92).unsqueeze(93).unsqueeze(94).unsqueeze(95).unsqueeze(96).unsqueeze(97).unsqueeze(98).unsqueeze(99).unsqueeze(100).unsqueeze(101).unsqueeze(102).unsqueeze(103).unsqueeze(104).unsqueeze(105).unsqueeze(106).unsqueeze(107).unsqueeze(", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    # Create a 2D tensor with the same shape as t\n    t_2D = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D = t_2D.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag = t_2D_diag.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix = t_2D_diag_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix = t_2D_diag_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix = t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix.repeat(t.shape[1], 1, 1)\n    # Create a 2D tensor with the same shape as t\n    t_2D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix = t.repeat(t.shape[0], 1)\n    # Create a 3D tensor with the same shape as t\n    t_3D_diag_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix = t_2", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\nab = torch.stack((a, b), 0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = a.reshape(1, -1)\nb = b.reshape(1, -1)\nab = np.vstack((a, b))\nab = torch.tensor(ab)\nprint(ab.shape) # (138, 514)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    # stack the tensors a and b along the first dimension\n    # then stack the result with a and b along the second dimension\n    # the result should be a tensor of shape (3, 11)\n    # return the result\n    ### END SOLUTION ###\n    return torch.stack((a, b), 0)\n\n# test the function\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[7, 8, 9], [10, 11, 12]])\nprint(solve(a, b))\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[ : , lengths : , : ] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[ : , lengths : , : ] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    # Convert the list of tensors to a tensor of tensors\n    tensor_of_tensors = torch.tensor(lt)\n    ### END SOLUTION ###\n    return tensor_of_tensors\n\n# Call the function\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = t[idx]\nprint(result)\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = t[idx]\nprint(result)\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\nscores = x.reshape(70,3)\nmax_scores = np.max(scores, axis=1)\nmax_ids = np.argmax(scores, axis=1)\nresult = x[np.arange(70), max_ids]\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "scores = x.mean(1)\nids = torch.argmax(scores, 1)\nresult = x.gather(1, ids.unsqueeze(1)).squeeze(1)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask with the indices of the selected elements\nmask = (ids == 1).astype(int)\n\n# Gather the selected slices\nresult = x[mask].gather(1, ids.unsqueeze(1)).squeeze(1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "y = torch.argmax(softmax_output, 1)\ny = y.view(-1, 1)\nprint(y)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "y = torch.argmax(softmax_output, 1)\nprint(y)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "y = torch.argmax(softmax_output, 1)\ny = y.view(-1, 1)\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    # Find the maximum value in each row of the softmax output\n    max_values = np.max(softmax_output, axis=1)\n    # Create a tensor with the same shape as softmax_output, where each element is the maximum value from the softmax output\n    max_tensor = torch.tensor(max_values).unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = torch.argmax(softmax_output, axis=1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_indices = max_indices.unsqueeze(1)\n    # Create a tensor with the same shape as softmax_output, where each element is the index of the maximum value from the softmax output\n    max_", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "# Find the index of the minimum value in the softmax output\ny = torch.argmax(softmax_output, 1)\n# Create a tensor with the same shape as the softmax output, with all values set to 0\ny = torch.zeros_like(softmax_output)\n# Set the value at the index of the minimum value to 1\ny[y == torch.argmax(softmax_output, 1)] = 1\nreturn y\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "# Convert labels to one-hot encoding\nlabels = torch.LongTensor(labels)\nlabels = F.one_hot(labels, num_classes=7)\n\n# Convert images to one-hot encoding\nimages = images.unsqueeze(0)\nimages = F.one_hot(images, num_classes=7)\n\n# Calculate cross entropy loss\nloss = F.nll_loss(images, labels, size_average=False)\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = np.sum(A == B)\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = np.sum(A == B)\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = np.sum(A != B)", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = np.sum(A == B)\n    ### END SOLUTION ###\n    return cnt_equal\n\n# Call the function\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "cnt_equal = np.sum(A[-x:] == B[-x:])", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = np.sum(A[-x:] != B[-x:])", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\na = torch.randn(1, 3, 10, 40, 1)\nchunk_dim = 10\ntensors_31 = torch.chunk(a, chunk_dim, dim=3)\ntensors_31 = tensors_31.reshape(tensors_31.shape[0], tensors_31.shape[1], tensors_31.shape[2], tensors_31.shape[3], 1)\nprint(tensors_31)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\n# Create a tensor with the shape of a\na = torch.randn(1, 3, 10, 40, 1)\n\n# Create a tensor with the shape of a\na_split = torch.chunk(a, chunk_dim, dim=2)\n\n# Create a tensor with the shape of a\ntensors_31 = a_split[0]\n\n# Print the shape of tensors_31\nprint(tensors_31.shape)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output = torch.where(mask == 1, clean_input_spectrogram, output)\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask == 0] = clean_input_spectrogram[mask == 0]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x), torch.abs(y))\nmin_y = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.where(x < y, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\nsigned_max = torch.where(x > y, max_x, max_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x), torch.abs(y))\n    min_y = torch.min(torch.abs(x), torch.abs(y))\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, min_x_sign, min_y_sign)\n    return min_x_sign\n    ### END SOLUTION ###\n\n# Call the function\nsolution = solve(x, y)\nprint(solution)\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\nprint(f\"Confidence score: {confidence_score}, Class name: {class_name}\")\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "a = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\na = a[:, :-1]\nb = b[:, 1:]\n\nresult = a + b\nresult = result / 2\nresult = result.numpy()\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    # combine the two tensors and have them partially overlap by a single column, with the average being taken for those elements that overlap.\n    # The first two columns are the first two columns of 'a'. The last two columns are the last two columns of 'b'. The middle column is the average of 'a's last column and 'b's first column.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a and the first column of b.\n    # The result should be a tensor with the same shape as a and b, but with the elements of a and b combined in a way that the middle column is the average of the last column of a", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "t = torch.tensor([[1, 2], [3, 4]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
