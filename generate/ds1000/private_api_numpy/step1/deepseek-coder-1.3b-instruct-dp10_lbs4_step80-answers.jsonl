{"id": 0, "code": "result = df.reindex(List)\nresult", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "\nresult = df.reindex(List)\nresult['Type'] = df['Type']\nresult['Type'] = result['Type'].astype(int)\nresult['Type'] = result['Type'].replace([1, 2, 3], [2, 3, 4])\nresult['Type'] = result['Type'].replace([2, 3, 4], [1, 2, 3])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [1, 2, 3, 4])\nresult['Type'] = result['Type'].replace([1, 2, 3, ", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a mask for the rows where the value in column Qu1 is not in the value_counts() of column Qu1\nmask = df['Qu1'].map(df['Qu1'].value_counts()) < 2\n\n# Apply the mask to the column Qu1\ndf.loc[mask, 'Qu1'] = 'other'\n\n# Create a mask for the rows where the value in column Qu2 is not in the value_counts() of column Qu2\nmask = df['Qu2'].map(df['Qu2'].value_counts()) < 2\n\n# Apply the mask to the column Qu2\ndf.loc[mask, 'Qu2'] = 'other'\n\n# Create a mask for the rows where the value in column Qu3 is not in the value_counts() of column Qu3\nmask = df['Qu3'].map(df['Qu3'].value_counts()) < 2\n\n# Apply the mask to the column Qu3\ndf.loc[mask, 'Qu3'] = 'other'\n\nresult = df\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a mask for the rows where the value in column Qu1 is not in the list of values with at least 3 occurrences\nmask = df['Qu1'].isin(df['Qu1'].value_counts()[df['Qu1'].value_counts() >= 3].index)\n\n# Apply the mask to the column Qu1\ndf.loc[mask, 'Qu1'] = 'other'\n\n# Create a mask for the rows where the value in column Qu2 is not in the list of values with at least 3 occurrences\nmask = df['Qu2'].isin(df['Qu2'].value_counts()[df['Qu2'].value_counts() >= 3].index)\n\n# Apply the mask to the column Qu2\ndf.loc[mask, 'Qu2'] = 'other'\n\n# Create a mask for the rows where the value in column Qu3 is not in the list of values with at least 3 occurrences\nmask = df['Qu3'].isin(df['Qu3'].value_counts()[df['Qu3'].value_counts() >= 3].index)\n\n# Apply the mask to the column Qu3\ndf.loc[mask, 'Qu3'] = 'other'\n\nresult = df\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "\n    for col in df.columns:\n        counts = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: 'other' if x in counts[counts >= 2].index else x)\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a function to replace values in a column based on value counts\ndef replace_values(column):\n    counts = df[column].value_counts()\n    for index, count in counts.iteritems():\n        if count >= 3:\n            df[column] = df[column].replace(index, 'other')\n        elif count >= 2:\n            df[column] = df[column].replace(index, index)\n    return df\n\n# Apply the function to Qu1 and Qu2\ndf = replace_values('Qu1')\ndf = replace_values('Qu2')\n\n# Create a final dataframe with the result\nresult = df\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a dictionary to map the values\nmap_dict = {'apple': 'other', 'potato': 'other', 'banana': 'other', 'cheese': 'cheese', 'sausage': 'sausage', 'egg': 'egg'}\n\n# Apply the mapping to the columns\ndf['Qu1'] = df['Qu1'].map(map_dict)\ndf['Qu2'] = df['Qu2'].map(map_dict)\ndf['Qu3'] = df['Qu3'].map(map_dict)\n\n# Create a dictionary to map the values for the second time\nmap_dict_2 = {'apple': 'other', 'banana': 'other', 'sausage': 'sausage', 'egg': 'egg'}\n\n# Apply the mapping to the columns\ndf['Qu2'] = df['Qu2'].map(map_dict_2)\ndf['Qu3'] = df['Qu3'].map(map_dict_2)\n\n# Set the values in the 'Qu1' column to 'other' if they are not 'apple'\ndf['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x != 'apple' else x)\n\n# Set the values in the 'Qu3' column to 'other' if they are not 'apple'\ndf['Qu3'] = df['Qu3'].apply(lambda x: 'other' if x != 'apple' else x)\n\nresult = df\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "\ndf['keep_if_dup'] = df['keep_if_dup'].map({'Yes': True, 'No': False})\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result[result['keep_if_dup']]\nresult = result[['id', 'url', 'keep_if_dup']]\n", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "\ndf['drop_if_dup'] = df['drop_if_dup'].map({'Yes': True, 'No': False})\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result[['id', 'url', 'drop_if_dup']]\nresult['drop_if_dup'] = result['drop_if_dup'].map({True: 'Yes', False: 'No'})\nresult\n", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\ndf['keep_if_dup'] = df['keep_if_dup'].map({'Yes': True, 'No': False})\nresult = df.drop_duplicates(subset='url', keep='last')\nresult = result[result['keep_if_dup']]\nresult = result[['id', 'url', 'keep_if_dup']]\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\nresult = df.groupby('name').apply(lambda x: {v1: {v2: v3} for v1, v2, v3 in x[['v1', 'v2', 'v3']].values}).to_dict()\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": "\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# First, we need to parse the message to extract the key-value pairs\ndf['job'] = df['message'].apply(lambda x: x.split(':')[1].strip())\ndf['money'] = df['message'].apply(lambda x: x.split(', money:')[1].split(']')[0].strip())\ndf['wife'] = df['message'].apply(lambda x: x.split(', wife:')[1].split(']')[0].strip())\ndf['group'] = df['message'].apply(lambda x: x.split(', group:')[1].split(']')[0].strip())\ndf['kids'] = df['message'].apply(lambda x: x.split(', kids:')[1].split(']')[0].strip())\n\n# Then, we drop the message column\ndf = df.drop(['message'], axis=1)\n\n# Finally, we expand the dataframe\nresult = df.set_index(['name', 'status', 'number']).unstack(level=-1).reset_index()\n\n# The result is a dataframe with the key-value pairs expanded out\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df['score'] * 10\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10 if df['product'].isin(products) else df['score']", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "\ndf['score'] = df['score'] * 10\n\nfor product_range in products:\n    df.loc[df['product'].between(product_range[0], product_range[1]), 'score'] = df.loc[df['product'].between(product_range[0], product_range[1]), 'score'] * 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "df['score'] = df['product'].apply(lambda x: 1 if x in products else df['score'].min() if x == products[0] else df['score'].max())\ndf['score'] = df['score'] / df['score'].max()\ndf", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.idxmax(axis=1)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = df.idxmax(axis=1)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col]], axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf = df[df['Date'].between('17-Jan-2019', '20-Feb-2019')]\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "df['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf = df.iloc[1:]\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "df['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf = df.iloc[:-1]\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf['#1'] = df['#1'].fillna(df['#1'].iloc[0])\ndf['#2'] = df['#2'].fillna(df['#2'].iloc[-1])\ndf", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\ndf['#1'] = df['#1'].shift(-1)\n\n# shift the last row of the first column up 1 row\ndf['#2'] = df['#2'].shift(1)\n\ndf = df.iloc[1:]\n\ndf", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'X' for col in df.columns]", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col for col in df.columns]\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "\n# Create a list of all column names that end with 'X'\nx_columns = [col for col in df.columns if col.endswith('X')]\n\n# Create a list of all column names that don't end with 'X'\nnon_x_columns = [col for col in df.columns if not col.endswith('X')]\n\n# Create a dictionary to map the new column names\nnew_columns = {**{col: col + 'X' for col in x_columns}, **{col: 'XHeader' + col for col in non_x_columns}}\n\n# Rename the columns\ndf.rename(columns=new_columns, inplace=True)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "result = df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\nresult['group_color'] = df.groupby('group')['group_color'].first()\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' in col})\nresult['group_color'] = df.groupby('group')['group_color'].first()\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\n# Create a dictionary to hold the aggregation functions\nagg_dict = {'group_color': 'first', 'val1': 'sum', 'val2': 'mean', 'val42': 'sum'}\n\n# Apply the aggregation function to the groupby object\nresult = df.groupby('group').agg(agg_dict)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "\nresult = df.loc[row_list, column_list].mean()\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "\nresult = df.loc[row_list, column_list].sum()\nresult", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "\nresult = df.loc[row_list, column_list].sum()\nresult = result.drop(result.idxmax())\n", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "\nresult = df.apply(lambda x: pd.Series(x.value_counts()), axis=0)\nresult = result.fillna(0)\nresult = result.astype(float)\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "\n# Get the value counts for each column\nresult = df.apply(lambda x: pd.Series(x.value_counts()))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "df = df.iloc[[0, 1]]\nresult = df.rename(columns={'Unnamed: 1': 'Concentration', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H'})\nresult = df.reset_index(drop=True)\nresult", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "df = df.iloc[[0, 1]]\nresult = df.merge(df.iloc[[2]], how='outer', left_index=True, right_index=True)\nresult.columns = ['Nanonose', 'Concentration', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\nresult", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "\ndf['0'] = df['0'].replace(np.nan, df['0'].mean()).astype(float)\ndf['1'] = df['1'].replace(np.nan, df['1'].mean()).astype(float)\ndf['2'] = df['2'].replace(np.nan, df['2'].mean()).astype(float)\n\nresult = df\nresult", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "\ndf['0'] = df['0'].replace({np.nan: df['0'].mean()})\ndf['1'] = df['1'].replace({np.nan: df['1'].mean()})\ndf['2'] = df['2'].replace({np.nan: df['2'].mean()})\n\nresult = df\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "\ndf['0'] = df['0'].mask(df['0'].isnull(), df['1'].notnull()).astype(int)\ndf['1'] = df['1'].mask(df['1'].isnull(), df['2'].notnull()).astype(int)\ndf['2'] = df['2'].mask(df['2'].isnull(), df['0'].notnull()).astype(int)\n\nresult = df\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# Create a mask for rows where value is less than threshold\nmask = df['value'] < thresh\n\n# Apply the mask to the dataframe\ndf.loc[mask] = df.loc[mask].sum()\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the index back to 'lab'\ndf.set_index('lab', inplace=True)\n\n# Set the value column to the original values\ndf['value'] = df['value']\n\n# Set the index back to 'lab'\ndf.set_index('lab', inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\n# Create a mask for rows where value is less than threshold\nmask = df['value'] < thresh\n\n# Apply the mask to the dataframe\ndf.loc[mask] = df.loc[mask].mean(axis=1).rename('value')\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the lab as the index\ndf.set_index('lab', inplace=True)\n\n# Fill the NaN", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# Create a mask for the rows to be replaced\nmask = (df['value'] >= section_left) & (df['value'] <= section_right)\n\n# Replace the rows with the average of the values in the section\ndf.loc[mask, 'value'] = df.loc[mask, 'value'].mean()\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the index back to 'lab'\ndf = df.set_index('lab')\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\nresult = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\nresult = df.assign(**{f'exp_{col}': lambda x: x[col] for col in df.columns})\nresult", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\ndf['inv_A'] = 1 / df['A']\ndf['inv_B'] = 1 / df['B']\n\nresult = df\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndf = df.add_prefix(\"sigmoid_\")\ndf = df.assign(**{f\"sigmoid_{col}\": sigmoid(df[col]) for col in df.columns})\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the last occurrence of the column-wise maximum\nmax_idx = df.idxmax().loc[min_idx]\n\n# Get the date from the index\nresult = df.index[max_idx]\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the maximum value in each column\nmax_idx = df.idxmax()\n\n# Create a mask where the maximum value is less than the minimum value\nmask = df.apply(lambda x: x < x.min())\n\n# Apply the mask to the maximum index\nresult = max_idx.where(mask)\n\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "\n# Create a pivot table with the minimum and maximum dates for each user\npivot = df.pivot_table(index='user', columns='dt', aggfunc={'val': 'min', 'dt': 'max'})\n\n# Expand the dates to include all the dates for each user\nresult = pd.DataFrame(pd.np.nan, index=pd.np.arange(len(df)), columns=pd.np.arange(len(pivot)))\nresult.update(pivot)\n\n# Fill in the values for the dates that are not in the pivot table\nresult.fillna(0, inplace=True)\n\n# Reset the index\nresult = result.reset_index()\n\n# Rename the columns\nresult.columns = ['user', 'dt', 'val']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "\n# Create a new dataframe with all the dates from the minimum to the maximum date\nnew_df = pd.DataFrame({'user': df['user'].unique(), 'dt': pd.date_range(start=df['dt'].min(), end=df['dt'].max())})\n\n# Merge the new dataframe with the original dataframe\nresult = pd.merge(new_df, df, on=['user', 'dt'], how='left')\n\n# Fill the NaN values with 0\nresult = result.fillna(0)\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "\n# Create a new dataframe with all the dates from the minimum to the maximum date\nnew_df = pd.DataFrame({'user': df['user'].unique(), 'dt': pd.date_range(start=df['dt'].min(), end=df['dt'].max())})\n\n# Merge the new dataframe with the original dataframe\nresult = pd.merge(new_df, df, on=['user', 'dt'], how='left')\n\n# Fill the NaN values with 233\nresult['val'].fillna(233, inplace=True)\n\n# Reset the index\nresult.reset_index(drop=True, inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "df['dt'] = df.groupby('user')['dt'].expanding().min()\nresult = df.merge(df.groupby('user')['val'].max(), on='user').drop('val', axis=1)\nresult.columns = ['dt', 'user', 'val']\nresult", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "df['dt'] = df.groupby('user')['dt'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('max')\nresult = df.set_index(['dt', 'user']).unstack(fill_value=0).stack().reset_index()\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\nresult", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "df['name'] = df['name'].astype('category').cat.codes\ndf['name'] += 1\nresult = df\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "df['a'] = df['a'].astype(str) + df['name'].astype(str)\ndf['a'] = df['a'].astype(int) + 1\nresult = df\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "\n    df['name'] = df['name'].astype('category').cat.codes\n    df['name'] += 1\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "df['ID'] = df['name'].astype('category').cat.codes\ndf = df.drop(['name'], axis=1)\ndf = df.rename(columns={'ID': 'b'})\ndf = df.rename(columns={'b': 'c'})\nresult = df\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.sort_values(['user', 'date'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value', fill_value=0)\ndf = df.pivot_table(index='user', columns='02/12/15', values='value', fill_value=0)\ndf = df.pivot_table(index='user', columns='someBool', values='value', fill_value=0)\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'others'})\ndf = df.rename(columns={'01/12/15': 'value'})\ndf = df.rename(columns={'02/12/15': 'value'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'value': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': '02/12/15'})\ndf = df.rename(columns={'value': '01/12/15'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': '02/12/15'})\ndf = df.rename(columns={'value': '01/12/15'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': '02/12/15'})\ndf = df.rename(columns={'value': '01/12/15'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': '02/12/15'})\ndf = df.rename(columns={'value': '01/12/15'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': '02/12/15'})\ndf = df.rename(columns={'value': '01/12/15'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': '02/12/15'})\ndf = df.rename(columns={'value': '01/12/15'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': '02/12/15'})\ndf = df.rename(columns={'value': '01/12/15'})\ndf = df.rename(columns={'value': 'someBool'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.ren", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.sort_values(['user', 'date'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "\ndf_sub = df.loc[df['c'] > 0.5, columns]\nresult = df_sub.values", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "\ndf_sub = df.loc[df['c'] > 0.45, columns]\nresult = df_sub.values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    df['sum'] = df[columns].apply(lambda x: x.sum(), axis=1)\n    return df[columns]\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df, columns=['b', 'e']))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\nresult = f(df, ['b', 'e'])\nprint(result)\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\n# Convert the date column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtracts the number of days from the date\ndf['days_from_date'] = (df['date'] - df['date'].min()).dt.days\n\n# Create a new column that subtract", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column that subtracts X weeks from the date\ndf['date_minus_X'] = df['date'] - pd.to_timedelta(X, unit='w')\n\n# Create a new column that checks if the date is within the last X weeks\ndf['is_within_last_X'] = (df['date'] >= df['date_minus_X'])\n\n# Filter the dataframe to only include rows where the date is within the last X weeks\nresult = df[df['is_within_last_X']]\n\n# Remove the date_minus_X and is_within_last_X columns\nresult = result.drop(['date_minus_X', 'is_within_last_X'], axis=1)\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the dataframe by date\ndf = df.sort_values('date')\n\n# Create a new column 'week' which represents the week of the date\ndf['week'] = df['date'].dt.week\n\n# Create a new column 'week_end' which represents the end of the week\ndf['week_end'] = df['date'].dt.weekday\n\n# Create a new column 'week_start' which represents the start of the week\ndf['week_start'] = df['date'].dt.weekday\n\n# Create a new column 'week_start_date' which represents the start date of the week\ndf['week_start_date'] = df['date'].dt.date\n\n# Create a new column 'week_end_date' which represents the end date of the week\ndf['week_end_date'] = df['date'].dt.date\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\ndf['col1'] = df['col1'].rolling(3).mean().reset_index(0, drop=True)\nresult = df\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\ndf['col1'] = df['col1'].groupby(df.index // 4).sum()\nresult = df['col1'].values", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df['col1']\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column with the cumulative sum of the first 3 rows\ndf['cumulative_sum'] = df.iloc[:3].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['cumulative_avg'] = df.iloc[-2:].mean()\n\n# Create a new column with the cumulative sum of the first 3 rows\ndf['cumulative_sum_2'] = df.iloc[3:].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['cumulative_avg_2'] = df.iloc[-2:].mean()\n\n# Drop the last 2 rows\ndf = df.iloc[:-2]\n\n# Append the new columns to the original dataframe\ndf = df.append(df.iloc[-1])\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the cumulative sum of the dataframe\ndf['cumulative_sum'] = df['col1'].cumsum()\n\n# Create a new column with the cumulative sum of the dataframe from the end\ndf['cumulative_sum_end'] = df['cumulative_sum'].iloc[-3:]\n\n# Create a new column with the cumulative sum of the dataframe from the head\ndf['cumulative_sum_head'] = df['cumulative_sum'].iloc[:3]\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end'] = df['cumulative_sum_end'].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head'] = df['cumulative_sum_head'].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['cumulative_sum_end'].sum()\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head'] = df['cumulative_sum_head'].sum()\n\n# Drop the cumulative sum and cumulative sum end columns\ndf = df.drop(['cumulative_sum', 'cumulative_sum_end'], axis=1)\n\n# Rename the new columns to the original column names\ndf = df.rename(columns={'cumulative_sum_head': 'col1', 'avg_end': 'col2', 'sum_end': 'col3', 'avg_head': 'col4', 'sum_head': 'col5'})\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df['A'] = df['A'].mask(df['A'] == 0).ffill()\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df['A'] = df['A'].mask(df['A'] == 0).ne(0).astype(int)\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "df['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['number'] = df['duration'].str.extract('(\\d+)').astype(int)\ndf['time'] = df['duration'].str.extract('([a-zA-Z]+)').str.replace(' ', '')\ndf['time_days'] = df['time'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].astype(int)\ndf['number'] = df['number'].astype(int)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].astype(int)\ndf['number'] = df['number'].astype(int)\ndf", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "\n    df['time'] = df['duration'].str.split(' ').str[0]\n    df['number'] = df['duration'].str.split(' ').str[1]\n    df['time_days'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n    df['time_days'] = df['time_days'].astype(int)\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1].astype(int)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number']\ndf\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = [all(getattr(df1, column) == getattr(df2, column) for column in columns_check_list) for _ in range(len(df1))]\nresult", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = [all(getattr(df1, column) == getattr(df2, column) for column in columns_check_list) for _ in range(len(df1))]\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='date')\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='datetime')\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "\n    df['date'] = pd.to_datetime(df['date'])\n    return df[['date', 'x', 'y']].values\n    ### END SOLUTION\n\ndf = pd.DataFrame({'id': ['abc', 'abc', 'abc'],\n                   'date': ['3/1/1994', '9/1/1994', '3/1/1995'],\n                   'x': [100, 90, 80],\n                   'y': [7, 8, 9]})\n\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "\n    df.index = pd.to_datetime(df.index)\n    df = df.swaplevel(0, 1)\n    return df\n    ### END SOLUTION\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df_melt = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf_pivot = df_melt.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf_pivot.columns.name = None\ndf_pivot.columns = ['Country', 'year', 'var1', 'var2']\ndf_pivot['var1'] = df_pivot['var1'].astype(int)\ndf_pivot['var2'] = df_pivot['var2'].astype(int)\ndf = df_pivot\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df_long = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf_long['year'] = df_long['year'].astype(int)\ndf_long = df_long.sort_values(['Country', 'year'], ascending=[True, False])\ndf_long = df_long.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf_long.columns.name = None\ndf_long", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\ndf = df.loc[abs(df['Value_E']) < 1]\ndf = df.loc[abs(df['Value_F']) < 1]\ndf = df.loc[abs(df['Value_G']) < 1]\ndf = df.loc[abs(df['Value_H']) < 1]\ndf = df.loc[abs(df['Value_I']) < 1]\ndf = df.loc[abs(df['Value_J']) < 1]\ndf = df.loc[abs(df['Value_K']) < 1]\ndf = df.loc[abs(df['Value_L']) < 1]\ndf = df.loc[abs(df['Value_M']) < 1]\ndf = df.loc[abs(df['Value_N']) < 1]\ndf = df.loc[abs(df['Value_O']) < 1]\ndf = df.loc[abs(df['Value_P']) < 1]\ndf = df.loc[abs(df['Value_Q']) < 1]\ndf = df.loc[abs(df['Value_R']) < 1]\ndf = df.loc[abs(df['Value_S']) < 1]\ndf = df.loc[abs(df['Value_T']) < 1]\ndf = df.loc[abs(df['Value_U']) < 1]\ndf = df.loc[abs(df['Value_V']) < 1]\ndf = df.loc[abs(df['Value_W']) < 1]\ndf = df.loc[abs(df['Value_X']) < 1]\ndf = df.loc[abs(df['Value_Y']) < 1]\ndf = df.loc[abs(df['Value_Z']) < 1]\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\ndf = df.loc[abs(df['Value_E']) < 1]\ndf = df.loc[abs(df['Value_F']) < 1]\ndf = df.loc[abs(df['Value_G']) < 1]\ndf = df.loc[abs(df['Value_H']) < 1]\ndf = df.loc[abs(df['Value_I']) < 1]\ndf = df.loc[abs(df['Value_J']) < 1]\ndf = df.loc[abs(df['Value_K']) < 1]\ndf = df.loc[abs(df['Value_L']) < 1]\ndf = df.loc[abs(df['Value_M']) < 1]\ndf = df.loc[abs(df['Value_N']) < 1]\ndf = df.loc[abs(df['Value_O']) < 1]\ndf = df.loc[abs(df['Value_P']) < 1]\ndf = df.loc[abs(df['Value_Q']) < 1]\ndf = df.loc[abs(df['Value_R']) < 1]\ndf = df.loc[abs(df['Value_S']) < 1]\ndf = df.loc[abs(df['Value_T']) < 1]\ndf = df.loc[abs(df['Value_U']) < 1]\ndf = df.loc[abs(df['Value_V']) < 1]\ndf = df.loc[abs(df['Value_W']) < 1]\ndf = df.loc[abs(df['Value_X']) < 1]\ndf = df.loc[abs(df['Value_Y']) < 1]\ndf = df.loc[abs(df['Value_Z']) < 1]\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\ndf = df.loc[abs(df['Value_E']) < 1]\ndf = df.loc[abs(df['Value_F']) < ", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\ndf = df[(df.abs().max(axis=1) > 1).all(axis=1)]", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\ndf = df[(abs(df['Value_B']) > 1) | (abs(df['Value_C']) > 1) | (abs(df['Value_D']) > 1)]", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df['A'] = df['A'].str.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "\n    df.replace({'&AMP;': '&'}, regex=True, inplace=True)\n    return df\n\nf()\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df['A'] = df['A'].str.replace('&AMP;', '&').str.replace('&LT;', '<').str.replace('&GT;', '>')\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "\ndef validate_single_space_name(name: str) -> str:\n    if ' ' in name:\n        return name.split(' ')[0]\n    else:\n        return name\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: None if ' ' not in x else x.split(' ')[1])\n\ndf", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "\ndf['1_name'] = df['name'].apply(lambda x: x.split(' ')[0])\ndf['2_name'] = df['name'].apply(lambda x: x.split(' ')[1])\n\ndf\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^(.*) (.*)$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return match_obj.groups()\n    else:\n        return None\n\ndf['first_name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(validate_single_space_name))\ndf = df.drop(columns=['name'])\ndf = df.rename(columns={'first_name': 'first_name', 'middle_name': 'middle_name', 'last_name': 'last_name'})\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = df2.merge(df1, on='Timestamp', how='left')\nresult['data'] = result['data'].fillna(result['stuff'])\nresult = result.drop(['stuff'], axis=1)\nresult = result.sort_values('Timestamp')\nresult\n", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1['Timestamp'], df2['Timestamp'], direction='nearest')\nresult['data'] = df2['stuff']\nresult = result.sort_values('Timestamp')\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= row['col3'] else row['col1'].max(), axis=1)\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, int) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, float) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field1'].replace('and', '')\ndf['Field1'] = df['Field1'].astype(float)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.notnull(x) and isinstance(x, str) else 'and')\ndf['Field1'] = df['Field", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: [int(i) for i in str(x).split() if str(i).isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "\n    result = []\n    for value in df['Field1']:\n        if not isinstance(value, int):\n            result.append(value)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "df = df.drop(test)\nresult = df\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\n# Calculate the pairwise distances between cars\ndf['distance'] = ((df['x'] - df['x'].shift())**2 + (df['y'] - df['y'].shift())**2).sqrt()\n\n# Find the nearest neighbour for each car\ndf['nearest_neighbour'] = df.groupby('time')['distance'].transform('idxmin')\n\n# Calculate the average of the distances for each frame\ndf['average_distance'] = df.groupby('time')['distance'].mean()\n\ndf\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\n# First, calculate the pairwise distances between cars\ndf['distance'] = ((df['x'].values[:, None, None] - df['x'].values[None, :, None])**2 + (df['y'].values[:, None, None] - df['y'].values[None, :, None])**2).sqrt()\n\n# Then, find the most common car for each time point\nmost_common_car = df.groupby('time').apply(lambda x: x.loc[x['car'].values.argmax(), 'car']).values\n\n# Finally, calculate the average of the distances for each time point\ndf['farmost_neighbour'] = df.groupby('time')['car'].apply(lambda x: x.loc[x.values == most_common_car].index[0])\ndf['euclidean_distance'] = df.groupby('time')['distance'].mean()\n\ndf", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: ', '.join(x.dropna().values), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values), axis=1)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndf['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values[::-1]), axis=1)\ndf\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df['Quantity'] = df.index[df.index.isin(sample_df.index)]\n\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change the value of the ProductId column of these rows to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index[~df.index.isin(sample_df.index)]\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# First, calculate the number of rows to select for each user\nnum_rows = df['UserId'].value_counts()\n\n# Then, select the rows for each user\nselected_rows = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n\n# Set the Quantity column to zero\ndf.loc[selected_rows.index, 'Quantity'] = 0\n\n# Keep the indexes of the altered rows\ndf.loc[selected_rows.index]\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\nresult = df.loc[df['index_original'] == True]\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\nresult = df.loc[df['index_original'] == True]\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "\n    df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\n    duplicate = df.loc[df['index_original'] == True]\n    return duplicate\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "df['index_original'] = df.duplicated(subset=['col1','col2', '3col'], keep=False)\nresult = df.loc[df['index_original'] == True]\nresult", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\nresult = df.loc[df['index_original'] == True]\nresult", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt']).drop(columns=['count'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count\nresult = result[result['count'] == max_counts['count']].reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "\n# First, find the maximum count in each group\nmax_counts = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n\n# Then, join the original DataFrame with the maximum counts\nresult = df.join(max_counts.set_index(['Sp','Mt'])['count'], on=['Sp','Mt'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count\nresult = result[result['count'] == result['count'].max()]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\n# First, find the minimum count for each group\nmin_counts = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\n\n# Then, join the original dataframe with the minimum counts\nresult = df.join(min_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt']).drop(columns=['count'])\n\n# Finally, reset the index and drop the original index\nresult = result.reset_index().drop(columns='level_1')\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp','Value'])['count'], on=['Sp','Value'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count\nresult = result[result['count'] == result['count'].max()]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df[df['Category'].isin(filter_list)]", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "\n# Create a list of tuples where each tuple contains the first column level, the second column level, and the third column level\nvalue_vars = [('A', 'B', 'E'),\n              ('A', 'B', 'F'),\n              ('A', 'C', 'G'),\n              ('A', 'C', 'H'),\n              ('A', 'D', 'I'),\n              ('A', 'D', 'J')]\n\n# Use the list of tuples to melt the data frame\nresult = pd.melt(df, value_vars=value_vars)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "\n# Define the tuples of column levels\ntuples = [(a, b, c) for a in df.columns[0] for b in df.columns[1] for c in df.columns[2]]\n\n# Use pd.melt to melt the data frame\nresult = pd.melt(df, id_vars=df.columns[0], value_vars=tuples, var_name='variable_0', value_name='value')\n\n# Rename the columns\nresult.columns = ['variable_1', 'variable_2', 'value']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] >= 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index()\nresult.columns = ['r', 'v']\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index()\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "\ndef relationship(df):\n    result = []\n    for c1 in df.columns:\n        for c2 in df.columns:\n            if c1 != c2:\n                if df[c1].nunique() == 1:\n                    result.append(f'{c1} {c2} one-to-many')\n                elif df[c2].nunique() == 1:\n                    result.append(f'{c1} {c2} one-to-one')\n                else:\n                    result.append(f'{c1} {c2} many-to-many')\n    return result\n\nresult = relationship(df)\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "\ndef relationship(column1, column2):\n    if len(column1) == len(column2):\n        return 'one-2-many'\n    elif len(column1) > len(column2):\n        return 'one-2-one'\n    else:\n        return 'many-2-many'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {relationship(df[column1], df[column2])}')\n\nresult\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "\ndef relationship(x, y):\n    if x.nunique() == 1:\n        return 'one-to-many'\n    elif y.nunique() == 1:\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\nresult = df.apply(relationship, axis=0)\nresult.index = df.columns\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "\ndef relationship(x, y):\n    if x.nunique() == 1:\n        if y.nunique() > 1:\n            return 'one-2-many'\n        else:\n            return 'one-2-one'\n    elif y.nunique() == 1:\n        if x.nunique() > 1:\n            return 'many-2-one'\n        else:\n            return 'many-2-many'\n    else:\n        return 'many-2-many'\n\n\nresult = df.apply(lambda x: relationship(x, df[x.name]), axis=1)\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "\n# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n\n# sort the dataframe by bank account\ndfiban_uniq = dfiban_uniq.sort_values(by='bank', ascending=False)\n\nresult = dfiban_uniq\nresult\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\nprint(\"Hello, World!\")\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\ndf.groupby('Family')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndf['Family'] = (df['SibSp'] == 1) & (df['Parch'] == 1)\ndf.loc[df['Family'] == True, 'Family'] = 'Has Family'\ndf.loc[df['Family'] == False, 'Family'] = 'No Family'\n\ndf['Family'] = df['Family'].replace({True: 'Has Family', False: 'No Family'})\n\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A')).reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list(), names=['Caps', 'Lower'])", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "df = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult = df\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "\ndef std_mean(x):\n    return np.std(np.mean(x))\n\nresult = df.groupby('a')['b'].apply(std_mean).reset_index(name='std')\nresult['mean'] = df.groupby('a')['b'].mean()\nresult\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "\ndef std_mean(x):\n    return np.std(np.mean(x))\n\nresult = df.groupby('b')['a'].apply(std_mean).reset_index(name='std')\nresult['mean'] = df.groupby('b')['a'].mean().reset_index(name='mean')\nresult", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "\nimport numpy as np\n\n# calculate softmax\ndf['softmax'] = np.exp(df['b']) / np.exp(df['b']).sum()\n\n# calculate min-max\ndf['min-max'] = (df['b'] - df['b'].min()) / (df['b'].max() - df['b'].min())\n\ndf", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "\n# First, we need to find the rows and columns that only contain zeros.\n# Then, we can remove these rows and columns from the dataframe.\n\n# Find the rows and columns that only contain zeros.\nrows_with_zeros = df.loc[:, df.eq(0).all()].index\ncols_with_zeros = df.loc[df.eq(0).all(), :].columns\n\n# Remove these rows and columns from the dataframe.\nresult = df.loc[~rows_with_zeros, ~cols_with_zeros]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "df = df.loc[:, (df.sum() != 0)]\nresult = df.loc[(df.sum() != 0)]\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "df = df.loc[df.max(axis=1) == 2]\ndf = df.loc[df.max(axis=0) == 2]\nresult = df\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "\n# Create a mask where the maximum value in each row is 2\nmask = df.eq(2)\n\n# Apply the mask to the dataframe\nresult = df.where(mask)\n\nprint(result)\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=[True, False])\nresult", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.reset_index().sort_values(by=['index', 0]).reset_index(drop=True)\ndf['index'] = df['index'].astype(str)\ndf['index'] = df['index'].str.replace('tf', '')\ndf['index'] = df['index'].str.replace('p', '')\ndf['index'] = df['index'].str.replace('1', '')\ndf['index'] = df['index'].str.replace('0', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('-', '')\ndf['index'] = df['index'].str.replace('=', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('?', '')\ndf['index'] = df['index'].str.replace('(', '')\ndf['index'] = df['index'].str.replace(')', '')\ndf['index'] = df['index'].str.replace('\\'', '')\ndf['index'] = df['index'].str.replace(':', '')\ndf['index'] = df['index'].str.replace(';', '')\ndf['index'] = df['index'].str.replace(',', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('-', '')\ndf['index'] = df['index'].str.replace('=', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('?', '')\ndf['index'] = df['index'].str.replace('(', '')\ndf['index'] = df['index'].str.replace(')', '')\ndf['index'] = df['index'].str.replace('\\'', '')\ndf['index'] = df['index'].str.replace(':', '')\ndf['index'] = df['index'].str.replace(';', '')\ndf['index'] = df['index'].str.replace(',', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('-', '')\ndf['index'] = df['index'].str.replace('=', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('?', '')\ndf['index'] = df['index'].str.replace('(', '')\ndf['index'] = df['index'].str.replace(')', '')\ndf['index'] = df['index'].str.replace('\\'', '')\ndf['index'] = df['index'].str.replace(':', '')\ndf['index'] = df['index'].str.replace(';', '')\ndf['index'] = df['index'].str.replace(',', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('-', '')\ndf['index'] = df['index'].str.replace('=', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('?', '')\ndf['index'] = df['index'].str.replace('(', '')\ndf['index'] = df['index'].str.replace(')', '')\ndf['index'] = df['index'].str.replace('\\'', '')\ndf['index'] = df['index'].str.replace(':', '')\ndf['index'] = df['index'].str.replace(';', '')\ndf['index'] = df['index'].str.replace(',', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('-', '')", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "\nresult = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str) == df['A']]\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt']).drop(columns=['count'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count\nresult = result[result['count'] == max_counts['count']].reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "\n# First, find the maximum count in each group\nmax_count = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum count dataframe\nresult = df.join(max_count.set_index(['Sp','Mt'])['count'], on=['Sp','Mt'])\n\n# Finally, drop the maximum count column\nresult = result.drop('count', axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\n# First, find the minimum count for each group\nmin_counts = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\n\n# Then, join the original dataframe with the minimum counts\nresult = df.join(min_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt']).drop(columns=['count'])\n\n# Finally, reset the index and drop the original index\nresult = result.reset_index().drop(columns='level_1')\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp','Value'])['count'], on=['Sp','Value'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count\nresult = result[result['count'] == result['count'].max()]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(np.nan)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'].fillna(pd.to_datetime('17/8/1926'), inplace=True)\ndf", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    df['Date'] = df['Member'].map(dict)\n    ### END SOLUTION\n    return df\n\nresult = f(example_dict, example_df)\nprint(result)\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).Date.transform('count')\ndf", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month'), 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year.rename('year'), 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby('Val').Val.transform('count')\ndf", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.month.rename('month')]).Date.transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year.rename('year')).Date.transform('count')\ndf['Count_w'] = df.groupby([df['Date'].dt.year.rename('year'), df['Date'].dt.week.rename('week')]).Date.transform('count')\ndf['Count_Val'] = df.groupby('Val').Val.transform('count')\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "\n# Create a new column 'Date' with the date format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create a new column 'Date_B' with the date format for B\ndf['Date_B'] = df['Date'].dt.strftime('%d.%m.%Y')\n\n# Create a new column 'Date_C' with the date format for C\ndf['Date_C'] = df['Date'].dt.strftime('%d.%m.%Y')\n\n# Create a new column 'B_zero' with the count of zero values in column B\ndf['B_zero'] = df['B'].apply(lambda x: x == 0)\n\n# Create a new column 'C_zero' with the count of zero values in column C\ndf['C_zero'] = df['C'].apply(lambda x: x == 0)\n\n# Create a new column 'B_non_zero' with the count of non-zero values in column B\ndf['B_non_zero'] = df['B'].apply(lambda x: x != 0)\n\n# Create a new column 'C_non_zero' with the count of non-zero values in column C\ndf['C_non_zero'] = df['C'].apply(lambda x: x != 0)\n\n# Print the result\nprint(df[['Date_B', 'B_zero', 'C_zero']])\nprint(df[['Date_B', 'B_non_zero', 'C_non_zero']])\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "\n# Create a new column 'Date' with the date format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create a new column 'Even' with the count of even numbers\ndf['Even'] = df['B'].apply(lambda x: x % 2 == 0)\n\n# Create a new column 'Odd' with the count of odd numbers\ndf['Odd'] = df['B'].apply(lambda x: x % 2 != 0)\n\n# Group by 'Date' and count the even and odd numbers\nresult1 = df.groupby('Date')['Even'].sum().reset_index()\nresult2 = df.groupby('Date')['Odd'].sum().reset_index()\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.mean)\nresult", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult['min'] = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.min)\n", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda x: x['var2'].str.split(',').str[0].repeat(x.shape[0]))\n\nresult = result.compute()\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split(',').str[0].repeat(df['var2'].str.len())).compute()\n\nresult", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\nresult = df.assign(var2=lambda df: df['var2'].str.split('-').str[0].repeat(df['var2'].str.len())).compute()\nresult", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf['row'] = df['row'].str.split(' ', 2, expand=True)[0]\ndf", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndf = df.set_index('Name').replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df.replace(0, np.nan)\ndf = df", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndf['2001'] = df['2001'].replace(0, np.nan)\ndf['2002'] = df['2002'].replace(0, np.nan)\ndf['2003'] = df['2003'].replace(0, np.nan)\ndf['2004'] = df['2004'].replace(0, np.nan)\ndf['2005'] = df['2005'].replace(0, np.nan)\ndf['2006'] = df['2006'].replace(0, np.nan)\n\ndf['2001'] = df['2001'].replace(np.nan, df['2001'].mean())\ndf['2002'] = df['2002'].replace(np.nan, df['2002'].mean())\ndf['2003'] = df['2003'].replace(np.nan, df['2003'].mean())\ndf['2004'] = df['2004'].replace(np.nan, df['2004'].mean())\ndf['2005'] = df['2005'].replace(np.nan, df['2005'].mean())\ndf['2006'] = df['2006'].replace(np.nan, df['2006'].mean())\n\ndf = df.cumsum()\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n\ndf = df.replace(0, df.mean())\n\ndf = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "\n    df = df.set_index('Name')\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.replace(0, 1)\n    df = df.multiply(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.add(1)\n    df = df.", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndf['2001'] = df['2001'].replace(0, 1).cumprod() / df['2001'].replace(0, 1).cumprod()\ndf['2002'] = df['2002'].replace(0, 1).cumprod() / df['2002'].replace(0, 1).cumprod()\ndf['2003'] = df['2003'].replace(0, 1).cumprod() / df['2003'].replace(0, 1).cumprod()\ndf['2004'] = df['2004'].replace(0, 1).cumprod() / df['2004'].replace(0, 1).cumprod()\ndf['2005'] = df['2005'].replace(0, 1).cumprod() / df['2005'].replace(0, 1).cumprod()\ndf['2006'] = df['2006'].replace(0, 1).cumprod() / df['2006'].replace(0, 1).cumprod()\n\ndf", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)).apply(lambda x: 1 if x > 0 else 0)\ndf['label'] = df['label'].replace({0: 0, 1: 1})\ndf", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf.loc[0, 'label'] = 1\ndf['label'] = df['label'].replace({-1: -1, 0: 0, 1: 1})\ndf\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: -1 if x < 0 else (1 if x > 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_timedelta(df['departure_time'] - df['arrival_time'])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = pd.to_timedelta(df['Duration'])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[1])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[2])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[3])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[4])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[5])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[6])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[7])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[8])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[9])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[10])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[11])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[12])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[13])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[14])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[15])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[16])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[17])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[18])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[19])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[20])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[21])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[22])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[23])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[24])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[25])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[26])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[27])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[28])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[29])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[30])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[31])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[32])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[33])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[34])\ndf['Duration'] = df['Duration'].apply(lambda x", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'] - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.departure_time - df.arrival_time\ndf['Duration'] = df['Duration'].apply(lambda x: x.seconds)\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df[df['key2'].str.endswith('e')].groupby('key1').size()\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.idxmax()\nmin_result = df.idxmin()\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.mode(axis=0)[0][0]\nmedian_result = df.median()[0]\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "\ndf = df[(df['closing_price'] < 100) | (df['closing_price'] > 101)]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "df1 = df.groupby(\"item\", as_index=False)[\"diff\"].min()\nresult = df[df.groupby(\"item\")[\"diff\"].transform(\"min\") == df[\"diff\"]]", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n\n# Calculate the number of NaN values to be filled\nfill_count = nan_count // 2\n\n# Create a mask for the first half of the NaN values\nmask = df['Column_x'].isna()\n\n# Fill the first half of the NaN values with 0\ndf.loc[mask[:fill_count], 'Column_x'] = 0\n\n# Create a mask for the second half of the NaN values\nmask = df['Column_x'].isna()\n\n# Fill the second half of the NaN values with 1\ndf.loc[mask[fill_count:], 'Column_x'] = 1\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n\n# Calculate the number of values to replace\nreplace_count = int(nan_count * 0.3)\n\n# Create a mask for the first 30% of NaN values\nmask = df['Column_x'].isna()\n\n# Create a mask for the middle 30% of NaN values\nmask[nan_count - replace_count:nan_count] = False\n\n# Create a mask for the last 30% of NaN values\nmask[nan_count:] = False\n\n# Replace the NaN values with the corresponding values\ndf['Column_x'] = df['Column_x'].where(mask)\n\n# Fill the NaN values with the mode of the column\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0])\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# First, we create a mask where the values are not NaN\nmask = df['Column_x'].notna()\n\n# Then, we calculate the number of 0 and 1 values in the column\nnum_zero = df['Column_x'][mask].sum()\nnum_one = df['Column_x'].shape[0] - num_zero\n\n# We calculate the number of 0 and 1 values we want to replace with\nnum_zero_want = num_zero // 2\nnum_one_want = num_one // 2\n\n# We create a new column with the desired values\ndf['Column_x'][mask] = np.where(df['Column_x'][mask] == 0, 0, 1)\n\n# We replace the NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df['Column_x'][mask].replace([np.nan], [0, 1])\n\n# We replace the remaining NaN values with the desired values\ndf['Column_x'][mask] = df", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame(list(zip(a['one'], b['one'])), columns=['one', 'two'])\nresult['two'] = list(zip(a['two'], b['two']))\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.concat([a, b, c], axis=1).groupby(level=0, axis=1).apply(lambda x: x.values.tolist())\nresult = pd.DataFrame(result, columns=['one', 'two'])\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\n# Create a new dataframe with the same columns as a and b\nresult = pd.DataFrame(columns=['one', 'two'])\n\n# Iterate over the rows of a and b\nfor i in range(len(a)):\n    # Create a tuple from the corresponding elements in a and b\n    tuple_elements = (a.iloc[i, 0], b.iloc[i, 0])\n    # Append the tuple to the result dataframe\n    result = result.append(pd.Series(tuple_elements, index=['one', 'two']), ignore_index=True)\n\n# Fill the vacancy with np.nan\nresult = result.fillna(np.nan)\n\nprint(result)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "\ndf['text'] = df['text'].apply(lambda x: ', '.join(x.split(', ')))\nresult = df.groupby(df.index).apply(lambda x: ', '.join(x['text']))\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "\nresult = df['text'].apply(lambda x: '-'.join(x.split('-'))).str.strip()\nresult = pd.DataFrame({'text': [result]})\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "\nresult = df['text'].apply(lambda x: ', '.join(x.split(', '))).reset_index(drop=True)\nresult['text'] = result['text'].str.strip()\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = pd.Series(df['text'].values.flatten(), index=df['text'])\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].str.cat(sep='-')\nresult = pd.Series(result, name='text')\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.merge(df1, df2, on=['id', 'city', 'district'], how='outer')\nresult['date'] = result['date_x']\nresult = result.drop(['date_x'], axis=1)\nresult.columns = ['id', 'city', 'district', 'date', 'value']\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "df2['date'] = df2['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\nresult = pd.merge(df1, df2, on=['id', 'city', 'district'], how='left').sort_values(['id', 'date'])\nresult", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "df2['city'] = df2['id'].map(df1.set_index('id')['city'])\ndf2['district'] = df2['id'].map(df1.set_index('id')['district'])\ndf2['date'] = df2['id'].map(df1.set_index('id')['date'])\ndf2 = df2.sort_values(['id', 'date'])\nresult = pd.concat([df1, df2], axis=0)\nresult", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer').fillna(D['B'].values)", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer').fillna(D['B'])", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "\n# Create a new column 'duplicated' in C that indicates if the values in 'A' are the same in C and D\nC['duplicated'] = C['A'].apply(lambda x: x in D['A'].values)\n\n# Merge C and D on 'A' and keep the duplicated column\nresult = pd.merge(C, D, on='A', how='outer').fillna(False)\n\n# Set the 'duplicated' column to boolean\nresult['duplicated'] = result['duplicated'].astype(bool)\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in C to the corresponding value in D\nC['B'] = D['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n# Set the 'B' column in D to the corresponding value in C\nD['B'] = C['B']\n\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "\nresult = df.groupby('user')[['time', 'amount']].apply(list).reset_index()\nresult['time'] = result['time'].apply(lambda x: sorted(x, key=lambda y: y[1]))\nresult['amount'] = result['amount'].apply(lambda x: sorted(x, key=lambda y: y[1]))\nresult['time'] = result['time'].apply(lambda x: [i[0] for i in x])\nresult['amount'] = result['amount'].apply(lambda x: [i[0] for i in x])\nresult = result[['user', 'time', 'amount']]\nresult\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "\ndf['amount-time'] = list(zip(df['time'], df['amount']))\nresult = df.groupby('user')['amount-time'].apply(list).reset_index()\nresult.columns = ['user', 'amount-time']\nresult['amount-time'] = result['amount-time'].apply(lambda x: [tuple(i) for i in x])\nresult\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "\ndf['amount-time'] = list(zip(df['time'], df['amount']))\nresult = df.groupby('user')['amount-time'].apply(list).reset_index()\nresult.columns = ['user', 'amount-time']\nresult['amount-time'] = result['amount-time'].apply(lambda x: [tuple(i) for i in x])\nresult\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\ndf = pd.DataFrame(series.values.reshape(-1, 1), index=series.index, columns=['column'])\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'file'})\ndf = df.set_index(['file'])\ndf_concatenated = df.transpose()\ndf_concatenated = df_concatenated.reset_index()\ndf_concatenated = df_concatenated.rename(columns={'index': 'file'})\ndf_concatenated = df_concatenated.set_index(['file'])\ndf_concatenated = df_concatenated.transpose()\ndf_concatenated", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\ndf = pd.DataFrame(series.tolist(), index=series.index).reset_index().rename(columns={'index': 'name'})\ndf['0'] = df['0'].apply(lambda x: x[0])\ndf['1'] = df['1'].apply(lambda x: x[1])\ndf['2'] = df['2'].apply(lambda x: x[2])\ndf['3'] = df['3'].apply(lambda x: x[3])\ndf = df[['name', '0', '1', '2', '3']]\ndf", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "\n# Find the column name that contains the string 'spike'\nfor col in df.columns:\n    if s in col:\n        result = col\n        break\n\n# Access the column with df[result]\ndf[result]", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "\n# Find the column names that contain the string 'spike'\ncolumns = [col for col in df.columns if s in col]\n\n# If there are columns that contain 'spike', select the first one\nif columns:\n    result = df[columns[0]]\nelse:\n    result = None\n\nprint(result)\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# First, we need to flatten the list of codes into a single series\nflattened_codes = df['codes'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a new dataframe with the flattened codes as columns\ndf_result = pd.DataFrame(flattened_codes.tolist(), columns=range(len(flattened_codes)))\n\n# Finally, we fill the NaN values with 0\ndf_result.fillna(0, inplace=True)\n\nresult = df_result\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# First, we need to flatten the list of codes into a single series\nflattened_codes = df['codes'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a new dataframe with the flattened codes as columns\ndf_result = pd.DataFrame(flattened_codes.tolist(), columns=range(1, len(flattened_codes) + 1))\n\n# Finally, we fill the NaN values with 0\ndf_result.fillna(0, inplace=True)\n\nresult = df_result\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# First, we need to flatten the list of codes into a single series\nflattened_codes = df['codes'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a new dataframe with the flattened codes as columns\ndf_result = pd.DataFrame(flattened_codes.tolist(), columns=['code_' + str(i) for i in range(1, len(flattened_codes) + 1)])\n\n# Finally, we fill the NaN values with 0\ndf_result.fillna(0, inplace=True)\n\nresult = df_result\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\nimport ast\n\ndf['col1'] = df['col1'].apply(lambda x: [ast.literal_eval(i) for i in x])\nresult = df['col1'].flatten().tolist()\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\nimport numpy as np\n\ndf['col1'] = df['col1'].apply(lambda x: ','.join(map(str, np.flip(x))))\nresult = df['col1'].str.cat(sep=',')\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\nimport ast\n\ndf['col1'] = df['col1'].apply(lambda x: ', '.join(map(str, x)))\nresult = df['col1'].values[0]\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "df['Time'] = df['Time'].apply(lambda x: x.floor('2min'))\ndf.groupby('Time').mean()\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "df['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit='m'))\ndf = df.groupby('Time').sum().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x.floor('3min'))\ndf = df.groupby('Time').mean().reset_index()\ndf['Time'] = df['Time'].apply(lambda x: x + pd.to_timedelta(3, unit=", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\")\ndf", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\ndf", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\")\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\ndf", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[df.index.get_level_values('a') != 2]\nresult = result[result.index.get_level_values('b') != 2]\nresult = result[result.index.get_level_values('a').isin([1,3])]\nresult = result[result.c < 7]\n", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[~filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[df.isnull().all(axis=1)]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.iloc[0].eq(df.iloc[8]).where(lambda x: x.ne(True), '').index.tolist()\n", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[df.iloc[0] != df.iloc[8]].tolist()", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(x, y) for x, y in zip(df.iloc[0], df.iloc[8]) if x != y or (x == np.nan and y != np.nan) or (x != np.nan and y == np.nan)]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.stack().reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'A_{i}' for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_1', 'level_2']).unstack(level=1).reset_index()\ndf.columns = [f'{c}_{i}' for c in df.columns for i in range(1, len(df.columns) + 1)]\ndf = df.set_index(['level_", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.stack().reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "\nresult = df.sort_index(level='time')\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "\nresult = df.sort_values(['time', 'VIM'])\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "df = df[(df.index.date != '2020-02-17') & (df.index.date != '2020-02-18')]\nresult = df\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "\n# Create a new column 'Day' to store the day of the week\ndf['Day'] = df.index.strftime('%A')\n\n# Filter the dataframe to keep only the rows where the day is not 'Friday'\nresult = df[df['Day'] != 'Friday']\n\n# Set the index back to the original date\nresult.reset_index(inplace=True)\n\n# Convert the 'Date' column to datetime format\nresult['Date'] = pd.to_datetime(result['Date'])\n\n# Set the 'Date' column as the index\nresult.set_index('Date', inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "\n# Create a mask for the correlation matrix where the value is above 0.3\nmask = (corr > 0.3).stack()\n\n# Create a new DataFrame where the mask is True\nresult = corr.mask(mask)\n\n# Set the value of the mask to NaN\nresult.mask(mask, np.nan)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "\nresult = corr.stack().reset_index()\nresult.columns = ['col1', 'col2', 'value']\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.rename(columns={df.columns[0]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\n# Create a new column 'frequent' that indicates the most frequent value in each row\ndf['frequent'] = df.apply(lambda row: max(set(row.values), key=list(row.values).count), axis=1)\n\n# Create a new column 'freq_count' that counts the number of times each value appears in the row\ndf['freq_count'] = df.apply(lambda row: list(row.values).count(row['frequent']), axis=1)\n\ndf", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\n# Create a new column 'frequent' and 'freq_count'\ndf['frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n\ndf", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\n# Create a new column 'frequent'\ndf['frequent'] = df.apply(lambda row: list(row[row == 1]), axis=1)\n\n# Create a new column 'freq_count'\ndf['freq_count'] = df['frequent'].apply(lambda x: len(x))\n\ndf", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "df['bar'] = df['bar'].replace('NULL', np.nan)\ndf['bar'] = df['bar'].astype(float)\ndf['bar'] = df['bar'].fillna(df['bar'].mean())\ndf['bar'] = df['bar'].astype(int)\n\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\n", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "df['bar'] = df['bar'].replace('NULL', 0)\ndf['bar'] = df['bar'].astype(float)\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nresult['bar'] = df.groupby(['id1', 'id2'])['bar'].transform('mean')\nresult['bar'] = result['bar'].replace('NULL', 0)\nresult", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = df_c[['EntityNum', 'foo', 'a_col']]\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='outer')\ndf_c = df_c.drop(['a_col', 'b_col'], axis=1)\nresult = df_c\n", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[np.logical_not(np.isnan(x))]", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = x.tolist()\nresult = [i for i in result if i is not np.nan]\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.eye(max(a), dtype=int)[a-1]", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((len(a), 4))\nb[:, a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((a.size, a.size))\nb[np.arange(a.size), a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nb[a[:, 0] - 1, a[:, 1] - 1] = 1", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=1)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=0)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape)\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": "\n    max_value = np.max(a)\n    max_indices = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    return max_value, max_indices\n    ### END SOLUTION\n\nresult = f(example_a)\nprint(result)\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2)[-1], a.shape)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "import numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\n\nmask = np.any(np.isnan(a), axis=0)\na = a[:, ~mask]\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "import numpy as np\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \nresult = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "result = np.unravel_index(a.argmin(), a.shape)\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape)\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.deg2rad(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "cos_value = np.cos(np.deg2rad(degree))\nresult = cos_value\nprint(result)\n", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.degrees(np.arcsin(value))\nprint(result)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "result = np.pad(A, (0, length - A.size), 'constant')", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "result = np.pad(A, (0, length - A.size), 'constant')", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "print(np.power(a, power))\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nresult = f(example_a, 5)\nprint(result)\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), 1)\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.array([numerator, denominator]).astype(int) / np.gcd.reduce(np.array([numerator, denominator]))\n    return tuple(result)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = (numerator / denominator, denominator / numerator) if denominator != 0 else (np.nan, np.nan)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.where(a > b, a, b)\nresult = np.where(result > c, result, c)\nprint(result)\n", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.flatten()[-1::-1])", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a.flatten()[-1::-1])", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.diag(a.flatten())[::-1]\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.diag(np.fliplr(a).flatten())\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": "\n    result = [i for sublist in X for i in sublist]\n    ### END SOLUTION\n    return result\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\nresult = list(result)\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "import numpy as np\n\ndef get_maximal_independent_vectors(a):\n    # Get the transpose of the matrix\n    a_t = a.T\n\n    # Compute the rank of the matrix\n    rank = np.linalg.matrix_rank(a_t)\n\n    # If the rank is less than the number of columns in the matrix,\n    # then the matrix is not full rank, so there are more independent vectors\n    if rank < a.shape[1]:\n        # Compute the null space of the matrix\n        null_space = np.linalg.nullspace(a_t)\n\n        # Return the null space as a set of independent vectors\n        return null_space\n\n    # If the rank is equal to the number of columns in the matrix,\n    # then the matrix is full rank, so there is only one independent vector\n    else:\n        # Return the identity matrix as a set of independent vectors\n        return np.identity(a.shape[1])\n\nresult = get_maximal_independent_vectors(a)\nprint(result)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]\n", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the standard deviation\nstd_dev = np.sqrt(np.sum((amean - bmean)**2) / (anobs + bnobs - 2))\n\n# calculate the t-statistic\nt_statistic = (amean - bmean) / std_dev\n\n# calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_statistic), anobs + bnobs - 2))\n\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A.view(np.int64), B.view(np.int64))]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.setdiff1d(np.setdiff1d(A.flatten(), B.flatten()), B.flatten())\noutput = output.reshape(output.shape[0], -1)\noutput = output.reshape(-1, 1)\noutput = np.concatenate((output, B), axis=1)\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)[:, ::-1]", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\nprint(result)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": "\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION\n    print(a)\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.array_equal(a[0], a[1:]), axis=1)", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.diff(a, axis=1) == 0, axis=1)", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.array_equal(a[0], a[1]))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.trapz(np.trapz(z, y, axis=0), x)", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": "\n    return (np.cos(x))**4 + (np.sin(y))**2\n\ndef integral(x, y):\n    # calculate the integral using Simpson's rule\n    # result = integral(x, y)\n    ### BEGIN SOLUTION\n    return np.sum(np.array([[np.trapz(f(x, y), y) for y in example_y] for x in example_x]))\n\nprint(integral(example_x, example_y))\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(data):\n    # sort the data\n    x = np.sort(data)\n    # calculate the number of data points\n    n = x.shape[0]\n    # calculate the x values\n    x_bar = np.ones(n) / n\n    # calculate the y values\n    y = np.arange(1, n + 1) / n\n    # calculate the cumulative distribution function\n    ecdf_data = np.cumsum(x) / np.sum(x)\n    # return the ecdf\n    return ecdf_data\n\nresult = ecdf(grades)\nprint(result)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(data):\n    # normalize data to sum to 1\n    data = data / np.sum(data)\n    # compute the cumulative sum of the normalized data\n    cumulative_sum = np.cumsum(data)\n    return cumulative_sum\n\nresult = ecdf(grades)\nresult(eval)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\n# compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow, high = 0, 1\nwhile ecdf(low) < threshold:\n  low += 0.01\nwhile ecdf(high) > threshold:\n  high -= 0.01\n\nprint(low, high)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.choice([0, 1], size=size, p=[one_ratio, 1-one_ratio])", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = a.argsort()[::-1]\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\nresult = result.tolist()\nresult", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[-N:][::-1]\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "\ndef extract_patches(a, patch_size):\n    h, w = a.shape\n    patches = []\n    for i in range(0, h - patch_size[0] + 1, patch_size[0]):\n        for j in range(0, w - patch_size[1] + 1, patch_size[1]):\n            patches.append(a[i:i + patch_size[0], j:j + patch_size[1]])\n    return patches\n\npatch_size = (2, 2)\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "\ndef sliding_window(a, window_size):\n    shape = a.shape\n    strides = a.strides\n    result = []\n    for i in range(shape[0] - window_size + 1):\n        for j in range(shape[1] - window_size + 1):\n            result.append(a[i:i+window_size, j:j+window_size])\n    return np.array(result)\n\nresult = sliding_window(a, 2)\nprint(result)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "\ndef extract_patches(a, patch_size):\n    h, w = a.shape\n    patches = []\n    for i in range(0, h - patch_size[0] + 1, patch_size[0]):\n        for j in range(0, w - patch_size[1] + 1, patch_size[1]):\n            patches.append(a[i:i + patch_size[0], j:j + patch_size[1]])\n    return patches\n\npatch_size = (2, 2)\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "\n# calculate the number of patches\nnum_patches_x = a.shape[0] // patch_size\nnum_patches_y = a.shape[1] // patch_size\n\n# calculate the shape of the result\nresult_shape = (num_patches_x, num_patches_y, patch_size, patch_size)\n\n# extract patches\nresult = a[:result_shape[0]*patch_size, :result_shape[1]*patch_size].reshape(result_shape)\n\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.pad(a, ((0, h - a.shape[0]), (0, w - a.shape[1])), mode='constant')\nresult = result.reshape((h, w))\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "\n# calculate the number of patches\nnum_patches_x = a.shape[0] // patch_size\nnum_patches_y = a.shape[1] // patch_size\n\n# calculate the shape of the result\nresult_shape = (num_patches_x, num_patches_y, patch_size, patch_size)\n\n# extract patches\nresult = a[:result_shape[0]*patch_size, :result_shape[1]*patch_size].reshape(result_shape)\n\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "import numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(string)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.random.uniform(min, max, n)\nresult = np.log(result)\nresult = np.exp(result)\nresult", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.random.uniform(np.log(min), np.log(max), n)", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.random.loguniform(min, max, n)\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.array([])\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = np.sub2ind(a.shape, index)\nprint(result)\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\ndf", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.array([np.sum(a[accmap==i]) for i in range(a.size)])", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = a[index]\nresult = np.max(result)\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.cumsum(a[accmap >= 0])", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = a[index]\nresult = np.min(result)\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.vectorize(elementwise_function)(x, y)\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, p=probabilit)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "result = np.pad(a, ((low_index, high_index), (low_index, high_index)), 'constant')[low_index:, low_index:]", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.iscomplex(x)]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, len(data) // bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, len(data) // bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.lib.arraysetops.partition(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = [np.mean(bin_data[i]) for i in range(len(bin_data))]\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    t = (x - x_min) / (x_max - x_min)\n    t = np.clip(t, 0, 1)\n    return t * t * (3 - 2 * t)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0.0, 1.0)\n    t = t * t * (3.0 - 2.0 * t)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t * (t - 1.0)\n    t = t * t *", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\n# First, we need to create a circular version of b\nb_circular = np.concatenate((b[-2:], b, b[:2]))\n\n# Then, we can perform the circular correlation\nresult = np.correlate(a, b_circular, mode='full')\n\n# Finally, we need to remove the extra elements from the result\nresult = result[:len(a)]\n\nprint(result)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.groupby(['major', 'timestamp']).apply(lambda x: x.values.reshape(1, 15, 5)).values", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = df.groupby('major').apply(lambda x: x.values.reshape(1, 4, 5)).values", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.array([list(np.unpackbits(np.uint8(num))) for num in a])\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.array([list(np.binary_repr(num, width=m)) for num in a])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[int(i) for i in j] for j in result])\nresult = np.array([[", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "result = np.array([[int(i^j) for j in a] for i in range(2**m)])\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\nresult = (\u03bc - 3*\u03c3, \u03bc + 3*\u03c3)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\nresult = (\u03bc - 2*\u03c3, \u03bc + 2*\u03c3)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": "\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    start = mean - 3*std_dev\n    end = mean + 3*std_dev\n    return (start, end)\n    ### END SOLUTION\n\nresult = f(a)\nprint(result)\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\nresult = (a >= lower_bound) & (a <= upper_bound)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.compressed(), percentile)\nprint(prob)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[:, 0] = 0", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.argmax(a, axis=1)\nmask = mask.reshape(-1, 1)\nmask = mask == np.arange(mask.shape[0])[:, None]\nmask = mask.astype(bool)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "import scipy.stats\n\nresult = scipy.stats.pearsonr(post, distance)\nprint(result)\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ij,ij->ij', X, X)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((M, N))\nfor i in range(M):\n    X[i] = np.sum(Y[i], axis=0)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a\n", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = A[~np.isin(A, B)]\nC = A", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[np.in1d(A, B)]", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "mask = np.isin(A, B)\nC = A[mask]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = rankdata(a, method='max') - 1", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.array(len(a) - rankdata(a)) - 1", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    result = rankdata(a, method='max') - 1\n    return result.astype(int)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.linalg.norm(X, ord=1, axis=1)\nresult = X / l1[:, np.newaxis]\nprint(result)", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = LA.norm(X, ord=2, axis=1)\nresult = X / x[:, np.newaxis]\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = X / x[:, np.newaxis]\nprint(result)\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].str.contains(target)\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "import scipy.spatial\n\ndistance_matrix = scipy.spatial.distance.cdist(a, a, metric='euclidean')\nresult = distance_matrix", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "from scipy.spatial.distance import pdist, squareform\n\nresult = squareform(pdist(a, 'euclidean'))\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "import scipy.spatial\n\ndistances = scipy.spatial.distance.cdist(a, a, metric='euclidean')\nresult = distances\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# Remove the adjacent (before removing) duplicate non-zero value and all the zero value\nresult = np.unique(a[a != 0])\n\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# Remove adjacent duplicates\na = np.concatenate(np.where(np.diff(a, prepend=0) != 0)[0]).reshape(-1, 1)\n\n# Remove zero values\na = a[a != 0]\n\nresult = a\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf = df.reset_index(drop=True)\ndf = df.reindex(columns=['lat', 'lon', 'val'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ### END SOLUTION\n    return df\n\ndf = f(lat, lon, val)\nprint(df)\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df.max(axis=1)\ndf = df.reset_index(drop=True)\ndf = df.reindex(columns=['lat', 'lon', 'val', 'maximum'])\ndf", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    shape = a.shape[:-1] + (a.shape[-1] - size[0] + 1, size[1])\n    strides = a.strides[:-1] + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    shape = a.shape[:-1] + (a.shape[-1] - size[0] + 1, size[1])\n    strides = a.strides[:-1] + a.strides[-1]\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:,-1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = any(np.array_equal(c, cnt) for cnt in CNTS)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = any(np.array_equal(c, cnt) for cnt in CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\ndf", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.eye(4, dtype=np.float64)\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[a != 0] = 0\n", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_datetime = pd.to_datetime(start)\nend_datetime = pd.to_datetime(end)\ndelta = end_datetime - start_datetime\nresult = pd.date_range(start=start_datetime, periods=n, freq=delta)\nresult = result.to_series()\nresult", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No such index\")\nelse:\n    print(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))[0]\nif indices.size > 0:\n    result = indices\nelse:\n    result = np.array([])\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "# calculate the coefficients a, b and c using the numpy.polyfit function\nresult = np.polyfit(x, y, 2)\nresult = result[::-1]\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "\n# Create a matrix with the x values as columns\nX = np.column_stack((np.ones(len(x)), np.array(x)))\n\n# Calculate the coefficients using the formula for the least squares solution\ncoefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n\n# Extract the coefficients in the order of highest to lowest\nresult = coefficients[::-1]\n\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = [0, 1, 2, 3]\ndf = df - temp_arr\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr >= -10\narr[mask] = arr[mask] + 5\n\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr[arr < n1] = 0\narr[(arr >= n1) & (arr < n2)] = 30\narr[(arr >= n2)] = 5\n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "print np.nonzero(np.logical_not(np.isclose(s1, s2)))[0].shape[0]\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = all(np.array_equal(a[0], x) for x in a)\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all() for arr in a)", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "\n# Calculate the number of rows and columns needed to match the shape\nrows_needed = shape[0] - a.shape[0]\ncols_needed = shape[1] - a.shape[1]\n\n# Zero pad the array to match the shape\nresult = np.pad(a, ((0, rows_needed), (0, cols_needed)))\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "\n# Calculate the maximum shape\nmax_shape = tuple(np.array(shape) - np.array(a.shape))\n\n# Zero pad the array\nresult = np.pad(a, max_shape, mode='constant')\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "\n# Calculate the number of rows and columns needed to pad the array\nrows_needed = max(shape[0], a.shape[0])\ncols_needed = max(shape[1], a.shape[1])\n\n# Create a new array with the desired shape and fill it with the element\nresult = np.full(shape, element)\n\n# Copy the original array to the new array\nresult[:a.shape[0], :a.shape[1]] = a\n\n# If the original array is larger than the new array, pad the bottom and right with the element\nif a.shape[0] < result.shape[0]:\n    result[-a.shape[0]:, :] = element\nif a.shape[1] < result.shape[1]:\n    result[:, -a.shape[1]:] = element\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # Calculate the difference between the shape of the original array and the shape of the padded array\n    diff = np.array(shape) - np.array(arr.shape)\n    # Use the numpy `numpy.pad` function to pad the array\n    result = np.pad(arr, pad_width=diff, mode='constant', constant_values=0)\n    ### END SOLUTION ###\n    return result\n\n# Test the function with the example array\nprint(f())\n", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "\n# calculate the padding needed\npadding = [(0, max_len - a.shape[0]), (0, max_len - a.shape[1])]\n\n# zero pad the array\nresult = np.pad(a, padding, mode='constant', constant_values=0)\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1]), b])", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[:, :, None]])", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "\nresult = df.loc[df['a'].between(1, 4), 'b'].values\nresult = np.where(result.size > 0, result, np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "\n# Create a mask where the image is not all zeros\nmask = np.any(im, axis=1)\n\n# Create a new array where the mask is True\nresult = im[mask]\n\n# Reshape the result to the desired shape\nresult = result.reshape(-1, 2)\n\nprint(result)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "\n# Find the indices of the non-zero elements\nindices = np.where(A != 0)\n\n# Get the minimum and maximum indices along the first and second axis\nmin_indices = np.min(indices, axis=1)\nmax_indices = np.max(indices, axis=1)\n\n# Truncate the array\nresult = A[min_indices[0]:max_indices[0]+1, min_indices[1]:max_indices[1]+1]\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Apply the mask to the image\nresult = im[mask]\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "\n# Create a mask where the image is not entirely black\nmask = np.any(im != 0, axis=1)\n\n# Apply the mask to the image\nresult = im[mask]\n\n# Reshape the result to the desired shape\nresult = result.reshape(-1, result.shape[-1])\n\nprint(result)\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\nsns.scatterplot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.figure(figsize=(10, 6))\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'r-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'go-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'b*-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'kp-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.figure(figsize=(10, 6))\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'r-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'go-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'b*-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'kp-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'cD-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'm^-')\n\n# draw a line with random y\nplt.plot(x, np.random.randn(10), 'yH-')\n\n# draw a line with random y\nplt.plot(x, np.", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, marker='d', linestyle='-', color='blue')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, marker='d', linestyle='-', linewidth=2, markersize=10)\nplt.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, 'r--')\nplt.fill_between(np.arange(2, 4), x[2:4], alpha=0.5, color='red')\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, color='blue')\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    height=5,\n    aspect=1.5,\n)\nplt.show()\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style(\"whitegrid\")\nsns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize=20)\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.setp(l, color='black')\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), x, 'r-', lw=5, markersize=30)\nplt.plot(range(10), y, 'r-', lw=5, markersize=30)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.array([0, 2, 4, 6, 8, 10])\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc=\"upper right\")\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.imshow(H, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# split the title into words\nwords = myTitle.split(' ')\n# get the longest word\nlongest_word = max(words, key=len)\n# get the number of lines needed\nnum_lines = len(words) // 2\n# create the title with the longest word repeated num_lines times\ntitle = longest_word * num_lines\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = -y\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5])\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.plot(x, y)\nplt.xticks(np.arange(min(x), max(x) + 1, 1))\nplt.yticks(np.arange(min(y), max(y) + 1, 1))\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, p: '{:.0f}'.format(x)))\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax.plot(x, y, linestyle='--')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_title('sin(x)')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)', color='orange')\naxs[1].set_title('cos(x)')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_title('sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_title('cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks([])\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(min(x), max(x) + 1, 1))\nplt.grid(axis='x')\nplt.show()\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 5))\nplt.xticks(np.arange(1, 3))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5)\nplt.grid(axis='x', which='both', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), constrained_layout=True)\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xticks([])\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.gca().xaxis.set_tick_params(labelsize=20)\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks([])\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().tick_params(axis='y')\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\nplt.yticks(np.arange(0, max(y) + 1, 1))\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green', scatter_kws={'color':'blue'})\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green')\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips)\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.ylabel(\"Y\", color=\"red\")\nplt.xticks(color=\"red\")\nplt.yticks(color=\"red\")\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='Y')\nplt.xlabel('X', color='red')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.xticks(x, rotation=90)\nplt.tick_params(axis='both', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "\n# \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u56fe\u5f62\u548c\u8f74\nfig, ax = plt.subplots()\n\n# \u7ed8\u5236\u5782\u76f4\u7ebf\nax.axvline(0.22058956, color='r', linestyle='-')\nax.axvline(0.33088437, color='g', linestyle='-')\nax.axvline(2.20589566, color='b', linestyle='-')\n\n# \u8bbe\u7f6e\u6807\u7b7e\u548c\u6807\u9898\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('\u5782\u76f4\u7ebf\u56fe')\n\n# \u663e\u793a\u56fe\u5f62\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, cmap='hot', interpolation='nearest')\nplt.xticks(range(len(xlabels)), xlabels, rotation=90)\nplt.yticks(range(len(ylabels)), ylabels)\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax.plot(time, temp, \"-\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(x, y, label='Y')\n\n# Set the title and labels for each subplot\naxs[0].set_title('Y')\naxs[1].set_title('Y')\n\n# Set the labels for each subplot\naxs[0].set_xlabel('X')\naxs[0].set_ylabel('Y')\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Y')\n\n# Add a legend to each subplot\naxs[0].legend()\naxs[1].legend()\n\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n                palette=\"deep\", size=\"body_mass_g\", sizes=(30, 300), legend=False)\n\n# add a title to the plot\nplt.title(\"Scatter plot of bill_length_mm and bill_depth_mm\")\n\n# add labels to the x and y axes\nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"bill_depth_mm\")\n\n# add a legend to the plot\nplt.legend(title=\"body_mass_g\")\n\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.figure(figsize=(10, 6))\nplt.scatter(b, a, c=c, cmap='viridis')\nplt.xlabel('b')\nplt.ylabel('a')\nplt.title('Scatter plot of a over b')\nplt.colorbar()\nfor i, j, k in zip(b, a, c):\n    plt.annotate(k, (j, i))\nplt.show()\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend', title_fontsize='14', label_fontsize='12', loc='best')\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, alpha=0.6, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\n# Make the first subplot three times wider than the second subplot but they should have the same height.\naxs[0].plot(x, y, 'r')\naxs[0].set_title('First subplot')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, 'b')\naxs[1].set_title('Second subplot')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n# Make the second subplot three times wider than the first subplot but they should have the same height.\naxs[0].plot(x, y, 'g')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, 'y')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n# Make the third subplot three times wider than the first subplot but they should have the same height.\naxs[0].plot(x, y, 'c')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, 'm')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n# Make the fourth subplot three times wider than the first subplot but they should have the same height.\naxs[0].plot(x, y, 'k')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, 'o')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n# Make the fifth subplot three times wider than the first subplot but they should have the same height.\naxs[0].plot(x, y, 'p')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, '*')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n# Make the sixth subplot three times wider than the first subplot but they should have the same height.\naxs[0].plot(x, y, '+')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, '^')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n# Make the seventh subplot three times wider than the first subplot but they should have the same height.\naxs[0].plot(x, y, 'v')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, '<')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n# Make the eighth subplot three times wider than the first subplot but they should have the same height.\naxs[0].plot(x, y, '>')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, '<')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n# Make the ninth subplot three times wider than the first subplot but they should have the same height.\naxs[0].plot(x, y, '>')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\n\naxs[1].plot(x, y, '<')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\n\n#", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Add a legend to the chart\nplt.legend()\n\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist([x, y], bins=10, histtype='bar', label=['x', 'y'])\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Grouped Histogram of x and y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.plot([a, c], [b, d])\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, axs = plt.subplots(2, 1)\n\n# create a colormap with x and y\ncmap = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_y = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_x = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n#", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[0].plot(a, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].plot(x, z, label='Y')\naxs[1].plot(a, y, label='Z')\naxs[1].set_title('Y and Z')\n\n# Set the labels for the axes\naxs[0].set_xlabel('X')\naxs[0].set_ylabel('Y')\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\n\n# Set the labels for the legend\naxs[0].legend()\naxs[1].legend()\n\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.figure(figsize=(10, 6))\nplt.plot([x[0] for x in points], [x[1] for x in points])\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log scale y-axis')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "plt.figure(figsize=(5, 5))\nfor line in lines:\n    plt.plot(*zip(*line), color=tuple(c[np.argmin([np.linalg.norm(np.array(line[0]) - np.array(line[1])) for line in lines])]))\n\n# Add a title and labels for the axes\nplt.title('Line Segments')\nplt.xlabel('X')\nplt.ylabel('Y')\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o-')\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-Log Plot')\nplt.grid(True)\nplt.xticks(np.arange(0, 1000, 100))\nplt.yticks(np.arange(0, 1000, 100))\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "fig, axs = plt.subplots(4, 1, figsize=(10, 10))\n\nfor i in range(4):\n    axs[i].plot(df.index, df[df.columns[i]])\n    axs[i].scatter(df.index, df[df.columns[i]])\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "\n# Calculate the cumulative sum of the data\ncumulative_sum = np.cumsum(data)\n\n# Normalize the data to sum up to 1\nnormalized_data = data / cumulative_sum[-1]\n\n# Create a histogram of the normalized data\nplt.hist(normalized_data, bins=len(data), edgecolor='black')\n\n# Set the y tick labels as percentage\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.ylabel('Percentage')\n\n# Set the x tick labels as the data values\nplt.xticks(range(1, len(data) + 1))\n\n# Set the x tick labels as the data values\nplt.xlabel('Data Value')\n\n# Set the title of the plot\nplt.title('Normalized Histogram of Data')\n\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, markeredgecolor='red', markerfacecolor='red', alpha=0.5)\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, y, label='y')\naxs[0].legend()\n\naxs[1].plot(x, z, label='z')\naxs[1].plot(a, z, label='z')\naxs[1].legend()\n\nfig.legend()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=False, sharey=False)\n\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['second'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(range(0, 10, 2), [2.1, 3, 7.6])\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\nplt.tight_layout()\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.gca().set_xticks(x, minor=True)\nplt.gca().xaxis.tick_top()\nplt.gca().xaxis.set_minor_locator(plt.MultipleLocator(1))\nplt.gca().yaxis.tick_left()\nplt.gca().yaxis.set_minor_locator(plt.MultipleLocator(1))\nplt.gca().yaxis.set_label_position(\"left\")\nplt.gca().yaxis.set_major_locator(plt.MultipleLocator(1))\nplt.gca().yaxis.set_major_formatter(plt.FormatStrFormatter('%.1f'))\nplt.gca().yaxis.set_minor_formatter(plt.NullFormatter())\nplt.gca().yaxis.set_ticks([-60])\nplt.gca().yaxis.set_label_coords(-0.1, 1.02)\nplt.gca().yaxis.set_tick_params(labelleft=True, labelright=False, left=True, right=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(top=True, bottom=False, topdown=False, bottomup=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='minor', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='major', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='minor', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='major', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='minor', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='major', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='minor', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='major', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='minor', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='major', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='minor', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='major', left=True, right=False, bottom=True, top=False, labelleft=True, labelright=False, labelsize=10)\nplt.gca().yaxis.set_tick_params(which='minor', left=True, right=False, bottom=True, top=False, label", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(x, alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0.05)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0.0, y=0.0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 1)\naxs.plot(x, y, label='y')\naxs.plot(x, x, label='x')\naxs.set_title('Figure')\naxs.legend()\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.figure(figsize=(10, 5))\nplt.plot(df['Type A'], df['Type B'])\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Line Chart of Type A and Type B')\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='||')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolors='none', hatch='|')\nplt.show()\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*', hatch='//')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*^', s=100, hatch='|')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, extent=(1, 5, 1, 4))\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(y, x, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.figure(figsize=(10, 6))\nplt.plot([3, 3], [0, 1], 'k-', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 10))\nplt.pie(height, labels=labels, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedgeprops={'width': 0.4})\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle=':')\nplt.grid(which='major', linestyle='-')\nplt.show()\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.title(\"Activity Distribution\")\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.title(\"Activity Distribution\")\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, '--o', markersize=10, markeredgewidth=2, markeredgecolor='red')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "plt.figure(figsize=(10, 5))\nplt.bar(np.arange(3), blue_bar, color='b', align='center')\nplt.bar(np.arange(3) + 0.2, orange_bar, color='orange', align='center')\nplt.xlim(0, 3.2)\nplt.ylim(0, 30)\nplt.tick_params(axis='both', which='both', length=0)\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "\ng = sns.catplot(x=\"bill_length_mm\", y=\"species\", hue=\"sex\", data=df, height=5, aspect=0.9, kind=\"bar\", sharey=False)\n\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(5, 5))\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontsize=16, fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"center right\", bbox_to_anchor=(0.3, 0.5), frameon=False)\nplt.show()\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc=\"upper right\")\nplt.plot(x, y, marker=\"*\", label=\"Line\")\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "fig, ax = plt.subplots()\n\n# plot the data\nax.matshow(data, aspect='auto')\n\n# create a colorbar\ncbar = plt.colorbar(plt.cm.ScalarMappable(cmap='jet', norm=plt.Normalize(np.min(data), np.max(data))))\n\n# add the colorbar to the plot\nfig.colorbar(cbar)\n\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(1)\nplt.plot(x, y)\nplt.title(\"Figure 1\", fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.gca().invert_xaxis()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n\n# Create a scatter plot\nplt.scatter(x, y, c='blue')\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolors='black')\nplt.show()\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_title('Subplot')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 6))\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 1], 'w-')\nplt.plot([0, 1], [0, 0], 'w-')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color='r')\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(a, z, label='Z')\naxs[0].set_title('Y')\naxs[1].set_title('Z')\naxs[1].set_position_above()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1)\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nsns.heatmap(df, cbar=False, cmap=\"viridis\", bbox=[0, 0, 1, 1])\nplt.show()\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(x)\nplt.yticks(y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.xticks([])\nplt.ylabel('Y')\nplt.yticks([])\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "plt.figure(figsize=(10, 6))\n\n# Create a scatter plot for \"time\" vs \"pulse\" with \"kind\" as hue and \"diet\" as col\nsns.scatterplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\n\n# Set the title of the plot\nplt.title(\"Group: Fat vs No Fat\")\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "plt.figure(figsize=(10, 6))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.9, kind=\"point\")\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Scatter Plot of Exercise Time vs Pulse by Kind and Diet\")\nplt.show()\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "plt.figure(figsize=(10, 4))\n\n# First subplot\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Time vs Pulse\")\n\n# Second subplot\nplt.subplot(1, 2, 2)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Time vs Pulse\")\nplt.ylabel(\"Pulse\")\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='upper right', bbox_to_anchor=(1.05, 1), borderaxespad=0)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "fig, ax = plt.subplots()\nax.plot(t, a, label='sin(t)')\nax.plot(t, b, label='cos(t)')\nax.plot(t, c, label='sin(t) + cos(t)')\nax.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=0.2, edgecolor=\"black\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\ng = sns.FaceGrid(axes, marginals=[axes], fig=fig)\n\nfor ax, b in zip(g.axes.flat, df[\"b\"].unique()):\n    ax.plot(df[\"a\"], df[df[\"b\"] == b][\"c\"], label=b)\n    ax.set_xticks(np.arange(1, 31, 2))\n    ax.set_xticklabels(np.arange(1, 31, 2))\n\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azim=100, elev=50)\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[0, 1], height_ratios=[0, 1])\naxes = []\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.axis('off')\n    axes.append(ax)\n\n# Plot the image in each subplot\nfor ax, x_ in zip(axes, x.flatten()):\n    ax.imshow(x_)\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nprint(x.numpy())", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\n# Convert the labels to one-hot encoding\none_hot = tf.one_hot(labels, depth=10, dtype=tf.int32)\n\n# Calculate the sum of each row to get the total count of each class\ntotal_count = tf.reduce_sum(one_hot, axis=0)\n\n# Create a tensor with the total count of each class\nresult = tf.constant([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]) + tf.constant([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]) + tf.constant([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]) + tf.constant([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]) + tf.constant([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\n# Convert the labels to one-hot encoding\none_hot = tf.one_hot(labels, depth=10, dtype=tf.int32)\n\n# Flatten the one-hot encoding\nflattened = tf.reshape(one_hot, [-1, 10])\n\n# Convert the flattened one-hot encoding to a tensor\nresult = tf.cast(flattened, tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "\n# Convert the list to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a tensor of zeros with the same shape as the labels tensor\nresult = tf.zeros_like(labels_tensor)\n\n# Set the values of the result tensor to the corresponding values in the labels tensor\nresult = tf.tensor_scatter_nd_update(result, tf.expand_dims(labels_tensor, 1), 1)\n\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": "\n    result = tf.one_hot(tf.constant(labels), depth=10, dtype=tf.int32)\n    ### END SOLUTION\n    return result.numpy()\n\nprint(f())\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\n# Convert the labels to tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a tensor of ones with the same shape as the labels tensor\nones_tensor = tf.ones_like(labels_tensor)\n\n# Create a tensor of zeros with the same shape as the labels tensor\nzeros_tensor = tf.zeros_like(labels_tensor)\n\n# Create a tensor of ones with the same shape as the labels tensor\nresult = tf.where(labels_tensor == 0, ones_tensor, zeros_tensor)\n\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.concat([[i], [i+1], [i+2]], axis=0)\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return [i for input_value in input for i in input_value]\n    ### END SOLUTION ###\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(f)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding = tf.ones_like(lengths) - 1\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\npadding_zeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_padding_ones = tf.ones_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding_padding_padding_padding_padding_padding_padding_padding_padding_padding_", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n\n# Convert the mask to a tensor of 1s and 0s\nmask = tf.cast(mask, tf.float32)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(mask)\n\n# Subtract the mask from the ones tensor\nresult = ones - mask\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": "\n    max_length = tf.constant(8)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\n    mask = tf.cast(mask, tf.float32)\n    return mask\n\nresult = f(example_lengths)\nprint(result)\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([tf.repeat(a, len(b))]*len(a))\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [4, 2])\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "\n    return tf.stack([tf.repeat(a, len(b))]*len(a))\n\nresult = f(example_a, example_b)\nprint(result)\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "result = tf.squeeze(a, [2])", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.expand_dims(a, axis=2)\nresult = tf.reshape(result, (50, 100, 1, 512))\nprint(result)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "result = tf.expand_dims(a, axis=1)\nresult = tf.expand_dims(result, axis=2)\nresult = tf.reshape(result, (1, 50, 100, 1, 512))\nresult = tf.squeeze(result)\nprint(result)\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\nresult = result.numpy()\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\n# Calculate the L2 distance element-wise\ndifference = tf.square(tf.sub(a, b))\n\n# Calculate the sum of the squares\nsum_of_squares = tf.reduce_sum(difference, axis=0)\n\n# Calculate the L2 distance\nl2_distance = tf.sqrt(sum_of_squares)\n\n# Print the result\nprint(l2_distance)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "distances = tf.square(a - b)\nresult = tf.reduce_sum(distances, axis=0)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    return tf.sqrt(tf.reduce_sum(tf.square(A - B), axis=0))\n\nresult = f(example_a, example_b)\nprint(result)\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "\nresult = tf.gather_nd(x, tf.stack([y, z], axis=1))\n\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = tf.gather_nd(x, tf.stack([row, col], axis=1))\nresult = m.numpy()\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "\n    return tf.gather_nd(x, tf.stack([y, z], axis=1))\n\nresult = f(example_x, example_y, example_z)\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\nresult = tf.einsum('ijk,ilk->ijl', A, B)\nresult = tf.reduce_sum(result, axis=-1)\n\nprint(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "\n# reshape A and B to match the shape of the dot product\nA = tf.reshape(A, [10, 20, 30, 1])\nB = tf.reshape(B, [10, 20, 30, 1])\n\n# compute the dot product\nresult = tf.reshape(tf.matmul(tf.reshape(A, [-1, 30]), tf.reshape(B, [-1, 30])), [-1, 20, 20])\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\nx = tf.constant(x)\nresult = tf.strings.unicode_transcode(x, \"UTF-8\")\n\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": "\n    return [str(i, 'utf-8') for i in x]\n    ### END SOLUTION\n\nresult = f(example_x)\nprint(result)\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "mask = tf.cast(tf.reduce_sum(x, axis=-1) > 0, tf.float32)\nresult = tf.reduce_sum(x, axis=-1) / tf.cast(mask, tf.float32)\nresult = tf.reshape(result, tf.shape(x))\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = tf.not_equal(x, 0)\nresult = tf.reduce_sum(x, axis=-2, keepdims=True) / tf.reduce_sum(mask, axis=-2, keepdims=True)\nresult = tf.where(mask, result, tf.zeros_like(result))\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    mask = tf.cast(tf.math.reduce_any(x != 0, axis=-1), tf.float32)\n    return tf.reduce_sum(x, axis=-1) / mask\n    ### END SOLUTION\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\n\nwith tf.compat.v1.Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\n# Get the indices of the maximum values in each row\nindices = tf.argmax(a, axis=1)\n\n# Convert the indices to a tensor of shape [100]\nresult = tf.squeeze(indices)\n\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\n# Get the indices of the maximum values in each column\nindices = tf.argmax(a, axis=1)\n\n# Convert the indices to a tensor of the same shape as the original tensor\nresult = tf.tensor_scatter_nd_update(tf.ones_like(a), indices, tf.range(10))\n\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    return tf.argmax(a, axis=1)\n\nresult = f(example_a)\nprint(result)\n### END SOLUTION ###\n!\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1) + 1", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save(\"my_model\")\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\n# Set the seed\ntf.random.set_seed(seed_x)\n\n# Generate 10 random integers from a uniform random variable\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=5, dtype=tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    tensor = tf.random.uniform(shape=[10], minval=1, maxval=4, dtype=tf.int32)\n    return tensor\n\nresult = f(seed_x=10)\nprint(result)\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "print(tf.__version__)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "coeff = np.polyfit(np.log(x), y, 1)\nresult = np.array([coeff[1], coeff[0]])\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "coeff = np.polyfit(x, y, 2)\nresult = coeff\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "test_statistic, p_value = stats.ks_2samp(x, y)\n", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + ((3 * a - b - c)**2 + sin(b) + cos(b) + 4)\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.norm.sf(z_scores)", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [scipy.stats.norm.sf(z, mu, sigma) for z in z_scores]\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = np.array([scipy.stats.norm.ppf(1 - p) for p in p_values])", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm([stddev], loc=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa.dot(sb)", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": "\n    return sA.dot(sB)\n\nresult = f(example_sA, example_sB)\nprint(result)\nprint(result.toarray())\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid from the points\nx, y, z = np.mgrid[0:3:100j, 0:3:100j, 0:-3:100j]\n\n# Create a LinearNDInterpolator object\ninterp = scipy.interpolate.LinearNDInterpolator((x, y, z), V)\n\n# Get the interpolated value\nresult = interp(request)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "interp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\nprint(result)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, mode='reflect')\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = M.diagonal()", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, 'uniform')\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # Compute the cumulative distribution function of the times\n    cdf = np.arange(len(times)) / len(times)\n    # Compute the K-S test statistic\n    ks_statistic = np.sum(np.abs(cdf - (np.arange(len(times)) / len(times))))\n    # Compute the p-value\n    p_value = 1 - stats.kstest(times, 'uniform').pvalue\n    ### END SOLUTION ###\n    return ks_statistic, p_value\nresult = f(example_times, example_rate, example_T)\nprint(result)\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "# Run the K-S test\nresult = stats.kstest(times, 'uniform')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "distances = scipy.spatial.distance.cdist(points1, points2)\nresult = scipy.optimize.linear_sum_assignment(distances)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "distances = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nresult = scipy.optimize.linear_sum_assignment(distances)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\nb = sparse.csr_matrix(b.toarray())\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "mask = img > threshold\nresult = ndimage.label(mask)\nresult = result[mask]\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "mask = img < threshold\nresult = ndimage.label(mask)\nresult = result[mask]\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    labels, num_labels = ndimage.label(img > threshold)\n    return num_labels\n    ### END SOLUTION ###\n\nprint(f(img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\n# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the coordinates of the regions\ncoords = np.argwhere(mask)\n\n# Calculate the center of mass of the regions\ncenter_of_mass = np.array([np.mean(coords[:, 0]), np.mean(coords[:, 1])])\n\n# Calculate the distance between the center of mass of the regions and the top left corner\ndistance = np.sqrt(np.sum((center_of_mass - np.array([0, 0]))**2))\n\nresult = [distance]\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T - np.diag(M.diagonal())\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    sA = (sA + sA.T) / 2\n    sA = sA + sA.T\n    ### END SOLUTION ###\n    return sA\n\nsA = f(example_sA)\nprint(sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\n# Create a mask of the square array\nmask = np.zeros(square.shape, dtype=bool)\nmask[10:-10, 10:-10] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask)\n\n# Invert the mask to get the isolated cells\nmask = np.logical_not(mask)\n\n# Apply the mask to the square array\nsquare = np.where(mask, 0, square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask where the image is not zero\nmask = square != 0\n\n# Use scipy.ndimage.measurements.label to create a labeled image\nlabeled_image, num_labels = scipy.ndimage.measurements.label(mask)\n\n# Create a new array where the isolated cells are set to zero\nnew_square = np.copy(square)\nnew_square[labeled_image == 0] = 0\n\n# Print the new square\nprint(new_square)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_value = np.max(col.toarray())\nmin_value = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\nmedian = np.median(col.toarray())\nmode = np.mode(col.toarray())\nprint(median, mode)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *coeffs):\n    return sum(coeff * np.cos(n * np.pi * x / tau) for n, coeff in enumerate(coeffs))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\nflat_array = example_array.flatten()\n\n# Create a boolean mask to exclude the zero values\nmask = flat_array != 0\n\n# Create a 2D array with the unique IDs and their distances\ndistances = scipy.spatial.distance.pdist(flat_array[mask], metric='euclidean')\ndistances = np.resize(distances, (len(flat_array[mask]), len(flat_array[mask])))\n\n# Create a 2D array with the unique IDs and their distances\nresult = np.array(list(zip(flat_array[mask], flat_array[mask], distances)))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Convert the array to a 1D array\nflat_array = example_array.flatten()\n# Create a dictionary to store the distances\ndistances = {}\n# Loop over the unique IDs in the array\nfor i in range(len(flat_array)):\n    # Loop over the unique IDs in the array\n    for j in range(i + 1, len(flat_array)):\n        # If the IDs are not the same\n        if flat_array[i] != flat_array[j]:\n            # Calculate the distance\n            distance = scipy.spatial.distance.cityblock(flat_array[i], flat_array[j])\n            # Store the distance in the dictionary\n            distances[(flat_array[i], flat_array[j])] = distance\n            distances[(flat_array[j], flat_array[i])] = distance\n# Convert the distances to a 2D array\nresult = np.array(list(distances.items()))\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate the pairwise Euclidean distances between all regions\n    distances = scipy.spatial.distance.pdist(example_array, metric='euclidean')\n    # Convert the distances from cells to meters\n    distances_in_meters = distances / np.max(example_array)\n    # Convert the distances to a 2D array in the form \"from ID, to ID, distance\"\n    result = np.array(list(zip(np.where(example_array == 0)[0], np.where(example_array == 0)[1], distances_in_meters)))\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_array))\n", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "result = np.empty((5, 100))\nfor i in range(5):\n    tck, u = interpolate.splprep([x[i], y[i]], s=4)\n    result[i] = interpolate.splev(x_val, tck)\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x = np.array([x1, x2, x3, x4])\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x)\nprint(statistic, critical_values, significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nresult = ss.anderson_ksamp([x1, x2])\nprint(result)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "\ndef tau1(x):\n    y = np.array(df['A']) # keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['AC'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['BC'] = df['B'].rolling(3).apply(lambda x: tau1(x))\n\ndf", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "result = bool(sa.data.sum() == 0)", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "result = bool(sa.count_nonzero()) == 0\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores)[1]", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n    ### END SOLUTION ###\n\np_value = f(pre_course_scores, during_course_scores)\nprint(p_value)\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "kurtosis_result = np.mean(a) - 3 * np.std(a)**2\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interpolated = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interpolated(x, y)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a grid of points to interpolate over\n    xi, ti = np.mgrid[-1:1:10j, -2:0:10j]\n    # Interpolate the function at these points\n    result = scipy.interpolate.interp2d(s, t, z, kind='cubic')(xi, ti)\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "import numpy as np\n\n# Create a list of the indices of the regions occupied by the extra points\nresult = np.array([vor.point_region[i] for i in extraPoints])\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "import numpy as np\n\n# Create a list of indices for the extra points\nextra_indices = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the voronoi diagram\nvor_points = [vor.vertices[i] for i in range(len(vor.vertices))]\n\n# Create a list of the regions of the voronoi diagram\nvor_regions = [vor.point_region[i] for i in range(len(vor.point_region))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions of the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "# Find the maximum size among all vectors\nmax_size = max([vector.size for vector in vectors])\n\n# Create a sparse matrix with zeros\nresult = sparse.csc_matrix((max_size, len(vectors)), dtype=np.float64)\n\n# Fill the sparse matrix with the vectors\nfor i, vector in enumerate(vectors):\n    result[i, :vector.size] = vector\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=[0, 1])", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[i, j] for i, j in zip(row, column)]\nprint(result)\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, :, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u, o2, x):\n    dev = abs((x - u) / o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer / 2\n    return P\n\nprob = NormalDistro(u, o2, x)\nprint(prob)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n!pip install scipy\n!pip install numpy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install statsmodels\n!pip install numpy\n!pip install scip", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a 2D array with random values\nA = np.random.rand(N, N)\n\n# Compute the DCT matrix\ndct_matrix = sf.dctn(A, norm='ortho')\n\n# Store the result in the variable 'result'\nresult = dct_matrix", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "offset = [-1, 0, 1]\nresult = sparse.diags(matrix, offset, shape=(5, 5)).toarray()\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "\ndef binomial_distribution(n, p):\n    return scipy.stats.binom.pmf(range(n + 1), n, p)\n\nresult = binomial_distribution(N, p)\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x), axis=0)\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "df['data'] = df.index\ndf['zscore'] = stats.zscore(df.values, axis=1)\nresult = df\nresult\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "df['data'] = df.apply(lambda row: pd.Series({'sample1': row['sample1'], 'sample2': row['sample2'], 'sample3': row['sample3']}), axis=1)\ndf['zscore'] = df.apply(lambda row: pd.Series({'sample1': stats.zscore(row['sample1']), 'sample2': stats.zscore(row['sample2']), 'sample3': stats.zscore(row['sample3'])}), axis=1)\ndf = df.drop(['sample1', 'sample2', 'sample3'], axis=1)\ndf = df.rename(columns={'sample1': 'data', 'sample2': 'data', 'sample3': 'data'})\ndf = df.rename(columns={'sample1': 'zscore', 'sample2': 'zscore', 'sample3': 'zscore'})\ndf = df.round({'data': 3, 'zscore': 3})\nresult = df\nresult\nEND SOLUTION\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y = np.arange(shape[0])\nx = np.arange(shape[1])\nmid = np.array([[np.sqrt(i**2 + j**2) for j in x] for i in y])\nresult = distance.cdist(mid, mid)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.ogrid[:shape[0], :shape[1]]\nresult = distance.cdist(np.dstack((x, y)), mid)", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    center = np.array([shape[0] / 2, shape[1] / 2])\n    mid = np.sqrt((x - center[1]) ** 2 + (y - center[0]) ** 2)\n    return mid\n    ### END SOLUTION ###\n!f(shape = (6, 6))\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((func(x) - y) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, method='Nelder-Mead')\nprint(out.x)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((func(x) - y) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, method='L-BFGS-B', bounds=[(-b / 2.0) for b in x_lower_bounds])['x']\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) if 0 < t < 2*np.pi else 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.hstack([sa, sb])", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\n# define the function\ndef integrand(x, c):\n    return 2 * c * x\n\n# define the array of constants\nc_values = np.array([1, 2, 3, 4, 5])\n\n# initialize the array to store the solutions\nI = np.zeros(len(c_values))\n\n# loop over the constants\nfor i, c in enumerate(c_values):\n    # integrate the function\n    result, error = scipy.integrate.quad(integrand, 0, 1, args=(c,))\n    # store the result\n    I[i] = result\n\nprint(I)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n\n# create an array of c values\nc_values = array([5, 10, 15])\n\n# create an array to store the solutions\nI = zeros(len(c_values))\n\n# loop over the c values and calculate the solution\nfor i, c in enumerate(c_values):\n    I[i] = f(c)\n\n# print the solutions\nprint(I)\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.toDense()\nV = V + x", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V += x", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V += x\nV += y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    column = sa[:, col].data\n    list_ = [x**2 for x in column]\n    # get the column length\n    length = math.sqrt(sum(list_))\n    # update the original column of the matrix\n    sa[:, col] = sa[:, col] / length\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].toarray()\n    # calculate the length\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    normalized_column = column / length\n    # update the original column in the sparse matrix\n    sa[:, col] = normalized_column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = (a > 0).astype(int)", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = (a == 0) | (a == 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argmin(distances, axis=1)\nresult = closest_indices", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# Calculate the distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n\n# Find the indices of the minimum distance for each cluster\nindices = np.argmin(distances, axis=1)\n\n# Extract the closest point to each cluster\nresult = data[indices]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argpartition(distances, k, axis=0)[:k, :]\nresult = closest_indices", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result.append(fsolve(eqn, x0=x, args=(a, b)))\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result.append(fsolve(eqn, x0=x, args=(a,)))\nresult = np.array(result)\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, 0] > 0]\nresult = result[result[:, 1] > 0]\nresult = result[result[:, ", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Fit the data to the bekkers function\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nkstest_result = sp.stats.kstest(sample_data, bekkers(range_start, *popt))\n\n# Print the result\nprint(kstest_result)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Fit the data to the bekkers function\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nkstest_result = sp.stats.kstest(sample_data, bekkers(range_start, *popt))\n\n# Check the p-value at 95% confidence level\nresult = kstest_result.pvalue < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "df['Time'] = pd.to_datetime(df['Time'])\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(lambda x: integrate.trapz(x['A'], x['Time']))\nintegral_df = integral_df.reset_index(drop=True)\nintegral_df", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "interp = scipy.interpolate.griddata(x, y, eval, method='linear')\nresult = interp\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "# calculate the number of categories\nn_categories = a.nunique()\n\n# calculate the number of observations\nn_observations = a.shape[0]\n\n# calculate the number of observations in each category\nn_observations_in_category = a.value_counts()\n\n# calculate the weights\nweights = n_observations_in_category / n_observations\n\n# normalize the weights\nweights /= weights.sum()\n\n# print the weights\nprint(weights)", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "# Create a mask for the elements to be considered\nmask = np.ones(arr.shape, dtype=bool)\nmask[n:-n] = False\n\n# Find the indices of the elements to be considered\nindices = np.where(mask)[0]\n\n# Find the relative extrema\nresult = indices[np.all(np.logical_and(arr[indices] <= arr[indices-n:indices+n], arr[indices] <= arr[indices-n-1:indices+n-1]), axis=1)]\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "\n# Create a 2D array with the same shape as the input array, filled with False values\nmask = np.zeros_like(arr, dtype=bool)\n\n# Create a 2D array with the same shape as the input array, filled with False values\nresult = []\n\n# Loop over the rows of the array\nfor i in range(arr.shape[0]):\n    # Loop over the columns of the array\n    for j in range(arr.shape[1]):\n        # Get the neighbouring elements\n        neighbours = arr[np.maximum(i - 1, 0):i + 2, np.maximum(j - 1, 0):j + 2]\n        # Check if the current element is less or equal to the neighbouring elements\n        if np.all(arr[i, j] <= neighbours):\n            # If it is, add the index to the result\n            result.append([i, j])\n\n# Convert the result to a numpy array\nresult = np.array(result)\n\nprint(result)\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "df = df[(np.abs(stats.zscore(df.select_dtypes(include=np.number))) < 3).all(axis=1)]\n", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": "\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n    return df\n    ### END SOLUTION\n\nprint(solve(data))\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a list of unique elements in the column\nunique_elements = df['Col3'].apply(lambda x: list(set(x)))\n\n# Create a new dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\n\n# Add the original dataframe and the one-hot-encoded dataframe\ndf_out = pd.concat([df, df_out], axis=1)\n\n# Remove the original column\ndf_out.drop('Col3', axis=1, inplace=True)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# Create a list of unique fruit names\nfruit_names = ['Apple', 'Orange', 'Banana', 'Grape']\n\n# Create a DataFrame with the unique fruit names as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(fruit_names))), columns=fruit_names)\n\n# Iterate over the rows of the DataFrame\nfor i in range(df.shape[0]):\n    # Iterate over the columns of the DataFrame\n    for j in range(df.shape[1]):\n        # If the value in the current column is in the list of fruit names\n        if df.iloc[i, j] in fruit_names:\n            # Set the value in the DataFrame to 1\n            df_out.iloc[i, fruit_names.index(df.iloc[i, j])] = 1\n\n# Print the DataFrame\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col4'].apply(lambda x: list(set(x)))\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col4'], prefix='Col4')\n\n# Add the unique elements to the dataframe\ndf_out = pd.concat([df, unique_elements], axis=1)\n\n# Drop the original column\ndf_out.drop('Col4', axis=1, inplace=True)\n\n# Print the dataframe\nprint(df_out)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col3'].apply(lambda x: list(set(x)))\n\n# Create a dataframe with a column for each unique element\ndf_out = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\n\n# Add the original dataframe and the one-hot-encoded dataframe\ndf_out = pd.concat([df, df_out], axis=1)\n\n# Remove the original column\ndf_out.drop('Col3', axis=1, inplace=True)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# First, we need to create a list of all unique elements in the last column\nunique_elements = set(df['Col3'].values.flatten())\n\n# Then, we create a new dataframe with the unique elements as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(unique_elements))), columns=unique_elements)\n\n# Now, we need to iterate over the rows of the original dataframe\nfor index, row in df.iterrows():\n    # For each row, we iterate over the elements in the last column\n    for element in row['Col3']:\n        # We set the value of the corresponding column to 1\n        df_out.loc[index, element] = 1\n\n# Finally, we drop the last column\ndf_out = df_out.drop('Col3', axis=1)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "\n# Convert the sparse matrix to a DataFrame\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Concatenate the original dataframe and the transformed dataframe\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "# Convert the csr_matrix to a pandas DataFrame\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Merge the original DataFrame and the transformed DataFrame\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # Convert the sparse matrix to a dense matrix\n    transform_output_dense = transform_output.toarray()\n    # Create a new dataframe with the original data and the transformed output\n    df_transformed = pd.concat([df, pd.DataFrame(transform_output_dense)], axis=1)\n    return df_transformed\n    ### END SOLUTION ###\n\nThis function should work for your case. It converts the sparse matrix to a dense matrix, then concatenates the original dataframe and the transformed output dataframe along the columns axis.\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.pop(1) # remove the second step\nclf.steps.insert(1, ('reduce_poly', PolynomialFeatures())) # insert a new step\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\nclf.steps.append(('new_step', PolynomialFeatures()))\nclf.fit(X, y)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps.append(('new_step', SVC()))\nclf.fit(np.array([[1, 2, 3], [4, 5, 6]]), np.array([1, 2]))\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PCA()))\nclf.steps\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n    'max_depth': [3, 4, 5, 6, 7],\n    'n_estimators': [100, 200, 300, 400, 500]\n}\nb = []\nc = []\nfor i in range(5):\n    gridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX, trainY)\n    b.append(gridsearch.best_score_)\n    c.append(gridsearch.predict(testX))\nprint(b)\nprint(c)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n    'max_depth': [3, 4, 5, 6, 7],\n    'n_estimators': [100, 200, 300, 400, 500]\n}\n\n# Define the GridSearchCV object\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n\n# Fit the GridSearchCV object\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the prediction\nc = gridsearch.predict(testX)\n\n# Get the score on the test set\nb = gridsearch.score(testX, testY)\n\n# Print the best parameters, best score, and prediction\nprint(\"Best parameters: \", best_params)\nprint(\"Best score: \", best_score)\nprint(\"Prediction: \", c)\nprint(\"Score on the test set: \", b)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    ### END SOLUTION ###\n\n# call the function\nsolve(data, scaler, scaled)\n", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = str(model)\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data)\n", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)\n", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nclf = grid_search.best_estimator_\nproba = clf.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Convert the DataFrame to a numpy array\ndata_array = data.values\n\n# Apply the preprocessing.scale function to the numpy array\nscaled_data = preprocessing.scale(data_array)\n\n# Convert the scaled numpy array back to a DataFrame\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Assign the scaled DataFrame to the variable df_out\ndf_out = scaled_data_df\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a list of column names\ncolumn_names = data.columns.tolist()\n\n# Create a scaler object\nscaler = preprocessing.StandardScaler()\n\n# Fit and transform the data\nscaled_data = scaler.fit_transform(data)\n\n# Create a new DataFrame\ndf_out = pd.DataFrame(scaled_data, columns=column_names, index=data.index)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = X.columns[model.get_support()]\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "import numpy as np\ncolumn_names = np.array(X.columns)[model.get_support()]\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns[(clf.feature_importances_ > 0.05)].tolist()\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\ndistances = km.transform(X)\nclosest_50_samples = X[np.argpartition(distances, 50, axis=0)[:50]]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[km.labels_ == km.labels_[p]]\nclosest_50_samples = closest_50_samples[:50]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\ndistances = km.transform(X)\nclosest_100_samples = X[np.argpartition(distances, 100, axis=0)[:100]]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    distances = np.linalg.norm(centers - X, axis=1)\n    closest_samples = np.argpartition(distances, p)[:p]\n    return X[closest_samples]\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Gaussian Kernel\ngaussian_kernel = svm.SVC(kernel='rbf')\n\n# Fit the model\ngaussian_kernel.fit(X_train, y_train)\n\n# Predict the outcomes\npredict = gaussian_kernel.predict(X_test)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict\npredict = model.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "tfidf_query = tfidf.transform(queries)\ncosine_similarities_of_queries = np.array([np.sqrt(1 - cosine_similarity(tfidf_query, tfidf.transform(documents))) for documents in documents])\ncosine_similarities_of_queries = pd.DataFrame(cosine_similarities_of_queries, index=documents, columns=queries)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "tfidf_query = tfidf.transform(queries)\ncosine_similarities_of_queries = np.asarray(tfidf_query.dot(tfidf_matrix.T))", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_query = TfidfVectorizer().fit_transform(queries)\n    cosine_similarities_of_queries = tfidf_query.dot(tfidf.fit_transform(documents)).toarray()\n    return cosine_similarities_of_queries\n    ### END SOLUTION ###\n\n# Call the function with your queries and documents\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a DataFrame\ndf = pd.DataFrame(new_features, columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6'])\n\n# Convert the DataFrame to a 2D array\nnew_features = df.values\n\n# Now you can use the new_features variable to perform feature selection\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# Convert the list of lists to a pandas DataFrame\ndf = pd.DataFrame(f)\n\n# Convert the DataFrame to a numpy array\nnew_f = df.values\n\n# Convert the numpy array to a 2D array with zeros\nnew_f = np.zeros((len(new_f), len(df.columns)))\n\n# Convert the numpy array to a 2D array with ones\nnew_f = np.ones((len(new_f), len(df.columns)))\n\n# Convert the numpy array to a 2D array with the sum of the numpy array\nnew_f = np.sum(new_f, axis=1)\n\n# Convert the numpy array to a 2D array with the maximum of the numpy array\nnew_f = np.max(new_f, axis=1)\n\n# Convert the numpy array to a 2D array with the minimum of the numpy array\nnew_f = np.min(new_f, axis=1)\n\n# Convert the numpy array to a 2D array with the mean of the numpy array\nnew_f = np.mean(new_f, axis=1)\n\n# Convert the numpy array to a 2D array with the standard deviation of the numpy array\nnew_f = np.std(new_f, axis=1)\n\n# Convert the numpy array to a 2D array with the median of the numpy array\nnew_f = np.median(new_f, axis=1)\n\n# Convert the numpy array to a 2D array with the sum of the numpy array along the first axis\nnew_f = np.sum(new_f, axis=0)\n\n# Convert the numpy array to a 2D array with the maximum of the numpy array along the first axis\nnew_f = np.max(new_f, axis=0)\n\n# Convert the numpy array to a 2D array with the minimum of the numpy array along the first axis\nnew_f = np.min(new_f, axis=0)\n\n# Convert the numpy array to a 2D array with the mean of the numpy array along the first axis\nnew_f = np.mean(new_f, axis=0)\n\n# Convert the numpy array to a 2D array with the standard deviation of the numpy array along the first axis\nnew_f = np.std(new_f, axis=0)\n\n# Convert the numpy array to a 2D array with the median of the numpy array along the first axis\nnew_f = np.median(new_f, axis=0)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to one-hot encoding\nnew_features = pd.get_dummies(new_features)\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D numpy array\n    features_array = np.array(features)\n\n    # Create a DataFrame from the array\n    df = pd.DataFrame(features_array, columns=features[0])\n\n    # Convert the DataFrame to a 2D numpy array\n    new_features = df.values\n\n    ### END SOLUTION ###\n    return new_features\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Convert the list of lists to a pandas DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D array with ones for the features that are present and zeros for the features that are not present\nnew_features = np.where(df == 1, 1, 0)\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from the data\ndistances = pd.DataFrame(data_matrix)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data\ncluster_labels = cluster.fit_predict(distances)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data\ncluster_labels = cluster.fit_predict(dist_matrix)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from the similarity matrix\ndistances = pd.DataFrame(simM)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the distance matrix and get the cluster labels\ncluster_labels = cluster.fit_predict(distances)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix\nZ = linkage(data_matrix, 'ward')\n\n# Perform clustering\ncluster_labels = fcluster(Z, t=2, criterion='maxclust')\ncluster_labels", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "import scipy.cluster.hierarchy as shc\n\n# Create a linkage matrix\nlinkage_matrix = shc.linkage(data_matrix, method='average')\n\n# Perform hierarchical clustering\ncluster_labels = shc.fcluster(linkage_matrix, t=2, criterion='maxclust')\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix\nZ = linkage(simM, 'ward')\n\n# Perform clustering\ncluster_labels = fcluster(Z, t=2, criterion='maxclust')\ncluster_labels", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import PowerTransformer\n\nbox_cox = PowerTransformer(method='box-cox')\nbox_cox_data = box_cox.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(punctuation='!\"?\"\\'')\ntransformed_text = vectorizer.fit_transform(text)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataframe into features and target\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    from sklearn.model_selection import train_test_split\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\n<code>\n!!solve(dataset)\n", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "\n# Reshape the data\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans instance with 2 clusters\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n\n# Get the labels\nlabels = kmeans.labels_\n\nEND SOLUTION\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# Reshape the data\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans instance with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit the model to the data\nkmeans.fit(X)\n\n# Get the labels\nlabels = kmeans.predict(X)\n\n# Get the centroids\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "svc = LinearSVC(penalty='l1', dual=False)\nsvc.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "selector = LinearSVC(C=1.0, penalty='l1', dual=False).fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selector.coef_ != 0]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    svc = LinearSVC(penalty='l1', dual=False)\n    svc.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\n    return selected_feature_names\n    ### END SOLUTION ###\n\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vocabulary = {'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'}\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nfeature_names = np.array(feature_names)\nfeature_names = np.sort(feature_names)\nX = np.array([np.where(feature_names == word, 1, 0) for word in feature_names])\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names = np.array(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = np.sort(feature_names)\nfeature_names = feature_names.tolist()\nfeature_names = [word for word in feature_names if word in vocabulary]\nfeature_names =", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nvectorizer.fit(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = vectorizer.transform(corpus).toarray()\nfeature_names, X\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[['Time', col]].dropna()\n    X, Y = df2['Time'].values.reshape(-1, 1), df2[col].values.reshape(-1, 1)\n    slope = LinearRegression().fit(X, Y)\n    slopes.append(slope.coef_[0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[['Time', col]].dropna()\n    X, Y = df2['Time'], df2[col]\n    slope = LinearRegression().fit(X.values.reshape(-1, 1), Y.values)\n    slopes.append(slope.coef_[0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed dataframe\ntransformed_df = df\nprint(transformed_df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n    ### END SOLUTION\n\ndf = Transform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "ElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\nprint (\"R^2 for training set:\")\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\")\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": "\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION\n\nnp_array = Transform(np_array)\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close[-1:], ma50[-1:], ma100[-1:], ma200[-1:]])\npredict", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = []\nfor i in X:\n    new_X.append([ord(i[0][0]), i[0][1]])\n\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = []\nfor i in X:\n    new_X.append([float(i[0]), float(i[1])])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array(X)\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Separating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n# logReg = LogisticRegression()\n# logReg.fit(X, y)\nlogReg.fit(X, y.values.reshape(-1, 1))\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Separate the features and the target\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n\n# Create a Logistic Regression model\nlogReg = LogisticRegression()\n\n# Fit the model with the data\nlogReg.fit(X, y)\n\n# Predict the target for the new data\npredict = logReg.predict(X)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Calculate the number of days in the dataframe\nnum_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the test set\ntest_days = int(num_days * 0.8)\n\n# Get the indices for the train and test sets\ntrain_indices = np.arange(num_days - test_days)\ntest_indices = np.arange(num_days - test_days, num_days)\n\n# Get the train and test dataframes\ntrain_dataframe = features_dataframe.loc[train_indices]\ntest_dataframe = features_dataframe.loc[test_indices]\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Calculate the number of days in the dataframe\nnum_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the test set\ntest_days = int(num_days * 0.2)\n\n# Calculate the number of days in the train set\ntrain_days = num_days - test_days\n\n# Create the train and test dataframes\ntrain_dataframe = features_dataframe.iloc[:train_days]\ntest_dataframe = features_dataframe.iloc[train_days:]\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    features_dataframe['date'] = pd.to_datetime(features_dataframe['date'])\n    features_dataframe = features_dataframe.sort_values('date')\n    train_dataframe = features_dataframe.iloc[:int(len(features_dataframe)*0.2)]\n    test_dataframe = features_dataframe.iloc[int(len(features_dataframe)*0.2):]\n    return train_dataframe, test_dataframe\n    ### END SOLUTION\n\nfeatures_dataframe = solve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2', 'X3']] = scaler.fit_transform(df[['X2', 'X3']])\ndf['X2_scale'] = df.groupby('Month')['X2'].transform('minmax')\ndf['X3_scale'] = df.groupby('Month')['X3'].transform('minmax')\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "\nmyData['new_A2'] = scaler.fit_transform(myData[['Month', 'A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['Month', 'A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results.sort_values(by='rank_test_score')\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values('mean_fit_time')\nfull_results", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport joblib\njoblib.dump(fitted_model, 'sklearn_model')\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix, tfidf_matrix)\ncosine_similarity_matrix", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptimizer = optim.param_groups[0]['lr']\nprint(optimizer)\noptim.param_groups[0]['lr'] = 0.001\nprint(optim.param_groups[0]['lr'])\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\noptimizer = optim.state_dict()\noptimizer['param_groups'][0]['lr'] = 0.001\noptim.load_state_dict(optimizer)\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define a function to update the learning rate\ndef update_learning_rate(optimizer, epoch):\n    if epoch > 10:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.0005\n    else:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.05\n\n# Apply the function to the optimizer\noptim.step()\noptim.zero_grad()\nupdate_learning_rate(optim, epoch)\noptim.step()\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the gensim word2vec model to a PyTorch tensor\nembedding_weights = torch.tensor(word2vec.wv.vectors)\n\n# Create a PyTorch embedding layer with the same vector size as your input tensor\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_weights)\n\n# Use the embedding layer to embed your input data\nembedded_input = embedding_layer(input_Tensor)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # Create a PyTorch embedding layer with the same size as the input tensor\n    embedding_layer = torch.nn.Embedding(input_Tensor.size(1), input_Tensor.size(1))\n\n    # Load the pre-trained word2vec weights into the embedding layer\n    embedding_layer.weight.data.copy_(torch.from_numpy(word2vec.wv.vectors))\n\n    # Embed the input data using the pre-trained weights\n    embedded_input = embedding_layer(input_Tensor)\n\n    return embedded_input\n    ### END SOLUTION ###\n\nget_embedded_input(input_Tensor)\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(np.array(x))\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "\nA_log = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n\n# Convert ByteTensor to LongTensor\nA_log = A_log.long()\n\n# Use logical indexing to get the columns that correspond to a 1 value in the index vector\nC = B[:, A_log]\n\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "\n# Convert the ByteTensor to a numpy array\nA_logical_np = A_logical.numpy()\n\n# Use numpy to select the columns\nC = B[:, A_logical_np]\n\n# Convert the result back to a torch tensor\nC = torch.from_numpy(C)\n\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "\nC = B[:, A_log.bool()]\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "\nA_log = torch.ByteTensor([0, 1, 0])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n\n# Convert ByteTensor to LongTensor\nA_log = A_log.long()\n\n# Use logical indexing to get the columns that correspond to a 0 value in the index vector\nC = B[:, A_log]\n\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    return C\n\nC = solve(A_log, B)\nprint(C)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "\n# Convert the ByteTensor to a numpy array\nA_log_np = A_log.numpy()\n\n# Use numpy to select the columns\nC = B[:, A_log_np]\n\n# Convert the numpy array back to a torch tensor\nC = torch.from_numpy(C)\n\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Convert the tensor to numpy array\nB_np = B.numpy()\n\n# Use numpy to select the columns\nC_np = B_np[:, idx.numpy()]\n\n# Convert the numpy array back to tensor\nC = torch.from_numpy(C_np)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    return torch.tensor(a, dtype=torch.float16)\n    ### END SOLUTION ###\n\n# Test the function\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\n# Create a tensor of zeros with the same shape as the `lens` tensor\nmask = torch.zeros_like(lens)\n\n# Fill the tensor with ones where the `lens` tensor is greater than 0\nmask[lens > 0] = 1\n\n# Print the mask\nprint(mask)\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\n# Create a tensor of zeros with the same shape as the `lens` tensor\nmask = torch.zeros_like(lens)\n\n# Create a tensor of ones with the same shape as the `lens` tensor\nmask[torch.arange(len(lens)), lens - 1] = 1\n\n# Print the mask\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask.long()\n    ### END SOLUTION ###\n\nget_mask(lens)\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with the same shape as the input tensor\nTensor_2D_diagonal = torch.eye(Tensor_2D.shape[0])\n\n# Multiply the 2D tensor with the diagonal tensor\nTensor_3D = Tensor_2D * Tensor_2D_diagonal\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    # Create a diagonal matrix from the tensor\n    diag_matrix = torch.diag(t)\n    # Create a matrix from the tensor\n    matrix = torch.eye(t.shape[0])\n    # Multiply the diagonal matrix and the matrix\n    result = torch.mm(diag_matrix, matrix)\n    ### END SOLUTION ###\n    return result\n\n# Call the function with your tensor\nresult = Convert(Tensor_2D)\nprint(result)\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]])\nab = torch.stack((a, b), 0)\nprint(ab.shape)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = a.reshape(-1, 1)\nb = b.reshape(-1, 1)\nab = torch.cat([a, b], 0)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    a = torch.cat([a, a], 0)\n    b = torch.cat([b, b], 0)\n    return a, b\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[torch.arange(10), lengths, :] = 0\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "\na[torch.arange(10), lengths, :] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[torch.arange(10), lengths - 1, :] = 0\n", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "\na[torch.arange(10), lengths - 1, :] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.tensor(lt, dtype=torch.float32)\n    return tensor_of_tensors\n    ### END SOLUTION ###\n\n# Call the function with your list of tensors\ntensor_of_tensors = Convert(list_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "\nresult = torch.index_select(t, 0, torch.tensor(idx))\nprint(result)\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "\nresult = t[np.arange(t.shape[0])[:, None], idx - 1]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "\nresult = t[range(len(t)), idx - 1]", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\n# Flatten the tensor to 1D\nids = ids.view(-1)\n\n# Gather the scores based on the ids\nresult = torch.gather(x, 1, ids.unsqueeze(-1).unsqueeze(-1)).squeeze(1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\n# Flatten the tensor\nx = x.view(-1, 114)\n\n# Get the maximum ids\nmax_ids = torch.argmax(x, 1)\n\n# Gather the selected slices\nresult = torch.gather(x, 1, max_ids.unsqueeze(-1).expand(-1, -1, 114))\nresult = result.squeeze(1)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask where the ids tensor is 1\nmask = ids == 1\n\n# Gather the scores from the x tensor where the mask is True\nselected_scores = x[mask].gather(1, ids.unsqueeze(-1)).squeeze(1)\n\n# Find the maximum score\nmax_score = selected_scores.max()\n\n# Create a mask where the selected scores are equal to the maximum score\nmask_max_score = selected_scores == max_score\n\n# Gather the ids where the mask is True\nresult = ids[mask_max_score]\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "\n# Get the maximum value from the softmax output\nmax_values, y = torch.max(softmax_output, 1)\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\ny = torch.tensor([0, 1, 2])\n\n# Create a", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "\n# Get the maximum value from the softmax output\ny = torch.argmax(softmax_output, dim=1)\n\n# Convert the tensor to a numpy array\ny = y.numpy()\n\nprint(y)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "\n# Get the maximum value in each row\nmax_values = torch.max(softmax_output, 1)[0]\n\n# Create a tensor of zeros with the same shape as softmax_output\ny = torch.zeros_like(softmax_output)\n\n# Set the values in y to the indices of the maximum values\ny.scatter_(0, max_values.unsqueeze(1), 1)\n\n# Print the tensor y\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    _, predicted = torch.max(softmax_output, 1)\n    return predicted.view(-1, 1)\n    ### END SOLUTION ###\n\nprint(solve(softmax_output))\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "\n# Get the maximum value in each row\nmax_values = torch.max(softmax_output, 1)[0]\n\n# Create a tensor of the same shape as the input with the maximum values\ny = torch.zeros_like(softmax_output)\n\n# Set the values in the tensor to the maximum values\ny.scatter_(0, max_values.unsqueeze(1), 1)\n\n# Convert the tensor to a long tensor\ny = y.long()\n\nprint(y)\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "\n# Convert labels to one-hot encoding\nlabels = F.one_hot(labels, num_classes=7).float()\n\n# Calculate cross entropy loss\nloss = F.cross_entropy(input=images, target=labels, reduction='none')\n\n# Average the loss over the batch dimension\nloss = loss.mean()\n\n# Print the loss\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = (A == B).sum().item()\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "\ncnt_equal = torch.sum((A == B).float())\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "\ncnt_not_equal = torch.sum((A != B).bool())\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = (A == B).sum().item()\n    ### END SOLUTION ###\n    return cnt_equal\n\nA = torch.randn(1000, 1)\nB = torch.randn(1000, 1)\n\nprint(Count(A, B))\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "\n# Convert tensors to numpy arrays\nA_np = A.numpy()\nB_np = B.numpy()\n\n# Get the last x elements of both arrays\nlast_x_A = A_np[-x:]\nlast_x_B = B_np[-x:]\n\n# Check if the last x elements are equal\ncnt_equal = np.sum(last_x_A == last_x_B)\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "\ncnt_not_equal = torch.sum((A[-x:] != B[-x:]).bool())\n", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensor_31 = a[:, :, :, i:i + chunk_dim, :]\n    tensors_31.append(tensor_31)\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 10, 31, 1)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = []\nfor i in range(3):\n    tensor_31 = a[:, :, i, :, :]\n    tensors_31.append(tensor_31)\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 31, 10, 1)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output[mask.bool()] = clean_input_spectrogram[mask.bool()]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask == 0] = clean_input_spectrogram[mask == 0]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x))\nmin_y = torch.min(torch.abs(y))\nsigned_min = torch.where(x < y, min_x, min_y) * sign_x\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.where(x >= 0, sign_x, -sign_y) * max\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x))\n    min_y = torch.min(torch.abs(y))\n    signed_min = torch.where(x < 0, -min_x, min_x)\n    signed_min = torch.where(y < 0, -min_y, min_y)\n    return signed_min\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    confidence_score = conf.item()\n    return confidence_score, class_names[classes.item()]\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# Get the last column of a and the first column of b\nlast_a, first_b = a[:, -1:], b[:, 0]\n\n# Compute the average of the last column of a and the first column of b\nresult = torch.cat([last_a, (first_b + last_a) / 2], dim=1)\n\n# Remove the last column of a and the first column of b\nresult = torch.cat([result[:, :-1], result[:, 1:]], dim=1)\n\n# Add the last column of a and the first column of b back to the result\nresult = torch.cat([result, last_a, first_b], dim=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a.squeeze()\n    b = b.squeeze()\n    result = torch.zeros_like(a)\n    result[:, :-1] = (a[:, 1:] + b[:, :-1]) / 2\n    result[:, -1] = a[:, -1]\n    return result\n    ### END SOLUTION ###\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "\n# Create a tensor with zeros\nt = torch.zeros(1, 4, 2)\n\n# Create a tensor with the values you want\nnew = torch.tensor([[0., 0., 0., 0.]])\n\n# Stack the tensors\nresult = torch.cat([t, new], dim=0)\n\nprint(result)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "\n# Create a tensor with the same shape as t, but filled with zeros\nnew = torch.zeros_like(t)\n\n# Stack t and new along the first dimension\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "\n# Create a tensor with the desired shape\nt = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n\n# Create a tensor with the desired shape\nnew = torch.tensor([[-1, -1, -1, -1]])\n\n# Stack the tensors along the first dimension\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data.unsqueeze(1), W.unsqueeze(0)).squeeze()\nresult = result.view(10, 2, 3)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
