{"id": 0, "code": "\nresult = df.reindex(List)\nresult", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "\nresult = df.loc[List]\nresult['Type'] = df['Type'].values[List]\nresult['Type'] = result['Type'].astype(int)\nresult['Type'] = result['Type'].replace([1, 2, 3], [2, 3, 4])\nresult['Type'] = result['Type'].replace([2, 3, 4], [1, 2, 3])\nresult['Type'] = result['Type'].replace([1, 2, 3, 4], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [3, 4, 5, 6])\nresult['Type'] = result['Type'].replace([3, 4, 5, 6], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [4, 5, 6, 7])\nresult['Type'] = result['Type'].replace([4, 5, 6, 7], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [5, 6, 7, 8])\nresult['Type'] = result['Type'].replace([5, 6, 7, 8], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [6, 7, 8, 9])\nresult['Type'] = result['Type'].replace([6, 7, 8, 9], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [7, 8, 9, 10])\nresult['Type'] = result['Type'].replace([7, 8, 9, 10], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [8, 9, 10, 11])\nresult['Type'] = result['Type'].replace([8, 9, 10, 11], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [9, 10, 11, 12])\nresult['Type'] = result['Type'].replace([9, 10, 11, 12], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [10, 11, 12, 13])\nresult['Type'] = result['Type'].replace([10, 11, 12, 13], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [11, 12, 13, 14])\nresult['Type'] = result['Type'].replace([11, 12, 13, 14], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [12, 13, 14, 15])\nresult['Type'] = result['Type'].replace([12, 13, 14, 15], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [13, 14, 15, 16])\nresult['Type'] = result['Type'].replace([13, 14, 15, 16], [0, 1, 2, 3])\nresult['Type'] = result['Type'].replace([0, 1, 2, 3], [14, 15, 16, 17])\nresult['Type'] = result['Type'].replace([14, 15, 16, 17], [0, 1, 2, 3])\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a mask for the rows where the value in column Qu1 is not in the value_counts of column Qu1\nmask = df['Qu1'].isin(pd.value_counts(df.Qu1)[pd.value_counts(df.Qu1) >= 2].index)\n\n# Create a mask for the rows where the value in column Qu2 is not in the value_counts of column Qu2\nmask2 = df['Qu2'].isin(pd.value_counts(df.Qu2)[pd.value_counts(df.Qu2) >= 2].index)\n\n# Create a mask for the rows where the value in column Qu3 is not in the value_counts of column Qu3\nmask3 = df['Qu3'].isin(pd.value_counts(df.Qu3)[pd.value_counts(df.Qu3) >= 2].index)\n\n# Apply the masks to the dataframe\ndf.loc[mask, 'Qu1'] = 'other'\ndf.loc[mask2, 'Qu2'] = 'sausage'\ndf.loc[mask3, 'Qu3'] = 'other'\n\nresult = df\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a mask for the values in Qu1 that have at least 3 occurrences\nmask = df['Qu1'].apply(lambda x: pd.value_counts(df.Qu1) >= x)\n\n# Apply the mask to the Qu1 column\ndf['Qu1'] = df['Qu1'].where(mask, 'other')\n\n# Create a mask for the values in Qu2 that have at least 3 occurrences\nmask = df['Qu2'].apply(lambda x: pd.value_counts(df.Qu2) >= x)\n\n# Apply the mask to the Qu2 column\ndf['Qu2'] = df['Qu2'].where(mask, 'other')\n\n# Create a mask for the values in Qu3 that have at least 3 occurrences\nmask = df['Qu3'].apply(lambda x: pd.value_counts(df.Qu3) >= x)\n\n# Apply the mask to the Qu3 column\ndf['Qu3'] = df['Qu3'].where(mask, 'other')\n\nresult = df\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "\n    for col in df.columns:\n        df.loc[df[col] == 'apple', col] = 'other'\n        df.loc[df[col] == 'egg', col] = 'other'\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a mask for the values in Qu1 that have at least 3 occurrences\nmask = df['Qu1'].apply(lambda x: pd.value_counts(df.Qu1) >= x)\n\n# Apply the mask to the Qu1 column\ndf['Qu1'] = df['Qu1'].where(mask, 'other')\n\n# Create a mask for the values in Qu2 and Qu3 that have at least 2 occurrences\nmask = df['Qu2'].apply(lambda x: pd.value_counts(df.Qu2) >= x) | df['Qu3'].apply(lambda x: pd.value_counts(df.Qu3) >= x)\n\n# Apply the mask to the Qu2 and Qu3 column\ndf['Qu2'] = df['Qu2'].where(mask, 'sausage')\ndf['Qu3'] = df['Qu3'].where(mask, 'sausage')\n\nresult = df\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a mask for the rows where Qu1 is 'apple' or 'potato'\nmask = (df['Qu1'] == 'apple') | (df['Qu1'] == 'potato')\n\n# Apply the mask to the Qu1 column\ndf.loc[mask, 'Qu1'] = 'other'\n\n# Create a mask for the rows where Qu2 is 'apple' or 'banana' or 'egg'\nmask = (df['Qu2'] == 'apple') | (df['Qu2'] == 'banana') | (df['Qu2'] == 'egg')\n\n# Apply the mask to the Qu2 and Qu3 column\ndf.loc[mask, 'Qu2'] = 'other'\ndf.loc[mask, 'Qu3'] = 'other'\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\ndf['keep_if_dup'] = df['keep_if_dup'].replace(['Yes', 'No'], [1, 0])\nresult = df.drop_duplicates(subset='url', keep='last')\nresult['keep_if_dup'] = result['keep_if_dup'].replace([1, 0], ['Yes', 'No'])\nresult\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\nresult = df.groupby('name')['v1', 'v2'].apply(lambda x: {k: {y: v} for k, y, v in x.values}).to_dict()\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": "\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\ndf = df.sort_values('datetime')\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# First, we need to parse the message to extract the key-value pairs\nimport json\n\n# Create a function to parse the message\ndef parse_message(message):\n    message = json.loads(message)\n    return message\n\n# Apply the function to the message column\ndf['message'] = df['message'].apply(parse_message)\n\n# Now, we can extract the key-value pairs and create a new dataframe\nresult = df[['name', 'status', 'number', 'message']].explode('message')\n\n# Finally, we can reset the index and rename the columns\nresult = result.reset_index(drop=True).rename(columns={'message': 'value'})\n\n# The 'value' column will contain the key-value pairs, and the 'name' and 'status' columns will contain the original values\nresult", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df['score'] * 10\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10\ndf['score'] = df['score'].where(df['product'].isin(products), df['score'])\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "\ndf['score'] = df['score'] * 10\n\nfor product in products:\n    df.loc[df['product'].isin(product), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\ndf['score'] = df['product'].apply(lambda x: 1 if x in products else df['score'].min() if x == products[0] else df['score'].max())\ndf['score'] = df['score'].apply(lambda x: (x - df['score'].min()) / (df['score'].max() - df['score'].min()))\ndf", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "\ndf['category'] = df.idxmax(axis=1)\ndf", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "\ndf['category'] = df.idxmax(axis=1)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col] == 1], axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf = df[df['Date'].between('17-Jan-2019', '20-Feb-2019')]\ndf", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\n\ndf = df.iloc[1:]\n\nprint(df)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\n\ndf = df.iloc[:-1, :]\n\nprint(df)\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\n\ndf['#1'] = df['#1'].fillna(df['#1'].iloc[0])\ndf['#2'] = df['#2'].fillna(df['#2'].iloc[-1])\n\ndf", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\ndf['#1'] = df['#1'].shift(-1)\n\n# shift the last row of the first column to the first row, first column\ndf['#2'] = df['#2'].shift(1)\n\n# drop the first row of the first column\ndf = df.drop(df.index[0])\n\n# print the dataframe\nprint(df)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'X' for col in df.columns]", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col for col in df.columns]\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "\n# Create a list of all column names that end with 'X'\nx_columns = [col for col in df.columns if col.endswith('X')]\n\n# Create a list of all column names that don't end with 'X'\nnon_x_columns = [col for col in df.columns if not col.endswith('X')]\n\n# Create a list of all column names that don't end with 'X' and add 'X' to the end\nnew_columns = [col + 'X' for col in non_x_columns]\n\n# Create a new dataframe with the new column names\nnew_df = df.rename(columns=dict(zip(non_x_columns + x_columns, new_columns)))\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "\ndef mean_columns(df):\n    return df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\n\nresult = mean_columns(df)\nprint(result)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "\ndef sum_columns(df):\n    return df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' in col})\n\nresult = sum_columns(df)\nprint(result)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndef aggregate_columns(df):\n    df['val1'] = df.filter(like='val1').sum(axis=1)\n    df['val2'] = df.filter(like='val2').mean(axis=1)\n    df['val42'] = df.filter(like='val42').sum(axis=1)\n    return df\n\nresult = df.groupby('group').apply(aggregate_columns)\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "\nresult = df.loc[row_list, column_list].mean()", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "\nresult = df.loc[row_list, column_list].sum()", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "\nresult = df.loc[row_list, column_list].sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult = result.sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = df.loc[row_list, result.index]\nresult", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "\nresult = df.apply(lambda x: x.value_counts() if x.name != 'temp' and x.name != 'name' else 0, axis=0)\nresult = result.fillna(0)\nresult = result.astype(float)\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "\n# Get the value counts for each column\nresult = df.apply(lambda x: x.value_counts())\n\n# Convert the result to a string\nresult = result.astype(str)\n\n# Remove the 'Name:' column\nresult = result.drop('name', axis=1)\n\n# Remove the 'null' column\nresult = result.drop('temp', axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0, 1]].combine_first(df.iloc[[2, 3]])\nresult.columns = ['Nanonose', 'Unnamed: 1', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\nresult", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0, 1]].append(df.iloc[[2, 3]])\nresult['Nanonose'] = ['Sample type', 'Water', 'Water', 'Water']\nresult['Unnamed: 1'] = ['Concentration', 9200, 9200, 9200]\nresult['A'] = [np.nan, 95.5, 94.5, 92.0]\nresult['B'] = [np.nan, 21.0, 17.0, 16.0]\nresult['C'] = [np.nan, 6.0, 5.0, 3.0]\nresult['D'] = [np.nan, 11.942308, 5.484615, 11.057692]\nresult['E'] = [np.nan, 64.134615, 63.205769, 62.586538]\nresult['F'] = [np.nan, 21.498560, 19.658560, 19.813120]\nresult['G'] = [np.nan, 5.567840, 4.968000, 5.192480]\nresult['H'] = [np.nan, 1.174135, 1.883444, 0.564835]\nresult", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "\ndef fill_nan(x):\n    x[x.notnull()] = 1\n    x[x.isnull()] = 2\n    return x\n\ndf = df.apply(fill_nan, axis=1)\nresult = df\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "\ndef replace_nan(x):\n    x[x.isnull()] = x[x.notnull()].values\n    return x\n\ndf = df.apply(replace_nan, axis=1)\nresult = df\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "\ndf = df.where(df.notnull(), df.mean())\nresult = df.where(df.isnull(), df.mean())\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# Create a mask for rows where value is less than threshold\nmask = df['value'] < thresh\n\n# Apply the mask to the dataframe\nresult = df.loc[mask]\n\n# Sum the rows where the mask is True\nresult['value'] = result.sum(axis=1)\n\n# Set the index back to 'lab'\nresult.set_index('lab', inplace=True)\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\n# Create a mask for rows where value is less than threshold\nmask = df['value'] < thresh\n\n# Apply the mask to the dataframe\ndf.loc[mask] = df.loc[mask].mean(axis=1).values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set the index to the lab column\ndf.index = df.index.values.reshape(-1, 1)\n\n# Set", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# Create a new column that indicates whether the value is in the given section\ndf['in_section'] = (df['value'] >= section_left) & (df['value'] <= section_right)\n\n# Calculate the average of the values in the given section\naverage = df.loc[df['in_section'], 'value'].mean()\n\n# Create a new row with the average value\nnew_row = pd.Series({'value': average})\n\n# Replace the rows in the original dataframe that are not in the given section with the new row\ndf.loc[~df['in_section'], 'value'] = new_row['value']\n\n# Drop the 'in_section' column\ndf.drop('in_section', axis=1, inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\nresult = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nfor col in df.columns:\n    result[f\"exp_{col}\"] = df[col]**2\n\nprint(result)\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\nresult = df.assign(inv_A=1/df['A'])\nresult", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndf = df.apply(sigmoid)\ndf.columns = ['sigmoid_' + c for c in df.columns]\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the last occurrence of the column-wise maximum\nmax_idx = df.idxmax()\n\n# Create a mask that includes the indices of the minimum and maximum values\nmask = (min_idx <= max_idx)\n\n# Apply the mask to the dataframe to get the last occurrence of the column-wise maximum\nresult = df.loc[mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Create a mask where the column is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=0)\n\n# Apply the mask to the index\nresult = min_idx[mask]\n\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "\n# Create a pivot table with the minimum and maximum dates as the index\npivot_table = df.pivot_table(index=['user', 'dt'], values='val', aggfunc='first')\n\n# Expand the index to have all the dates\nresult = pd.DataFrame(pivot_table.index.levels[0].values, columns=['user'])\n\n# Fill in the dates with 0\nresult['dt'] = pivot_table.index.levels[1].values\n\n# Fill in 0 for the val column\nresult['val'] = 0\n\n# Reset the index\nresult = result.reset_index()\n\n# Rename the columns\nresult = result.rename(columns={'user': 'user', 'dt': 'dt', 'val': 'val'})\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "\n# Create a new dataframe with all the dates from the minimum to the maximum date\nnew_df = pd.DataFrame({'user': df['user'].unique(), 'dt': pd.date_range(df['dt'].min(), df['dt'].max())})\n\n# Merge the new dataframe with the original dataframe\nresult = pd.merge(new_df, df, on=['user', 'dt'], how='left')\n\n# Fill the missing values with 0\nresult['val'] = result['val'].fillna(0)\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "\n# Create a new dataframe with the minimum and maximum dates\ndf_minmax = pd.DataFrame({'user': ['a','a','b','b'], 'dt': [df['dt'].min(), df['dt'].max(), df['dt'].min(), df['dt'].max()]})\n\n# Expand the date column with all the dates there\ndf_minmax['dt'] = pd.to_datetime(df_minmax['dt'])\n\n# Fill in 233 for the val column\ndf_minmax['val'] = df_minmax['val'].fillna(233)\n\n# Reset the index\ndf_minmax.reset_index(drop=True, inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Fill in the missing values with 233\ndf_minmax.fillna(233, inplace=True)\n\n# Reset the index\ndf_minmax.reset_index(drop=True, inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['user', 'dt'], inplace=True)\n\n# Set the index to user and dt\ndf_minmax.set_index(['", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "\n# Create a pivot table with the minimum and maximum dates for each user\npivot = df.pivot_table(index='user', columns='dt', aggfunc={'val': 'min', 'dt': 'max'})\n\n# Reset the index and rename the columns\npivot = pivot.reset_index().rename(columns={'dt': 'max_dt', 'val': 'max_val'})\n\n# Merge the pivot table with the original data frame\nresult = pd.merge(df, pivot, on=['user', 'max_dt'], how='left')\n\n# Fill the missing values with the maximum value for the user\nresult['val'] = result.groupby('user')['val'].transform('max')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "df['dt'] = df.groupby('user')['dt'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('max')\nresult = df.pivot(index='dt', columns='user', values='val')\nresult.index = result.index.strftime('%d-%b-%Y')\nresult", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "\n# Create a unique ID for each name\ndf['name'] = df['name'].astype('category').cat.codes\n\n# Replace the name with the unique ID\ndf['name'] = df['name'].replace({'Aaron': 1, 'Brave': 2, 'David': 3})\n\n# Create a new column with the unique ID\ndf['unique_id'] = df['name'].values\n\n# Drop the original name column\ndf = df.drop(columns='name')\n\n# Rename the unique ID column to 'name'\ndf = df.rename(columns={'unique_id': 'name'})\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "\n# Create a unique ID for each name\nunique_ids = df['name'].unique()\n\n# Create a dictionary to map the unique ID to the unique ID for each name\nid_map = {name: id for id, name in enumerate(unique_ids, 1)}\n\n# Replace the 'a' column with the unique ID for each name\ndf['a'] = df['name'].map(id_map)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "\n    df['name'] = df['name'].astype('category').cat.codes\n    df['name'] += 1\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\n# Create a unique ID for each name\ndf['ID'] = df['name'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df['ID'].factorize()[0]\n\n# Combine name and a and replace each of them with a unique ID\ndf['ID'] = df['ID'].astype(int)\ndf['ID'] = df['ID'].add(1).astype(int)\n\n# Create a new column 'ID' with the unique ID for each name\ndf['ID'] = df", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.pivot_table(index='user', columns='date', values='value', fill_value=0)\ndf['date'] = df['date'].apply(lambda x: x.strftime('%m/%d/%y'))\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'date', '01/12/15': 'value', '02/12/15': 'value2', 'someBool': 'someBool2'})\ndf = df[['date', 'value', 'value2', 'someBool', 'someBool2']]\n", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "\ndf['others'] = df['01/12/15']\ndf['value'] = df['02/12/15']\ndf = df.drop(['01/12/15', '02/12/15'], axis=1)\ndf = df.rename(columns={'user': 'user_column', 'someBool': 'bool_column'})\ndf = df.set_index('user_column')\ndf = df.unstack(fill_value=0)\ndf = df.stack()\ndf = df.reset_index()\ndf = df.rename(columns={'user_column': 'user', 'bool_column': 'bool_column', 0: 'value'})\ndf = df.drop(['bool_column'], axis=1)\ndf = df.rename(columns={0: 'value'})\ndf = df.set_index(['user', 'bool_column'])\ndf = df.unstack(fill_value=0)\ndf = df.stack()\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'user_column', 'bool_column': 'bool_column', 0: 'value'})\ndf = df.drop(['bool_column'], axis=1)\ndf = df.rename(columns={0: 'value'})\ndf = df.set_index(['user', 'bool_column'])\ndf = df.unstack(fill_value=0)\ndf = df.stack()\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'user_column', 'bool_column': 'bool_column', 0: 'value'})\ndf = df.drop(['bool_column'], axis=1)\ndf = df.rename(columns={0: 'value'})\ndf = df.set_index(['user', 'bool_column'])\ndf = df.unstack(fill_value=0)\ndf = df.stack()\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'user_column', 'bool_column': 'bool_column', 0: 'value'})\ndf = df.drop(['bool_column'], axis=1)\ndf = df.rename(columns={0: 'value'})\ndf = df.set_index(['user', 'bool_column'])\ndf = df.unstack(fill_value=0)\ndf = df.stack()\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'user_column', 'bool_column': 'bool_column', 0: 'value'})\ndf = df.drop(['bool_column'], axis=1)\ndf = df.rename(columns={0: 'value'})\ndf = df.set_index(['user', 'bool_column'])\ndf = df.unstack(fill_value=0)\ndf = df.stack()\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'user_column', 'bool_column': 'bool_column', 0: 'value'})\ndf = df.drop(['bool_column'], axis=1)\ndf = df.rename(columns={0: 'value'})\ndf = df.set_index(['user', 'bool_column'])\ndf = df.unstack(fill_value=0)\ndf = df.stack()\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'user_column', 'bool_column': 'bool_column', 0: 'value'})\ndf = df.drop(['bool_column'], axis=1)\ndf = df.rename(columns={0: 'value'})\ndf = df.set_index(['user', 'bool_column'])\ndf = df.unstack(fill_value=0)\ndf = df.stack()\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'user_column', 'bool_column': 'bool_column', 0: 'value'})\ndf = df.drop(['bool_column'], axis=1)\ndf = df.rename(columns={0: 'value'})\ndf = df.set_index(['user', 'bool_column'])\ndf = df.unstack(fill_value=0)\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf['date'] = df['date'].str.split(' ', 1).str[0]\ndf = df.pivot(index='user', columns='date', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'user', '01/12/15': 'date1', '02/12/15': 'date2'})\ndf = df.fillna(0)\ndf", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "\n# Select the rows where the value for column 'c' is greater than 0.5\ndf_sub = df[df['c'] > 0.5]\n\n# Select the columns 'b' and 'e'\nresult = df_sub[columns]\n\n# Convert the result to a numpy array\nresult = np.array(result)\n\nprint(result)\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "\ndf_subset = df[df['c'] > 0.45][columns]\nresult = df_subset.values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n\ndf = pd.DataFrame(np.random.rand(4, 5), columns=list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    df_sub = df.loc[df['c'] > 0.5, columns]\n    df_sub['sum'] = df_sub[columns[0]] + df_sub[columns[1]]\n    return df_sub\n\ndf = pd.DataFrame(np.random.rand(4, 5), columns=list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\n# Convert the date column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column that calculates the difference in days between the current row and the previous row\ndf['diff'] = df['date'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days'] = df['diff'].dt.days\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row and the previous row\ndf['days_since_previous'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current row", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\n# Convert the date column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks'] = (df['date'] - df['date'].shift()).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_last'] = (df['date'].shift() - df['date']).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_first'] = (df['date'] - df['date'].shift(1)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_first_row'] = (df['date'].shift(1) - df['date']).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_last_row'] = (df['date'].shift() - df['date'].shift(1)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_first_row_minus_one'] = (df['date'].shift(1) - df['date'].shift(2)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_last_row_minus_one'] = (df['date'].shift() - df['date'].shift(-1)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_first_row_minus_one_minus_one'] = (df['date'].shift(1) - df['date'].shift(2)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_last_row_minus_one_minus_one'] = (df['date'].shift() - df['date'].shift(-2)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_first_row_minus_one_minus_one_minus_one'] = (df['date'].shift(1) - df['date'].shift(2)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_last_row_minus_one_minus_one_minus_one'] = (df['date'].shift() - df['date'].shift(-3)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_first_row_minus_one_minus_one_minus_one_minus_one'] = (df['date'].shift(1) - df['date'].shift(2)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_last_row_minus_one_minus_one_minus_one_minus_one'] = (df['date'].shift() - df['date'].shift(-4)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_first_row_minus_one_minus_one_minus_one_minus_one_minus_one'] = (df['date'].shift(1) - df['date'].shift(2)).dt.days // 7\n\n# Create a new column that calculates the number of weeks between the current date and the date of the previous row\ndf['weeks_since_last_row_minus_one_minus_one_minus_one_minus_one_minus_one'] = (df['date'].shift() - df['date", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\n# Convert the date to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column 'week' which represents the week of the date\ndf['week'] = df['date'].dt.week\n\n# Create a new column 'week_end' which represents the end of the week\ndf['week_end'] = df['date'].dt.weekday\n\n# Create a new column 'week_start' which represents the start of the week\ndf['week_start'] = df['date'].dt.weekday\n\n# Create a new column 'week_start_date' which represents the start date of the week\ndf['week_start_date'] = df['date'].dt.date\n\n# Create a new column 'week_end_date' which represents the end date of the week\ndf['week_end_date'] = df['date'].dt.date\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df['col1']\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\ndf['col1'] = df['col1'].rolling(3).mean().reset_index(drop=True)\nresult = df", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\ndf['col1'] = df['col1'].groupby(df.index // 4).sum()\nresult = df['col1'].values", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df['col1'].values", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-2:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-1:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-2:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-1:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-2:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-1:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-2:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-1:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-2:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-1:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-2:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-1:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the cumulative sum of the dataframe\ndf['cumulative_sum'] = df['col1'].cumsum()\n\n# Create a new column with the cumulative sum of the dataframe from the end\ndf['cumulative_sum_end'] = df['cumulative_sum'].iloc[-3:]\n\n# Create a new column with the cumulative sum of the dataframe from the head\ndf['cumulative_sum_head'] = df['cumulative_sum'].iloc[:3]\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end'] = df['col1'].iloc[-3:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head'] = df['col1'].iloc[:3].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head'] = df['col1'].iloc[:3]\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end'] = df['col1'].iloc[-3:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head'] = df['col1'].iloc[:3].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head'] = df['col1'].iloc[:3]\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end'] = df['col1'].iloc[-3:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head'] = df['col1'].iloc[:3].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head'] = df['col1'].iloc[:3]\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end'] = df['col1'].iloc[-3:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head'] = df['col1'].iloc[:3].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head'] = df['col1'].iloc[:3]\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end'] = df['col1'].iloc[-3:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head'] = df['col1'].iloc[:3].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['col1'].iloc[-3:]\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head'] = df['col1'].iloc[:3]\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df['A'] = df['A'].replace(0, df['A'].dropna().iloc[-1])\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).max())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).min())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).mean())\ndf['A'] = df['A'].fillna(df['A'].ne(0).rolling(2).median())\n\ndf['A'] = df['A'].fillna(df['", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['number'] = df['duration'].str.extract('(\\d+)').astype(int)\ndf['time'] = df['duration'].str.extract('([a-zA-Z]+)').str.replace(' ', '')\ndf['time_days'] = df['time'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\ndf", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "\n    df['time'] = df['duration'].str.split(' ').str[0]\n    df['number'] = df['duration'].str.split(' ').str[1]\n    df['time_days'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number'].astype(int)\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.all(df1[columns_check_list] == df2[columns_check_list], axis=1)", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "\nresult = [all(getattr(df1, column) == getattr(df2, column) for column in columns_check_list) for _ in range(len(df1))]\n\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='date')\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='datetime')\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "\n    df['date'] = pd.to_datetime(df['date'])\n    return df[['date', 'x', 'y']].values\n    ### END SOLUTION\n\ndf = pd.DataFrame({'id': ['abc', 'abc', 'abc'],\n                   'date': ['3/1/1994', '9/1/1994', '3/1/1995'],\n                   'x': [100, 90, 80],\n                   'y': [7, 8, 9]})\n\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index(['id', 'date'])\n    df = df.swaplevel(0, 1)\n    return df\n    ### END SOLUTION\n\ndf = pd.DataFrame({'x': [100, 90, 80], 'y': [7, 8, 9], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df_long = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf_long['year'] = df_long['year'].astype(int)\ndf_long = df_long.pivot_table(index=['Country', 'Variable'], columns='year', values='value').reset_index()\ndf_long.columns = ['Country', 'Variable', 'year', 'value']\ndf_long", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df_long = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf_long['year'] = df_long['year'].astype(int)\ndf_long = df_long.sort_values(['Country', 'year', 'Variable'], ascending=[True, True, True])\ndf_long = df_long.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf_long.columns.name = None\ndf_long", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\ndf = df[(abs(df['Value_B']) > 1) | (abs(df['Value_C']) > 1) | (abs(df['Value_D']) > 1)]", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\ndf = df[(abs(df['Value_B']) > 1) | (abs(df['Value_C']) > 1) | (abs(df['Value_D']) > 1)]", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df['A'] = df['A'].str.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "\n    df.replace({'&AMP;': '&'}, regex=True, inplace=True)\n    return df\n\nf()\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "\ndf['A'] = df['A'].str.replace('&AMP;', '&''<''>')\ndf['A'] = df['A'].str.replace('&LT;', '&''<''>')\ndf['A'] = df['A'].str.replace('&GT;', '&''>''')\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "\ndef validate_single_space_name(name: str) -> str:\n    if ' ' in name:\n        return name.split(' ')[0]\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: validate_single_space_name(x) if ' ' in x else None)\n\ndf", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x if validate_single_space_name(x) else None)\n\ndf", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^(.*) (.*)$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return match_obj.groups()\n    else:\n        return None\n\ndf['first_name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(validate_single_space_name))\n\ndf = df.drop(columns=['name'])\n\ndf", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = df2.merge(df1, on='Timestamp', how='left')\nresult['data'] = result['data'].fillna(result['stuff'])\nresult = result.drop(['stuff'], axis=1)\nresult = result.sort_values('Timestamp')\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1['Timestamp'], df2['Timestamp'], on='Timestamp', direction='nearest')\nresult['data'] = df2['stuff']\nresult = result.sort_values('Timestamp')\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= row['col3'] else row['col1'].max(), axis=1)\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.is_numeric(x) else \"and\")\n\nresult = df['Field1'].tolist()\n\nprint(result)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: [int(i) for i in x.split(',') if i.isnumeric()])\n\nresult = df['Field1'].tolist()\n\nprint(result)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "\n    result = []\n    for value in df['Field1']:\n        if not isinstance(value, int):\n            result.append(value)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\n# Calculate the pairwise distances between cars\ndf['distance'] = ((df['x'] - df['x'].shift())**2 + (df['y'] - df['y'].shift())**2).sqrt()\n\n# Find the nearest neighbour for each car\ndf['nearest_neighbour'] = df.groupby('car')['distance'].transform('min')\n\n# Calculate the average of the distances for each frame\ndf['average_distance'] = df.groupby('time')['distance'].mean()\n\n# Drop the distance column\ndf = df.drop('distance', axis=1)\n\n# Rename the nearest neighbour column\ndf = df.rename(columns={'nearest_neighbour': 'nearest_neighbour_car'})\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\n# Create a new column 'distance' to store the euclidean distance between each car and its neighbour\ndf['distance'] = df.groupby('time')['car'].apply(lambda x: np.sqrt((x.x - x.x.shift())**2 + (x.y - x.y.shift())**2))\n\n# Create a new column 'farmost_neighbour' to store the car of the farthest neighbour for each time point\ndf['farmost_neighbour'] = df.groupby('time')['distance'].apply(lambda x: x.idxmax())\n\n# Calculate the average of the distances for each time point\ndf['average_distance'] = df.groupby('time')['distance'].mean()\n\n# Drop the 'distance' column\ndf = df.drop('distance', axis=1)\n\n# Print the df\nprint(df)\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: ', '.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndf['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values[::-1]), axis=1)\ndf", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Set Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df['UserId'] = df.index[df.index.isin(sample_df.index)]\n\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change the value of the ProductId column to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df['index'] = df.index[df.index.isin(sample_df.index)]\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# Create a new DataFrame with the 20% of rows of each user\ndf_sample = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n\n# Set the Quantity column of these rows to zero\ndf_sample['Quantity'] = 0\n\n# Keep the indexes of the altered rows\ndf_sample.reset_index(drop=True, inplace=True)\n\n# Print the resulting DataFrame\nprint(df_sample)\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "\n# Create a boolean series to identify duplicates\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n\n# Create a new dataframe with the original index and the duplicate boolean series\nduplicate = df.loc[duplicate_bool == True].reset_index(drop=True)\n\n# Add a new column 'index_original' to the duplicate dataframe to refer to the original index\nduplicate['index_original'] = duplicate.index\n\n# Print the duplicate dataframe\nprint(duplicate)\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "\n# Create a boolean series to identify duplicates\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\n\n# Create a new dataframe with the original index and the duplicate boolean series\nduplicate = df.loc[duplicate_bool == True].reset_index(drop=True)\n\n# Add a new column 'index_original' to the duplicate dataframe\nduplicate['index_original'] = duplicate.index\n\n# Print the duplicate dataframe\nprint(duplicate)\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "\n    df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\n    df['index_original'] = df['index_original'].apply(lambda x: df.index[x])\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "\n# First, create a boolean mask to identify the duplicates\nmask = df.duplicated(subset=['col1','col2', '3col'], keep=False)\n\n# Then, create a new column 'index_original' that contains the index of the first duplicate\ndf['index_original'] = df[mask].index[0]\n\n# Finally, select the rows that are not duplicates\nresult = df[~mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "\n# Create a boolean series where the last occurrence of each group is True\nduplicate_bool = df.groupby(['col1','col2']).apply(lambda x: x.index[-1] == x.index).values\n\n# Create a new dataframe with the duplicate rows and the index of the last occurrence\nduplicate = df.loc[duplicate_bool]\nduplicate['index_original'] = df.index[duplicate_bool]\n\nresult = duplicate\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# First, find the maximum count for each group\nmax_count = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Then, find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "\n# First, find the maximum count for each group\nmax_count = df.groupby(['Sp','Mt'])['count'].max()\n\n# Then, find the rows where the count is equal to the maximum count\nresult = df[df['count'] == max_count]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\n# First, find the minimum count for each group\nmin_count = df.groupby(['Sp', 'Mt'])['count'].min()\n\n# Then, find the rows where count equals the minimum count\nresult = df[df['count'] == min_count]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# First, find the maximum count for each group\nmax_count = df.groupby(['Sp','Value'])['count'].max()\n\n# Then, find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "\n# Create a list of tuples where each tuple contains the first column level, the second column level, and the third column level\nvalue_vars = [('A', 'B', 'E'),\n              ('A', 'B', 'F'),\n              ('A', 'C', 'G'),\n              ('A', 'C', 'H'),\n              ('A', 'D', 'I'),\n              ('A', 'D', 'J')]\n\n# Use the list of tuples to melt the data frame\nresult = pd.melt(df, value_vars=value_vars)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "\n# Define the tuples of column levels\ntuples = [(a, b, c) for a in df.columns[0] for b in df.columns[1] for c in df.columns[2]]\n\n# Create a new DataFrame with the tuples\ndf_new = pd.DataFrame(tuples, columns=['column_level_0', 'column_level_1', 'column_level_2'])\n\n# Merge the original DataFrame with the new DataFrame\ndf_new = pd.merge(df_new, df, left_on=['column_level_0', 'column_level_1', 'column_level_2'], right_on=['col1', 'col2', 'col3'])\n\n# Drop the original columns\ndf_new.drop(columns=['col1', 'col2', 'col3'], inplace=True)\n\n# Rename the columns\ndf_new.rename(columns={'column_level_0': 'variable_0', 'column_level_1': 'variable_1', 'column_level_2': 'variable_2'}, inplace=True)\n\n# Reset the index\ndf_new.reset_index(drop=True, inplace=True)\n\n# Set the value_vars\ndf_new.set_index(['variable_0', 'variable_1', 'variable_2'], append=True, inplace=True)\n\n# Melt the DataFrame\nresult = df_new.reset_index().melt(id_vars=['variable_0', 'variable_1', 'variable_2'], var_name='value')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] >= 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index(drop=True)\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "\ndef relationship(df):\n    result = []\n    for c1 in df.columns:\n        for c2 in df.columns:\n            if c1 != c2:\n                if df[c1].nunique() == 1:\n                    result.append(f'{c1} {c2} one-to-many')\n                elif df[c2].nunique() == 1:\n                    result.append(f'{c1} {c2} many-to-one')\n                else:\n                    result.append(f'{c1} {c2} many-to-many')\n    return result\n\nresult = relationship(df)\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "\ndef relationship(df):\n    result = []\n    for c1 in df.columns:\n        for c2 in df.columns:\n            if c1 != c2:\n                if df[c1].nunique() == 1:\n                    result.append(f'{c1} {c2} one-2-many')\n                elif df[c2].nunique() == 1:\n                    result.append(f'{c1} {c2} one-2-one')\n                else:\n                    result.append(f'{c1} {c2} many-2-many')\n    return result\n\nresult = relationship(df)\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "\ndef relationship(x, y):\n    if x.nunique() == 1:\n        return 'one-to-many'\n    elif y.nunique() == 1:\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\nresult = df.apply(relationship, axis=0)\nresult", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "\ndef relationship(x, y):\n    if x.shape[0] == 1:\n        return 'one-2-many'\n    elif y.shape[0] == 1:\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\nresult = df.apply(relationship, axis=1)\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "\n# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s: s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n\n# sort the dataframe by bank account\ndfiban_uniq = dfiban_uniq.sort_values(by='bank', ascending=False)\n\n# keep the records with a bank account\ndfiban_uniq = dfiban_uniq[dfiban_uniq['bank'].notna()]\n\nresult = dfiban_uniq\nresult\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\nprint(\"Hello, World!\")\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\nresult = df.groupby('Family')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndf['Family'] = (df['SibSp'] == 1) & (df['Parch'] == 1)\ndf.loc[df['Family'] == True, 'Family'] = 'Has Family'\ndf.loc[df['Family'] == False, 'Family'] = 'No Family'\n\ndf['Family'] = df['Family'].replace({True: 'Has Family', False: 'No Family'})\n\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\nresult", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "\ndef std_mean(x):\n    return np.std(np.mean(x))\n\nresult = df.groupby('a')['b'].apply(std_mean).reset_index(name='mean')\nresult['std'] = df.groupby('a')['b'].apply(std_mean)\n\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "\ndef std_mean(x):\n    return np.std(np.mean(x))\n\nresult = df.groupby('b')['a'].apply(std_mean).reset_index(name='mean')\nresult['std'] = df.groupby('b')['a'].apply(std_mean)\n\nprint(result)\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "\nimport numpy as np\n\n# calculate softmax\ndf['softmax'] = np.exp(df['b']) / np.exp(df['b']).sum()\n\n# calculate min-max\ndf['min-max'] = (df['b'] - df['b'].min()) / (df['b'].max() - df['b'].min())\n\ndf", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "\n# Create a mask to remove rows and columns with all zeros\nmask = df.ne(0)\n\n# Apply the mask to the dataframe\nresult = df[mask]\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "\n# Create a mask to exclude rows and columns with sum of 0\nmask = df.sum(axis=1) != 0\n\n# Apply the mask to the dataframe\nresult = df[mask]\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "\n# Create a mask to select the rows and columns with maximum value 2\nmask = (df == 2)\n\n# Apply the mask to the dataframe\nresult = df.loc[mask]\n\n# Remove the rows and columns with maximum value 2\nresult = result.loc[:, ~mask.columns]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "\n# Create a mask where the maximum value in each row is 2\nmask = df.max(axis=1) == 2\n\n# Apply the mask to the dataframe\nresult = df.where(mask)\n\nprint(result)\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=[True, False])\nresult", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.reset_index().sort_values(by=['index', 0]).reset_index(drop=True)\ndf['index'] = df['index'].astype(str)\ndf['index'] = df['index'].str.replace('tf', '')\ndf['index'] = df['index'].str.replace('p', '')\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('1', '')\ndf['index'] = df['index'].str.replace('0', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('-', '')\ndf['index'] = df['index'].str.replace('n', '')\ndf['index'] = df['index'].str.replace('e', '')\ndf['index'] = df['index'].str.replace('r', '')\ndf['index'] = df['index'].str.replace('o', '')\ndf['index'] = df['index'].str.replace('a', '')\ndf['index'] = df['index'].str.replace('i', '')\ndf['index'] = df['index'].str.replace('t', '')\ndf['index'] = df['index'].str.replace('u', '')\ndf['index'] = df['index'].str.replace('d', '')\ndf['index'] = df['index'].str.replace('s', '')\ndf['index'] = df['index'].str.replace('v', '')\ndf['index'] = df['index'].str.replace('m', '')\ndf['index'] = df['index'].str.replace('y', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "\nresult = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str) == 's']\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# First, find the maximum count for each group\nmax_count = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Then, find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "\n# First, find the maximum count for each group\nmax_count = df.groupby(['Sp','Mt'])['count'].max()\n\n# Then, find the rows where the count is equal to the maximum count\nresult = df[df['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\n# First, find the minimum count for each group\nmin_count = df.groupby(['Sp', 'Mt'])['count'].min()\n\n# Then, find the rows where count equals the minimum count\nresult = df[df['count'] == min_count]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# First, find the maximum count for each group\nmax_count = df.groupby(['Sp','Value'])['count'].max()\n\n# Then, find the rows where count equals the maximum count\nresult = df[df['count'] == max_count]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(pd.to_datetime('17/8/1926'))\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    df['Date'] = df['Member'].map(dict)\n    ### END SOLUTION\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val']).Val.transform('count')\ndf", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_w'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "\n# Create a new column 'zero' that counts the number of zero values in each date\ndf['zero'] = df['B'] == 0\n\n# Create a new column 'non_zero' that counts the number of non-zero values in each date\ndf['non_zero'] = df['B'] != 0\n\n# Print the result\nprint(df[['Date', 'zero', 'non_zero']])\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "\n# Create a new column 'Even' that counts the number of even numbers in each date\ndf['Even'] = df.groupby('Date')['B'].apply(lambda x: x.sum() if x.sum() % 2 == 0 else 0)\n\n# Create a new column 'Odd' that counts the number of odd numbers in each date\ndf['Odd'] = df.groupby('Date')['B'].apply(lambda x: x.sum() if x.sum() % 2 == 1 else 0)\n\n# Print the result\nprint(df[['Date', 'Even', 'Odd']])\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult['D'] = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.min)\nresult", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split(',').str[0].repeat(df['var2'].str.len())).compute()\n\nresult", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split(',').str[0].repeat(df['var2'].str.len())).compute()\n\nresult", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split('-')).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[0]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split('-').str[1:]).compute()\n\nresult = result", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf['fips'] = df['fips'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "\ndf[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf['row'] = df['row'].str.split(' ', 2, expand=True)[2]\ndf = df[['fips', 'medi', 'row']]\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndf = df.set_index('Name').replace(0, 1)\ndf = df.cumsum() / df.replace(0, 1)\ndf = df.reset_index()\ndf", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndf['2001'] = df['2001'].replace(0, 1)\ndf['2002'] = df['2002'].replace(0, 1)\ndf['2003'] = df['2003'].replace(0, 1)\ndf['2004'] = df['2004'].replace(0, 1)\ndf['2005'] = df['2005'].replace(0, 1)\ndf['2006'] = df['2006'].replace(0, 1)\n\ndf = df.cumsum()\n\ndf = df.div(df.sum(axis=1), axis=0)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, ", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "\n    df = df.set_index('Name')\n    df = df.replace(0, 1)\n    df = df.cumsum() / df.sum(axis=1)[:, None]\n    df = df.reset_index()\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndf['2001'] = df['2001'].replace(0, 1)\ndf['2002'] = df['2002'].replace(0, 1)\ndf['2003'] = df['2003'].replace(0, 1)\ndf['2004'] = df['2004'].replace(0, 1)\ndf['2005'] = df['2005'].replace(0, 1)\ndf['2006'] = df['2006'].replace(0, 1)\n\ndf = df.cumsum()\n\ndf = df.div(df.sum(axis=1), axis=0)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, 1)\n\ndf = df.replace(0, ", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)).apply(lambda x: 1 if x > 0 else 0)\ndf['label'] = df['label'].replace({0: 0, 1: 1})\ndf", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['label'] = df['label'].replace({0: 0, 1: 1, -1: -1})\ndf['label'] = df['label'].replace(df['DateTime'].shift(1), df['DateTime'])\ndf['label'] = df['label'].replace(df['DateTime'], 1)\n\ndf['DateTime'] = df['DateTime'].replace({'2000-01-04': 1, '2000-01-05': 1, '2000-01-06': 1, '2000-01-07': 0, '2000-01-08': -1})\n\ndf['label'] = df['DateTime'].replace({1: 1, 0: 0, -1: -1})\n\ndf\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: -1 if x < 0 else (1 if x > 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_timedelta(df['departure_time'].sub(df['arrival_time']))\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = pd.to_timedelta(df['Duration'])\ndf", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = pd.to_datetime(df['departure_time']) - pd.to_datetime(df['arrival_time'])\ndf['Duration'] = df['Duration'].apply(lambda x: x.total_seconds())\ndf", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'] - df['arrival_time']\ndf['Duration'] = df['Duration'].apply(lambda x: x.seconds)\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "\nresult = df[df['key2'] == 'one'].groupby('key1').size()", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "\nresult = df[df['key2'].endswith('e')].groupby('key1').size()\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.idxmax()\nmin_result = df.idxmin()\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df['value'].mode()[0]\nmedian_result = df['value'].median()", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "\nresult = df[df['closing_price'].between(99, 101)]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "\ndf1 = df.groupby(\"item\", as_index=False)[\"diff\"].min()\nresult = df[df.groupby(\"item\")[\"diff\"].transform(\"min\") == df[\"diff\"]]", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df['SOURCE_NAME']\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n\n# Calculate the number of NaN values to be filled\nfill_count = nan_count // 2\n\n# Create a mask for the first half of the NaN values\nmask = df['Column_x'].isna()\n\n# Fill the first half of the NaN values with 0\ndf.loc[mask[:fill_count], 'Column_x'] = 0\n\n# Create a mask for the second half of the NaN values\nmask = df['Column_x'].isna()\n\n# Fill the second half of the NaN values with 1\ndf.loc[mask[fill_count:], 'Column_x'] = 1\n\ndf", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n\n# Calculate the number of NaN values to be replaced\nreplacement_count = int(nan_count * 0.3)\n\n# Create a mask for the first 30% of NaN values\nmask = df['Column_x'].isna()\n\n# Create a mask for the middle 30% of NaN values\nmask[nan_count - replacement_count:nan_count] = False\n\n# Create a mask for the last 30% of NaN values\nmask[nan_count:] = False\n\n# Replace the NaN values with the corresponding value\ndf['Column_x'] = df['Column_x'].where(mask)\n\n# Fill the NaN values with the mode of the column\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0])\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# First, create a binary column with the same length as df['Column_x']\ndf['Column_x'] = df['Column_x'].astype(int)\n\n# Then, create a binary column with the same length as df['Column_x']\ndf['Column_x_binary'] = df['Column_x'].astype(bool)\n\n# Now, create a binary column with the same length as df['Column_x']\ndf['Column_x_binary_rounded'] = df['Column_x_binary'].astype(int)\n\n# Finally, replace the NaN values in df['Column_x'] with the rounded binary values\ndf['Column_x'] = df['Column_x_binary_rounded'].fillna(df['Column_x_binary'])\n\n# Print the df\nprint(df)\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame(list(zip(a['one'], b['one'])), columns=['one', 'two'])\nresult['two'] = list(zip(a['two'], b['two']))\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "\n# Create a list of dataframes\ndataframes = [a, b, c]\n\n# Create a list of tuples\ntuples = [(x['one'].values[0], x['two'].values[0], x['one'].values[1], x['two'].values[1]) for x in dataframes]\n\n# Create a dataframe from the tuples\nresult = pd.DataFrame(tuples, columns=['one1', 'two1', 'one2', 'two2'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\n# Create a list of tuples from the dataframes a and b\ntuples = list(zip(a['one'], b['one']))\n\n# Create a dataframe a_b with the tuples\na_b = pd.DataFrame(tuples, columns=['one', 'two'])\n\n# Fill the vacancy with np.nan\na_b['one'] = a_b['one'].fillna(np.nan)\na_b['two'] = a_b['two'].fillna(np.nan)\n\n# Print the result\nprint(a_b)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "\nresult = df.assign(text=lambda x: ', '.join(x['text']))\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "\nresult = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame()\nresult.columns = ['text']\nresult", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "\nresult = df['text'].apply(lambda x: ', '.join(x.split(', '))).reset_index(drop=True)\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = pd.Series(df['text'].values.flatten(), index=df['text'])\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "\nresult = df['text'].apply(lambda x: '-'.join(x.split('-')[::-1])).reset_index(drop=True)\nresult", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "\nresult = pd.concat([df1, df2], axis=0)\nresult = result.groupby(['id', 'city', 'district', 'date']).first().reset_index()\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n# First, we need to create a new column 'date_formatted' in df1 to match the format of df2\ndf1['date_formatted'] = df1['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n\n# Then, we merge df1 and df2 on 'id' and 'date_formatted'\nresult = pd.merge(df1, df2, on=['id', 'date_formatted'], how='left')\n\n# If there are any rows in df1 that don't have a corresponding row in df2, we fill the NaN values with 0\nresult = result.fillna(0)\n\n# Finally, we sort the result by 'id' and 'date_formatted'\nresult = result.sort_values(['id', 'date_formatted'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\ndf1['date'] = pd.to_datetime(df1['date'])\ndf2['date'] = pd.to_datetime(df2['date'])\n\ndf1 = df1.sort_values(['id', 'date'])\ndf2 = df2.sort_values(['id', 'date'])\n\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date'])\nresult = result.reset_index(drop=True)\n\nresult['date'] = result['date'].dt.strftime('%Y/%m/%d')\nresult\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['B'] = result['B_x']\nresult.drop(['B_x', 'B_y'], axis=1, inplace=True)\nresult.rename(columns={'B_y': 'B'}, inplace=True)\nresult.sort_values('A', inplace=True)\nresult.reset_index(drop=True, inplace=True)\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "\nresult = pd.merge(C, D, on='A', how='outer')\nresult['B_x'] = result['B_x'].fillna(result['B'])\nresult['B_y'] = result['B_y'].fillna(result['B'])\nresult = result.drop(['B'], axis=1)\nresult.columns = ['A', 'B_x', 'B_y']\nresult", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "\n# Create a new column 'duplicated' in C that is True if 'A' is in D and False otherwise\nC['duplicated'] = C['A'].isin(D['A'])\n\n# Merge C and D on 'A' and keep the duplicated column\nresult = pd.merge(C, D, on='A', how='outer').drop('A', axis=1)\n\n# Fill the NaN values in the duplicated column with False\nresult['duplicated'] = result['duplicated'].fillna(False)\n\n# Rename the duplicated column to 'dulplicated'\nresult.rename(columns={'duplicated': 'dulplicated'}, inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist())\nresult = result.sort_values(by=['time', 'amount'])\nresult = result.reset_index(drop=True)\nresult\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "\n# Create a new column 'amount-time-tuple'\ndf['amount-time-tuple'] = list(zip(df['time'], df['amount']))\n\n# Sort the dataframe by 'user' and 'time'\ndf = df.sort_values(['user', 'time'])\n\n# Reset the index of the dataframe\ndf = df.reset_index(drop=True)\n\n# Convert the 'amount-time-tuple' column to a list\ndf['amount-time-tuple'] = df['amount-time-tuple'].apply(list)\n\n# Set the 'amount-time-tuple' column as the new dataframe\ndf = df[['user', 'amount-time-tuple']]\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "\n# Create a new column 'amount-time-tuple'\ndf['amount-time-tuple'] = list(zip(df['time'], df['amount']))\n\n# Sort the dataframe by 'user' and 'amount-time-tuple'\ndf = df.sort_values(['user', 'amount-time-tuple'])\n\n# Reset the index of the dataframe\ndf = df.reset_index(drop=True)\n\n# Drop the 'amount-time-tuple' column\ndf = df.drop(columns='amount-time-tuple')\n\n# Rename the 'time' and 'amount' columns to 'time-amount'\ndf = df.rename(columns={'time': 'time-amount', 'amount': 'amount-time'})\n\n# Reset the index of the dataframe\ndf = df.reset_index(drop=True)\n\n# Print the dataframe\nprint(df)\n\n# Save the dataframe to a csv file\ndf.to_csv('result.csv')\n\n# END SOLUTION\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\ndf = pd.DataFrame(series.values.reshape(-1, 1), index=series.index, columns=['column'])\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'file'})\ndf = df.set_index(['file'])\ndf_concatenated = df.transpose()\ndf_concatenated", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\ndf = pd.DataFrame(series.values.tolist(), index=series.index, columns=['0', '1', '2', '3'])\ndf.columns = ['name'] + df.columns.tolist()\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'name'})\ndf", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "\nresult = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "\n# Find the column name that contains the string 'spike'\ncolumn_name = df.columns[df.columns.str.contains(s)].values[0]\n\n# Create a dataframe with the column name\nresult = df[[column_name]]\n\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "\n# Find the column names that contain the string 'spike'\ncolumns = [col for col in df.columns if s in col]\n\n# Create a new dataframe with the columns\nresult = df[columns]\n\n# Rename the columns\nresult.columns = [col.replace('-2', '') for col in result.columns]\n\nprint(result)\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# First, we need to convert the list to a DataFrame\ndf_codes = pd.DataFrame(df['codes'].to_list(), columns=['code_0', 'code_1', 'code_2', 'code_3'])\n\n# Then, we need to fill the NaN values with 0\ndf_codes = df_codes.fillna(0)\n\n# Finally, we need to transpose the DataFrame to get the columns as the first level of the index\nresult = df_codes.transpose()\n\n# The result is a DataFrame with the columns as the first level of the index\nprint(result)\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# First, we need to convert the list to a DataFrame\ndf_codes = pd.DataFrame(df['codes'].to_list(), columns=['code_' + str(i) for i in range(1, len(df['codes']) + 1)])\n\n# Then, we merge the original DataFrame with the converted DataFrame\nresult = pd.merge(df, df_codes, left_index=True, right_index=True)\n\n# Finally, we drop the original 'codes' column\nresult.drop('codes', axis=1, inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# First, we need to flatten the list of codes into a single series\nflat_codes = df['codes'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a new dataframe with the flattened list of codes\ndf_flat = pd.DataFrame(flat_codes.tolist(), columns=['code_' + str(i) for i in range(1, len(flat_codes) + 1)])\n\n# Finally, we merge the original dataframe with the flattened dataframe\nresult = pd.merge(df, df_flat, left_index=True, right_index=True)\n\n# The result is a dataframe where the codes are sorted and split into columns\nprint(result)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\nimport pandas as pd\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\nresult = df['col1'].apply(lambda x: [item for sublist in x for item in sublist]).tolist()\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\nimport pandas as pd\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3],[4,5]]))\n\nresult = ','.join(str(i) for sublist in df['col1'].values for i in sublist[::-1])\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\nimport pandas as pd\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\nresult = ','.join(str(i) for sublist in df['col1'].values for i in sublist)\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "\n# Convert the 'Time' column to a datetime object\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Set the 'Time' column as the index of the DataFrame\ndf.set_index('Time', inplace=True)\n\n# Create a new column 'Value' with the mean of the 'Value' column\ndf['Value'] = df['Value'].resample('2min').mean()\n\n# Reset the index of the DataFrame\ndf.reset_index(inplace=True)\n\n# Rename the 'Time' column to 'index'\ndf.rename(columns={'Time': 'index'}, inplace=True)\n\n# Set the 'index' column as the index of the DataFrame\ndf.set_index('index', inplace=True)\n\n# Print the DataFrame\nprint(df)\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "\n# Convert the time to minutes\ndf['Time'] = df['Time'].map(lambda x: x.replace(minute=x.minute // 3 * 3))\n\n# Group by the time and sum the values\ndf = df.groupby('Time').sum()\n\n# Reset the index\ndf = df.reset_index()\n\n# Rename the columns\ndf.columns = ['Time', 'Value']\n\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[df.index.get_level_values('a') != 2]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[~filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[df.isna().all(axis=1)]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.columns[df.iloc[0] == df.iloc[8]]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[df.isna().all(axis=1)]", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(x, y) for x, y in zip(df.iloc[0], df.iloc[8]) if not np.isnan(x) or not np.isnan(y)]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.stack().reset_index(level=1, drop=True).unstack().reset_index(level=1, drop=True)\ndf.columns = [f'{c}_{i}' for i, c in enumerate(df.columns)]\ndf = df.iloc[[0]]\n", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.transpose().reset_index(drop=True)\ndf.columns = [f'{col}_{i}' for i, col in enumerate(df.columns)]\ndf = df.transpose().reset_index(drop=True)\ndf.columns = [f'{col}_{i}' for i, col in enumerate(df.columns)]\ndf", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "\nresult = df.sort_values(by=['time'])\nresult", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "\nresult = df.sort_values(['time', 'VIM'])\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-1", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "\n# Create a new column 'Day' that contains the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Filter the rows where the day is not 'Friday'\nresult = df[df['Day'] != 'Friday']\n\n# Set the 'Date' column as the index of the result\nresult.set_index('Date', inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "\n# Create a mask for the correlation matrix where the value is above 0.3\nmask = (corr > 0.3).stack()\n\n# Create a new DataFrame where the values are above 0.3\nresult = corr.where(mask)\n\n# Reset the index and column names\nresult = result.reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr.stack().reset_index()\nresult = result[result[0] > 0.3]\nresult = result[result[1] > 0.3]\nresult = result[result[0] != result[1]]\nresult = result.drop(columns=[0, 1])\nresult = result.rename(columns={0: 'column1', 1: 'column2'})\nresult = result.groupby(['column1', 'column2']).size().reset_index(name='count')\nresult = result[result['count'] > 1]\nresult = result[result['count'] > 1]\nresult = result.drop(columns=['count'])\nresult = result.rename(columns={'column1': 'column1', 'column2': 'column2'})\nresult = result.set_index(['column1', 'column2'])\nresult = result.unstack(level=-1)\nresult = result.where(result > 0.3)\nresult = result.stack().reset_index()\nresult = result[result[0] > 0.3]\nresult = result[result[1] > 0.3]\nresult = result[result[0] != result[1]]\nresult = result.drop(columns=[0, 1])\nresult = result.rename(columns={0: 'column1', 1: 'column2'})\nresult = result.groupby(['column1', 'column2']).size().reset_index(name='count')\nresult = result[result['count'] > 1]\nresult = result[result['count'] > 1]\nresult = result.drop(columns=['count'])\nresult = result.rename(columns={'column1': 'column1', 'column2': 'column2'})\nresult = result.set_index(['column1', 'column2'])\nresult = result.unstack(level=-1)\nresult = result.where(result > 0.3)\nresult = result.stack().reset_index()\nresult = result[result[0] > 0.3]\nresult = result[result[1] > 0.3]\nresult = result[result[0] != result[1]]\nresult = result.drop(columns=[0, 1])\nresult = result.rename(columns={0: 'column1', 1: 'column2'})\nresult = result.groupby(['column1', 'column2']).size().reset_index(name='count')\nresult = result[result['count'] > 1]\nresult = result[result['count'] > 1]\nresult = result.drop(columns=['count'])\nresult = result.rename(columns={'column1': 'column1', 'column2': 'column2'})\nresult = result.set_index(['column1', 'column2'])\nresult = result.unstack(level=-1)\nresult = result.where(result > 0.3)\nresult = result.stack().reset_index()\nresult = result[result[0] > 0.3]\nresult = result[result[1] > 0.3]\nresult = result[result[0] != result[1]]\nresult = result.drop(columns=[0, 1])\nresult = result.rename(columns={0: 'column1', 1: 'column2'})\nresult = result.groupby(['column1', 'column2']).size().reset_index(name='count')\nresult = result[result['count'] > 1]\nresult = result[result['count'] > 1]\nresult = result.drop(columns=['count'])\nresult = result.rename(columns={'column1': 'column1', 'column2': 'column2'})\nresult = result.set_index(['column1', 'column2'])\nresult = result.unstack(level=-1)\nresult = result.where(result > 0.3)\nresult = result.stack().reset_index()\nresult = result[result[0] > 0.3]\nresult = result[result[1] > 0.3]\nresult = result[result[0] != result[1]]\nresult = result.drop(columns=[0, 1])\nresult = result.rename(columns={0: 'column1', 1: 'column2'})\nresult = result.groupby(['column1', 'column2']).size().reset_index(name='count", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.rename(columns={df.columns[0]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\n# Create a new column 'frequent' that indicates the most frequent value in each row\ndf['frequent'] = df.apply(lambda row: max(row), axis=1)\n\n# Create a new column 'freq_count' that counts the number of times the most frequent value occurs in each row\ndf['freq_count'] = df.apply(lambda row: row['frequent'].value_counts()[row['frequent']], axis=1)\n\ndf", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\n# Create a new column 'frequent' and 'freq_count'\ndf['frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n\n# Drop the original columns\ndf = df.drop(['bit1', 'bit2', 'bit3', 'bit4', 'bit5'], axis=1)\n\n# Rename the new columns\ndf = df.rename(columns={'frequent': 'frequent_value', 'freq_count': 'frequency_count'})\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\n# Create a new column 'frequent' that contains the frequent values in each row\ndf['frequent'] = df.apply(lambda row: list(row[row == 1]), axis=1)\n\n# Create a new column 'freq_count' that contains the count of frequent values in each row\ndf['freq_count'] = df['frequent'].apply(lambda x: len(x))\n\ndf", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "\ndf['bar'] = df['bar'].replace('NULL', 0)\ndf['bar'] = df['bar'].astype(float)\n\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nresult['bar'] = df.groupby(['id1', 'id2'])['bar'].mean()\n\nresult", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "\n# Convert the 'bar' column to numeric if it's not already\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce')\n\n# Fill the NaN values with 0\ndf['bar'].fillna(0, inplace=True)\n\n# Group by 'id1' and 'id2', and calculate the mean of 'foo' and 'bar'\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\n\n# Reset the index of the result\nresult.reset_index(inplace=True)\n\n# Convert the 'bar' column to float if it's not already\nresult['bar'] = result['bar'].astype(float)\n\n# Fill the NaN values with 0\nresult['bar'].fillna(0, inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = df_c[['EntityNum', 'foo', 'a_col']]\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result.drop(['a_col'], axis=1)\nresult", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[~np.isnan(x)]", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "import numpy as np\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nresult = x.tolist()\nresult = [i for i in result if not np.isnan(i)]\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((len(a), 4))\nb[:, a] = 1", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((len(a), 4))\nb[:, a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((len(a), 5))\nb[:, a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((len(a), 3))\nb[:, a.astype(int)] = 1", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nb[a[:, 0] - 1, a[:, 1] - 1] = 1", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=1)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=0)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape)\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    return max_value, max_index\n    ### END SOLUTION ###\n\nresult = f(example_a)\nprint(result)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2)[-2], a.shape)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "import numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\n\nz = np.any(np.isnan(a), axis=0)\n\na = np.delete(a, z, axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "import numpy as np\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \nresult = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "result = np.unravel_index(np.argmin(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.argwhere(a == a.min())\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.deg2rad(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "cos_value = np.cos(np.deg2rad(degree))\nresult = np.rad2deg(cos_value)\nprint(result)\n", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.degrees(np.arcsin(value))\nprint(result)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "result = np.pad(A, (0, length - A.shape[0]), 'constant')", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "result = np.pad(A, (0, length - A.shape[0]), 'constant')", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "print(np.power(a, power))\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nresult = f(example_a, 5)\nprint(result)\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), 1)\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.array([numerator, denominator])\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "import numpy as np\nnumerator = 98\ndenominator = 42\n\nif denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator // denominator, denominator // denominator)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.where(a > b, a, b)\nresult = np.where(result > c, result, c)\n", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a[-1, :])[::-1]", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a[-1, :])[::-1]", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.concatenate((np.diag(a.shape[0] - np.arange(a.shape[0])), np.diag(a.shape[0] - np.arange(a.shape[0]) - 1)), axis=1)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(a.shape[0]), np.diag(a.shape[1] - 1, k=1)), axis=1)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = [x for x in np.ravel(X)]\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\nresult = np.array(result)\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\nprint(result)", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "import numpy as np\n\ndef get_maximal_independent_vectors(a):\n    # Get the transpose of the matrix\n    a_transpose = a.transpose()\n\n    # Get the eigenvalues of the transposed matrix\n    eigenvalues = np.linalg.eigvals(a_transpose)\n\n    # Get the indices of the maximum eigenvalue\n    max_index = np.argmax(eigenvalues)\n\n    # Get the maximum eigenvalue\n    max_eigenvalue = eigenvalues[max_index]\n\n    # Get the indices of the maximum eigenvalue\n    max_index = np.argwhere(eigenvalues == max_eigenvalue)\n\n    # Get the maximum independent vectors\n    result = a[:, max_index]\n\n    return result\n\nresult = get_maximal_independent_vectors(a)\nprint(result)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b, equal_var=True, nan_policy='omit')\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "n = max(len(a), len(b))\nt_statistic, p_value = scipy.stats.ttest_ind(a[:n], b[:n], equal_var=False, nan_policy='omit')\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the standard deviation\nstd_dev = np.sqrt(np.sum((amean - bmean)**2) / (anobs + bnobs - 2))\n\n# calculate the t-statistic\nt_statistic = (amean - bmean) / std_dev\n\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_statistic, anobs + bnobs - 2)\n\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.setdiff1d(np.setdiff1d(A.flatten(), B.flatten()), B.flatten())\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=2)\nresult = a[result]\nresult = np.sort(result, axis=1)\nresult = a[result]\nresult = np.sort(result, axis=0)\nresult = a[result]\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "import numpy as np\npairs = [(2, 3), (3, 4), (4, 5)]\narray_of_arrays = np.array([np.arange(a*b).reshape(a,b) for (a, b) in pairs])\nresult = np.array(array_of_arrays, copy=True)\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.array_equal(a[0], a[1]), axis=1)", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.equal(a[:, 1:], a[:, 0]), axis=1)", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.array_equal(a[0], a[1]))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    return (np.cos(x))**4 + (np.sin(y))**2\n    ### END SOLUTION ###\n\n# create a grid\nx, y = np.meshgrid(example_x, example_y)\n\n# calculate the integral\nresult = np.sum(f(x, y))\n\nprint(result)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(data):\n    # sort the data\n    x = np.sort(data)\n    # calculate the number of data points\n    n = x.shape[0]\n    # calculate the x values\n    x_values = np.arange(1, n + 1) / n\n    # calculate the y values\n    y_values = np.arange(1, n + 1) / n\n    # return the x values and y values\n    return x_values, y_values", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\n# compute ECDF\necdf_values = ecdf(grades)\n\n# find the longest interval [low, high) that satisfies ECDF(x) < threshold\nlow, high = 0, 1\nwhile ecdf_values(high) < threshold:\n  low = high\n  high *= 2\n\n# print the longest interval\nprint(low, high)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.choice([0, 1], size=size, p=[one_ratio, 1-one_ratio])", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\nresult = result.tolist()\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[-N:][::-1]\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "\ndef extract_patches(array, patch_shape):\n    array_shape = array.shape\n    patch_shape = np.array(patch_shape)\n    patch_strides = array.strides\n    view_shape = tuple(np.concatenate((patch_shape, array_shape)))\n    view_strides = tuple(np.concatenate((patch_strides, patch_strides)))\n    result = np.lib.stride_tricks.as_strided(array, view_shape, view_strides)\n    return result.reshape(-1, patch_shape[0], patch_shape[1])\n\npatch_shape = (2, 2)\nresult = extract_patches(a, patch_shape)\nprint(result)\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "\ndef extract_patches(array, patch_shape):\n    array_shape = array.shape\n    patch_shape = np.array(patch_shape)\n    patch_strides = array.strides\n    total_patches = np.array([array_shape[i] - patch_shape[i] + 1 for i in range(len(patch_shape))])\n    patch_strides = np.array([array_shape[i] // patch_shape[i] * patch_shape[i] for i in range(len(patch_shape))])\n    patches = np.lib.stride_tricks.as_strided(array, shape=total_patches, strides=patch_strides)\n    return patches\n\npatch_shape = (2, 2)\nresult = extract_patches(a, patch_shape)\nprint(result)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "\ndef extract_patches(array, patch_shape):\n    array_shape = array.shape\n    patch_shape = np.array(patch_shape)\n    patch_strides = array.strides\n    view_shape = tuple(np.concatenate((patch_shape, array_shape)))\n    view_strides = tuple(np.concatenate((patch_strides, patch_strides)))\n    result = np.lib.stride_tricks.as_strided(array, view_shape, view_strides)\n    return result.reshape(-1, patch_shape[0], patch_shape[1])\n\npatch_shape = (2, 2)\nresult = extract_patches(a, patch_shape)\nprint(result)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "\n# calculate the number of patches\nnum_patches = (a.shape[0] - patch_size + 1) * (a.shape[1] - patch_size + 1)\n\n# create a 3-d array to store the patches\nresult = np.zeros((num_patches, patch_size, patch_size))\n\n# loop over the array and store the patches\nfor i in range(a.shape[0] - patch_size + 1):\n    for j in range(a.shape[1] - patch_size + 1):\n        result[i, :, :] = a[i:i+patch_size, j:j+patch_size]\n\n# remove the patches that are not divisible by patch size\nresult = result[:num_patches, :, :]\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\nresult", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "\n# calculate the number of patches\nnum_patches = (a.shape[0] - patch_size + 1) * (a.shape[1] - patch_size + 1)\n\n# create a 3-d array to store the patches\nresult = np.zeros((num_patches, patch_size, patch_size))\n\n# loop over the array and store the patches\nfor i in range(a.shape[0] - patch_size + 1):\n    for j in range(a.shape[1] - patch_size + 1):\n        result[i, :, :] = a[i:i+patch_size, j:j+patch_size]\n\n# remove the patches that are not divisible by patch size\nresult = result[:num_patches, :, :]\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "import numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(string)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.random.uniform(min, max, n)\nresult = np.log(result)\n", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.random.loguniform(min, max, n)", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.log(np.random.uniform(min, max, n))\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.array([])", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3, 0))", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = np.sub2ind(a.shape, index)\nprint(result)", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.array([np.sum(a[accmap == i]) for i in np.unique(accmap)])", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0], axis=0)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = a[index]\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.array([[elementwise_function(x[i][j], y[i][j]) for j in range(len(x[i]))] for i in range(len(x))])", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, p=probabilit)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "result = np.pad(a, ((low_index, high_index), (low_index, high_index)), 'constant')[low_index:high_index, low_index:high_index]", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[x.real != 0]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.array_split(data, np.arange(bin_size, len(data) + 1, bin_size))\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (3 * x**2 - 2 * x**3)", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0, 1)\n    t = t * t * (3 - 2 * t)\n    return x_min + (x_max - x_min) * t", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\n# Create a circular correlation\nresult = np.correlate(a, b, mode='full')\n\n# Remove the zero-padding\nresult = result[len(a):]\n\nprint(result)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.groupby(['major', 'timestamp']).values.reshape(-1, 15, 5)\nresult = np.array(result, dtype=np.float32)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = df.groupby('major').apply(lambda x: x.values.reshape(1, 4, 5)).values", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.array([np.unpackbits(np.uint8(num)) for num in a])\nresult = result.reshape(-1, m)\n", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.array([np.unpackbits(np.uint8(num)) for num in a])\nresult = result[:, :m]\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "\n# Convert the integer array to binary array of length m\nbinary_array = np.unpackbits(np.uint8(a))\n\n# Create a matrix of zeros with shape (1, m)\nresult = np.zeros((1, m))\n\n# Compute exclusive OR of all the rows\nfor i in range(m):\n    result[:, i] = np.bitwise_xor(binary_array[:, i], result[:, i])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc - 3*\u03c3\n\u03bc + 3*\u03c3", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc - 2*\u03c3\n\u03bc + 2*\u03c3", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    third_std_dev = np.std(a, ddof=1)\n    start = mean - 3 * third_std_dev\n    end = mean + 3 * third_std_dev\n    return (start, end)\n    ### END SOLUTION ###\n\nresult = f(example_a)\nprint(result)\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\nresult = (a >= lower_bound) & (a <= upper_bound)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.compressed(), percentile)\nprint(prob)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[:, 0] = 0", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.argmax(a, axis=1)\nmask = np.where(mask == 1, True, False)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "import scipy.stats\n\nresult = scipy.stats.pearsonr(post, distance)\nprint(result)", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ij,ij->ij', X, X)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.dot(Y.transpose(1, 2, 0), Y)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "C = A[~np.isin(A, B)]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[np.in1d(A, B)]", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "\n# Create a boolean array that indicates whether each element in A is in B\nmask = np.isin(A, B)\n\n# Use the mask to index A and get the result\nC = A[mask]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = rankdata(a, method='max') - 1", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.array(len(a) - rankdata(a) + 1, dtype=int)\nprint(result)\n", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    result = np.array(len(a) - rankdata(a))\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.linalg.norm(X, ord=1, axis=1)\nresult = X / l1[:, np.newaxis]", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "result = np.array([LA.norm(v, ord=2) for v in X])\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = X / x[:, np.newaxis]\nprint(result)\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].str.contains(target)\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "import scipy.spatial\n\ndistance_matrix = scipy.spatial.distance.cdist(a, a, metric='euclidean')\nresult = distance_matrix", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "import scipy.spatial\n\nresult = scipy.spatial.distance.cdist(a, a, metric='euclidean')", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "import scipy.spatial\n\nresult = scipy.spatial.distance.cdist(a, a, metric='euclidean')", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# Remove the adjacent (before removing) duplicate non-zero value\na = a[a != 0]\n\n# Remove the zero value\na = a[a != 0]\n\nresult = a\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# Create a mask to exclude the adjacent duplicate non-zero values and all the zero values\nmask = np.concatenate(([True], (a[:-1] != a[1:]) & (a[:-1] != 0) & (a[1:] != 0), [True]))\n\n# Apply the mask to the array\nresult = a[mask]", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf = df.sort_values(['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ### END SOLUTION\n    return df\n\ndf = f(lat, lon, val)\nprint(df)\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df.max(axis=1)\ndf = df.reset_index(drop=True)\ndf = df.sort_values(by=['lat', 'lon'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "\ndef rolling_window(a, size):\n    shape = a.shape\n    strides = a.strides\n    result = np.lib.stride_tricks.as_strided(a, shape=(shape[0]-size[0]+1, shape[1]-size[1]+1), strides=(strides[0], strides[1]))\n    return result", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "\ndef rolling_window(a, size):\n    shape = a.shape\n    strides = a.strides\n    result = []\n    for i in range(shape[0] - size[0] + 1):\n        for j in range(shape[1] - size[1] + 1):\n            result.append(a[i:i + size[0], j:j + size[1]])\n    return np.array(result)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:, :, -1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = c in CNTS\nprint(result)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = any(np.array_equal(c, cnt) for cnt in CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.matrix(np.eye(4, 4))\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[1:, 1:] = 0", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_datetime = pd.to_datetime(start)\nend_datetime = pd.to_datetime(end)\n\n# Calculate the duration of the series\nduration = end_datetime - start_datetime\n\n# Calculate the number of seconds in the duration\nduration_seconds = duration.total_seconds()\n\n# Calculate the number of intervening elements\nintervening_elements = n\n\n# Calculate the frequency of the series\nfrequency = duration_seconds / intervening_elements\n\n# Create the series\nresult = pd.date_range(start_datetime, end_datetime, periods=n)\n\n# Convert the result to pandas.DatetimeIndex\nresult = pd.DatetimeIndex(result)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No such index\")\nelse:\n    print(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))\nresult = indices[0]", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "\n# calculate the coefficients a, b and c using the numpy.polyfit function\nresult = np.polyfit(x, y, 2)\n\n# print the coefficients in the order of highest to lowest\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "\n# Create a matrix with the x values as columns\nx_matrix = np.column_stack((np.ones(len(x)), x))\n\n# Calculate the coefficients using the formula for the least squares solution\ncoefficients = np.linalg.inv(x_matrix.T.dot(x_matrix)).dot(x_matrix.T).dot(y)\n\n# Create a function that calculates the output for a given x value\ndef f(x, coefficients):\n    return sum(coefficient * x ** i for i, coefficient in enumerate(coefficients))\n\n# Calculate the squared error for each x value\nsquared_error = np.sum((f(x, coefficients) - y) ** 2)\n\n# Calculate the parameters by minimizing the squared error\nparameters = coefficients[np.argmin(squared_error)]\n\n# Print the parameters\nprint(parameters)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = [0, 1, 2, 3]\ndf = df - temp_arr\nprint(df)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr >= -10\narr[mask] = arr[mask] + 5\n\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr[arr < n1] = 0\narr[(arr >= n1) & (arr < n2)] = 30\narr[(arr >= n2)] = 5\n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(np.abs(s1 - s2) > 1e-10)[0].shape[0]", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "print np.nonzero(np.logical_not(np.isclose(s1, s2)))[0].shape[0]\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a[1]) and np.array_equal(a[0], a[2])", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all() for arr in a)", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "\n# calculate the number of rows and columns to pad\nrows_pad = shape[0] - a.shape[0]\ncols_pad = shape[1] - a.shape[1]\n\n# zero pad the array\nresult = np.pad(a, ((0, rows_pad), (0, cols_pad)), mode='constant')\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "\n# calculate the maximum shape\nmax_shape = np.array(shape)\n\n# calculate the maximum size\nmax_size = np.array(a.shape)\n\n# calculate the padding\npadding = (max_shape - max_size) // 2\n\n# zero pad the array\nresult = np.pad(a, padding, mode='constant')\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "\n# calculate the maximum shape\nmax_shape = np.array(shape) + np.array(a.shape) - 1\n\n# calculate the padding\npadding = np.array(shape) - np.array(a.shape)\n\n# pad the array\nresult = np.pad(a, padding, mode='constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # calculate the difference between the shape of the original array and the shape of the padded array\n    diff = np.array(shape) - np.array(arr.shape)\n    # use the numpy pad function to pad the array\n    result = np.pad(arr, diff, mode='constant')\n    ### END SOLUTION ###\n    return result\n\n# test the function with a small array\nprint(f(arr))\n# test the function with a large array\nprint(f(np.ones((100, 13))))\n# test the function with a large array\nprint(f(np.ones((1000, 13))))\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "\n# calculate the maximum shape\nmax_shape = np.array(shape)\n\n# calculate the maximum size\nmax_size = np.array(a.shape)\n\n# calculate the padding\npadding = (max_shape - max_size) // 2\n\n# zero pad the array\nresult = np.pad(a, padding, mode='constant')\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), b[:, :, 0], b[:, :, 1]]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[:, :, b[:, 0]], axis=2)\n", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[:, :, None]])", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = df.loc[df['a'] > 1, 'b'].values\nresult = np.where(result.size > 0, result, np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "\n# Create a mask where the image is not all zeros\nmask = np.any(im, axis=1)\n\n# Create a new array where the mask is True\nresult = im[mask]\n\n# Reshape the result to the desired shape\nresult = result.reshape(-1, 2)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "\n# Create a mask that is True where the array is not zero, and False where it is.\nmask = A != 0\n\n# Use the mask to index the array.\nresult = A[mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Create a mask where the image is", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "\n# Create a mask where the image is not all zero\nmask = np.any(im != 0, axis=1)\n\n# Create a new array where the mask is applied\nresult = im[mask]\n\n# Reshape the result to the desired shape\nresult = result.reshape(-1, 5)\n\nprint(result)\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.figure(figsize=(10, 6))\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'r-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'go-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'b*-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'k^-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm-.')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c--')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm-.')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'k^-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'b*-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'go-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'r-')\n\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.figure(figsize=(10, 6))\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'r-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'go-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'b*-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'k^-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm-.')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c--')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm-.')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'k^-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'b*-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'go-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'r-')\n\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, marker='d')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.plot(x, y, marker='d', linestyle='-', linewidth=2)\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, 'r')\nplt.plot(x[2:4], 'r')\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = 2 * x + 1\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    height=5,\n    aspect=1.5,\n)\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.legend(title='xyz', title_fontsize=20)\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"black\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), x, 'r-', lw=5, markersize=30)\nplt.plot(range(10), y, 'r-', lw=5, markersize=30)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.array([0, 2, 4, 6, 8, 10])\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='hot')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = -y\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5])\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 10))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().yaxis.set_major_formatter(plt.matplotlib.ticker.FormatStrFormatter('%.0f'))\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax.plot(x, y, '--')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y2')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\nax[0].plot(x, y1, label='sin(x)')\nax[0].set_title('sin(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y')\nax[0].legend()\n\nax[1].plot(x, y2, label='cos(x)')\nax[1].set_title('cos(x)')\nax[1].set_xlabel('x')\nax[1].set_ylabel('y')\nax[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks([])\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4), [''])\nplt.grid(axis='x', which='both', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 5))\nplt.xticks(np.arange(1, 3))\nplt.grid(True, which='both')\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), constrained_layout=True)\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xticks([])\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.gca().set_xticks(np.arange(0, 10, 2))\nplt.gca().set_xticks(np.arange(0, 10, 2), minorticks=np.arange(0, 10, 2))\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks([])\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().tick_params(axis='y')\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel(\"Y\")\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green', scatter_kws={'color':'blue'})\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green')\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\")\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 6))\n\n# Create a bar plot of s1 and s2\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\n\n# Set the x-axis tick labels to rotate 45 degrees\nplt.xticks(rotation=45)\n\n# Set the xlabel to be the celltype\nplt.xlabel(\"celltype\")\n\n# Set the legend\nplt.legend()\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.ylabel(\"Y\", color=\"red\")\nplt.xticks(color=\"red\")\nplt.yticks(color=\"red\")\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.xticks(x, rotation=90)\nplt.tick_params(axis='both', labelsize=10)\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "\n# create a figure and a set of subplots\nfig, ax = plt.subplots()\n\n# create a vertical line at the given coordinates\nax.axvline(x=0.22058956, color='r')\nax.axvline(x=0.33088437, color='g')\nax.axvline(x=2.20589566, color='b')\n\n# set the labels and title\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Vertical Lines at x = 0.22058956, 0.33088437, 2.20589566')\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, cmap='hot', interpolation='nearest')\nplt.xticks(range(len(xlabels)), xlabels, rotation=90)\nplt.yticks(range(len(ylabels)), ylabels)\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax.plot(time, temp, \"-\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "plt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\nplt.title(\"Scatter plot of bill_length_mm and bill_depth_mm\")\nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"bill_depth_mm\")\nplt.show()\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.figure(figsize=(10, 6))\nplt.scatter(b, a, c=c, alpha=0.7)\nplt.annotate(c, (b[0], a[0]), text='c')\nplt.annotate(c, (b[1], a[1]), text='c')\nplt.annotate(c, (b[2], a[2]), text='c')\nplt.xlabel('b')\nplt.ylabel('a')\nplt.title('Scatter plot of a over b')\nplt.show()\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend', title_fontsize=14, label_fontsize=12, loc='best')\nplt.legend(title='Legend', title_fontsize=14, label_fontsize=12, loc='best', bbox_to_anchor=(1.05, 1), borderaxespad=0)\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, alpha=0.5, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(2, 3, figsize=(10, 10))\n\n# Make the first subplot three times wider than the second subplot but they should have the same height.\naxs[0, 0].plot(x, y)\naxs[0, 1].plot(x, y)\naxs[0, 2].plot(x, y)\naxs[1, 0].plot(x, y)\naxs[1, 1].plot(x, y)\naxs[1, 2].plot(x, y)\n\n# Set the labels for the axes\naxs[0, 0].set_xlabel('x')\naxs[0, 1].set_xlabel('x')\naxs[0, 2].set_xlabel('x')\naxs[1, 0].set_xlabel('x')\naxs[1, 1].set_xlabel('x')\naxs[1, 2].set_xlabel('x')\n\n# Set the labels for the axes\naxs[0, 0].set_ylabel('y')\naxs[0, 1].set_ylabel('y')\naxs[0, 2].set_ylabel('y')\naxs[1, 0].set_ylabel('y')\naxs[1, 1].set_ylabel('y')\naxs[1, 2].set_ylabel('y')\n\n# Set the labels for the axes\naxs[0, 0].set_title('Subplot 1')\naxs[0, 1].set_title('Subplot 2')\naxs[0, 2].set_title('Subplot 3')\naxs[1, 0].set_title('Subplot 4')\naxs[1, 1].set_title('Subplot 5')\naxs[1, 2].set_title('Subplot 6')\n\n# Set the labels for the axes\naxs[0, 0].set_xlabel('x')\naxs[0, 1].set_xlabel('x')\naxs[0, 2].set_xlabel('x')\naxs[1, 0].set_xlabel('x')\naxs[1, 1].set_xlabel('x')\naxs[1, 2].set_xlabel('x')\n\n# Set the labels for the axes\naxs[0, 0].set_ylabel('y')\naxs[0, 1].set_ylabel('y')\naxs[0, 2].set_ylabel('y')\naxs[1, 0].set_ylabel('y')\naxs[1, 1].set_ylabel('y')\naxs[1, 2].set_ylabel('y')\n\n# Set the labels for the axes\naxs[0, 0].set_title('Subplot 1')\naxs[0, 1].set_title('Subplot 2')\naxs[0, 2].set_title('Subplot 3')\naxs[1, 0].set_title('Subplot 4')\naxs[1, 1].set_title('Subplot 5')\naxs[1, 2].set_title('Subplot 6')\n\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Set the legend of the chart\nplt.legend()\n\n# Set the title of the chart\nplt.title('Histogram of x and y')\n\n# Set the x-axis label of the chart\nplt.xlabel('Value')\n\n# Set the y-axis label of the chart\nplt.ylabel('Frequency')\n\n# Show the chart\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist([x, y], bins=10, alpha=0.5, label=['x', 'y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.plot([a, c], [b, d])\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2, 1)\n\n# create a colormap with x and y\ncmap = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_y = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_x = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[0].plot(a, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].plot(x, z, label='Y')\naxs[1].plot(a, y, label='Z')\naxs[1].set_title('Y and Z')\n\n# Set the labels for the two subplots\naxs[0].set_xlabel('X')\naxs[0].set_ylabel('Y')\naxs[1].set_xlabel('A')\naxs[1].set_ylabel('Z')\n\n# Set the legend for the two subplots\naxs[0].legend()\naxs[1].legend()\n\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.plot([x[0] for x in points], [x[1] for x in points])\nplt.yscale('log')\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.plot(x, y)\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "plt.figure(figsize=(5, 5))\nfor line in lines:\n    plt.plot(*line, c=c[lines.index(line)])\n\n# Add a title to the plot\nplt.title(\"Line Segments\")\n\n# Add a legend to the plot\nplt.legend([f\"Line {i}\" for i in range(len(lines))])\n\n# Display the plot\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o-')\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-Log Plot')\nplt.grid(True)\nplt.xticks(np.arange(0, 1000, 100))\nplt.yticks(np.arange(0, 1000, 100))\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\n\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "\n# Calculate the total sum of the data\ntotal_sum = np.sum(data)\n\n# Normalize the data\nnormalized_data = [d / total_sum for d in data]\n\n# Create a histogram of the normalized data\nplt.hist(normalized_data, bins=len(data), edgecolor='black')\n\n# Set the y tick labels as percentage\nplt.yticks(range(1, len(data) + 1), [str(int(d * 100)) + '%' for d in normalized_data])\n\n# Set the x tick labels as the data\nplt.xticks(range(1, len(data) + 1), data)\n\n# Set the x label and y label\nplt.xlabel('Data')\nplt.ylabel('Frequency')\n\n# Set the title\nplt.title('Histogram of Normalized Data')\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, alpha=0.5, linestyle='-')\nplt.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, a, label='a')\naxs[1].plot(z, z, label='z')\n\nfig.legend(loc='upper right')\n\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=False, sharey=False, figsize=(10, 10))\n\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['second'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(range(0, 10, 2), [2.1, 3, 7.6])\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\nplt.tight_layout()\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.gca().set_xticks(x, minor=True)\nplt.gca().set_yticks(y, minor=True)\nplt.gca().tick_params(axis='y', which='minor', left=False, right=False, top=False, bottom=False)\nplt.gca().tick_params(axis='x', which='minor', left=False, right=False, top=False, bottom=False)\nplt.gca().xaxis.tick_top()\nplt.gca().yaxis.tick_left()\nplt.gca().xaxis.set_label_position(\"top\")\nplt.gca().yaxis.set_label_position(\"left\")\nplt.gca().xaxis.set_tick_params(rotation=-60)\nplt.gca().yaxis.set_tick_params(rotation=-60)\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(x, alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0.05)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n\naxs.plot(x, y, label='y')\naxs.plot(x, y, label='y')\n\naxs.set_title('Figure')\naxs.legend()\n\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df['Type A'], df['Type B'])\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Line Chart of Type A and Type B')\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='||')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='black', hatch='|')\n\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*', hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*' + '|')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, extent=(1, 5, 1, 4))\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.figure(figsize=(10, 5))\nplt.plot([3], [0], 'o-', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 10))\nplt.pie(height, labels=labels, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedgeprops={'width': 0.4})\n\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle='-', linewidth=0.5, color='gray')\nplt.grid(which='major', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.title('Activity Distribution')\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.title('Activity Distribution')\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'r-o', markersize=10, markeredgewidth=2, markeredgecolor='black')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "plt.figure(figsize=(10, 5))\nplt.bar(np.arange(3), blue_bar, color='b', align='center')\nplt.bar(np.arange(3), orange_bar, color='orange', align='center')\nplt.xlabel('Bar')\nplt.ylabel('Value')\nplt.title('Blue and Orange Bars')\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='y')\naxs[1].plot(a, z, label='z')\naxs[0].legend()\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.axis([0, 10, 0, 10])\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "\nsns.catplot(x=\"bill_length_mm\", y=\"species\", hue=\"sex\", data=df, height=5, aspect=0.9, kind=\"bar\", share=False)\n\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(5, 5))\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)\nplt.plot([0.5, 0.5], [0, 1], 'k-', linewidth=2)\nplt.plot([0, 1], [0.5, 0.5", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"center right\", bbox_to_anchor=(0.3, 0.5), frameon=False)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc=\"upper right\")\nplt.plot(x, y, marker=\"*\", label=\"Line\")\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.plot(x, y)\nplt.title('Figure 1', fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.plot(x[::-1], y[::-1])\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.show()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n\n# Create a scatter plot of x over y\nplt.scatter(x, y)\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolors='black')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor ax in axs.flat:\n    ax.plot(x, y)\n\nplt.show()\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o-')\nplt.errorbar(x, y, yerr=error, fmt='o-')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 1], 'w-')\nplt.plot([0, 1], [0, 0], 'w-')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[0].legend()\n\naxs[1].plot(z, a, label='Z')\naxs[1].set_title('Z')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs.flat:\n    ax.plot(x, y)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1)\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 6))\nsns.heatmap(df, cmap=\"viridis\", cbar=False, bbox=[0, 0, 1, 1])\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(x)\nplt.yticks(y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks([])\nplt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\nplt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "plt.figure(figsize=(10, 6))\n\n# Create a scatter plot for \"time\" and \"pulse\" with \"kind\" as the hue\nscatter_plot = sns.scatterplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\n\n# Set the title of the plot\nplt.title(\"Group: Fat\")\n\n# Remove the legend from the plot\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n# Show the plot\nplt.show()\n\n# Create a scatter plot for \"time\" and \"pulse\" with \"kind\" as the hue\nscatter_plot = sns.scatterplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\n\n# Set the title of the plot\nplt.title(\"Group: No Fat\")\n\n# Remove the legend from the plot\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "plt.figure(figsize=(10, 6))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.9, kind=\"point\")\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Scatter Plot of Exercise Time vs Pulse by Kind and Diet\")\nplt.show()\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "plt.figure(figsize=(10, 6))\n\n# Create the first subplot\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.ylabel(\"Pulse\")\n\n# Create the second subplot\nplt.subplot(1, 2, 2)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.ylabel(\"Pulse\")\nplt.ylabel(\"Pulse\")\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(frameon=False)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "fig, ax = plt.subplots(3, 1, figsize=(10, 10))\n\nax[0].plot(t, a, label='sin(t)')\nax[0].set_xlabel('t')\nax[0].set_ylabel('a')\nax[0].legend()\n\nax[1].plot(t, b, label='cos(t)')\nax[1].set_xlabel('t')\nax[1].set_ylabel('b')\nax[1].legend()\n\nax[2].plot(t, c, label='a + b')\nax[2].set_xlabel('t')\nax[2].set_ylabel('c')\nax[2].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=0.2, edgecolor=\"black\")\nstripplot.set_title(\"Bill Length vs Sex for Different Species\")\nstripplot.set_xlabel(\"Sex\")\nstripplot.set_ylabel(\"Bill Length (mm)\")\nstripplot.legend(title=\"Species\", loc=\"upper left\")\nplt.show()\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n\nfor ax, b in zip(axes, df[\"b\"].unique()):\n    subset = df[df[\"b\"] == b]\n    sns.FaceGrid(ax, gs=ax.get_gridspec(), width_ratios=[1], height_ratios=[1])\n    ax.plot(subset[\"a\"], subset[\"c\"], label=b)\n    ax.set_xticks(np.arange(min(subset[\"a\"]), max(subset[\"a\"]), 2))\n    ax.set_xticklabels(np.arange(min(subset[\"a\"]), max(subset[\"a\"]), 2))\n\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azimuth=100, elevation=50)\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.ticklabel(False)\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[0, 1], height_ratios=[1, 0])\n\naxes = []\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.axis('off')\n    axes.append(ax)\n\nfor i, ax in enumerate(axes):\n    ax.imshow(x[i])\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nprint(x.numpy())", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nprint(x.numpy())", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\nlabels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\n\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\n# Convert the labels to one-hot encoding\nlabels = tf.one_hot(labels, depth=10, dtype=tf.int32)\n\n# Calculate the sum of each class\nresult = tf.reduce_sum(labels, axis=0)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "\nlabels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nresult = tf.cast(result, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\nlabels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nresult = tf.cast(result, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.concat([[i], [i+1], [i+2]], axis=0)\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return [input, input+1, input+2]\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(lambda input: tf.py_function(f, [input], [tf.int32]))\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n\n# Convert the mask to a tensor of integers\nmask = tf.cast(mask, tf.int32)\n\n# Create the result tensor by multiplying the mask with the lengths tensor\nresult = tf.multiply(mask, lengths)\n\n# Print the result tensor\nprint(result)\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(mask)\n\n# Multiply the mask and ones to get the desired result\nresult = tf.where(mask, ones, 0)\n\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    max_length = tf.constant(8)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\n    result = tf.cast(mask, tf.float32)\n    return result\n\nresult = f(example_lengths)\nprint(result)\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([tf.repeat(a, len(b))] + [tf.repeat(b, len(a))])", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "\n    return tf.stack([tf.repeat(a, len(b))]*len(a))\n\nresult = f(example_a, example_b)\nprint(result)\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "result = tf.reshape(a, (50, 100, 512))", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.expand_dims(a, axis=2)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "result = tf.expand_dims(a, axis=1)\nresult = tf.expand_dims(result, axis=2)\nresult = tf.reshape(result, (1, 50, 100, 1, 512))\nresult = tf.squeeze(result)\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\n# Calculate the L2 distance element-wise\ndifference = tf.square(a - b)\n\n# Calculate the sum of the squares\nsum_of_squares = tf.reduce_sum(difference, axis=0)\n\n# Calculate the L2 distance\nl2_distance = tf.sqrt(sum_of_squares)\n\n# Print the result\nprint(l2_distance)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "\n# Calculate the L2 distance column-wise\ndifference = tf.square(tf.sub(a, b))\n\n# Reduce the difference by column\nresult = tf.reduce_sum(difference, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    return tf.square(tf.sub(A, B))\n\nresult = f(example_a, example_b)\nprint(result)\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "\nresult = tf.gather_nd(x, tf.stack([y, z], axis=1))\n\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "\nm = tf.gather_nd(x, tf.stack([row, col], axis=1))\n\nresult = m.numpy()\n\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "\n    return x[y,z]\n\nresult = f(example_x, example_y, example_z)\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\nresult = tf.einsum('ijk,ik->ij', A, B)\n\nprint(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "\n# Create a tensor of shape [Batch_size, Num_vectors, Num_vectors]\nC = tf.matmul(A, B, transpose_a=True)\n\n# Flatten the tensor to get the shape [Batch_size, Num_vectors, Num_vectors]\nC = tf.reshape(C, [A.shape[0], A.shape[1], A.shape[1]])\n\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\nx = tf.constant(x)\nresult = tf.strings.unicode_transcode(x, \"UTF-8\")\n\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    result = tf.strings.unicode_transcode(x, \"UTF-8\")\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "\n# Get the number of non-zero entries in the second to last dimension\nnum_non_zero = tf.math.count_nonzero(x, axis=-2, dtype=tf.float32)\n\n# Compute the average of the second to last dimension\nresult = tf.math.divide(tf.reduce_sum(x, axis=-2), num_non_zero)\n\n# Reshape the result to the original shape\nresult = tf.reshape(result, tf.shape(x))\n\nprint(result)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = tf.not_equal(x, 0)\nresult = tf.reduce_sum(x, axis=-2, keepdims=True) / tf.reduce_sum(mask, axis=-2, keepdims=True)\nresult = tf.where(mask, result, tf.zeros_like(result))\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    non_zero_indices = tf.where(x != 0)\n    non_zero_values = tf.gather_nd(x, non_zero_indices)\n    non_zero_count = tf.reduce_sum(tf.cast(non_zero_values != 0, tf.int32), axis=1)\n    result = tf.reduce_sum(non_zero_values, axis=1) / tf.cast(non_zero_count, tf.float32)\n    return tf.reshape(result, tf.shape(x))\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\nimport tensorflow as tf\n\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\n\nwith tf.compat.v1.Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\n# Get the indices of the maximum values in each row\nindices = tf.argmax(a, axis=1)\n\n# Convert the indices to a tensor of the same shape as the original tensor\nresult = tf.tensor_scatter_nd_update(tf.ones_like(a), indices, tf.range(100))\n\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\n# Get the indices of the maximum values in each column\nindices = tf.argmax(a, axis=1)\n\n# Convert the indices to a tensor of the same shape as the original tensor\nresult = tf.tensor_scatter_nd_update(tf.ones_like(a), indices, tf.range(10))\n\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    return tf.argmax(a, axis=1)\n\nresult = f(example_a)\nprint(result)\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save(\"my_model\")\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\n# Define the seed\ntf.random.set_seed(seed_x)\n\n# Define the range of the random integers\nrange_x = tf.constant([1, 2, 3, 4])\n\n# Generate the random integers\nresult = tf.random.uniform(shape=[10], minval=1, maxval=4, dtype=tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "\n# Define the seed\ntf.random.set_seed(seed_x)\n\n# Define the range of the random integers\nrange_values = [2, 3, 4, 5]\n\n# Generate the random integers\nresult = tf.random.uniform(shape=(114,), minval=range_values[0], maxval=range_values[-1])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    tensor = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n    return tensor\n\nresult = f(seed_x=10)\nprint(result)\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "import tensorflow as tf\n\n# get the version of tensorflow\nresult = tf.__version__\nprint(result)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "\n# Fit the polynomial of order 1\npoly1 = np.polyfit(x, y, 1)\nresult = np.array([poly1[0], poly1[1]])\n\n# Fit the polynomial of order 2\npoly2 = np.polyfit(x, y, 2)\nresult = np.array([poly2[0], poly2[1], poly2[2]])\n\n# Fit the polynomial of order 3\npoly3 = np.polyfit(x, y, 3)\nresult = np.array([poly3[0], poly3[1], poly3[2], poly3[3]])\n\n# Fit the polynomial of order 4\npoly4 = np.polyfit(x, y, 4)\nresult = np.array([poly4[0], poly4[1], poly4[2], poly4[3], poly4[4]])\n\n# Fit the polynomial of order 5\npoly5 = np.polyfit(x, y, 5)\nresult = np.array([poly5[0], poly5[1], poly5[2], poly5[3], poly5[4], poly5[5]])\n\n# Fit the polynomial of order 6\npoly6 = np.polyfit(x, y, 6)\nresult = np.array([poly6[0], poly6[1], poly6[2], poly6[3], poly6[4], poly6[5], poly6[6]])\n\n# Fit the polynomial of order 7\npoly7 = np.polyfit(x, y, 7)\nresult = np.array([poly7[0], poly7[1], poly7[2], poly7[3], poly7[4], poly7[5], poly7[6], poly7[7]])\n\n# Fit the polynomial of order 8\npoly8 = np.polyfit(x, y, 8)\nresult = np.array([poly8[0], poly8[1], poly8[2], poly8[3], poly8[4], poly8[5], poly8[6], poly8[7], poly8[8]])\n\n# Fit the polynomial of order 9\npoly9 = np.polyfit(x, y, 9)\nresult = np.array([poly9[0], poly9[1], poly9[2], poly9[3], poly9[4], poly9[5], poly9[6], poly9[7], poly9[8], poly9[9]])\n\n# Fit the polynomial of order 10\npoly10 = np.polyfit(x, y, 10)\nresult = np.array([poly10[0], poly10[1], poly10[2], poly10[3], poly10[4], poly10[5], poly10[6], poly10[7], poly10[8], poly10[9], poly10[10]])\n\n# Fit the polynomial of order 11\npoly11 = np.polyfit(x, y, 11)\nresult = np.array([poly11[0], poly11[1], poly11[2], poly11[3], poly11[4], poly11[5], poly11[6], poly11[7], poly11[8], poly11[9], poly11[10], poly11[11]])\n\n# Fit the polynomial of order 12\npoly12 = np.polyfit(x, y, 12)\nresult = np.array([poly12[0], poly12[1], poly12[2], poly12[3], poly12[4], poly12[5], poly12[6], poly12[7], poly12[8], poly12[9], poly12[10], poly12[11], poly12[12]])\n\n# Fit the polynomial of order 13\npoly13 = np.polyfit(x, y, 13)\nresult = np.array([poly13[0], poly13[1], poly13[2], poly13[3], poly13[4], poly13[5], poly13[6], poly13[7], poly13[8], poly13[9], poly13[10], poly13[11], poly13[12", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the polynomial of order 1\nA, B = np.polyfit(x, y, 1)\nresult = np.array([A, B])\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + (3*a - b - c)**2 + sin(b) + cos(b) + 4)\n\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.norm.sf(z_scores)", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [scipy.stats.norm.sf(z) for z in z_scores]\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = np.array([scipy.stats.norm.ppf(1 - p) for p in p_values])", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu], loc=stddev)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm([stddev], loc=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb\nprint(result)\n", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    return sA * sB\n\nresult = f(example_sA, example_sB)\nprint(result)\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "\n# Create a 3D grid from the points\nx, y, z = np.mgrid[0:3:100j, 0:3:100j, 0:3:100j]\n\n# Create a LinearNDInterpolator from the points\ninterpolator = scipy.interpolate.LinearNDInterpolator((x, y, z), V)\n\n# Get the interpolated value for the request point\nresult = interpolator(request)\n\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "interpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\nprint(result)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, mode='reflect')\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = M.diagonal()", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, 'uniform')\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # compute the K-S test\n    result = stats.kstest(times, 'uniform')\n    # return the result\n    return result\n    ### END SOLUTION ###\n\nresult = f(example_times, example_rate, example_T)\nprint(result)\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "# compute the K-S test\nresult = stats.kstest(times, 'uniform')\n# print the result\nprint(result)\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "distances = scipy.spatial.distance.cdist(points1, points2, metric='euclidean')\nresult = scipy.optimize.linear_sum_assignment(distances)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "distances = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nresult = scipy.optimize.linear_sum_assignment(distances)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\nb = sparse.csr_matrix(b.toarray())\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "\n# Create a binary mask where the values are greater than the threshold\nmask = img > threshold\n\n# Use the ndimage.label function to create a region label map\nlabel_map = ndimage.label(mask)\n\n# Count the number of regions\nresult = label_map[mask].max()\n\nprint(result)\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "mask = img < threshold\nresult = np.sum(mask)", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use the numpy function `np.greater_equal` to count the number of cells which value exceeds a given threshold\n    result = np.greater_equal(img, threshold).sum()\n    ### END SOLUTION ###\n    return result\nprint(f(img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\n# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the coordinates of the regions\ncoordinates = np.argwhere(mask)\n\n# Calculate the center of mass of the regions\ncenter_of_mass = np.mean(coordinates, axis=0)\n\n# Calculate the distance between the center of mass of the regions and the top left corner\ndistance = center_of_mass - np.array([0, 0])\n\n# Append the distance to the result list\nresult.append(distance)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "\n# make the matrix symmetric\nM = M + M.T\n\n# convert back to csr\nM = M.tocsr()\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    sA = (sA + sA.T) / 2\n    sA = sA + sA.T\n    ### END SOLUTION ###\n    return sA\n\nsA = f(example_sA)\nprint(sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\n# Create a binary array with the same shape as the original array\nbinary = np.zeros_like(square, dtype=bool)\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = True\n\n# Erode the binary array to remove small isolated regions\nbinary = scipy.ndimage.binary_erosion(binary)\n\n# Dilate the binary array to create larger isolated regions\nbinary = scipy.ndimage.binary_dilation(binary)\n\n# Set the values of the original array to 0 where the binary array is 0\nsquare[binary == False] = 0\n\n# Print the original array\nprint(square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask where the image is not zero\nmask = square != 0\n\n# Create a mask where the image is zero\nmask_zero = square == 0\n\n# Create a mask where the image is not zero and not the last row or column\nmask_not_last_row_not_last_column = np.logical_and(np.logical_and(mask, ~np.islast(square, axis=0)), ~np.islast(square, axis=1))\n\n# Create a mask where the image is not zero and not the first row or column\nmask_not_first_row_not_first_column = np.logical_and(np.logical_and(mask, ~np.isfirst(square, axis=0)), ~np.isfirst(square, axis=1))\n\n# Create a mask where the image is not zero and not the last row or column and not the first row or column\nmask_not_last_row_not_last_column_not_first_row_not_first_column = np.logical_and(np.logical_and(mask, ~np.islast(square, axis=0)), np.logical_and(~np.islast(square, axis=0), ~np.isfirst(square, axis=0)))\n\n# Create a mask where the image is not zero and not the last row or column and not the first row or column and not the last column or row\nmask_not_last_row_not_last_column_not_first_row_not_first_column_not_last_column_not_row = np.logical_and(np.logical_and(mask, ~np.islast(square, axis=0)), np.logical_and(~np.islast(square, axis=0), ~np.isfirst(square, axis=0)))\n\n# Create a mask where the image is not zero and not the last row or column and not the first row or column and not the last column or row and not the last row or column\nmask_not_last_row_not_last_column_not_first_row_not_first_column_not_last_column_not_row_not_last_column_not_row = np.logical_and(np.logical_and(mask, ~np.islast(square, axis=0)), np.logical_and(~np.islast(square, axis=0), ~np.isfirst(square, axis=0)))\n\n# Create a mask where the image is not zero and not the last row or column and not the first row or column and not the last column or row and not the last row or column and not the last column or row\nmask_not_last_row_not_last_column_not_first_row_not_first_column_not_last_column_not_row_not_last_column_not_row_not_last_column_not_row = np.logical_and(np.logical_and(mask, ~np.islast(square, axis=0)), np.logical_and(~np.islast(square, axis=0), ~np.isfirst(square, axis=0)))\n\n# Create a mask where the image is not zero and not the last row or column and not the first row or column and not the last column or row and not the last row or column and not the last column or row and not the last row or column\nmask_not_last_row_not_last_column_not_first_row_not_first_column_not_last_column_not_row_not_last_column_not_row_not_last_column_not_row_not_last_column_not_row = np.logical_and(np.logical_and(mask, ~np.islast(square, axis=0)), np.logical_and(~np.islast(square, axis=0), ~np.isfirst(square, axis=0)))\n\n# Create a mask where the image is not zero and not the last row or column and not the first row or column and not the last column or row and not the last row or column and not the last column or row and not the last row or column\nmask_not_last_row_not_last_column_not_first_row_not_first_column_not_last_column_not_row_not_last_column_not_row_not_last_column_not_", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_value = np.max(col.toarray())\nmin_value = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\nmedian = np.median(col.toarray())\nmode = np.mode(col.toarray())\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *coeffs):\n    return sum(coeff * np.cos(n * np.pi / tau * x) for n, coeff in enumerate(coeffs))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.pdist(example_array, 'euclidean')\n\n# Convert distances to meters\ndistances_in_meters = distances / np.max(example_array)\n\n# Convert distances to a 2D array\nresult = np.array(list(zip(range(len(distances)), range(len(distances)), distances_in_meters)))\n\n# Print result\nprint(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Calculate pairwise Manhattan distances\ndistances = scipy.spatial.distance.cdist(np.arange(example_array.shape[0]), np.arange(example_array.shape[1]), metric='cityblock')\n\n# Create a 2D array to store the result\nresult = np.empty((distances.shape[0], distances.shape[1], distances.shape[0]))\n\n# Loop over all pairs of regions\nfor i in range(distances.shape[0]):\n    for j in range(distances.shape[1]):\n        # Find the nearest edge of the current region\n        nearest_edge = np.argmin(np.abs(example_array[i, :] - example_array[:, j]))\n        # Store the result\n        result[i, j] = nearest_edge\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate the pairwise Euclidean distances between all regions\n    distances = scipy.spatial.distance.pdist(example_array, 'euclidean')\n    # Convert the distances from cells to meters\n    distances_in_meters = distances / np.max(example_array)\n    # Create a 2D array with the from ID, to ID, distance\n    result = np.array([[i, j, distances_in_meters[i*example_array.shape[0] + j]] for i in range(example_array.shape[0]) for j in range(example_array.shape[1])])\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_array))\n!", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "result = np.empty((5, 100))\nfor i in range(5):\n    tck, u = interpolate.splprep([x[i], x[-1, i]], y[:, i], s=4)\n    result[i] = interpolate.splev(x_val, tck)\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x = np.array([x1, x2, x3, x4])\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x)\nprint(statistic)\nprint(critical_values)\nprint(significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\n\nresult = ss.anderson_ksamp([x1, x2])\n\nprint(result.significance)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "\ndef tau1(x):\n    y = np.array(df['A']) # keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['AC'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['BC'] = df['B'].rolling(3).apply(lambda x: tau1(x))\n\ndf", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "result = bool(sa.data.sum() == 0)", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "result = bool(sa.count_nonzero()) == 0\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue\n", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n    ### END SOLUTION ###\n    return p_value\n\nprint(f())\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "import scipy.stats\n\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interpolated = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interpolated(x, y)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a grid of points\n    xi, yi = np.mgrid[-1:1:10j, -2:0:10j]\n    # Compute the function values at the grid points\n    zi = (xi + yi)*np.exp(-6.0 * (xi * xi + yi * yi))\n    # Create a function from the grid points and function values\n    f = scipy.interpolate.interp2d(xi, yi, zi, kind='cubic')\n    # Compute the interpolated values\n    result = f(s, t)\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "import numpy as np\n\n# Create a list of the indices of the regions occupied by the extra points\nresult = np.array([vor.point_region[point] for point in extraPoints])\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "import numpy as np\n\n# Create a list of indices for the extra points\nextra_indices = [vor.points.tolist().index(point) for point in extraPoints]\n\n# Create a list of the indices for the regions occupied by the extra points\nresult = np.array(extra_indices)\n\nprint(result)\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "\n# Find the maximum size of the vectors\nmax_size = max([vector.size for vector in vectors])\n\n# Create a sparse matrix with padded zeros\nresult = sparse.csc_matrix(np.pad(vectors, ((0, 0), (0, max_size - vector.size)), 'constant'))\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=[1, 1])", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = M[row, column].toarray()", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, :, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u, o2, x):\n    dev = abs((x - u) / o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer / 2\n    return P\n\nprob = NormalDistro(u, o2, x)\nprint(prob)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n!pip install scipy\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install numpy\n!pip install matplotlib\n!pip install num", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "\n# Create a 2D array with random values\nA = np.random.rand(N, N)\n\n# Compute the DCT of the array\ndct_A = sf.dct(A)\n\n# Normalize the DCT matrix\nresult = dct_A / np.sqrt(np.sum(dct_A**2))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "offset = [-1, 0, 1]\nresult = sparse.diags(matrix, offset, shape=(5, 5)).toarray()\nprint(result)\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "\ndef binomial_distribution(n, p):\n    return scipy.stats.binom.pmf(range(n + 1), n, p)\n\nresult = binomial_distribution(N, p)\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x), axis=0)\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "df['data'] = df.index\ndf['zscore'] = stats.zscore(df.values, axis=1)\nresult = df\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "df['data'] = df.apply(lambda row: pd.Series({'sample1': row['sample1'], 'sample2': row['sample2'], 'sample3': row['sample3']}), axis=1)\ndf['zscore'] = df.apply(lambda row: pd.Series({'sample1': stats.zscore(row['sample1']), 'sample2': stats.zscore(row['sample2']), 'sample3': stats.zscore(row['sample3'])}), axis=1)\n\ndf = df.drop(['sample1', 'sample2', 'sample3'], axis=1)\n\ndf = df.rename(columns={'sample1': 'data', 'sample2': 'data', 'sample3': 'data'})\n\ndf = df.rename(columns={'sample1': 'zscore', 'sample2': 'zscore', 'sample3': 'zscore'})\n\ndf = df.round({'data': 3, 'zscore': 3})\n\nresult = df", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y = np.arange(shape[0])\nx = np.arange(shape[1])\nmid = np.array([[0, 0], [shape[0] - 1, 0], [0, shape[1] - 1], [shape[0] - 1, shape[1] - 1]])\nresult = distance.cdist(np.dstack((y, x)), mid)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.ogrid[:shape[0], :shape[1]]\nresult = distance.cdist(np.dstack((x, y)), np.array([[5., 4., 3., 3., 4., 5.], [4., 3., 2., 2., 3., 4.], [3., 2., 1., 1., 2., 3.], [3., 2., 1., 1., 2., 3.], [4., 3., 2., 2., 3., 4.], [5., 4., 3., 3., 4., 5.]]))", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    center = np.array([[shape[0] / 2, shape[1] / 2]])\n    return distance.cdist(center, [y, x], 'euclidean')\n    ### END SOLUTION ###\n!f(shape = (6, 6))\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((func(x) - y) ** 2)\n\nout = scipy.optimize.minimize(residual, x0)['x']\nprint(out)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return (y - func(x)) ** 2\n\nout = scipy.optimize.minimize(residual, x0, method='L-BFGS-B', bounds=[x_lower_bounds])\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) * (t < 2 * np.pi)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_sinusoidal(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_sinusoidal, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.hstack([sa, sb])", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\n# load the data\nc = np.loadtxt('cooltemp.dat')\n\n# create an empty array to store the solutions\nI = np.zeros(len(c))\n\n# loop over the values of c\nfor n in range(len(c)):\n    # equation\n    eqn = 2*c[n]*x\n    # integrate \n    result, error = scipy.integrate.quad(eqn, 0, 1)\n    I[n] = result\n\nprint(I)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n\n# create an array of c values\nc = array([5, 10, 15])\n\n# create an array to store the solutions\nI = array([])\n\n# loop over the c values\nfor c_value in c:\n    # calculate the solution\n    solution = f(c_value)\n    # append the solution to the array\n    I = append(I, solution)\n\n# print the solutions\nprint(I)\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.reshape(-1, 1)\nV_coords = V.keys()\nx_coords = x.keys()\nV_values = V.values()\nx_values = x.values()\nV_coords = V_coords[x_values != 0]\nx_coords = x_coords[x_values != 0]\nV_values = V_values[x_values != 0]\nx_values = x_values[x_values != 0]\nV = sparse.dok_matrix((V_coords, V.shape[1]), dtype = V.dtype)\nV.set_coords(V_coords, V_values)\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V += x", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V += x\nV += y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor Col in xrange(sa.shape[1]):\n    Column = sa[:,Col].data\n    List = [x**2 for x in Column]\n    # get the column length\n    Len = math.sqrt(sum(List))\n    # update the original column of the matrix\n    sa[:,Col] = sa[:,Col] / Len\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # extract column\n    column = sa[:, col].toarray()\n    # calculate length\n    length = np.sqrt(np.sum(column**2))\n    # normalize column\n    normalized_column = column / length\n    # update original column\n    sa[:, col] = normalized_column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "\n# Convert the matrix to binary matrix\na = (a > 0).astype(int)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "\n# Convert the matrix to binary matrix\na = (a > 0).astype(int)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argmin(distances, axis=1)\nresult = closest_indices", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_points = np.argmin(distances, axis=1)\nresult = data[closest_points]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argpartition(distances, k, axis=0)[:k, :]\nresult = closest_indices", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result.append(fsolve(eqn, x0=x, args=(a, b)))\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result.append(fsolve(eqn, x0=x, args=(a,)))\nresult = np.array(result)\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[:, 1:]\nresult = result.reshape(-1, 2)\nresult", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Fit the function to the data\npopt, pcov = sp.optimize.curve_fit(bekkers_func, range_start, range_end, p0=[estimated_a, estimated_m, estimated_d], bounds=([0, 0, 0], [np.inf, np.inf, np.inf]))\n\n# Calculate the K-S test\nkstest_result = sp.stats.kstest(sample_data, bekkers_func, p0=popt)\n\n# Print the result\nprint(kstest_result)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Define the function to be fitted\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Fit the function to the data\npopt, pcov = sp.optimize.curve_fit(bekkers_func, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nkstest_result = sp.stats.kstest(sample_data, bekkers_func, args=(popt[0], popt[1], popt[2]))\n\n# Check the p-value at 95% confidence level\nresult = kstest_result.pvalue < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "\ndef rolling_integral(df):\n    df['Time'] = pd.to_datetime(df['Time'])\n    df.set_index('Time', inplace=True)\n    integral_df = df.rolling(window='25S').apply(lambda x: integrate.trapz(x['A'], x['Time']))\n    return integral_df\n\nintegral_df = rolling_integral(df)\nprint(integral_df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "\n# Create a grid data object\ngrid_data = scipy.interpolate.GridData(x, y, eval)\n\n# Interpolate the value at the evaluation points\nresult = grid_data.evaluate()\n\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "\n# calculate the number of categories\nn_categories = a.nunique()\n\n# calculate the number of observations\nn_observations = a.shape[0]\n\n# calculate the number of observations in each category\nn_observations_in_category = a.value_counts()\n\n# calculate the weights\nweights = n_observations_in_category / n_observations\n\n# print the weights\nprint(weights)\n", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x, y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "\n# Create a mask to select the elements that are less or equal to the neighbouring n elements\nmask = np.concatenate((np.array([True]), np.array([False]*n), np.array([True])))\n\n# Apply the mask to the array\nresult = np.where(mask)[0]\n\nprint(result)\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "\n# Create a 2D array with the same shape as the input array\nresult = np.where(arr <= np.roll(arr, shift=-n, axis=1))\n\n# The result is a tuple of two arrays, we need to combine them into a single array\nresult = np.array(list(zip(result[0], result[1])))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "df = df[(np.abs(stats.zscore(df.select_dtypes(include=np.number))) < 3).all(axis=1)]\n", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                      columns=data['feature_names'] + ['target'])\n    return df\n\n# Call the function with the data\ndf = solve(data)\nprint(df)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a list of unique elements in the column\nunique_elements = df['Col3'].apply(lambda x: list(set(x)))\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(unique_elements)\n\n# Add the original dataframe to the one-hot-encoded dataframe\ndf_out = pd.concat([df, df_out], axis=1)\n\n# Drop the original column\ndf_out = df_out.drop(['Col3'], axis=1)\n\n# Rename the columns\ndf_out = df_out.rename(columns=lambda x: x + '_' + df['Col2'].values[0])\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# Create a list of unique names\nnames = ['Apple', 'Orange', 'Banana', 'Grape']\n\n# Create a DataFrame with the unique names as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(names))), columns=names)\n\n# Iterate over the rows of the DataFrame\nfor i in range(df.shape[0]):\n    # Iterate over the names\n    for name in names:\n        # If the name is in the list of names in the current row, set the value to 1\n        if name in df.loc[i, 'Col3']:\n            df_out.loc[i, name] = 1\n\n# Print the DataFrame\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col4'].unique()\n\n# Create a DataFrame with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col4'], values=unique_elements, prefix='Col4')\n\n# Drop the original column\ndf_out = df_out.drop('Col4', axis=1)\n\n# Add the original column back\ndf_out = pd.concat([df, df_out], axis=1)\n\n# Print the resulting DataFrame\nprint(df_out)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col3'].apply(lambda x: list(set(x)))\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'].apply(lambda x: pd.Series(x)).stack()).sum(level=0)\n\n# Add the original dataframe and the one-hot-encoded dataframe\ndf_out = pd.concat([df, df_out], axis=1)\n\n# Remove the original column\ndf_out = df_out.drop('Col3', axis=1)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col3'].apply(lambda x: list(set(x)))\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'].apply(lambda x: pd.Series(x)).stack()).sum(level=0)\n\n# Add the unique elements to the dataframe\ndf_out = df_out.add_prefix('Col3_')\n\n# Add the original dataframe to the dataframe with one-hot-encoded values\ndf_out = pd.concat([df, df_out], axis=1)\n\n# Remove the original column\ndf_out = df_out.drop('Col3', axis=1)\n\n# Print the dataframe\nprint(df_out)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "\n# Convert the sparse matrix to a DataFrame\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Concatenate the original DataFrame and the transformed DataFrame\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "\n# Convert the transform_output to a DataFrame\ntransform_output_df = pd.DataFrame(transform_output.toarray())\n\n# Merge the original DataFrame and the transformed DataFrame\ndf = pd.concat([df_origin, transform_output_df], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # Create a new dataframe with the original data and the transformed output\n    df_result = pd.concat([df, pd.DataFrame(transform_output)], axis=1)\n    # Return the result\n    return df_result\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\nclf.steps.append(('new_step', PolynomialFeatures()))\nclf.fit(np.array([[1, 2, 3], [4, 5, 6]]), np.array([1, 2]))\nprint(clf.named_steps)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps.append(('new_step', SVC()))\nclf.steps.append(('another_step', PolynomialFeatures()))\nprint(clf.steps)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PCA()))\nclf.steps\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n    'max_depth': [3, 4, 5, 6, 7],\n    'n_estimators': [100, 200, 300, 400, 500]\n}\nb = []\nc = []\nfor i in range(5):\n    gridsearch = GridSearchCV(model, paramGrid, verbose=0, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, iid=False)\n    gridsearch.fit(trainX, trainY)\n    b.append(gridsearch.best_score_)\n    c.append(gridsearch.predict(testX))\nprint(b)\nprint(c", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n    'max_depth': [3, 4, 5, 6],\n    'n_estimators': [100, 200, 300, 400]\n}\nb = []\nc = []\nfor i in range(10):\n    gridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n    gridsearch.fit(trainX, trainY)\n    b.append(gridsearch.best_score_)\n    c.append(gridsearch.best_estimator_.predict(testX))\nprint(b)\nprint(c)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    t = scaler.inverse_transform(scaled[:, 0])\n    return t\n\nt = solve(data, scaler, scaled)\nprint(t)\n", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nclf = grid_search.best_estimator_\nproba = clf.predict_proba(X_test)", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n\n# example variable\ntexts = [\"Hello World\", \"Python is cool\", \"Data science is fun\"]\n\n# fit the vectorizer\ntfidf.fit(texts)\n\n# transform the texts\ntfidf_matrix = tfidf.transform(texts)\n\n# print the transformed texts\nprint(tfidf_matrix.toarray())\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Convert the DataFrame to a numpy array\ndata_array = data.values\n\n# Apply the preprocessing.scale function to the numpy array\nscaled_data = preprocessing.scale(data_array)\n\n# Convert the scaled numpy array back to a DataFrame\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Assign the scaled DataFrame to the variable df_out\ndf_out = scaled_data_df\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a numpy array from the DataFrame\narray = data.values\n\n# Apply preprocessing.scale to the array\narray_scaled = preprocessing.scale(array)\n\n# Create a new DataFrame from the scaled array\ndf_out = pd.DataFrame(array_scaled, columns=data.columns, index=data.index)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = X.columns[model.get_support()]\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "import numpy as np\ncolumn_names = np.array(X.columns)[model.get_support()]\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns[model.get_support()].tolist()\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[km.labels_ == 0]\nclosest_50_samples = closest_50_samples[:50]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    closest_samples = centers[p]\n    return closest_samples\n    ### END SOLUTION ###\n\n# Call the function with the desired p and X\nclosest_samples = get_samples(p, X, km)\nprint(closest_samples)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = pd.concat([X_train, X_train.drop(columns=[0])], axis=1)\n\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\nfrom sklearn.model_selection import train_test_split\n\n# split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# create a svm regression model\nmodel = svm.SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X_train, y_train)\n\n# predict the model\npredict = model.predict(X_test)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "\n# Calculate the tf-idf of the queries\ntfidf_queries = TfidfVectorizer().fit_transform(queries)\n\n# Calculate the cosine similarity of the queries with the documents\ncosine_similarities_of_queries = np.array([np.dot(tfidf_queries[i], tfidf.transform(documents)) for i in range(len(queries))])\n\n# Convert the cosine similarity to a 3*5 matrix\ncosine_similarities_of_queries = np.reshape(cosine_similarities_of_queries, (3, 5))\n\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "\n# Calculate the tf-idf of the queries\ntfidf_queries = TfidfVectorizer().fit_transform(queries)\n\n# Calculate the cosine similarity of the queries with the documents\ncosine_similarities_of_queries = np.dot(tfidf_queries, tfidf_queries.T) / (np.sqrt(np.sum(tfidf_queries**2, axis=1))[:, np.newaxis] * np.sqrt(np.sum(tfidf_queries**2, axis=1)))\n\n# Convert the result to a 3*5 matrix\ncosine_similarities_of_queries = cosine_similarities_of_queries.toarray()\n\n# Print the result\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_query = TfidfVectorizer().fit_transform(queries)\n    cosine_similarities_of_queries = tfidf_query.dot(tfidf.fit_transform(documents)).toarray()\n    return cosine_similarities_of_queries\n    ### END SOLUTION ###\n\n# Call the function with your data\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# Convert the list of lists to a pandas DataFrame\ndf = pd.DataFrame(f)\n\n# Create a binary matrix where each row corresponds to a feature and each column corresponds to a sample\nbinary_matrix = df.to_numpy().astype(bool)\n\n# Convert the binary matrix to a 2D array\nnew_f = binary_matrix.reshape(-1, 1)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to one hot encoding\nnew_features = pd.get_dummies(new_features)\n\n# Print the new features\nprint(new_features)\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Create a DataFrame from the array\n    df = pd.DataFrame(features_array, columns=features[0])\n    # Convert the DataFrame to a 2D array\n    features_array_2d = df.values\n    # Return the 2D array\n    return features_array_2d\n    ### END SOLUTION ###\n\nfeatures = solve(features)\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Convert the list of features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Create a binary matrix where each row corresponds to a feature and each column corresponds to a variant\nbinary_matrix = df.apply(lambda x: pd.get_dummies(x, drop_first=True))\n\n# Convert the binary matrix to a numpy array\nnew_features = binary_matrix.to_numpy()\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from the data\ndistances = pd.DataFrame(data_matrix)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data\ncluster_labels = cluster.fit_predict(distances)\ncluster_labels", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the distance matrix\ncluster_labels = cluster.fit_predict(dist_matrix)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from the similarity matrix\ndistances = np.array(simM)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model to the distance matrix\ncluster_labels = cluster.fit_predict(distances)\ncluster_labels", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# Perform hierarchical clustering\ncluster_labels = fcluster(linkage_matrix, 2, criterion='maxclust')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "import scipy.cluster.hierarchy as shc\n\n# Create a distance matrix\ndist_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n# Create a linkage matrix\nlinkage_matrix = shc.linkage(dist_matrix, method='ward')\n\n# Perform hierarchical clustering\ncluster_labels = shc.fcluster(linkage_matrix, 2, criterion='maxclust')\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix\nlinkage_matrix = linkage(simM, 'ward')\n\n# Perform hierarchical clustering\ncluster_labels = fcluster(linkage_matrix, 2, criterion='maxclust')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import PowerTransformer\n\nbox_cox = PowerTransformer(method='box-cox')\nbox_cox_data = box_cox.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\nbox_cox = BoxCoxTransformer()\nbox_cox_data = box_cox.fit_transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(analyzer='word', token_pattern='[\\w]', lowercase=True, stop_words='english')\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n\n# Define x and y\nx_train_all = x_train.values\nx_test_all = x_test.values\ny_train_all = y_train.values\ny_test_all = y_test.values\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features (x) and the target (y)\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n\n# Define x and y\nx_train_all = x_train.values\nx_test_all = x_test.values\ny_train_all = y_train.values\ny_test_all = y_test.values\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    from sklearn.model_selection import train_test_split\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION\n!!solve(dataset)\n", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "\nmse_values = df['mse'].values\nreshaped_mse_values = mse_values.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2, n_init=10).fit(reshaped_mse_values)\nlabels = kmeans.predict(reshaped_mse_values)\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# Reshape the data\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans instance with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit the model to the data\nkmeans.fit(X)\n\n# Get the labels\nlabels = kmeans.predict(X)\n\n# Get the centroids\ncentroids = kmeans.cluster_centers_\n\n# Print the labels and centroids\nprint(labels)\nprint(centroids)\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "\n# Fit the model\nmodel = LinearSVC(penalty='l1', dual=False)\nmodel.fit(X, y)\n\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[model.coef_ != 0]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "selector = LinearSVC(penalty='l1', dual=False).fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selector.coef_ != 0]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    svc = LinearSVC(penalty='l1', dual=False)\n    svc.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\n    return selected_feature_names\n    ### END SOLUTION ###\n\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nfeature_names = sorted(feature_names, key=lambda x: (x.lower(), x))\nprint(feature_names)\nprint(X.toarray())\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nfeature_names = sorted(feature_names, key=lambda x: (x.lower(), x))\nprint(feature_names)\nprint(X.toarray())\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vocabulary = ['Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX']\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n\n# Reverse the order of the feature names\nfeature_names = feature_names[::-1]\n\n# Create a DataFrame\ndf = pd.DataFrame(X, columns=feature_names)\n\n# Convert the DataFrame to a numpy array\nX = df.values\n\n# Reverse the order of the rows\nX = np.flip(X, axis=0)\n\n# Convert the numpy array to a list of lists\nX = X.tolist()\n\n# Print the result\nprint(X)\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vocabulary = ['Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX']\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nvectorizer.fit(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = vectorizer.transform(corpus).toarray()\nfeature_names, X", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[['Time', col]].dropna()\n    X, Y = df2['Time'].values.reshape(-1, 1), df2[col].values.reshape(-1, 1)\n    lr = LinearRegression().fit(X, Y)\n    slopes.append(lr.coef_[0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(transformed_df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n    ### END SOLUTION\n\ndf = Transform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION ###\n\n# Test the function\nnp_array = np.array([[1, 2], [3, 4]])\nprint(Transform(np_array))\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close[-1:], ma50[-1:], ma100[-1:], ma200[-1:]])\npredict", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array(X)\nnew_X = new_X.astype(float)\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array(X)\nnew_X = new_X.astype(float)\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array(X)\nnew_X = new_X.astype(float)\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Separating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n\n# Predict\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the dataframe to numpy arrays\nX = dataframe.iloc[:, :-1].astype(float).values\ny = dataframe.iloc[:, -1].values\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model with the data\nlogReg.fit(X, y)\n# Predict the class of the animal\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by=\"date\")\n\n# Calculate the number of days in the dataframe\nnum_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the test set\ntest_days = num_days * 0.8\n\n# Calculate the number of days in the train set\ntrain_days = num_days - test_days\n\n# Create a range of days for the train set\ntrain_range = np.arange(train_days)\n\n# Create a range of days for the test set\ntest_range = np.arange(train_days, num_days)\n\n# Create a dataframe for the train set\ntrain_dataframe = features_dataframe.loc[train_range]\n\n# Create a dataframe for the test set\ntest_dataframe = features_dataframe.loc[test_range]\n\n# Sort the train and test dataframes by date\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by=\"date\")\n\n# Calculate the number of days in the dataframe\nnum_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the test set\ntest_days = num_days * 0.2\n\n# Calculate the number of days in the train set\ntrain_days = num_days - test_days\n\n# Create a range of days for the train set\ntrain_range = np.arange(train_days)\n\n# Create a range of days for the test set\ntest_range = np.arange(train_days, num_days)\n\n# Create a dataframe for the train set\ntrain_dataframe = features_dataframe.loc[train_range]\n\n# Create a dataframe for the test set\ntest_dataframe = features_dataframe.loc[test_range]\n\n# Sort the train and test dataframes by date\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    train_dataframe = features_dataframe[:int(len(features_dataframe)*0.2)]\n    test_dataframe = features_dataframe[int(len(features_dataframe)*0.2):]\n    ### END SOLUTION\n    return train_dataframe, test_dataframe\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2', 'X3']] = scaler.fit_transform(df[['X2', 'X3']])\ndf['X2_scale'] = df['X2']\ndf['X3_scale'] = df['X3']\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "\nscaler = MinMaxScaler()\n\ncols = myData.columns[2:4]\n\nmyData['new_' + cols] = scaler.fit_transform(myData[cols])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values('mean_fit_time')\nfull_results", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport joblib\njoblib.dump(fitted_model, 'sklearn_model')\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix, tfidf_matrix)\ncosine_similarity_matrix", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\noptimizer = optim.param_groups[0]['lr']\noptim.param_groups[0]['lr'] = 0.001\nprint(optimizer)\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define a custom scheduler\nclass CustomScheduler(torch.optim.lr_scheduler._LazyScheduler):\n    def __init__(self, optimizer, lr_factor, patience):\n        self.optimizer = optimizer\n        self.lr_factor = lr_factor\n        self.patience = patience\n        self.counter = 0\n        self.best_lr = lr_factor * 0.005\n        self.best_loss = np.inf\n        super().__init__(optimizer)\n\n    def step(self):\n        loss = self.optimizer.param_groups[0]['lr']\n        if loss > self.best_loss:\n            self.counter += 1\n            if self.counter > self.patience:\n                self.best_lr = self.lr_factor * self.best_lr\n                self.best_loss = loss\n                self.counter = 0\n        else:\n            self.best_lr = self.best_lr\n            self.best_loss = loss\n        self.optimizer.param_groups[0]['lr'] = self.best_lr\n        super().step()\n\n# Create an instance of the custom scheduler\nscheduler = CustomScheduler(optim, lr_factor=0.05, patience=5)\n\n# Train the model\nfor epoch in range(10):\n    for batch in train_loader:\n        optim.step()\n        scheduler.step()\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the word2vec model to a PyTorch embedding layer\nembedding = torch.tensor(word2vec.wv.vectors)\n\n# Create a tensor with your input data\ninput_Tensor = torch.tensor(input_Tensor)\n\n# Embed the input data using the pre-trained word2vec weights\nembedded_input = torch.nn.functional.embedding(input_Tensor, embedding)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # Create a PyTorch embedding layer with the same shape as the input tensor\n    embedding_layer = torch.nn.Embedding(input_Tensor.shape[1], input_Tensor.shape[2])\n\n    # Load the pre-trained word2vec embedding into the embedding layer\n    embedding_layer.weight.data.copy_(torch.tensor(word2vec.wv.vectors))\n\n    # Embed the input data using the pre-trained weights\n    embedded_input = embedding_layer(input_Tensor)\n    ### END SOLUTION ###\n    return embedded_input\n\n# Test the function\ninput_Tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nembedded_input = get_embedded_input(input_Tensor)\nprint(embedded_input)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.numpy())", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "\nA_log = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n\n# Create a ByteTensor with the same shape as B, filled with 0\nA_log_tensor = torch.ByteTensor(B.shape).fill_(0)\n\n# Set the values in A_log_tensor to 1 where A_log is 1\nA_log_tensor[A_log] = 1\n\n# Use the ByteTensor to index B\nC = B[:, A_log_tensor]\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "\n# Create a ByteTensor from the logical index\nA_logical = torch.ByteTensor(A_logical)\n\n# Create a LongTensor from the original tensor\nB = torch.LongTensor(B)\n\n# Use the ByteTensor to slice the tensor\nC = B[:, A_logical]\n\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "\nC = B[:, A_log.bool()]\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "\nA_log = torch.ByteTensor([0, 1, 0])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n\n# Create a logical index\nlogical_index = A_log.nonzero().squeeze()\n\n# Use the logical index to slice the tensor\nC = B[:, logical_index]\n\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    return C\n\nA_log = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = solve(A_log, B)\nprint(C)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "\n# Create a ByteTensor from the logical index\nA_log = torch.ByteTensor(A_log)\n\n# Create a LongTensor from the original tensor\nB = torch.LongTensor(B)\n\n# Use the ByteTensor to index the tensor\nC = B[:, A_log]\n\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Convert the index tensor to a numpy array\nidx_array = idx.numpy()\n\n# Use numpy's index_select to get the desired columns\nC = torch.from_numpy(np.array(B[:, idx_array]))\n\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "\n# Convert numpy array to pandas DataFrame\ndf = pd.DataFrame(x_array)\n\n# Convert pandas DataFrame to torch tensor\nx_tensor = torch.tensor(df.values, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    return torch.tensor(a, dtype=torch.float16)\n    ### END SOLUTION ###\n\n# Test the function\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\n# Create a tensor of zeros with the same shape as the lens\nmask = torch.zeros_like(lens)\n\n# Create a tensor of ones with the same shape as the lens\nones = torch.ones_like(lens)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\n# Create a tensor of zeros with the same shape as the lens tensor\nmask = torch.zeros_like(lens)\n\n# Create a tensor of ones with the same shape as the lens tensor\nones = torch.ones_like(lens)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length\nrange_tensor = torch.arange(lens.max()).unsqueeze(1)\n\n# Create a tensor of range from 0 to the maximum length", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\n# Create a tensor of zeros with the same shape as the lens\nmask = torch.zeros_like(lens)\n\n# Create a tensor of ones with the same shape as the lens\nones = torch.ones_like(lens)\n\n# Create a tensor of indices with the same shape as the lens\nindices = torch.arange(lens.numel()).unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = indices + 1\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_indices = shifted_indices.unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of the same shape as the lens, where the indices are shifted by 1\nshifted_", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, len in enumerate(lens):\n        mask[i, :len] = 1\n    return mask\n\nlens = [3, 5, 4]\nmask = get_mask(lens)\nprint(mask)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with the same shape as the input tensor\nTensor_2D_repeat = Tensor_2D.repeat(1, 1, 1, 1)\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor_2D_repeat.shape[2])\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = torch.zeros(Tensor_2D_repeat.shape[0], Tensor_2D_repeat.shape[1], Tensor", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    index_in_batch = t.shape[0]\n    diag_ele = t.shape[1]\n    Matrix = torch.diag(t)\n    result = torch.stack([Matrix for _ in range(index_in_batch)])\n    return result\n    ### END SOLUTION ###\n\n# Test the function\nt = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(Convert(t))\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]])\nab = torch.stack((a, b), 0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "\n# Convert tensors to numpy\na_np = a.numpy()\nb_np = b.numpy()\n\n# Stack numpy arrays\nab_np = np.stack((a_np, b_np), axis=0)\n\n# Convert numpy array to tensor\nab = torch.from_numpy(ab_np)\n\n# Print the shape of the tensor\nprint(ab.shape)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    a = torch.tensor(a)\n    b = torch.tensor(b)\n    ab = torch.stack((a, b), 0)\n    return ab.numpy()\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[torch.arange(10), lengths, :] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "\na[torch.arange(10), lengths, :] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "\na[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.tensor(lt, dtype=torch.float32)\n    return tensor_of_tensors\n    ### END SOLUTION ###\n\n# Call the function with your list of tensors\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "\nresult = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "\nresult = t[np.arange(t.shape[0])[:, None], idx]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "\nresult = t[range(len(t)), idx - 1]", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\n# Flatten the tensor to 1D\nids = ids.flatten()\n\n# Gather the scores based on the ids\nresult = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1, -1, 2)).squeeze(1)\n\n# Get the maximum score\nmax_score = result.max(1)[0]\n\n# Get the indices of the maximum score\nmax_indices = torch.where(result == max_score)[0]\n\n# Gather the maximum score\nresult = torch.gather(x, 1, max_indices.unsqueeze(-1).expand(-1, -1, 2)).squeeze(1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\n# Flatten the tensor\nx = x.reshape(-1, 114)\n\n# Find the maximum ids\nmax_ids = torch.argmax(x, 1)\n\n# Gather the selected slices\nresult = torch.gather(x, 1, max_ids.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, 114))\nresult = result.squeeze(1)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask where the ids are 1\nmask = ids == 1\n\n# Gather the scores of the selected slices\nresult = x[mask].gather(1, ids[mask].unsqueeze(-1)).squeeze(1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "\n# Get the maximum value from the softmax output\nmax_values = torch.max(softmax_output, 1)\n\n# Create a tensor with the indices of the maximum values\nindices = torch.argmax(softmax_output, 1)\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels corresponding to the indices\ny = class_labels[indices]\n\nprint(y)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "\n# Get the maximum value from the softmax output\nmax_values = torch.max(softmax_output, 1)\n\n# Create a tensor with the indices of the maximum values\nindices = torch.argmax(softmax_output, 1)\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass_labels = torch.tensor([0, 1, 2])\n\n# Create a tensor with the class labels\nclass", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "\n# Get the maximum value from the softmax output\nmax_values = torch.max(softmax_output, 1)[0]\n\n# Create a tensor of zeros with the same shape as the softmax output\ny = torch.zeros_like(softmax_output)\n\n# Set the values in the tensor to the maximum value\ny[torch.arange(y.size()), max_values.long()] = 1\n\n# Print the tensor y\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    _, predicted = torch.max(softmax_output, 1)\n    return predicted.view(-1, 1)\n    ### END SOLUTION ###\n\n# Test the function\nprint(solve(softmax_output))\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "\n# Get the maximum value from the softmax output\nmax_value = torch.max(softmax_output, 1)[0]\n\n# Create a tensor of the same shape as the softmax output with the maximum value\ny = torch.full_like(softmax_output, max_value)\n\n# Create a tensor of the same shape as the softmax output with the indices of the maximum value\nindices = torch.argmax(softmax_output, 1)\n\n# Create a tensor of the same shape as the softmax output with the indices of the maximum value\ny = torch.full_like(softmax_output, indices)\n\n# Subtract the indices from the maximum value to get the class labels\ny = y - indices\n\n# Add 1 to the class labels to get the class labels as 0, 1, 2\ny = y + 1\n\n# Convert the class labels to long tensor\ny = y.long()\n\nreturn y\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "\n# Convert labels to one-hot encoding\nlabels = F.one_hot(labels, num_classes=7).float()\n\n# Calculate cross entropy loss\nloss = F.cross_entropy(input=images, target=labels, reduction='none')\n\n# Calculate mean of the loss over the batch dimension\nloss = loss.mean(dim=0)\n\n# Print the loss\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "\ncnt_equal = (A == B).sum().item()\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "\ncnt_equal = torch.sum((A == B).float())\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "\ncnt_not_equal = torch.sum(A != B)\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = (A == B).sum().item()\n    ### END SOLUTION ###\n    return cnt_equal\n\nA = torch.randn(1000, 1)\nB = torch.randn(1000, 1)\n\nprint(Count(A, B))\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "\ncnt_equal = (A[-x:] == B[-x:]).sum().item()\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "\ncnt_not_equal = torch.sum((A[-x:] != B[-x:]).bool())", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensor_31 = a[:, :, :, i:i + chunk_dim, :]\n    tensors_31.append(tensor_31)\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 10, 31, 1)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensor_31 = a[:, :, i, :, :]\n    tensors_31.append(tensor_31)\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 31, 10, 1)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "\noutput[mask.bool()] = clean_input_spectrogram[mask.bool()]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x))\nmin_y = torch.min(torch.abs(y))\nsigned_min = torch.where(x < y, min_x, min_y) * sign_x\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\n\nsigned_max = torch.where(x > 0, max_x, -max_x) * torch.where(y > 0, max_y, -max_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x))\n    min_y = torch.min(torch.abs(y))\n    signed_min = torch.where(x < 0, -min_x, min_x)\n    signed_min = torch.where(y < 0, -min_y, min_y)\n    return signed_min\n\nprint(solve(x, y))\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    confidence_score = conf.item()\n    return confidence_score, class_names[classes.item()]\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# Get the last column of a and the first column of b\nlast_column_a = a[:, -1:]\nfirst_column_b = b[:, 0]\n\n# Compute the average of the last column of a and the first column of b\naverage = (last_column_a + first_column_b) / 2\n\n# Create a tensor with the average\nresult = torch.cat([last_column_a, average, first_column_b], dim=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a[:, :-1]\n    b = b[:, 1:]\n    result = torch.cat([a, b], dim=1)\n    return result\n    ### END SOLUTION ###\n\n# Test the function\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\nprint(solve(a, b))\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "\n# Create a tensor with the same shape as t, but with all elements set to 0\nresult = torch.zeros_like(t)\n\n# Set the first row of the result tensor to be the same as the first row of t\nresult[:1, :] = t[:1, :]\n\n# Set the second row of the result tensor to be the same as the second row of t\nresult[1:, :] = t[1:, :]\n\n# Set the third row of the result tensor to be the same as the third row of t\nresult[2:, :] = t[2:, :]\n\n# Set the fourth row of the result tensor to be the same as the fourth row of t\nresult[3:, :] = t[3:, :]\n\n# Set the fifth row of the result tensor to be the same as the fifth row of t\nresult[4:, :] = t[4:, :]\n\n# Set the sixth row of the result tensor to be the same as the sixth row of t\nresult[5:, :] = t[5:, :]\n\n# Set the seventh row of the result tensor to be the same as the seventh row of t\nresult[6:, :] = t[6:, :]\n\n# Set the eighth row of the result tensor to be the same as the eighth row of t\nresult[7:, :] = t[7:, :]\n\n# Set the ninth row of the result tensor to be the same as the ninth row of t\nresult[8:, :] = t[8:, :]\n\n# Set the tenth row of the result tensor to be the same as the tenth row of t\nresult[9:, :] = t[9:, :]\n\n# Set the eleventh row of the result tensor to be the same as the eleventh row of t\nresult[10:, :] = t[10:, :]\n\n# Set the twelfth row of the result tensor to be the same as the twelfth row of t\nresult[11:, :] = t[11:, :]\n\n# Set the thirteenth row of the result tensor to be the same as the thirteenth row of t\nresult[12:, :] = t[12:, :]\n\n# Set the fourteenth row of the result tensor to be the same as the fourteenth row of t\nresult[13:, :] = t[13:, :]\n\n# Set the fifteenth row of the result tensor to be the same as the fifteenth row of t\nresult[14:, :] = t[14:, :]\n\n# Set the sixteenth row of the result tensor to be the same as the sixteenth row of t\nresult[15:, :] = t[15:, :]\n\n# Set the seventeenth row of the result tensor to be the same as the seventeenth row of t\nresult[16:, :] = t[16:, :]\n\n# Set the eightteenth row of the result tensor to be the same as the eightteenth row of t\nresult[17:, :] = t[17:, :]\n\n# Set the nineteenth row of the result tensor to be the same as the nineteenth row of t\nresult[18:, :] = t[18:, :]\n\n# Set the twentieth row of the result tensor to be the same as the twentieth row of t\nresult[19:, :] = t[19:, :]\n\n# Set the twenty-first row of the result tensor to be the same as the twenty-first row of t\nresult[20:, :] = t[20:, :]\n\n# Set the twenty-second row of the result tensor to be the same as the twenty-second row of t\nresult[21:, :] = t[21:, :]\n\n# Set the twenty-third row of the result tensor to be the same as the twenty-third row of t\nresult[22:, :] = t[22:, :]\n\n# Set the twenty-fourth row of the result tensor to be the same as the twenty-fourth row of t\nresult[23:, :] = t[23:, :]\n\n# Set the twenty-fifth row of the result tensor to be the same as the twenty-f", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "\nt = torch.tensor([[1, 2], [3, 4]])\nnew = torch.tensor([[0, 0, 0, 0]])\n\nresult = torch.cat([new.expand(1, 4), t], dim=0)\nprint(result)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "\n# Create a tensor with the shape of (1, 4, 2) and fill it with -1\nt = torch.full((1, 4, 2), -1)\n\n# Create a tensor with the shape of (1, 1, 2) and fill it with the first row of t\nnew = torch.unsqueeze(t[0], 0)\n\n# Stack t and new\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W.unsqueeze(2)).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
