{"id": 0, "code": "result = df.reindex(List)", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "\nresult = df.reindex(List)\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# First, we need to count the occurrence of each value in each column\ncounts = df.apply(pd.value_counts)\n\n# Then, we need to check if the count is greater than or equal to 2\ncondition = counts >= 2\n\n# If the condition is true, we replace the value with 'other'\nresult = df.replace('other', df.index.values[condition.any(axis=1)])\n\n# If the condition is not true, we leave the value as it is\nresult = df.replace('other', df.index.values[~condition.any(axis=1)])\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# First, we need to count the occurrence of each value in each column\ncounts = df.apply(pd.value_counts)\n\n# Then, we need to check if the count is greater than or equal to 3\ncondition = counts.ge(3)\n\n# If it is, we replace the value with 'other'\nresult = df.where(condition, 'other')\n\nprint(result)\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    for col in df.columns:\n        if df[col].value_counts().max() >= 2:\n            df[col] = df[col].replace(df[col].value_counts().index[0], 'other')\n    return df\n    ### END SOLUTION ###\n\nprint(f(example_df))\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# First, we need to count the occurrence of each value in each column\ncounts = df.apply(pd.value_counts)\n\n# Then, we need to check if the count is greater or equal to 3, if it is, we replace the value with 'other'\ndf['Qu1'] = np.where(counts.Qu1 >= 3, 'other', df['Qu1'])\n\n# For Qu2, we check if the count is greater or equal to 2, if it is, we replace the value with 'other'\ndf['Qu2'] = np.where(counts.Qu2 >= 2, 'other', df['Qu2'])\n\n# For Qu3, we check if the count is greater or equal to 2, if it is, we replace the value with 'other'\ndf['Qu3'] = np.where(counts.Qu3 >= 2, 'other', df['Qu3'])\n\nresult = df\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# First, we need to count the occurrence of each value in each column\ncounts = df.apply(pd.value_counts)\n\n# Then, we need to replace the values in each column according to the count\ndf = df.replace(counts.loc['Qu1'] >= 3, 'other')\ndf = df.replace(counts.loc['Qu2'] >= 2, 'other')\ndf = df.replace(counts.loc['Qu3'] >= 2, 'other')\n\nresult = df\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "df['keep_if_dup'] = df.groupby('url')['keep_if_dup'].transform(lambda x: x.drop_duplicates().iat[0])\nresult = df.drop_duplicates(subset='url', keep='first')\n", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "df['drop_if_dup'] = df.groupby('url')['drop_if_dup'].transform(lambda x: x.drop_if_dup.first() if x.drop_if_dup.first() == 'Yes' else x)\nresult = df.drop(columns=['drop_if_dup'])\n", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "df['keep_if_dup'] = df['keep_if_dup'].replace({'Yes': 'first', 'No': 'last'})\nresult = df.sort_values('keep_if_dup').drop_duplicates(subset='url', keep='last')\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndf_pivot = df.pivot(index='name', columns='v1', values='v2').reset_index()\ndf_pivot.columns.name = None\n\nresult = df_pivot.groupby('name').apply(lambda x: {y: z for y, z in zip(x['v1'], x['v2'])}).to_dict()\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    ### END SOLUTION ###\n    return df\n\nf(example_df)\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# First, we need to parse the message to extract the key-value pairs\ndf['message'] = df['message'].str.replace('[', '').str.replace(']', '').str.split(', ')\n\n# Then, we need to expand the dataframe to include all the key-value pairs\nresult = df.explode('message').reset_index(drop=True)\n\n# Now, we need to reformat the dataframe to include only the key-value pairs\nresult = result.iloc[:, 1].str.split(', ', expand=True).iloc[:, 1:]\nresult.columns = result.iloc[:, 0]\nresult.iloc[:, 0] = result.iloc[:, 0].str.split(': ').str[0]\nresult.iloc[:, 1] = result.iloc[:, 1].str.split(': ').str[1]\nresult = result.iloc[:, ::-1].reset_index(drop=True)\n\n# Finally, we need to reformat the dataframe to include only the key-value pairs\nresult = result.iloc[:, 1].str.split(', ', expand=True).iloc[:, 1:]\nresult.columns = result.iloc[:, 0]\nresult.iloc[:, 0] = result.iloc[:, 0].str.split(': ').str[0]\nresult.iloc[:, 1] = result.iloc[:, 1].str.split(': ').str[1]\nresult = result.iloc[:, ::-1].reset_index(drop=True)\n\nresult\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "\ndf['score'] = df.apply(lambda row: row['score'] * 10 if row['product'] in products else row['score'], axis=1)\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "\ndf['score'] = df['score'] * df.loc[~df['product'].isin(products), 'score'].values[0]\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "\nfor product in products:\n    mask = df['product'].between(product[0], product[1])\n    df.loc[mask, 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\ndf['score'] = df.apply(lambda row: 1 if row['product'] in products else 0, axis=1)\ndf = df.drop('product', axis=1)\ndf = df.sort_values('product')\ndf = df.reset_index(drop=True)\ndf = df.groupby('product').apply(lambda x: x.max() - x.min())\ndf = df.reset_index(drop=True)\ndf = df.add(1).div(df.max()).sub(1)\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.idxmax(axis=1)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "\ndf['category'] = df.loc[:, 'A':'D'].idxmax(axis=1)\ndf = df.drop(columns=['A', 'B', 'C', 'D'])\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndef convert_to_list(row):\n    categories = []\n    if row['A'] == 1:\n        categories.append('A')\n    if row['B'] == 1:\n        categories.append('B')\n    if row['C'] == 1:\n        categories.append('C')\n    if row['D'] == 1:\n        categories.append('D')\n    return categories\n\ndf['category'] = df.apply(convert_to_list, axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\ndf['Day'] = df['Date'].dt.strftime('%A')\ndf = df.join(pd.DataFrame(List, columns=['Date']))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Date': 'Date'})\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf = df.iloc[1:]\ndf['#2'] = df['#2'].shift(-1)\ndf = df.iloc[1:]\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf.loc['1980-01-01'] = df.loc['1980-01-01'].shift(-1)\ndf = df.iloc[1:]\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "\ndf_shifted = df.shift(1, axis=1)\ndf_shifted.columns = ['#1', '#2']\ndf_shifted.iloc[0] = df.iloc[0]\ndf_shifted.ilof[-1] = df.ilof[-1]\ndf_shifted", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\ndf.rename(index={'1980-01-01': '1980-01-02', '1980-01-05': '1980-01-01'}, inplace=True)\ndf = df.loc[::-1]\ndf = df.reset_index(drop=True)\ndf = df.rename(index={'1980-01-02': '1980-01-01', '1980-01-01': '1980-01-02'}, inplace=True)\ndf = df.loc[::-1]\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX'}, inplace=True)\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.rename(columns={old: old + 'X' for old in df.columns}, inplace=True)", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "\n# Create a function to rename the columns\ndef rename_columns(df):\n    df.rename(columns={col: 'XHeader' + col for col in df.columns}, inplace=True)\n    return df\n\ndf = rename_columns(df)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "\ndef mean_columns(df):\n    for col in df.columns:\n        if 'val' in col:\n            mean_val = df[col].mean()\n            df = df.drop(col, axis=1)\n            df.insert(0, col, mean_val)\n    return df\n\nresult = mean_columns(df)\nprint(result)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "\ndef sum_columns(df):\n    sum_dict = {}\n    for col in df.columns:\n        if 'val' in col:\n            sum_dict[col] = df[col].sum()\n    return sum_dict\n\nresult = sum_columns(df)\nprint(result)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\n# First, we need to create a function to get the mean of the last 2 characters in the column name\ndef get_mean(col):\n    return df[col].mean()\n\n# Then, we need to create a function to get the sum of the last 2 characters in the column name\ndef get_sum(col):\n    return df[col].sum()\n\n# Now, we can use the apply function to apply these functions to each column in the dataframe\nresult = df.groupby('group').apply(lambda x: pd.Series({\n    'group_color': get_mean('group_color'),\n    'val1': get_sum('val1'),\n    'val2': get_mean('val2'),\n    'val42': get_sum('val42')\n}))\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "\nresult = df.groupby(row_list)['a','b','d'].mean()", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "\nresult = df.groupby(column_list)['q'].sum().reset_index()\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "\nresult = df.groupby(column_list).sum().iloc[row_list]\nresult = result.drop(result.idxmax())\n", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "\n# First, you need to separate the dataframe into two separate dataframes for 'id', 'temp', and 'name' columns.\ndf1 = df.loc[:,['id', 'temp', 'name']]\ndf2 = df.loc[:,['id', 'temp', 'name']].drop_duplicates()\n\n# Then, you can use the 'value_counts' function to count the number of occurrences of each value in each column.\nresult = df1.transpose().stack().value_counts()\n", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "df.isnull().sum()\n", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "\n# First, you need to separate the dataframes for each column\ntemp_df = df[['temp', 'id']].dropna()\nname_df = df['name']\n\n# Then, you can count the occurrences of each value in each dataframe\ntemp_counts = temp_df['temp'].value_counts()\nname_counts = name_df['name'].value_counts()\n\n# Concatenate the two dataframes\nresult = pd.concat([temp_counts, name_counts], axis=1, keys=['temp', 'name']).reset_index(level=0, drop=True)\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "df = df.iloc[1:].reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "df = df.iloc[1:].reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "\ndf['0'] = df['0'].apply(lambda x: x if pd.notnull(x) else 1)\ndf['1'] = df['1'].apply(lambda x: x if pd.notnull(x) else 2)\ndf['2'] = df['2'].apply(lambda x: x if pd.notnull(x) else 2)\n\nresult = df\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "\ndf['0'] = df['0'].replace(np.nan, np.nan).astype(float)\ndf['1'] = df['1'].replace(np.nan, np.nan).astype(float)\ndf['2'] = df['2'].replace(np.nan, np.nan).astype(float)\n\ndf = df.fillna(method='ffill')\n\ndf = df.replace(np.nan, np.nan)\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "\ndf['0'] = df['0'].apply(lambda x: np.nan if np.isnan(x) else x)\ndf['1'] = df['1'].apply(lambda x: np.nan if np.isnan(x) else x)\ndf['2'] = df['2'].apply(lambda x: np.nan if np.isnan(x) else x)\n\nresult = df\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# First, we need to find the rows with value less than the threshold\nrows_to_sum = df[df['value'] < thresh]\n\n# Then, we sum the values in these rows\nsummed_rows = rows_to_sum['value'].sum()\n\n# Finally, we replace the rows with the summed values in the original dataframe\nresult = df.replace(rows_to_sum, summed_rows)\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\n# First, we need to find all rows with value less than threshold\nrows_to_average = df[df['value'] < thresh]\n\n# Then, we calculate the average of these rows\naverage_row = rows_to_average.mean(axis=0)\n\n# We need to add a new row with the average\naverage_row['value'] = np.nan\ndf = df.append(average_row, ignore_index=True)\n\n# Now, we need to set the index back to 'lab'\ndf.index = df['lab']\n\n# Finally, we reset the index\ndf = df.reset_index(drop=True)\n\nresult = df\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# First, we need to find the index of the section that we want to average\nsection_index = df.index.get_loc(section_left)\n\n# Then, we need to calculate the average of the values in the section\naverage_value = df.iloc[section_index:].mean().sum()\n\n# Finally, we need to add the average to the end of the section\ndf.loc[df.index.max() + 1] = average_value\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\nresult = df\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\n# create a new dataframe with the inverted columns\ndf_inv = df.iloc[:, ::-1].copy()\n\n# add a new column for each original column with the name 'inv_' + the original column name\ndf_inv.columns = [f'inv_{col}' for col in df.columns]\n\n# concatenate the original dataframe and the inverted dataframe\nresult = pd.concat([df, df_inv], axis=1)\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndef add_exponential(df):\n    for col in df.columns:\n        df[\"exp_\" + col] = df[col].apply(lambda x: x**0.5)\n    return df\n\nresult = add_exponential(df)\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\n# create a new dataframe with the inverted columns\ndf_inv = df.iloc[:, ::-1].copy()\n\n# add a new column for each original column with the name 'inv_' + the original column name\ndf_inv.columns = [f'inv_{col}' for col in df.columns]\n\n# add the original dataframe to the inverted dataframe\nresult = pd.concat([df, df_inv], axis=1)\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndf = df.add_suffix('_sigmoid')\n\nfor col in df.columns:\n    df[col] = df[col].apply(sigmoid)\n\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "\n# First, find the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Then, find the index of the last occurrence of the maximum value in each column\nlast_max_idx = df.idxmax().loc[min_idx]\n\n# Finally, select the row with the last occurrence of the maximum value\nresult = df.iloc[last_max_idx]\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n# First, find the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Then, find the index of the first occurrence of the maximum value in each column\nmax_idx = df.idxmax().loc[min_idx]\n\n# Finally, create a mask that includes only the rows with the minimum and maximum indices\nmask = np.where((df.index >= min_idx) & (df.index <= max_idx))\n\n# Apply the mask to the DataFrame\nresult = df.loc[mask]", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\nresult = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt', 'user'])\nresult['val'] = 0\nresult = result.set_index(['dt', 'user'])\nresult = result.reindex(df['dt'].unique(), sort=False).reset_index()\nresult['val'] = df.groupby('user')['val'].values\nresult = result.sort_values('dt')\nresult = result.reset_index(drop=True)\nresult = result.set_index('dt')\nresult = result.unstack(level=0)\nresult = result.reindex(df['dt'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('user')\nresult = result.unstack(level=0)\nresult = result.reindex(df['user'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('val')\nresult = result.unstack(level=0)\nresult = result.reindex(df['val'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('user')\nresult = result.unstack(level=0)\nresult = result.reindex(df['user'].unique(), fill-value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('val')\nresult = result.unstack(level=0)\nresult = result.reindex(df['val'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('user')\nresult = result.unstack(level=0)\nresult = result.reindex(df['user'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('val')\nresult = result.unstack(level=0)\nresult = result.reindex(df['val'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('user')\nresult = result.unstack(level=0)\nresult = result.reindex(df['user'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('val')\nresult = result.unstack(level=0)\nresult = result.reindex(df['val'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('user')\nresult = result.unstack(level=0)\nresult = result.reindex(df['user'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('val')\nresult = result.unstack(level=0)\nresult = result.reindex(df['val'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('user')\nresult = result.unstack(level=0)\nresult = result.reindex(df['user'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('val')\nresult = result.unstack(level=0)\nresult = result.reindex(df['val'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('user')\nresult = result.unstack(level=0)\nresult = result.reindex(df['user'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('val')\nresult = result.unstack(level=0)\nresult = result.reindex(df['val'].unique(), fill_value=0)\nresult = result.reset_index(drop=True)\nresult = result.set_index('user')\nresult = result.unstack(level=0)\nresult = result.reindex(df['user'].unique(), fill_value=0", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\nresult = pd.DataFrame({'user': df['user'].unique(), 'dt': [min_date, max_date], 'val': [0,0]})\nresult['dt'] = pd.to_datetime(result['dt'])\nresult = result.set_index(['user', 'dt'])\nresult = result.reindex(pd.MultiIndex.from_product([df['user'].unique(), df['dt']]))\nresult['val'] = df.pivot(index='user', columns='dt', values='val')\nresult = result.reset_index()\nresult = result.sort_values('dt')\nresult.reset_index(drop=True, inplace=True)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "\ndf['dt'] = df.groupby('user')['dt'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('first')\n\ndf = df.sort_values('dt').reset_index(drop=True)\n\nresult = df\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "df = df.set_index(['dt', 'user']).unstack().reset_index()\ndf['val'] = df['val'].expanding().max()\ndf = df.sort_values('dt').reset_index(drop=True)\ndf = df.set_index(['dt', 'user']).unstack().reset_index()\ndf = df.sort_values('dt').reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "df = df.set_index(['dt', 'user']).unstack(fill_value=0).stack().reset_index()\ndf['dt'] = df['dt'].dt.strftime('%d-%b-%Y')\ndf = df.sort_values(['dt', 'user'])\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "\ndf['name'] = df['name'].astype('category').cat.codes\nresult = df\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "\ndf['a'] = df.groupby('name')['a'].cumcount() + 1\nresult = df\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    df['name'] = df.groupby('name')['a', 'b', 'c'].transform('first')\n    df.sort_values('name', inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    return df\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\ndf['ID'] = df.groupby('name')['a'].cumcount() + 1\nresult = df[['ID', 'b', 'c']]\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.melt(id_vars=['user', 'someBool'], \n            value_vars=['01/12/15', '02/12/15'], \n            var_name='date', \n            value_name='value')\ndf = df.sort_values('user')\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.melt(id_vars=['user', '01/12/15', 'someBool'], \n            value_vars=['02/12/15', 'value'], \n            var_name='others', \n            value_name='value')\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.melt(id_vars=['user', 'someBool'], \n            value_vars=['01/12/15', '02/12/15'], \n            var_name='date', \n            value_name='value')\ndf = df.sort_values('user')\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "\nmask = df['c'] > 0.5\nresult = df.loc[mask, columns].to_numpy()\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "\nmask = df['c'] > 0.45\nresult = df.loc[mask, columns].to_numpy()\n", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": " ###\n    return df.loc[df['c'] > 0.5, ['b', 'e']]\n    ### END SOLUTION ###\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    df_subset = df.loc[df['c'] > 0.5, ['b', 'e']]\n    df_subset['sum'] = df_subset['b'] + df_subset['e']\n    return df_subset\n    ### END SOLUTION\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\nf(df)\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\n# First, convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Then, sort the dataframe by 'date'\ndf = df.sort_values('date')\n\n# Initialize an empty list to store the result\nresult = []\n\n# Iterate over the rows in the dataframe\nfor index, row in df.iterrows():\n    # If the date is not in the result list, append it\n    if row['date'] not in result:\n        result.append(row['date'])\n\n# Convert the result list to a datetime series\nresult = pd.Series(result)\n\n# Convert the datetime series back to a list\nresult = result.dt.strftime('%m/%d/%y')\n\n# Convert the list back to a dataframe\nresult = pd.DataFrame(result.tolist(), columns=['date'])\n\n# Rename the 'date' column to 'ID'\nresult.columns = ['ID']\n\n# Sort the dataframe by 'ID'\nresult = result.sort_values('ID')\n\nresult\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\n# First, convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Then, sort the dataframe by 'date'\ndf = df.sort_values('date')\n\n# Initialize an empty list to store the result\nresult = []\n\n# Iterate over the dataframe\nfor index, row in df.iterrows():\n    # If the date is not in the result list, append it\n    if row['date'] not in result:\n        result.append(row['date'])\n\n# Convert the result list to datetime\nresult = pd.to_datetime(result)\n\n# Sort the result list\nresult = result.sort_values()\n\n# Convert the result list to a dataframe\nresult = pd.DataFrame(result, columns=['date'])\n\n# Reset the index of the result dataframe\nresult = result.reset_index(drop=True)\n\n# Rename the 'date' column to 'ID'\nresult.columns = ['ID', 'date', 'close']\n\n# Set 'ID' as the index of the result dataframe\nresult.set_index('ID', inplace=True)\n\nEND SOLUTION\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndf['date'] = pd.to_datetime(df['date'])\ndf.set_index('date', inplace=True)\n\ndef remove_overlapping_rows(df):\n    df_new = pd.DataFrame()\n    for i in range(len(df)):\n        group = df.iloc[i]\n        for j in range(i+1, len(df)):\n            if group.index[0] <= df.index[j] - pd.DateOffset(weeks=X):\n                break\n            group = group.append(df.iloc[j])\n        df_new = df_new.append(group)\n    return df_new\n\nresult = remove_overlapping_rows(df)\nresult.reset_index(inplace=True)\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\nresult.set_index('ID', inplace=True)\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\ndf['col1'] = df.groupby(df.index // 3).cumcount() / (df.groupby(df.index // 3).size()-1)\nresult = df\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\ndf['col1'] = df.groupby(df.index // 3).cumcount() + 1\nresult = df\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\ndf['col1'] = df.groupby(df.index // 4).cumsum()\nresult = df\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\n\ndf['col1'] = df['col1'].rolling(3).mean()\nresult = df\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# First, we need to bin the dataframe by 3 rows\nbins = [0, 3, 6]\nlabels = ['low', 'medium', 'high']\n\n# Then, we calculate the sum and average for each bin\ndf['bin'] = pd.cut(df['col1'], bins=bins, labels=labels)\n\n# Finally, we reshape the dataframe to get the desired output\nresult = df['bin'].value_counts().unstack().fill(0)\n", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# First, we need to create a new column that increments by 3 each time\ndf['new'] = list(range(1, len(df)+1)) % 3\n\n# Then, we group by 'new' and calculate sum and mean\nresult = df.groupby('new')['col1'].agg(['sum', 'mean']).reset_index()\n\n# We rename the columns\nresult.columns = ['new', 'sum', 'mean']\n\n# We reorder the columns\nresult = result[['new', 'mean', 'sum']]\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "\ndf['A'] = df['A'].astype(str).str.replace('0', '').astype(int)\ndf['A'] = df['A'].fillna(method='ffill').fillna(method='bfill')\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "\ndf['A'] = df['A'].astype(str).fillna(method='ffill').astype(int)\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\ndf['A'] = df['A'].astype(str).str.replace('0', '').astype(int).fillna(method='ffill').fillna(method='bfill')\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "\n# Split the 'duration' column into 'number' and 'time'\ndf[['number', 'time']] = df['duration'].str.split(' ', n=1, expand=True)\n\n# Convert 'number' to integer\ndf['number'] = df['number'].astype(int)\n\n# Create 'time_days' column\ndf['time_days'] = df['time'].replace(['year', 'day', 'week', 'month'], [365, 1, 7, 30])\n\n# Remove 'duration' column\ndf.drop('duration', axis=1, inplace=True)\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "\ndf['time'] = df['duration'].str.split().str[0]\ndf['number'] = df['duration'].str.split().str[1]\ndf['time_day'] = df['time'].replace(['year', 'week', 'day', 'month'], [365, 7, 1, 30])\ndf = df.drop(columns=['time', 'duration'])\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    df['time'] = df['duration'].str.split(' ').str[0]\n    df['number'] = df['duration'].str.split(' ').str[1]\n    df['time_days'] = df['time'].replace(['year', 'day', 'week', 'month'], [365, 1, 7, 30])\n    return df\n    ### END SOLUTION ###\n\nf(example_df)\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "\ndf['time'] = df['duration'].str.split().str[0]\ndf['number'] = df['duration'].str.split().str[1]\ndf['time_day'] = df['time'].replace(['year', 'day', 'week', 'month'], [365, 1, 7, 30])\ndf['time_day'] = df['time_day'] * df['number'].astype(int)\ndf", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "\nresult = [(df1[column] == df2[column]).all() for column in columns_check_list]\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "\nresult = [(df1[column] == df2[column]).all() for column in columns_check_list]\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_level_value(1, pd.to_datetime(df.index.get_level_value(1)))\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = df.index.set_level_value(1, pd.to_datetime(df.index.get_level_value(1)))\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    df['date'] = pder.to_datetime(df['date'])\n    return df[['date', 'x', 'y']].values\n    ### END SOLUTION ###\n\ndf = pd.DataFrame({'x': [100, 90, 80], 'y': [7, 8, 9], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    df['date'] = pder.to_datetime(df['date'])\n    df = df.set_index(['id', 'date'])\n    df = df.sort_index()\n    df = df.swaplevel(0,1)\n    return df\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = df.melt(id_vars=['Country', 'Variable'], value_name='value', var_names='year')\ndf = df.pivot(index=['Country', 'Variable', 'year'], columns='year', values='value').reset_index()\ndf.columns.name = None\ndf = df.sort_values(['Country', 'year'])\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = df.melt(id_vars=['Country', 'Variable'], value_name='year')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(['Country', 'year', 'Variable']).reset_index(drop=True)\n", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\nmask = df.apply(lambda x: abs(x) < 1).all(axis=1)\ndf = df.loc[mask]", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\nmask = df.apply(lambda x: abs(x) > 1).any()\ndf = df.loc[mask]", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a mask for the columns that have a non-zero value\nmask = df.ne(0).any()\n\n# Apply the mask to the dataframe\ndf = df.loc[mask]\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df = df.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "\ndf['A'] = df['A'].str.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    df.replace(to_replace='&AMP;', inplace=True, regex=True)\n    ### END SOLUTION ###\n\nf()\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "\ndf = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: x if validate_single_space_name(x) is None else x.split(' ')[-1])\n\ndf = df.drop(columns=['name'])\n\ndf = df.rename(columns={'first_name': 'name', 'last_name': 'name'})\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['name'] = df['name'].apply(validate_single_space_name)\n\ndf['1_name'] = df['name'].str.split(' ').str[0]\ndf['2_name'] = df['name'].str.split(' ').str[1]\n\ndf = df.drop(columns=['name'])\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "\ndef split_name(name):\n    if ' ' in name:\n        first_name, *middle_names, last_name = name.split(' ')\n        return first_name, middle_names, last_name\n    else:\n        return name, [], name\n\ndf['first name'], df['middle_name'], df['last name'] = zip(*df['name'].apply(split_name))\n\ndf = df.drop(columns=['name'])\n\ndf = df.rename(columns={'first name': 'first name', 'middle_name': 'middle_name', 'last name': 'last_name'})\n\ndf", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge_asof(df1.sort_values('Timestamp'), df2.sort_values('Timestamp'), left_on='Timestamp', right_on='Timestamp', direction='forward')", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge(df1, df2, on='Timestamp')\n", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where((df['col2'] <= df['col3']) & (df['col2'] <= df['col1']), df['col1'], np.maximum(df['col2'], df['col3']))\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "\ndf['state'] = np.where((df['col2'] > 50) & (df['col3'] > 50), df['col1'], df['col1'] + df['col2'] + df['col3'])\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\n# First, we need to filter out non-integer values\nnon_integer_values = df[~df['Field1'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]\n\n# Then, we need to convert non-integer values to string\nnon_integer_values['Field1'] = non_integer_values['Field1'].astype(str)\n\n# Finally, we need to convert DataFrame to a list\nresult = non_integer_values.to_dict(orient='records')\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\nnew_list = []\n\nfor i in range(0, len(df)):\n    if df.iloc[i]['Field1'].is_integer() == False:\n        new_list.append(df.iloc[i]['Field1'])\n\nprint(new_list)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    result = []\n    for i in range(len(df)):\n        if not df.iloc[i]['Field1'].is_integer():\n            result.append(df.iloc[i]['Field1'])\n        else:\n            result.append(df.iloc[i]['Field1'])\n    return result\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum() * 100\ndf['val2'] = df['val2'] / df['val2'].sum() * 100\ndf['val3'] = df['val3'] / df['val3'].sum() * 100\ndf['val4'] = df['val4'] / df['val4'].sum() * 100\n\ndf", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum() * 100\ndf['val2'] = df['val2'] / df['val2'].sum() * 100\ndf['val3'] = df['val3'] / df['val3'].sum() * 100\ndf['val4'] = df['val4'] / df['val4'].sum() * 100\n\ndf", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "df.drop(test, axis=0, inplace=True)\nresult = df\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    result = df.loc[test]\n    ### END SOLUTION ###\n    return result\n\ndf = pder.DataFrame({\n    'alleles': ['A/C', 'A/T', 'T/A', 'C/A', 'C/T'],\n    'chrom': [0, 0, 0, 0, 0],\n    'pos': [3, 7, 12, 15, 18],\n    'strand': ['+', '+', '+', '+', '+'],\n    'assembly#': [NaN, NaN, NaN, NaN, NaN],\n    'protLSID': [NaN, NaN, NaN, NaN, NaN],\n    'assayLSID': [NaN, NaN, NaN, NaN, NaN]\n})\n\ntest = ['TP3', 'TP12', 'TP18', 'TP3']\n\nprint(f(df, test))\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\n# First, we need to calculate the Euclidean distance between each pair of cars.\ndf['euclidean_distance'] = df.groupby('time')['car'].apply(lambda group: group.apply(lambda x: ((group-x)**2).sum()))\n\n# Then, we find the nearest car for each car at each time point.\ndf['nearest_neighbour'] = df.groupby('time')['euclidean_distance'].transform('idxmin')\n\n# Finally, we calculate the average of the distances for each time point.\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\n# First, we need to find the index of the maximum distance for each car\nmax_distance_index = df.groupby('car')['euclidean_distance'].idxmax()\n\n# Then, we join the original dataframe with this index\ndf = df.join(max_distance_index)\n\n# Finally, we group by 'time' and calculate the average of 'euclidean_distance'\ndf = df.groupby('time').mean()\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "\ndf['keywords_all'] = df.apply(lambda row: ','.join(row.dropna().astype(str).values), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "\ndf['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().astype(str).values), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "\ndf['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply('-'.join)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndf['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply('-'.join)\ndf.index = df['users']\ndf = df.drop(columns=['users', 'keywords_0', 'keywords_1', 'keywords_2', 'keywords_3'])\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# First, select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Then, change the value of the Quantity column to zero\nsample_df['Quantity'] = 0\n\n# Finally, keep the indexes of the altered rows\nsample_df = sample_df.reset_index(drop=True)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# First, select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Then, change the value of the 'ProductId' column to 0\nsample_df['ProductId'] = 0\n\n# Finally, keep the indexes of the altered rows\nsample_df = sample_df.reset_index(drop=True)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# First, we find the number of rows to select for each user\nnum_rows = df.groupby('UserId')['Quantity'].transform('count')\n\n# Then, we select the 20% of rows for each user\nselected_rows = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n\n# We set the Quantity to 0\ndf.loc[selected_rows.index, 'Quantity'] = 0\n\n# We keep the original indexes\ndf['UserId'] = df['UserId']\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "\n# First, we need to find the first duplicate\nfirst_duplicate_index = df.duplicated(subset=['col1','col2'], keep=False).idxmax()\n\n# Then, we need to find the index of the first duplicate in the original dataframe\noriginal_index = df.loc[first_duplicate_index].name\n\n# Add a new column with the original index\ndf['index_original'] = original_index\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "\n# First, we need to find the index of the last duplicate\nlast_duplicate_index = df.duplicated(subset=['col1','col2'], keep=False).last(True)\n\n# Then, we need to add a new column with the index of the last duplicate\ndf['index_original'] = df.index[last_duplicate_index]\n\n# Finally, we need to sort the dataframe by 'index_original'\ndf = df.sort_values('index_original')\n\nresult = df\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate_bool.groupby([df.index.get_level_values()[0], df.index.get_level_values()[1]]).cumcount()\n    return duplicate\n    ### END SOLUTION ###\n\nf(example_df)\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "\n# First, we need to find the first occurrence of each combination of 'col1', 'col2', and '3col'\nfirst_occurrence = df.groupby(['col1','col2','3col']).apply(lambda x: x.index.values[0])\n\n# Then, we join this with the original dataframe\nresult = df.join(first_occurrence.rename('index_original'))\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "\n# First, we need to find the last occurrence of each row\nlast_occurrence = df.iloc[:, ::-1].cumsum(axis=1).argmax(axis=1)\n\n# Then, we add a new column 'index_original' to the dataframe with the index of the last occurrence\ndf['index_original'] = df.iloc[:, ::-1].cumsum(axis=1).idxmax(axis=1)\n\n# Finally, we select the rows with the last occurrence\nresult = df.iloc[last_occurrence]\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# First, find the maximum 'count' value in each group\nmax_count = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, join this with the original DataFrame to get the rows with the max 'count' value\nresult = pd.merge(df, max_count, on=['Sp', 'Mt'], how='inner')\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "\n# First, find the maximum 'count' value in each group\nmax_count = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n\n# Then, join this with the original DataFrame to get the rows with the max 'count'\nresult = df.merge(max_count, how='inner').reset_index(drop=True)\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\n# First, find the minimum 'count' value in each group\nmin_count = df.groupby(['Sp', 'Mt'])['count'].min()\n\n# Then, find the rows with the minimum 'count' value in each group\nresult = df[df['count'] == min_count]\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# First, find the maximum 'count' value in each group\nmax_count = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, join this with the original DataFrame to get the rows with the max 'count' value\nresult = df.merge(max_count, how='inner', on=['Sp','Value'])\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df[df['Category'].isin(filter_list)]", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df[~df['Category'].isin(filter_list)]", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "\nresult = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                 ('A', 'B', 'F'),\n                                 ('A', 'C', 'G'),\n                                 ('A', 'C', 'H'),\n                                 ('A', 'D', 'I'),\n                                 ('A', 'D', 'J')])\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "\nresult = df.reset_index().melt(id_vars=df.columns[0], value_vars=df.columns[1:])\nresult.columns = ['variable_0', 'variable_1', 'value']\nresult['variable_2'] = df.columns[2]\nresult = result.sort_values('variable_0')\nresult\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cumsum()\ndf['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].apply(lambda x: x if x >= 0 else 0)\ndf", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "\nresult = df.groupby('l')['v'].sum().reset_index()\nresult.columns = ['l', 'v']\nresult['v'] = result['v'].astype('float')\nresult.loc[result['v'].isnull(), 'v'] = np.nan\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "\nresult = df.groupby('r')['v'].sum().reset_index()\nresult.columns = ['r', 'v']\nresult.loc[result['v'] == -1, 'v'] = np.nan\nresult = result.set_index('r')\n", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "\nresult = df.groupby('l')['v'].sum().reset_index()\nresult.columns = ['l', 'v']\nresult['v'] = result['v'].fillna(result['v'].astype(str))\nresult['v'] = result['v'].astype(float)\nresult.loc[result['v'].isnull(), 'v'] = np.nan\nresult = result.loc[:, ['l', 'v']]\n", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "\ndef relationship(df):\n    result = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                if df[col1].nunique() == 1 and df[col2].nunique() == 1:\n                    result.append([col1, col2, 'one-to-one'])\n                elif df[col1].nunique() == 1 and df[col2].nunique() > 1:\n                    result.append([col1, col2, 'one-to-many'])\n                elif df[col1].nunique() > 1 and df[col2].nunique() == 1:\n                    result.append([col1, col2, 'many-to-one'])\n                else:\n                    result.append([col1, col2, 'many-to-many'])\n    return result\n\nresult = relationship(df)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "\ndef relationship(df):\n    result = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                if df[col1].nunique() == 1 and df[col2].nunique() == 1:\n                    result.append([col1, col2, 'one-2-one'])\n                elif df[col1].nunique() == 1 and df[col2].nunique() > 1:\n                    result.append([col1, col2, 'one-2-many'])\n                elif df[col1].nunique() > 1 and df[col2].nunique() == 1:\n                    result.append([col1, col2, 'many-2-one'])\n                else:\n                    result.append([col1, col2, 'many-2-many'])\n    return result\n\nresult = relationship(df)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "\ndef relationship(df):\n    relationship = pd.DataFrame(index=df.columns, columns=df.columns)\n    for i in df.columns:\n        for j in df.columns:\n            if i != j:\n                if df[i].nunique() == 1 and df[j].nunique() == 1:\n                    if df[i].duplicated() == False and df[j].duplicated() == False:\n                        relationship.loc[i, j] = 'one-to-one'\n                    elif df[i].duplicated() == True and df[j].duplicated() == False:\n                        relationship.loc[i, j] = 'one-to-many'\n                    elif df[i].duplicated() == False and df[j].duplicated() == True:\n                        relationship.loc[i, j] = 'many-to-one'\n                    else:\n                        relationship.loc[i, j] = 'many-to-many'\n    return relationship\n\nresult = relationship(df)\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "\ndef relationship(df):\n    result = pd.DataFrame(index=df.columns, columns=df.columns)\n    for i in df.columns:\n        for j in df.columns:\n            if i != j:\n                if df[i].nunique() == 1 and df[j].nunique() == 1:\n                    result.iloc[i,j] = 'one-2-one'\n                elif df[i].nunique() == 1 and df[j].nunique() > 1:\n                    result.iloc[i,j] = 'one-2-many'\n                elif df[i].nunique() > 1 and df[j].nunique() == 1:\n                    result.iloc[i,j] = 'many-2-one'\n                else:\n                    result.iloc[i,j] = 'many-2-many'\n    return result\n\nresult = relationship(df)\nprint(result)\n", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "\n# First, we need to identify the rows with a bank account.\nrows_with_bank = df[df['bank'].notna()]\n\n# Then, we need to sort the rows by 'firstname' and 'lastname' in ascending order.\nsorted_rows = rows_with_bank.sort_values(['firstname', 'lastname'])\n\n# Finally, we select the first row of each group.\nresult = sorted_rows.groupby(['firstname', 'lastname', 'email']).first()\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n\nprint(\"Hello, World\")\n\nThis is a simple Python script that prints \"Hello, World\" to the console.\n\nTo run this script, you can copy it into a Python file, save it, and then run it in your Python environment.\n\nIf you're using a Python IDE like PyCharm, you can also run the script by clicking the \"Run\" button.\n", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\n# First, we need to create a new column that indicates the family status\ndf['Family'] = np.where((df['SibSp'] > 0) | (df['Parch'] > 0), 'Has Family', 'No Family')\n\n# Then, we group by the 'Family' column and calculate the mean 'Survived'\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\n# First, we need to create a new column that indicates the group to which the row belongs\ndf['Group'] = np.where((df['Survived'] > 0) | (df['Parch'] > 0), 'Has Family', 'No Family')\n\n# Then, we group by the new column and calculate the mean\nresult = df.groupby('Group')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\n# First, we need to create a new column that indicates the family status\ndf['Family'] = np.where((df['SibSp'] == 1) & (df['Parch'] == 1), 'Has Family',\n                       np.where((df['SibSp'] == 0) & (df['Parch'] == 0), 'No Family',\n                                np.where((df['SibSp'] == 0) & (df['Parch'] == 1), 'New Family',\n                                         'Old Family')))\n\n# Then, we group by the new column and take the mean of the 'Survived' column\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "\ndf = df.sort_values('cokey')\ndf = df.sort_values('A')\ndf = df.sort_values('B')\n\nresult = df\n", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "\ndf = df.sort_values('cokey')\ndf = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\nresult = df\n", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list(), names=['Caps', 'Lower'])", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list(), names=['Caps', 'Middle', 'Lower'])", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list(), names=['Caps', 'Middle', 'Lower'])", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "df = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult = df\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "\nmean_std = df.groupby('a')['b'].agg(['mean', 'std'])\nresult = mean_std\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "\nmean_std = df.groupby('b')['a'].agg(['mean', 'std'])\nresult = mean_std\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "\n# First, we need to calculate softmax and min-max normalization.\n# For softmax, we use the formula: exp(x) / sum(exp(x))\n# For min-max, we use the formula: (x - min(x)) / (max(x) - min(x))\n\ndf['softmax'] = df['b'].apply(lambda x: np.exp(x) / np.sum(np.exp(df['b'])))\ndf['min-max'] = df['b'].apply(lambda x: (x - np.min(df['b'])) / (np.max(df['b']) - np.min(df['b'])))\n\ndf", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "\nresult = df.loc[:, (df.sum() != 0) & (df.columns != 'D')]", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "\n# First, we need to find the sum of each row and column.\nsums = df.sum()\n\n# Then, we need to find the index of the sum that is 0.\nindex = sums.idxmax()\n\n# Finally, we can remove the rows and columns with sum 0.\nresult = df.drop(index)\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "\ndf = df.loc[:, (df.max().idxmax())]\nresult = df\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "\ndf.loc[df.max().idxmax()] = [0,0,0,0]\nresult = df\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values().reset_index(drop=True)\n", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = pd.DataFrame(s).reset_index().rename(columns={'index': 'value', 0: '1'}).sort_values(by='1', ascending=False).drop('1', axis=1)\n", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "\ndf['A'] = pd.to_numeric(df['A'], errors='coerce')\ndf = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]\nresult = df\n", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].str.isnumeric()]", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# First, find the maximum 'count' value in each group\nmax_count = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, join this with the original DataFrame to get the rows with the max 'count' value\nresult = pd.merge(df, max_count, on=['Sp', 'Mt'], how='inner')\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "\n# First, find the maximum 'count' value in each group\nmax_count = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n\n# Then, join this with the original DataFrame to get the rows with the max 'count'\nresult = df.merge(max_count, how='inner').reset_index(drop=True)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\n# First, find the minimum 'count' value in each group\nmin_count = df.groupby(['Sp', 'Mt'])['count'].min()\n\n# Then, find the rows with the minimum 'count' value in each group\nresult = df[df['count'] == min_count]\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# First, find the maximum 'count' value in each group\nmax_count = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, join this with the original DataFrame to get the rows with the max 'count' value\nresult = df.merge(max_count, how='inner', on=['Sp','Value'])\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\ndf = df.drop('Member', axis=1)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17/8/1926')\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    df['Date'] = df['Member'].map(dict)\n    df['Date'] = df['Date'].fillna(df['Group'])\n    return df\n    ### END SOLUTION\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17-Aug-1926')\ndf = df.rename(columns={'Date': 'Dates'})\ndf['Dates'] = pd.to_datetime(df['Dates'])\ndf['Dates'] = df['Dates'].dt.strftime('%d-%b-%Y')\ndf = df.set_index('Member')\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby('Date').Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year).Date.transform('count')\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby('Date').Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year).Date.transform('count')\ndf['Count_Val'] = df.groupby([df['Date'].dt.year, df['Val']]).Date.transform('count')\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby('Date').Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_y'] = df.groupby(df['Date'].dt.year).Date.transform('count')\ndf['Count_w'] = df.groupby(df['Date'].dt.weekday).Date.transform('count')\ndf['Count_Val'] = df.groupby([df['Date'].dt.year, df['Val']]).Date.transform('count')\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "\n# First, you need to convert the 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Then, you can use the `resample` function to group by 'Date' and 'B' and 'C' and count non-zero values\nresult1 = df.groupby('Date')['B', 'C'].sum().reset_index()\nresult2 = df.groupby('Date')['B', 'C'].sum().reset_index()\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "\n# First, you need to convert the 'Date' column to datetime and then to date format\ndf['Date'] = pd.to_datetime(df['Date']).dt.date\n\n# Then, you can use groupby to group by date and then apply a function to count the even and odd values\nresult1 = df.groupby('Date')['B'].apply(lambda x: (x % 2 == 0).sum()).reset_index(name='Count')\nresult2 = df.groupby('Date')['B'].apply(lambda x: (x % 2 != 0).sum()).reset_index(name='Count')\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "pivot_df = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult = pivot_df\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "\npivot_df = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult = pivot_df\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "\npivot_df = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc=np.max)\npivot_df['E'] = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc=np.min)\nresult = pivot_df\n", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\ndf['var2'] = df['var2'].str.split(',').apply(lambda x: [item.strip() for item in x])\ndf = df.explode('var2').reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\ndf['var2'] = df['var2'].str.split(',').apply(pd.Series).stack()\ndf = df.drop(1)\ndf.columns = ['var1', 'var2']\nresult = df\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\ndf['var2'] = df['var2'].str.split('-').str[0]\ndf = df.explode('var2')\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "\ndef count_special_char(string):\n    special_char = sum(c.isalpha() for c in string)\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df['str'].apply(count_special_char)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf = df.drop(columns=['row'])\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "\ndf[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf = df.drop(columns=['row'])\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndf = df.set_index('Name').apply(lambda x: x.cumsum() / (1.0 * x.count()))\ndf = df.reset_index()\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndf['2001'] = df['2001'].astype(float) / df['2001'].sum()\ndf['2002'] = df['2002'].astype(float) / df['2002'].sum()\ndf['2003'] = df['2003'].astype(float) / df['2003'].sum()\ndf['2004'] = df['2004'].astype(float) / df['2004'].sum()\ndf['2005'] = df['2005'].astype(float) / df['2005'].sum()\ndf['2006'] = df['2006'].astype(float) / df['2006'].sum()\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    df = df.fillna(0)\n    df = df.cumsum().div(df.count())\n    return df\n    ### END SOLUTION ###\n\nf(example_df)\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndf['2001'] = df['2001'].astype(float) / df['2001'].sum()\ndf['2002'] = df['2002'].astype(float) / df['2002'].sum()\ndf['2003'] = df['2003'].astype(float) / df['2003'].sum()\ndf['2004'] = df['2004'].astype(float) / df['2004'].sum()\ndf['2005'] = df['2005'].astype(float) / df['2005'].sum()\ndf['2006'] = df['2006'].astype(float) / df['2006'].sum()\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "\ndf['label'] = (df['Close'] - df['Close'].shift(1)) > 0\ndf['label'] = df['label'].replace({True: 1, False: 0})\ndf['label'] = df['label'].astype(int)\ndf['label'] = df['label'].replace({1: 1, 0: 0})\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\ndf.loc[0] = [df.iloc[0]['DateTime'], df.iloc[0]['Close'], 1]\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: +1 if x > 0 else -1 if x < 0 else 0)\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "\ndf['Duration'] = pd.to_timedelta(df['departure_time'] - df['arrival_time'])\ndf['Duration'] = df['Duration'].dt.seconds\ndf['Duration'] = df['Duration'].astype(str) + ' seconds'\ndf\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "\ndf['Duration'] = df['departure_time'] - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.seconds\ndf", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "\ndf['Duration'] = pd.to_timedelta(df['departure_time'] - df['arrival_time']).dt.seconds\ndf['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf\n", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "\nresult = df[df['key2'] == 'one'].groupby('key1')['key2'].count()", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "\nresult = df.groupby('key1')['key2'].apply(lambda x: x.tolist().count('e') if 'e' in x.tolist() else 0).reset_index(name='count')\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "min_result = df.idxmin()\nmax_result = df.idxmax()\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "\nmode_result = df.idxmax()\nmedian_result = df.median()\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "\nmask = (df['closing_price'] >= 99) & (df['closing_price'] <= 101)\nresult = df.loc[mask]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "\nmask = (df['closing_price'] < 100) | (df['closing_price'] > 101)\nresult = df[~mask]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "\ndf1 = df.groupby(\"item\")[\"diff\"].min()\nresult = df[df.groupby(\"item\")[\"diff\"].idxmin()]", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[1]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[1]\n    return df\n    ### END SOLUTION ###\n\nf(example_df)\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# First, we need to count the number of NaN values in each row\ncounts = df['Column_x'].isnull().sum(axis=1)\n\n# Then, we create a mask for the first 50% of the rows with '0' and the last 50% with '1'\nmask = np.concatenate([np.zeros(int(0.5 * len(counts))), np.ones(int(0.5 * len(counts)))]).reshape(-1, 1)\n\n# We apply the mask to the dataframe\ndf['Column_x'] = df['Column_x'].mask(mask)\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# First, we need to calculate the number of NaN values in each row\nnan_counts = df['Column_x'].isnull().sum(axis=1)\n\n# Then, we calculate the number of rows to be filled with '0'\nfill_rows = int(nan_counts * 0.3)\n\n# We fill the first 'fill_rows' rows with '0'\ndf.loc[df.index[:fill_rows], 'Column_x'] = 0\n\n# We calculate the number of rows to be filled with '0.5'\nfill_rows = int(nan_counts * 0.3)\n\n# We fill the middle 'fill_rows' rows with '0.5'\ndf.loc[df.index[fill_rows:fill_rows*2], 'Column_x'] = 0.5\n\n# We fill the last 'fill_rows' rows with '1'\ndf.loc[df.index[-fill_rows:], 'Column_x'] = 1\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# First, we need to count the number of 0's and 1's in the column\ncount_0 = df['Column_x'].value_counts()[0]\ncount_1 = df['Column_x'].value_counts()[1]\n\n# Then, we calculate the number of 0's and 1's we want to fill with\nnum_0 = count_0 / 2\nnum_1 = count_1 / 2\n\n# We create a mask for the first 'num_0' entries to be 0's and the next 'num_1' entries to be 1's\nmask = np.concatenate((np.zeros(int(num_0)), np.ones(int(num_1))))\n\n# We fill the NaN's with the mask\ndf['Column_x'] = df['Column_x'].mask(df['Column_x'].isna(), mask)\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame(list(zip(a['one'], a['two'])), columns=['one', 'two'])\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "\nresult = pd.concat([a, b, c], axis=1)\nresult.columns = pd.MultiIndex.from_product([[x]*2 for x in result.columns])\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\n# First, we need to create a function that can handle this.\ndef create_df(a, b):\n    # First, we need to find the length of the longest dataframe.\n    max_len = max(a.shape[0], b.shape[0])\n\n    # Then, we create a new dataframe with the maximum length.\n    result = pd.DataFrame(np.nan, index=np.arange(max_len), columns=['one', 'two'])\n\n    # We then fill in the dataframe with the tuples.\n    result['one'] = a['one']\n    result['two'] = a['two']\n    result.loc[a.index]['one'] = b['one']\n    result.loc[a.index]['two'] = b['two']\n\n    return result\n\nresult = create_df(a, b)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\n", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\n", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\n", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "\ndf['text'] = df['text'].apply(lambda x: ','.join(x))\nresult = df['text'].str.cat(sep=', ')\n", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "\ndf['text'] = df['text'].apply(lambda x: '-'.join(x.split('-')[::-1]))\nresult = df.iloc[0]\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "\ndf['text'] = df['text'].str.split(', ').apply(lambda x: ' , '.join(x))\nresult = df.iloc[0]\n", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = pd.DataFrame({'text': ['abc, def, ghi, jkl']})", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "\ndf['text'] = df['text'].astype('str').str.split('-').str[::-1].apply('-'.join)\nresult = df['text'].str.cat(sep='-')\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "\ndf1 = df1.merge(df2, on=['id', 'date', 'value'], how='left')\nresult = df1.sort_values(['id', 'city', 'district', 'date', 'value'])\n", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\ndf1['date'] = pd.to_datetime(df1['date']).dt.strftime('%d-%b-%Y')\ndf2['date'] = pd.to_datetime(df2['date']).dt.strftime('%d-%b-%Y')\n\nresult = pd.merge(df1, df2, on=['id', 'city', 'district'], how='outer')\nresult['date'] = result[['date_x', 'date_y']].apply(lambda x: x.dt.strftime('%d-%b-%Y') if x is not None, axis=1)\nresult.fillna('', inplace=True)\nresult.drop(['date_x', 'date_y'], axis=1, inplace=True)\nresult.rename(columns={'date_x': 'date'}, inplace=True)\nresult.sort_values('date', inplace=True)\nresult.reset_index(drop=True, inplace=True)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\ndf1 = df1.merge(df2, on=['id', 'date', 'city', 'district'], how='left')\nresult = df1.sort_values(['id', 'date', 'city', 'district'])\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "\nmerged = pd.merge(C, D, how='outer', indicator=False)\nmerged.sort_values('A', inplace=True)\nmerged.reset_index(drop=True, inplace=True)\nresult = merged\n", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "\nmerged = pd.merge(C, D, how='outer', on='A')\nmerged['B_x'] = merged['B_x'].fillna(merged['B_y'])\nmerged['B_y'] = merged['B_y'].fillna(merged['B_x'])\nresult = merged.loc[:, ['A', 'B_x', 'B_y']]\n", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "\nmerged = pd.merge(C, D, on='A', how='outer')\n\nmerged['duplicated'] = merged['A'].duplicated(keep=False)\n\nresult = merged.loc[merged['A'].duplicated(keep=False)]\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "\ndf['time'] = df.groupby('user')['time'].apply(list)\ndf['amount'] = df.groupby('user')['amount'].apply(list)\ndf = df.sort_values('time')\nresult = df\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "\ndf['amount-time'] = df.groupby('user')['time', 'amount'].apply(list)\nresult = df.groupby('user')['amount-time'].apply(list)\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "\ndf['amount-time'] = df.groupby('user')['amount', 'time'].apply(list)\nresult = df.groupby('user')['amount-time'].apply(list)\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\ndf = pd.DataFrame(series.values, index=series.index, columns=['column1', 'column2', 'column3', 'column4'])\ndf = df.stack().unstack().transpose().reset_index(name='file')\ndf.columns = ['file', 'column']\ndf = df.rename(columns={0: 'value'})\ndf = df.pivot(index='file', columns='column', values='value')\ndf_concatenated = df\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\ndf = pd.DataFrame(series.values, index=series.index).reset_index(level=0, drop=True)\ndf.columns = ['name', 'value']\ndf = df.rename(index=lambda x: x+1)\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=True)\ndf = df.rename(index=lambda x: 'file'+str(x+1))\ndf = df.sort_index()\ndf = df.reset_index(drop=", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "\ndef find_column(df, s):\n    for col in df.columns:\n        if s in col:\n            return col\n    return None\n\nresult = find_column(df, s)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "\ndef find_column(df, s):\n    for col in df.columns:\n        if s in col:\n            return col\n\nresult = find_column(df, 'spike')\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "\ndef find_column(df, s):\n    for col in df.columns:\n        if s in col:\n            return col\n\nresult = find_column(df, 'spike')\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# First, we need to flatten the list to a single list\nflat_codes = [item for sublist in df['codes'] for item in sublist]\n\n# Then, we create a new dataframe with the flattened list\ndf_new = pd.DataFrame(flat_codes, columns=['code_0', 'code_1', 'code_2', 'code_3', 'code_4', 'code_5', 'code_6', 'code_7', 'code_8', 'code_9'])\n\n# Now, we can split the dataframe into multiple dataframes with the same structure\ndfs = df_new.groupby(df_new.index // 10).apply(lambda x: x.reset_index(drop=True))\n\nresult = dfs\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# First, we need to flatten the list to a single list\nflat_codes = [item for sublist in df['codes'] for item in sublist]\n\n# Then, we create a new dataframe with the flattened list\ndf_new = pd.DataFrame(flat_codes, columns=['code_'+str(i+1) for i in range(len(flat_codes)//3)])\n\n# We need to rename the columns to match the original dataframe\ndf_new.columns = df_new.columns.map('code_'+df_new.columns.str[:-1].str.replace('code_', ''))\n\nresult = df_new\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# First, we need to flatten the list to a single list\nflat_codes = [item for sublist in df['codes'] for item in sublist]\n\n# Then, we need to reshape it to the desired shape\nreshaped_codes = pd.DataFrame(flat_codes).groupby(level=0).apply(lambda x: x.iloc[0]).reset_index(level=0, drop=True)\n\n# Now, we can reshape it to the desired shape\nresult = reshaped_codes.unstack(level=0).fillna(pd.np.nan)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\n# First, you need to flatten the list in each row\nflattened = df['col1'].apply(lambda x: x[0] if isinstance(x, list) else [x])\n\n# Then, you can flatten the entire DataFrame\nresult = flattened.apply(lambda x: x[0] if isinstance(x, list) else [x])\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\nresult = ','.join(str(i) for i in df['col1'].values.tolist()[0])\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\n# First, you need to flatten the list.\nflatten_list = [item for sublist in df['col1'].values for item in sublist]\n\n# Then, you can join the items in the list into a string.\nresult = ','.join(map(str, flatten_list))\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "df = df.groupby(df['Time'].dt.floor('2min')).mean()\ndf = df.reset_index()\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "df = df.groupby(df['Time'].dt.floor('30min')).sum()\ndf = df.reset_index()\ndf['Time'] = df['Time'].dt.floor('30min')\ndf = df.sort_values('Time')\ndf", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME']).dt.strftime('%d-%b-%Y %a %H:%M:%S')", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "\nresult = df.reindex(filt.index).groupby(level=0).filter(filt)", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "\nresult = df.loc[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[df.iloc[0].isnull()].tolist()", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.iloc[0].isnull().ne(df.iloc[8]).idxwhere(df.iloc[0].isnull().ne(df.iloc[8]))", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = [col for col in df.columns if df[col].eq(df[col].replace(np.nan, np.nan)).any()]", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(x, y) for x in df.iloc[0].where(df.iloc[0] != np.nan).dropna().tolist() for y in df.iloc[8].where(df.iloc[8] != np.nan).dropna().tolist()]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.stack().reset_index()\ndf.columns = [f'{x}_{y}' for x, y in df['level_1'].values.tolist()]\ndf = df.iloc[1:].reset_index(drop=True)\n", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.stack().reset_index(level=0).set_index(0).unstack().reset_index().rename(columns={'level_0':'A_'+str(0), 'level_1':'B_'+str(0), 'index':'E_'+str(0)})\ndf = df.iloc[:1].T.squeeze().reset_index().T.rename(columns={0:'A_'+str(0), 1:'B_'+str(0), 2:'C_'+str(0), 3:'D_'+str(0), 4:'E_'+str(0)})\n", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "\nresult = df.sort_index(ascending=[True, True, True])\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "\ndf.sort_index(ascending=[True, True, True])\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "\nto_delete = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\nto_delete['Date'] = pd.to_datetime(to_delete['Date'])\n\nmask = df['Date'].apply(lambda x: x not in to_delete['Date'])\n\nresult = df[mask]\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\nresult = df.loc['2020-02-17':'2020-02-18']", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "\n# Filter out the correlation values that are less than 0.3\nmask = corr.lt(0.3)\nresult = corr.loc[mask].stack()\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "\n# Filter the correlation matrix to only include the columns with a correlation above 0.3\ncorrelations = corr.stack()[corr.stack() > 0.3].reset_index(level=0)\n\n# Pivot the DataFrame to get the desired output\nresult = pd.pivot(correlations, index='index', columns='0_levelname')['level_1'].reset_index(drop=True)\n\n# Convert the result to a Series\nresult = result.set_index('index').T.squeeze().to_series()\n", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "df.rename(columns={'A': 'Test'}, inplace=True)\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.rename(columns={'A': 'Test'}, inplace=True)\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\n# First, we need to find the most frequent value in each row.\n# We can use the mode function in pandas to find the most common value in each row.\nfrequent = df.mode(axis=1)[0]\n\n# Then, we need to count the number of times each value appears in each row.\n# We can use the value_counts function in pandas to count the number of times each value appears in each row.\nfreq_count = df.count()\n\n# We need to create a new dataframe with these two columns.\nnew_df = pd.concat([frequent, freq_count], axis=1)\nnew_df.columns = ['frequent', 'freq_count']\n\nprint(new_df)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\nfrequent = df.idxmax(axis=1)\nfreq_count = df.apply(lambda x: list(x).count(x.name))\n\ndf['frequent'] = frequent\ndf['freq_count'] = freq_count\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    freq_dict = {0: '[0]', 1: '[1]'}\n    freq_list = []\n    for i in row:\n        if row[i] > 1:\n            freq_list.append(freq_dict[row[i]])\n    return freq_list\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(lambda row: len(row), axis=1)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "\ndf['foo'] = df.groupby(['id1','id2'])['foo'].transform('mean')\ndf['bar'] = df.groupby(['id1','id2'])['bar'].transform('mean')\n\nresult = df[['foo','bar']].groupby(['id1','id2']).mean()\n", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "\ndf['foo'] = df['foo'].apply(lambda x: 0 if x == 'NULL' else x)\ndf['bar'] = df['bar'].apply(lambda x: 0 if x == 'NULL' else x)\n\ndf = df.groupby(['id1','id2'])['foo','bar'].mean()\n\nresult = df\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "\ndf_c = pd.merge(df_a, df_b, on='EntityNum')\ndf_c = df_c.drop(['b_col'], axis=1)\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "\ndf_c = pd.merge(df_a, df_b, on='EntityNum', how='inner')\n", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape\n", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[np.logical_not(np.isnan(x))]", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "x = x.tolist()\nx = [i for i in x if i != np.nan]\nresult = x\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((len(a), 4))\nfor i in range(len(a)):\n    b[i, a[i]] = 1\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((len(a), max(a) + 1))\nfor i, val in enumerate(a):\n    b[i, val] = 1\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((len(a), a.max() + 1))\nb[:, 0] = 1 - a\nb[:, 1:] = a\n", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "\nmask = np.argwhere(a == np.max(a))[0][0]\nb = np.zeros((len(a), 3))\nb[:, mask] = 1\nb[:, [(x) for x in range(3) if x != mask]] = a[a != a[mask]]", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((6,5))\nb[a.flatten() - 1] = 1", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (int(len(A)/ncol), ncol))", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.reshape(A, (int(len(A)/ncol), ncol))", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.reshape(A, (int(len(A)/ncol), ncol))", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=0)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.shift(a, shift)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(a.argmin(), a.shape)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravelindex(np.argmax(a), a.shape)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravelindex(np.argmax(a), a.shape)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.amax(a)\n    max_position = np.unravelindex(np.argmax(a), a.shape)\n    return max_position\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "max_val = np.amax(a)\nsecond_max_val = np.partition(a.flatten(), -2)[-2]\nresult = np.where(a == second_max_val)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "\na = a[:, ~np.isnan(a).any(axis=0)]\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravelindex(np.argmin(a), a.shape)\nresult = min_index\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravelindex(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.unravel_index(np.argmin(a), a.shape)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.radians(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.radians(degree))", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "def is_degree(number):\n    if np.sin(np.radians(number)) > 0.99:\n        return 0\n    else:\n        return 1\nresult = is_degree(number)", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.degrees(np.arcsin(value))", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "result = np.pad(A, (0, length - A.size), 'constant')", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "result = np.pad(A, (0, length - A.size), 'constant')", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.divide(numerator, denominator)\n    return (result.astype(int))\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = (numerator // denominator, numerator % denominator)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = a + b / 2 + c / 3\n", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum.outer(a, b, c)", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a)\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = a[::-1].diagonal()\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.diag(a)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "\nresult = np.diag(a)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = [i for sublist in X for i in sublist]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = [i for sublist in X for i in sublist]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i][j])\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = [X[i, :].tolist() for i in range(X.shape[0])]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.sum(a[0, :] * multiply_number, axis=0)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.sum(a[row] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.divide(a[row, :], divide_number) * np.prod(a[row, :])", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "result = a[0]\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b)", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "def calculate_p_value(a, b):\n    t_statistic, p_value = scipy.stats.ttest_ind(a, b)\n    return p_value\np_value = calculate_p_value(a, b)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the pooled variance\npooled_variance = (anobs * avar + bnobs * bvar) / (anobs + bnobs)\n# calculate the combined mean\ncombined_mean = (anobs * amean + bnobs * bmean) / (anobs + bnobs)\n# calculate the combined standard deviation\ncombined_std_dev = np.sqrt(pooled_variance)\n# calculate the t-statistic\nt_stat = (combined_mean - bmean) / combined_std_dev\n# calculate the p-value\np_value = scipy.stats.ttest_1samp(sample1, sample2).pvalue\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "mask = np.in1d(A.ravel(), B.ravel())\noutput = A[mask]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.setdiff1d(A.ravel(), B.ravel(), false=True)\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.zeros_like(b)\nfor i in range(3):\n    for j in range(3):\n        for k in range(3):\n            result[i, j, k] = np.argmin(a[i, j, k])\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "\nresult = np.delete(a, del_col, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, dtype='object')", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.equal(a[0], a))", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.equal(a.T, a))", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.equal(a, a[0]))\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "result = np.sum(np.sin(x)**2 * np.cos(y)**4)", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    result = (np.cos(x))**4 + (np.sin(y))**2\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "\ndef ecdf(x):\n    n = x.size\n    x_sort = np.sort(x)\n    y = np.arange(1, n+1) / n\n    return x_sort, y\n\nresult = ecdf(grades)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "\ndef ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return nper.cumsum(x)\n\nresult = ecdf(grades)(eval)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  x = np.sort(x)\n  n = len(x)\n  theta = np.arange(1, n+1) / n\n  return theta, np.cumsum(x) / n\n\necdf_x, cdf_x = ecdf(grades)\n\nthreshold_index = np.where(ecdf_x > threshold)[0][0]\n\nlow, high = grades[threshold_index], grades[threshold_index+1]\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "randomLabel = np.random.choice([0, 1], size=size, p=[one_ratio, 1-one_ratio])", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = a.tolist()[::-1]", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = [a.index(i) for i in np.sort(a)]\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = a.argsort()[-N:].tolist()\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.linalg.matrix_power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "\n# First, we need to reshape the array to (2, 6, 2)\na = a.reshape(2, 6, 2)\n\n# Then, we need to transpose the array to get the desired output\na = a.transpose((0, 2, 1))\n\nresult = a\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "\ndef sliding_window(a, window_size):\n    num_rows, num_cols = a.shape\n    for i in range(num_rows - window_size + 1):\n        for j in range(num_cols - window_size + 1):\n            yield a[i:i+window_size, j:j+windower_size]\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "\n# First, we need to reshape the array to (2, 6, 2)\na = a.reshape(2, 6, 2)\n\n# Then, we need to transpose the array to get the desired output\na = a.transpose(0, 1, 2)\n\n# Finally, we need to reshape the array to (3, 4, 2)\nresult = a.reshape(3, 4, 2)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "\n# First, we need to reshape the array to a 3-D array with the shape (2, 2, 6, 7)\na = a.reshape((2, 2, *a.shape))\n\n# Then, we can use slicing to extract patches of size 2\nresult = a[:, :, :, :patch_size]\nresult = result.reshape(-1, *result.shape[2:])\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i][j] = a[i][j]\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "\n# First, we need to reshape the array to a 3-D array with the shape (2, 2, 6, 7)\na = a.reshape((2, 2, *a.shape))\n\n# Then, we can use slicing to extract patches of size 2\nresult = a[:, :, :, :patch_size]\nresult = result.reshape(-1, *result.shape[2:])\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high+1]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high+1]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high+1]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.array(eval(string))", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "\ndef loguni(n, min, max, base=np.e):\n    samples = np.random.uniform(min, max, n)\n    return np.power(base, samples)\n\nresult = loguni(n, min, max)", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "\n# Generate a sequence of uniformly distributed random numbers\nx = np.random.uniform(0, 1, n)\n\n# Convert these random numbers to log-uniform distributed numbers\nresult = np:-exp1(min + x * (max - min))", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.power(np.random.uniform(min, max, n), 1/np.log(np.e))\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = pd.Series(np.nan, index=A.index)\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[0] = a * A[0]\nfor i in range(1, len(A)):\n    B[i] = a * A[i] + b * B[i-1] + c * B[i-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.array([])\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, *index)", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = np.sub2ind(dims, index)\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "df = pd.DataFrame(data=np.zeros((2,3)), index=index, columns=columns)", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.zeros(a.shape[0])\nfor i in range(a.shape[0]):\n    result[i] = np.sum(a[accmap == i])\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = a[index]\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap])\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = a[index]\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.array([[elementwise_function(x[i], y[i]) for i in range(len(x))] for i in range(len(x))])\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, p=probabilit)", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "result = np.pad(a, ((0, 0), (low_index, high_index)), mode='constant')", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[x.real != x.real]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, len(data) // bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, len(data) // bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, len(data) // bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "\n# First, we need to find the number of bins\nnum_bins = len(data) // bin_size\n\n# Then, we create the bins\nbins = np.array_split(data, num_bins)\n\n# We calculate the mean of each bin\nbin_data_mean = [np.mean(bin) for bin in bins]\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "\ndef bin_data(data, bin_size):\n    return [data[i:i+bin_size] for i in range(0, len(data), bin_size)]\n\nbin_data_mean = [np.mean(bin) for bin in bin_data(data, bin_size)]\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "def bin_data(data, bin_size):\n    bin_data = np.array_split(data, len(data) // bin_size)\n    bin_data = [np.array(b) for b in bin_data]\n    return bin_data\n\ndef calculate_means(bin_data):\n    means = []\n    for b in bin_data:\n        means.append(np.mean(b))\n    return means\n\nbin_data = bin_data(data, bin_size)\nmeans = calculate_means(bin_data)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    t = (x - x_min) / (x_max - x_min)\n    t = max(0, min(1, t))\n    return t * t * (3 - 2 * t)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = (x - x_min) / (x_max - x_min)\n    t = np.clip(t, 0.0, 1.0)\n    t = t * t * (3 - 2 * t)\n    return x_min + (x_max - x_min) * t\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\n# First, you need to create a circular version of b\nb_circ = np.concatenate((b, b[:2]))\n\n# Then, you can perform the circular correlation\nresult = np.correlate(b_circ, a, mode='full')\n\n# Finally, you need to remove the first part of the result\nresult = result[len(b):]\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.values.reshape(-1, 4, 15, 5)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = df.values.reshape(df.index.levels[0].size, df.index.levels[1].size, -1)\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.array([a[i:i+m] for i in range(0, len(a), m)]).reshape(-1, m)\n", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.array([a[i:i+m] for i in range(0, len(a), m)]).flatten()\n", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "\ndef convert_to_binary(arr, m):\n    result = []\n    for num in arr:\n        binary = format(num, 'b').zfill(m)\n        result.append(list(map(int, list(binary))))\n    return np.array(result)\n\nresult = convert_to_binary(a, m)\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nresult = (mean-3*std_dev, mean+3*std_dev)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nresult = (mean-2*std_dev, mean+2*std_dev)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    start = mean - 3*std_dev\n    end = mean + 3*std_dev\n    return (start, end)\n    ### END SOLUTION ###\nf()\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\nmask = (a >= lower_bound) & (a <= upper_bound)\nresult = mask\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "\nfor i in zero_rows:\n    a[i, :] = 0\nfor i in zero_cols:\n    a[:, i] = 0\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, :] = 0\na[:, 0] = 0", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "max_values = np.amax(a, axis=1)\nmask = np.where(np.arange(a.shape[0])[:, np.newaxis] == max_values)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "min_value = np.min(a, axis=1)[:, np.newaxis]\nmask = a == min_value", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "\n# Convert the lists to numpy arrays\npostal_codes = np.array(post)\ndistances = np.array(distance)\n\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(postal_codes, distances)[0, 1]\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ijk->ijm', X)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.dot(np.linalg.inv(Y.transpose(0,1,2).reshape(-1,Y.shape[-1]).transpose(0,1,2)).reshape(Y.shape).transpose(0,1,2))", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a\n", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "\nC = A[~np.isin(A, B)]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[np.in1d(A, B)]", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "\nmask = np.where((A[:, None] == B) | (A == B[..., None]))\nC = A[np.where(mask[:, 0] != -1)]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = rankdata(a)[::-1]", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "rankdata(a).astype(int)\n", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    result = rankdata(a)[::-1]\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack([x_dists, y_dists])\n", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack([x_dists, y_dists])\n", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][:][second][third]", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))\n", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.sum(X, axis=1)\nresult = X / l1[:, np.newaxis]", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.array([LA.norm(v,ord=2) for v in X.T]).T\nresult = x / np.expand_dims(x, axis=0).T\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v, ord=nperf.inf) for v in X])\nresult = x / np.max(x, axis=1, keepdims=True)", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = [df['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "result = np.sqrt(np.sum((a - a.T)**2, axis=1))\n", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "\ndef calculate_distance(a, i):\n    return np.sqrt(np.sum((a[i] - a)**2))\n\nresult = np.array([calculate_distance(a, i) for i in range(len(a))])\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "\ndef calculate_distance(a, i):\n    distance = np.sqrt(np.sum((a[i] - a)**2))\n    return distance\n\nresult = np.array([calculate_distance(a, i) for i in range(len(a))])\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# First, we need to remove all the zeros.\na = a[a != 0]\n\n# Then, we need to remove the adjacent duplicates.\na = np.diff(a)\n\n# Finally, we need to append the last element of the array to the result.\nresult = np.append(result, a[-1])\n\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# First, we need to find the indices of non-zero elements\nnon_zero_indices = np.where(a != 0)\n\n# Then, we create a new array with only the non-zero elements\nresult = a[non_zero_indices]\n\n# Finally, we reshape the array to the desired shape\nresult = result.reshape(-1, 1)\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame(np.column_stack((lat[0], lon[0], val[0])), columns=['lat', 'lon', 'val'])\n\nfor i in range(1, len(lat)):\n    df = df.append(pd.DataFrame(np.column_stack((lat[i], lon[i], val[i])), columns=['lat', 'lon', 'val']), ignore_index=True)\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    df = pd.DataFrame({'lat': lat[0], 'lon': lon[0], 'val': val[0]})\n    for i in range(1, len(lat)):\n        df = df.append(pd.DataFrame({'lat': lat[i], 'lon': lon[i], 'val': val[i]}, ignore_index=True), ignore_index=True)\n    return df\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame({\n    'lat': lat[0],\n    'lon': lon[0],\n    'val': val[0]\n})\n\nfor i in range(1, len(lat)):\n    df = df.append(pd.DataFrame({\n        'lat': lat[i],\n        'lon': lon[i],\n        'val': val[i]\n    }), ignore_index=True)\n\ndf['maximum'] = df[['lat', 'lon', 'val']].idxmax(axis=1)\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "\ndef rolling_window(data, size):\n    shape = data.shape\n    strides = data.strides\n    subshape = (slice(None),) + size\n    s = data.shape\n    subs = s[slice(None)]\n    subs = subs[..., None]\n    subs = subs[..., None, None]\n    subs = subs.transpose(1, 2, 3, 0)\n    subs = subs.reshape(-1, *subs[0])\n    subs = subs.transpose(0, 1, 2, 3)\n    subs = subs.reshape(-1, subs.shape[0] * subs.shape[1], subs.shape[2])\n    subs = subs.reshape(-1, subs.shape[0] * subs.shape[1])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "\ndef rolling_window(data, size):\n    shape = data.shape\n    strides = data.strides\n    subshape = (slice(None),) + size\n    s = data.shape\n    subs = s[slice(None)]\n    subs = subs[..., None]\n    subs = subs[..., None, None]\n    subs = subs.transpose(1, 2, 3, 0)\n    subs = subs.reshape(-1, *subs[0])\n    subs = subs.transpose(0, 1, 2, 3)\n    subs = subs.reshape(-1, subs.shape[0] * subs.shape[1], subs.shape[2])\n    subs = subs.reshape(-1, subs.shape[0] * subs.shape[1])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.shape[1:])\n    subs = subs.reshape(-1, *subs.", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:, :, -1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[..., -1]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = any(np.array_equal(c, contour) for contour in CNTS)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "def is_member(arr, list_of_arrays):\n    for i in list_of_arrays:\n        if np.array_equal(arr, i):\n            return True\n    return False\n\nresult = is_member(c, CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.diag(i)\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[a != 0] = 0\n", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\ntimestamps = pd.date_range(start=start_date, end=end_date, periods=n)\nresult = timestamps.to_series()\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where(x == a)[0][0] if a in x else -1\nresult = np.where(y == b)[0][0] if b in y else -1\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where(x == a)[0]\nresult = np.concatenate([indices, np.where(y == b)[0]])\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "\n# First, we need to find the coefficients a, b and c that minimize the squared error.\n# We can do this by using the numpy.polyfit function.\na = np.polyfit(x, y, 2)\nb = np.polyfit(x, y, 1)\nc = np.polyfit(x, y, 0)\n\n# Then, we can create a function with these coefficients and calculate the squared error.\nf = lambda x: a * x ** 2 + b * x + c\nresult = [a, b, c]\n\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "\n# First, we need to find the coefficients of the polynomial that best fits the data.\n# We do this by minimizing the sum of the squares of the differences between the observed and predicted values.\n\n# We use numpy's polyfit function to do this.\ncoefficients = np.polyfit(x, y, degree)\n\n# Now we can use these coefficients to construct the polynomial.\npolynomial = np.poly1d(coefficients)\n\n# Finally, we can evaluate the polynomial at the points where the data is known.\ny_predicted = polynomial(x)\n\n# We subtract the predicted values from the observed values to get the squared errors.\n# We take the square root of these to get the final result.\nresult = np.sqrt(np.sum((y - y_predicted) ** 2))\n\n# The result is the final approximation of the parameters of the polynomial.\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "df = df.sub(a)", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,ik->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "\nmask = (arr < -10)\nmask2 = (arr >= 15)\narr[mask] = 0\narr[mask2] = arr[mask2] + 5\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "\nmask1 = arr < n1\nmask2 = arr >= n2\nmask3 = mask1 & mask2\n\narr[mask1] = 0\narr[mask2] = 30\narr[mask3] = arr[mask3] + 5\n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "result = np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "result = np.sum(np.logical_or(s1 != s2, np.isnan(s1), np.isnan(s2)))", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a[1])\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all() for arr in a)\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), mode='constant')\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    diff_x = shape[1] - arr.shape[1]\n    diff_y = shape[0] - arr.shape[0]\n    if diff_x > 0 and diff_y > 0:\n        result = np.pad(arr, ((0,0), (0,0)), 'constant', constant_values=0)\n    elif diff_x <= 0 and differ_y > 0:\n        result = np.pad(arr, ((0,0), (0,0)), 'constant', constant_values=0)\n    elif diff_x > 0 and differ_y <= 0:\n        result = np.pad(arr, ((0,0), (0,0)), 'constant', constant_values=0)\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "\ndef pad_array(a, shape):\n    num_rows, num_cols = a.shape\n    pad_rows = (shape[0] - num_rows) // 2\n    pad_cols = (shape[1] - num_cols) // 2\n    padded_a = np.pad(a, ((0, 0), (pad_cols, pad_cols)), 'constant')\n    padded_a = np.pad(padded_a, ((0, 0), (0, pad_rows)), 'constant')\n    return padded_a\n\nresult = pad_array(a, shape)\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), b[:, :, 0], b[:, :, 1]]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[..., b[:, 0]])\n", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[..., b[:, 0]])\n", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "\nresult = df.loc[df['a'].between(1, 4)].set_index('a')['b']\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "mask = np.array([[0,1,1,1],\n                 [1,1,0,1],\n                 [0,0,1,1]], dtype=np.bool)\nresult = np.ma.array(im, mask=mask)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "mask = A != 0\nresult = A[mask]", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\n# First, we need to find the non-zero elements in the image.\n# We can do this by using the np.where function to find the indices of non-zero elements.\n\nnon_zero_indices = np.where(im != 0)\n\n# Then, we need to find the minimum and maximum indices of the non-zero elements.\n# We can do this by using the np.min and np.max functions.\n\nmin_row, min_col = np.min(non_zero_indices[0]), np:-min(non_zero_indices[1])\nmax_row, max_col = np.max(non_zero_indices[0]), np.max(non_zero_indices[1])\n\n# Finally, we can create the result by using the np.array function.\n\nresult = im[min_row:max_row+1, min_col:max_col+1]\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "mask = np.any(im != 0, axis=0)\nresult = im[mask]\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.plot(x, label='x-y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "ax = plt.gca()\nax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))\nax.yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.ticklabel_format(axis='x', style='plain')\nplt.show()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.plot(x, np.sin(x))\nplt.plot(x, np.cos(x))\nplt.show()\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.plot(x, np.sin(x))\nplt.plot(x, np.cos(x))\nplt.show()\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, marker='D')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.plot(x, y, marker='D')\nplt.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, 'r')\nplt.fill_between(range(2,12), x[2:12], color='red', alpha=0.5)\nplt.gca().set(xlim=(0, 12), ylim=(-10, 10))\nplt.show()\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = 2 * x + 1\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = [0, 1]\ny = [0, 2]\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    style=\"Gender\",\n    palette=\"muted\",\n    height=5,\n    aspect=0.6,\n)\nplt.show()\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "plt.figure(figsize=(10,6))\nplt.plot(x, y, 'r')\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({\n    'x': x,\n    'y': y\n})\n\nsns.lineplot(data=df)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\nplt.show()\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.plot(x, y)\nplt.title('Cosine Function')\nplt.xlabel('x')\nplt.ylabel('cos(x)')\nplt.legend(['cos(x)'])\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\nplt.show()\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color='black')\nplt.tick_params(axis='both', colors='black')\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\nplt.show()\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), \"o-\", color='red', lw=5, markersize=30)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "plt.xticks([0, 2*np.pi, 4*np.pi])\nplt.xlabel(\"x-axis\")\nplt.ylabel(\"y-axis\")\nplt.title(\"Plot of cos(x)\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='best')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.imshow(H, cmap='viridis')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\nplt.xticks(rotation=45)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "plt.xticks(rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "plt.title(myTitle, fontsize=20, loc='left')\nplter = plt.gca().axes.get_ylabel()\nplt.title(myTitle, fontsize=20, loc='right')\nplt.show()\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "plt.yaxis = 'inverted'\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5, 3])\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 10))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "ax = plt.axes()\nax.set_xticks(np.arange(0, 10, 1))\nax.set_yticks(nperation=1))\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.ticklabel_format(axis='y', style='plain')\nplt.show()\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax.plot(x, y, linestyle='--')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y1, label='sin')\nplt.plot(x, y2, label='cos')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Graph of Sine and Cosine Functions')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y1, label='sin')\nplt.plot(x, y2, label='cos')\nplt.title('Sine and Cosine Waves')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(loc='upper center')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\nplt.show()\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.ticklabel_format(axis='x', rotation=45)\nplt.xticks(rotation=45)\nplt.show()\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(0, 10, 2))\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(0, max(plt.gca().get_ylim()), 1))\nplt.axhline(y=3, color='r', linestyle='--')\nplt.axhline(y=4, color='r', linestyle=':')\nplt.show()\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(0, max(y)+1, 2))\nplt.xticks(np.arange(0, max(x)+1, 2))\nplt.show()\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xticks([])\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xticks(np.arange(0, 10, 2))\nplt.yticks(np.arange(0, 10, 2))\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks([])\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.yticks(rotation='right')\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\nplt.yticks(range(10))\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', \n              scatter_kws={'color':'green'}, line_kws={'color':'blue'})\nplt.show()\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', \n              scatter_kws={'color':'green'}, line_kws={'color':'blue'})\nplt.show()\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips)\nplt.show()\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(9, 6))\nplt.bar(df[\"s1\"], df[\"celltype\"], color='blue')\nplt.xlabel(\"s1\")\nplt.xticks(rotation=90)\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(9, 6))\nplt.bar(df[\"s1\"], df[\"s2\"], label=df[\"celltype\"])\nplt.xlabel(df[\"celltype\"], rotation=45)\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.xticks(np.arange(0,11,1))\nplt.yticks(np.arange(0,11,1))\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Plot of Y over X')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.xticks(rotation='vertical')\nplt.tick_params(labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "plt.plot([0.22058956, 0.33088437, 2.20589566], [0.0, 0.0, 0.0])\nplt.ylabel('Frequency')\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.imshow(rand_mat, interpolation='nearest', cmap='YlGnBu', aspect='auto')\nplt.xticks(range(4), xlabels, rotation=90)\nplt.yticks(range(4), ylabels)\nplt.gca().invert_yaxis()\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nax = fig.add_subplot(121)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y)\naxs[0].set_title('Y')\naxs[1].plot(x, y)\naxs[1].set_title('Y')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "plt.figure(figsize=(8,6))\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, marker_size=30)\nplt.show()\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.scatter(a, b, c)\nplt.annotate(c, (a[0], b[0]), textcoords=\"offset points\")\nplt.show()\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='y over x')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(loc='best', title='Legend')\nplt.title('y over x')\nplt.legend(loc='best', title='Legend', fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, color='blue', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, ax1 = plt.subplots(2)\nax2 = plt.*t.gca().add_subplot(ax1)\n\nfig.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist([x, y], label=['x', 'y'], \n         color=['blue', 'green'], \n         alpha=0.5, \n         rwidth=0.8)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.plot([a, c], [b, d], 'r')\nplt.xlim([0, 5])\nplt.ylim([0, 5])\nplt.show()\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2)\nax[0].imshow(x, cmap='viridis')\nax[1].imshow(y, cmap='viridis')\n\n# create a colorbar for the first subplot\ncax = ax[0].imshow(x, cmap='viridis')\nplt.colorbar(cax)\n\n# create a colorbar for the second subplot\ncax = ax[1].imshow(y, cmap='viridis')\nplt.colorbar(cax)\n\nplt.show()\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\n\n# Add a legend to the plot\nplt.legend()\n\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "plt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.plot(x, y, label='Y over X')\nplt.plot(x, z, label='Z over X')\nplt.title('Y and Z')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(a, y, label='Y over A')\nplt.plot(a, z, label='Z over A')\nplt.title('Y and Z')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.plot([x[0] for x in points], [x[1] for x in points], 'o-')\nplt.gca().set_yscale('log')\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.plot(x, y)\nplt.title('Graph of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=18)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.plot(x, y)\nax.set_xticks(np.arange(0,11,1))\nax.set_yticks(np.arange(0,11,1))\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Graph')\nplt.show()\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "plt.figure(figsize=(6, 6))\nplt.plot(*zip(*lines), color=tuple(c[0]))\nplt.plot(*zip(*lines[1]), color=tuple(c[1]))\nplt.plot(*zip(*lines[2]), color=tuple(c[2]))\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.plot(x, y)\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y vs x on a log-log plot')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 7))\n\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title('Line Plot of Data')\n    plt.legend(loc='best')\n    plt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "plt.hist(data, bins=[1000, 2000, 3000, 4000, 5000, 6000, 7000],\n         align='left',\n         rwidth=0.8,\n         color='black',\n         histtype='bar',\n         alpha=0.7)\n\nplter = plt.gca().ticklabel_format(style='plain', axis='y')\nplt.setp(plt.gca().get_yticks(), labelonly=True)\nplt.ylabel('Counts')\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y:'%1.0f' % (y/1000)))\nplt.title('Histogram of Data')\nplt.show()\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, 'r--o')\nplt.title('Line Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[1].plot(x, y, label='y')\naxs[1].plot(a, z, label='a')\naxs[0].legend(loc='upper right')\naxs[1].legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "plt.figure(figsize=(10, 6))\n\nplter = sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, scatter=False, color='red')\nplt.title('Regression plot of \"bill_depth_mm\" over \"bill_length_mm\"')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, color='blue')\nplt.title('Regression plot of \"flipper_length_mm\" over \"bill_length_mm\"')\nplt.show()\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax2 = plt.gca().axes[1]\nax2.set_xlabel('Second')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label='$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "xticks = plt.gca().get_xticks()\nplt.gca().xaxis.set_ticks(xticks[::2])\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\nplt.ticklabel_format(rotation=-60)\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.xticks(rotation=-60)\nplt.ticklabel_format(rotation=-60)\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(np.arange(2010, 2020, 1), alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.tick_params(axis='x', which='both', length=0)\nplt.tick_params(axis='y', which='both', length=0)\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.tick_params(axis='x', which='both', length=0)\nplt.tick_params(axis='y', which='both', length=0)\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "plt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(x, y, 'r')\nplt.title('Figure')\nplt.subplot(1, 2, 2)\nplt.plot(y, x, 'r')\nplt.title('Figure')\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df['Type A'], df['Type B'])\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='///')\nplt.show()\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none', lw=3)\nplt.plot([0, 10], [0, 10], 'k--')\nplt.show()\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*')\nplt.show()\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*', hatch='v')\nplt.show()\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.xlim([0, 5])\nplt.ylim([0, 5])\nplt.imshow(data, extent=[0, 5, 0, 5])\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(y, x, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.figure(figsize=(9,6))\nplt.bar(d.keys(), d.values(), color=[c[i] for i in d.keys()])\nplt.xlabel(\"Keys\")\nplt.ylabel(\"Values\")\nplt.title(\"Bar plot\")\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.axvline(x=3, color='r', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 10))\nplt.pie(height, labels=labels, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.title(\"Pie Chart\")\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedgeprops={'width' : 0.4})\nplt.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle=':', color='black', alpha=0.5)\nplt.grid(which='major', linestyle='--', color='black', alpha=0.3)\nplt.show()\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.axis('equal')  # Ensure that the pie chart is a circle\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.axis('equal')  # Ensure that the pie chart is a circle\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'r--o')\nplt.show()\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color='green')\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "plt.bar(np.arange(len(blue_bar)), blue_bar, color='b', align='center')\nplt.bar(np.arange(len(orange_bar)), orange_bar, color='orange', align='center')\nplt.xticks(np.arange(len(blue_bar)), ['Group A', 'Group B', 'Group C'])\nplt.title('Bar plot')\nplt.xlabel('Groups')\nplt.ylabel('Values')\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "plt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(x, y, label='Line 1')\nplt.plot(a, z, label='Line 2')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.plot(x, z, label='Line 1')\nplt.plot(a, y, label='Line 2')\nplt.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\nplt.show()\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.xticks(np.arange(0, max(x)+1, 1))\nplt.yticks(np.arange(0, max(y)+1, 1))\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(x=\"species\", y=\"bill_length_mm\", hue=\"sex\", data=df, height=5, aspect=0.8, kind='bar')\nplt.show()\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.gca().add_patch(plt.Circle((0.5, 0.5), 0.2, fill=False, color='red'))\nplt.gca().set_aspect('equal')\nplt.show()\n", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.plot(x, y, label='Line')\nplt.legend(loc='upper left')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.plot(x, y, label='Line')\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='best')\nplt.show()\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.show()\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.imshow(data, cmap='viridis')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.title(r\"Figure \\textbf{1}\")\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.set(style=\"whitegrid\")\nplt.figure(figsize=(6, 6))\nplot = sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"id\", palette=\"deep\")\nplt.legend(loc='upper right', bbox_to_anchor=(0.6, 0.6))\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(y, x)\nplt.gca().invert_yaxis()\nplt.show()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.scatter(x, y, xlim=(0, 10), ylim=(0, 10), clip_on=False)\nplt.show()\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', s=50)\nplt.show()\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "plt.figure(figsize=(15, 15))\nplt.subplot(221)\nplt.plot(x, y, 'r')\nplt.subplot(222)\nplt.plot(x, y, 'g')\nplt.subplot(223)\nplt.plot(x, y, 'b')\nplt.subplot(224)\nplt.plot(x, y, 'y')\nplt.show()\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), color='blue')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.errorbar(x, y, yerr=error, fmt='o')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.axhline(0, color='white', linestyle='--')\nplt.axvline(0, color='white', linestyle='--')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', capsize=5, color='red')\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "plt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.plot(x, y)\nplt.title('Y')\nplt.subplot(122)\nplt.plot(z, a)\nplt.title('Z')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fmts = 'float')\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 8))\nsns.heatmap(df, cbar=False, cmap='viridis')\nplt.title('Penguins Dataset')\nplt.show()\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.plot(x, y)\nplt.xticks(rotation=45)\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.title(\"Line Chart\")\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.plot(x, y)\nplt.xticks(np.arange(0, max(x)+1, 1))\nplt.margins(x=0.05)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks([])\nplt.tick_params(axis='x', which='both', bottom='off', top='off')\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.8, palette=\"viridis\")\nplt.title(\"Group: Fat\")\n\n# Make catplots of scatter plots by using \"time\" as x, \"pulse\" as y, \"kind\" as hue, and \"diet\" as col\n# Change the subplots titles to \"Group: Skin\" and \"Group: No Skin\"\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.8)\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Exercise Time\")\nplt.title(\"Catplot with 'time' as x-axis, 'pulse' as y-axis, 'kind' as hue, and 'diet' as col\")\nplt.show()\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.8, palette=\"dark\")\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\nplt.show()\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='best', bbox_to_anchor=(0.0, 1.02), borderaxespad=0.)\nplt.show()\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "fig = plt.figure()\nax1 = fig.add_subplot(211)\nax1.plot(t, a, 'b')\nax1.set_title('Sine Waves')\nax1.set_xlabel('Time')\nax1.set_ylabel('Amplitude')\n\nax2 = fig.add_subplot(212)\nax2.plot(t, c, 'r')\nax2.set_title('Sine Waves')\nax2.set_xlabel('Time')\nax2.set_ylabel('Amplitude')\n\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "plt.figure(figsize=(8,6))\nsns.stripplot(x=\"species\", y=\"bill_length_mm\", hue=\"sex\", data=df, palette=\"deep\")\nplt.title(\"Penguins Stripplot\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Bill Length (mm)\")\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "fig = sns.FacetGrid(df, row=\"b\", hue=\"c\")\nplots = fig.add_subplot(111)\nplots.set_xticks(np.arange(1, 31, 2))\nplots.set_xticklabels(nper.arange(1, 31, 2))\nplots.tick_params(axis='x', which='both', length=0)\nplots.tick_params(axis='x', which='both', labelsize=0)\nplots.set_xlabel(\"\")\nplots.set_ylabel(\"\")\nplots.set_title(\"\")\nplots.set_ylabel(\"\")\nplots.set_yticks([])\nplots.set_yticklabels([])\nplots.set_xlim(0, 31)\nplots.set_ylim(0, 1)\nplots.legend(loc=\"upper left\")\nplots.grid(True)\nplots.set_xticks(np.arange(1, 31, 2))\nplots.set_xticklabels(np.arange(1, 31, 2))\nplots.tick_params(axis='x', which='both', length=0)\nplots.tick_params(axis='x', which='both', labelsize=0)\nplots.set_xlabel(\"\")\nplots.set_ylabel(\"\")\nplots.set_title(\"\")\nplots.set_ylabel(\"\")\nplots.set_yticks([])\nplots.set_yticklabels([])\nplots.set_xlim(0, 31)\nplots.set_ylim(0, 1)\nplots.legend(loc=\"upper left\")\nplots.grid(True)\nplots.set_xticks(np.arange(1, 31, 2))\nplots.set_xticklabels(np.arange(1, 31, 2))\nplots.tick_params(axis='x', which='both', length=0)\nplots.tick_params(axis='x', which='both', labelsize=0)\nplots.set_xlabel(\"\")\nplots.set_ylabel(\"\")\nplots.set_title(\"\")\nplots.set_ylabel(\"\")\nplots.set_yticks([])\nplots.set_yticklabels([])\nplots.set_xlim(0, 31)\nplots.set_ylim(0, 1)\nplots.legend(loc=\"upper left\")\nplots.grid(True)\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azim=100)\nax.set_zlim(0, 1)\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol)\n\nfor i in range(nrow * ncol):\n    ax = plt.subplot(gs[i])\n    plt.imshow(x[i])\n    ax.axis('off')\n\nplt.show()\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\nresult = tf.one_hot(tf.constant(labels), depth=10)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\nlabels_tensor = tf.constant(labels)\nresult = tf.one_hot(labels_tensor, depth=10)\nresult = tf.argmax(result, axis=1)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "\nresult = tf.constant(labels)\nresult = tf.one_hot(result, depth=10)\nresult = tf.argmax(result, axis=1)\nresult = tf.cast(result, tf.int32)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    result = tf.one_hot(tf.constant(labels), depth=10)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_labels))\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\nlabels_tensor = tf.constant(labels)\nresult = tf.one_hot(labels_tensor, depth=10)\nresult = tf.argmax(result, axis=1)\nresult = tf.cast(result, tf.int32)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "\ndef my_map_func(i):\n  return [i, i+1, i+2]\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.py_function(\n  func=my-map-func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    result = [i+1 for i in input]\n    ### END SOLUTION ###\n    return result\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(map_func=lambda input: tf.py_function(\n  func=f, inp=[input], Tout=[tf.int64]\n))\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\n\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\nmax_length = tf.constant(8)\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\nresult = tf.cast(mask, dtype=tfer.float32)\nresult", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\nmax_length = tf.constant(8)\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\nresult = tf.cast(mask, dtype=tfer.float32)\nresult", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\nmask = tf.pad(mask, [[0, 1], [0, 0]], constant_values=1.0)\nmask = tf.transpose(mask)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    mask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n    mask = tf.cast(mask, tf.int32)\n    return mask\n    ### END SOLUTION ###\n\nresult = f()\nprint(result)\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\nmax_length = tf.constant(8)\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\nresult = tf.cast(mask, dtype=tfer.float32)\nresult", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([tf.expand_dims(a, 0), tf.expand_dims(b, 0)], axis=0)\nresult = tf.transpose(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    cartesian_product = tf.stack([tf.expand_dims(a, 0), tf.expand_dims(b, 0)], axis=0)\n    result = tf.concat([tf.expand_dims(cartesian_product[0], 1), tf:-1)\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "result = tf.squeeze(a, [2])", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.expand_dims(a, axis=2)", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "result = tf.expand_dims(tf.expand_dims(a, axis=0), axis=2)", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\ndiff = tf.square(tf.subtract(a, b))\nsum_of_squares = tf.reduce_sum(diff, axis=0)\nresult = tf.sqrt(sum_of_squares)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "result = tf.reduce_sum(tf.square(tf.sub(a, b)), axis=0)", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    return tf.reduce_sum(tf.square(tf.sub(A, B)))\n    ### END SOLUTION ###\n\nprint(f(example_a, example_b))\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "\nm = tf.tensor_gather(x, y)[z]\n\nresult = sess.run(m)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "\nm = tf.tensor_gather(x, row)\nresult = tf.tensor_scatter(x, row, col)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    result = x[y,z]\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\nC = tf.eager_tensor(np.dot(A, B))\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "C = tf.eager_tensor(np.dot(A, B))\nprint(C)", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\nx = tf.constant(x)\nresult = tf.strings.unicode_transcode(x)\n\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    result = tf.strings.regex_decode(x, \"UTF-8\")\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "mask = tf.math.reduce_sum(x, axis=-1) != 0\nresult = tf.reduce_sum(x, axis=-1) / tf.math.reduce_sum(mask, axis=-1)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = tf.math.not_equal(x, 0)\nresult = tf.reduce_sum(x, axis=-1) / tf.reduce_sum(mask, axis=-1)\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": " ###\n    mask = tf.math.reduce_any(x, axis=-1)\n    masked_x = tf.boolean_mask(x, mask)\n    non_zero_count = tf.math.reduce_sum(tf.cast(mask, dtype=tfer.float32), axis=-1)\n    result = tf.math.divide(tf.reduce_sum(masked_x, axis=-1), non_zero_count)\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\ntry:\n    Session = tf.compat.v1.Session\nexcept AttributeError:\n    Session = tf.Session\n\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nwith Session() as sess:\n   result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\nresult = tf.argmax(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\nresult = tf.argmax(a, axis=0)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    max_indices = tf.argmax(a, axis=1)\n    ### END SOLUTION ###\n    return max_indices\n\nprint(f(example_a))\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=0)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save('export/1')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=4)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "with tf.random.Generator(seed_x) as gen:\n    result = gen.uniform(shape=[114], minval=2, maxval=5)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    result = tf.random.uniform(shape=[10], minval=1, maxval=4)\n    return result\n    ### END SOLUTION\n\nprint(f(10))\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "print(tf.__version__)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "coefficients = np.polyfit(x, y, 1)\nresult = coefficients\nEND SOLUTION\n\nThe function polyfit() fits a polynomial to the data and returns the coefficients of the polynomial. The first argument is the x-coordinates, the second is the y-coordinates, and the third is the order of the polynomial.\n\nIn your case, you want to fit a polynomial of order 1 to your data, so you should use 1 as the third argument.\n\nThe result is an array of the coefficients of the polynomial, which in this case is [A, B].\n\nNote: The polyfit function returns the coefficients in the order in which they were specified. If you want the coefficients in the order [B, A], you should first fit the polynomial in the order [1, 0] and then reverse the coefficients.\n\nFor example:\n\n```python\ncoefficients = np.polyfit(x, y, 1)\ncoefficients = coefficients[::-1]\n```\n\nThis will give you the coefficients in the order [B, A].\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "coefficients = np.polyfit(x, y, 1)\nresult = np.array(coefficients)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\npopt, pcov = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array(popt)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "test_stat, p_value = stats.ks_2samp(x, y)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat = np.concatenate([np.ones(len(x)), np.zeros(len(y))])\nt_stat = stats.kstest(test_stat, 'norm')\nresult = t_stat.pvalue < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c - 2)**2 + ((3 * a - b - c)**2 + sin(b) + cos(b) + 4)\nresult = optimize.minimize(f, initial_guess)\nresult.x[0], result.x[1], result.x[2]", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = [1 - scipy.stats.norm.cdf(z) for z in z_scores]\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [scipy.stats.norm.cdf(z, mu, sigma) for z in z_scores]", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = np.log(1 - p_values) / np.sqrt(2)", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([stddev],loc=mu)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm([stddev], loc=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa.multiply(sb)", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    result = sA.multiply(sB)\n    return result\n    ### END SOLUTION ###\n\nf(example_sA, example_sB)\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "interp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "interp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle)\nxrot = data_rot.shape[1]/2\nyrot = data_rot.shape[0]/2\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "\n# To extract the main diagonal of a sparse matrix, you can use the following line of code:\nresult = M.diagonal()\n", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "def ks_test(x, y):\n    statistic, p_value = stats.kstest(x, y)\n    return statistic, p_value\n\nresult = ks_test(times, np.linspace(0, T, len(times)))\n", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    ks_test = stats.kstest(times, 'uniform')\n    return ks_test\n    ### END SOLUTION ###\nf()\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "def test_uniformity(times, T):\n    # Convert the times to numpy array\n    times = np.array(times)\n    # Perform the Kolmogorov-Smirnov test\n    statistic, p_value = stats.kstest(times, 'uniform')\n    # Check if the p-value is less than the chosen significance level\n    result = p_value < 0.05\n    return result\nresult = test_uniformity(times, T)\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.vstack((c1, c2))", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack((c1, c2))", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "dist_matrix = scipy.spatial.distance.cdist(points1, points2, 'euclidean')\nresult = scipy.optimize.linear_sum_assignment(dist_matrix)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "dist_matrix = scipy.spatial.distance.cdist(points1, points2, metric='euclidean')\nresult = scipy.optimize.linear_sum_assignment(dist_matrix)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "labels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "labels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    labels, num_labels = ndimage.label(img > threshold)\n    return num_labels\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\nlabels, num_labels = ndimage.label(img > threshold)\ncenters = ndimage.center_of_mass(img, labels, range(1, num_labels + 1))\ndistances = np.sqrt(np.sum((centers - np.array([0, 0]))**2, axis=-1))\nresult = distances.tolist()\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "\nM = M.tolil()\nM = M.T\nM = M.tolil()\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    sA = (sA + sA.T)/2\n    return sA.A\n    ### END SOLUTION ###\n\nf(example_sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "labels, num_labels = scipy.ndimage.label(square)\nsquare_copy = np.zeros_like(square, dtype=np.bool)\nsquare_copy[x, y] = True\nblobs = scipy.ndimage.measurements.label(square_copy)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# First, we need to find all the connected components in the image.\nlabels, ncomponents = scipy.ndimage.label(square)\n\n# Then, we need to find the size of each component.\nsizes = np.bincount(labels)\n\n# We only want to keep the components of size 1.\nkeep = sizes == 1\n\n# We create a mask where the components of size 1 are kept, and the rest is set to 0.\nmask = np.zeros(square.shape, dtype=np.bool)\nmask[x, y] = True\n\n# We apply the mask to the square.\nsquare[np.logical_not(mask)] = 0\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col)\nstandard_deviation = np.std(col)", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_val = np.amax(col)\nmin_val = np.amin(col)\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "\n# Convert the csr_matrix to a dense array\ndense_arr = np.array(col.todense())\n\n# Get the median\nMedian = np.median(dense_arr)\n\n# Get the mode\nMode = np:-mode(dense_arr)", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "popt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert to a 1D array\nexample_array = example_array.ravel()\n\n# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.cdist(example_array, example_array, metric='euclidean')\n\n# Convert back to 2D array\ndistances = distances.reshape(len(example_array), len(example_array))\n\nresult = distances\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Convert to 1-D array\nexample_array = example_array.ravel()\n\n# Calculate pairwise distances\ndistances = scipy.spatial.distance.cdist(example_array, example_array, metric='cityblock')\n\n# Convert to 2-D array\ndistances = distances.reshape(len(example_array), len(example_array))\n\n# Find minimum distance\nmin_distance = distances.min()\n\n# Find the IDs of the minimum distance\nmin_ids = np.where(distances == min_distance)\n\n# Reshape to 2-D array\nmin_ids = min_ids.reshape(len(example_array), len(example-array))\n\nresult = min_ids\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate pairwise Euclidean distances\n    distances = scipy.spatial.distance.cdist(example_array, metric='euclidean')\n    # Convert distances to meters\n    distances_m = distances * 0.001\n    # Convert to a 2D array\n    distances_m = distances_m.reshape(-1, 2)\n    ### END SOLUTION ###\n    return distances_m\n", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "\ntck = interpolate.splrep(x, y)\nresult = interpolate.splev(x_val, tck)\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x = np.array([x1, x2, x3, x4])\nstatistic, critical_values = ss.anderson_ksamp(x)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x = np.array([x1, x2])\nresult = ss.anderson_ksamp(x)\nprint(result)\n", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "\ndef tau1(x):\n    y = np.array(df['A'])\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = df['A'].rolling(3).apply(tau1)\ndf", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "result = bool(sa.nnz == 0)", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "result = all(sa.toarray()[i][j] == 0 for i in range(sa.shape[0]) for j in range(sa.shape[1]))\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "test_result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = test_result.pvalue\n", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n    ### END SOLUTION ###\n    return p_value\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "kurtosis_result = np.mean(np.power(a - np.mean(a), 4)/np.power(np.std(a), 4))", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interp = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interp(x, y)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    result = np.piecewise(z, [s<-1, s>1, t<-2, t>0])\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "# First, we need to find the closest point to the extra points\ndistances = scipy.spatial.distance.cdist(points, extraPoints)\nclosest_points = np.argpartition(distances, 2)\n\n# Then, we need to find the region of the closest point\nregions = vor.regions\nregion_of_closest_point = [regions[closest_points[i][0]] for i in range(len(closest_points))]\n\n# Finally, we need to count the number of points in each region\nresult = [len(region_of_closest_point[i]) for i in range(len(region_of_closest-point_indices))]\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "# First, we need to find the closest point to the extra points\ndistances = scipy.spatial.distance.cdist(points, extraPoints)\nclosest_points = np.argpartition(distances, 2)\n\n# Then, we need to find the region of the closest points\nregions = vor.regions\nregion_of_closest_points = [regions[closest_points[i][0]] for i in range(len(closest_points))]\n\n# Finally, we count the number of points in each region\nresult = [len(region_of_closest_points[i]) for i in range(len(region_of_closest-points))]\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "\n# Find the maximum size among all vectors\nmax_size = max([len(v) for v in vectors])\n\n# Create a sparse matrix with zeros\nsparse_matrix = sparse.coo_matrix((np.ones(len(vectors)), dtype=np.int64))\n\n# Fill in the sparse matrix with vectors\nfor i, v in enumerate(vectors):\n    sparse_matrix[i, :len(v)] = v\n\nresult = sparse_matrix\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant')", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column].toarray()[0]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[i, j] for i, j in zip(row, column)]\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "f = scipy.interpolate.interp1d(x, array[:, :, 0])\nnew_array = f(x_new)", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u, o2, x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner[0]\n    P = P_inner[0] + P_outer/2\n    return P\nprob = NormalDistro(u, o2, x)", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner[0]\n    P = P_inner[0] + P_outer/2\n    return P\n    ### END SOLUTION ###\n\nThe error is due to the fact that the function `scipy.integrate.quad` is not a function, it's a method of the `scipy.integrate` module. You should call the function like this:\n\n```python\nP_inner = scipy.integrate.quad(NDfx, -dev, dev)\n```\n\nor\n\n```python\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:-\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx, -dev, dev)\n```\n\nor\n\n```python:\nP_inner = scipy.integrate(NDfx", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "\n# First, we need to generate the DCT matrix\ndct_matrix = np.zeros((N, N))\n\n# Then, we calculate the DCT\nfor i in range(N):\n    for j in range(N):\n        dct_matrix[i, j] = sf.dct([[np.cos(np.pi * i * (j - 0.5))]])[0][0]\n\nresult = dct_matrix\n", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "diags(matrix, [1, 0, -1], 'c')\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "\nM = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        M[i,j] = scipy.stats.binom.pmf(j, i, p)\nresult = M\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x))\n", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "z_scores = (df - df.mean()) / df.std()\nresult = z_scores.apply(lambda x: x.apply(lambda y: y / x.std() if x.std() != 0 else 0))\n", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "\n# First, calculate the z-scores for each column\nz_scores = df.apply(lambda x: stats.zscore(x))\n\n# Then, create a new dataframe with the original data and the z-scores\nresult = pd.concat([df.reset_index().rename(columns={'probegenes': 'data'}), z_scores], axis=1)\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "\n# First, calculate the z-score for each column\nz_scores = stats.zscore(df)\n\n# Then, create a new dataframe with the original data and the z-score\nresult = pd.concat([df.reset_index().rename(columns={'probegenes': 'data'}), pd.DataFrame(z_scores, index=df.index)], axis=1)\n\n# Rename the 'data' column to 'sample1', 'sample2', and 'sample3'\nresult.rename(columns={'probegenes': 'sample1', 'sample2': 'sample2', 'sample3': 'sample3'}, inplace=True)\n\n# Round the z-score to 3 decimal places\nresult['zscore'] = result['zscore'].apply(lambda x: round(x, 3))\n\nresult\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "\ny = np.linspace(0, 1, shape[0])\nx = np.linspace(0, 1, shape[1])\n\nresult = np.array([[distance.cdist(np.array([y[i], x[j]]), np.array([[0, 0]])) for j in range(shape[1])] for i in range(shape[0])])\n\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "\ndef get_distance_2(shape):\n    rows, cols = shape\n    mid = np.array([[rows/2, cols/2]])\n    y, x = np.mgrid[:rows, :cols]\n    dist_fn = lambda x, y: np.abs(x - mid[0, 0]) + np.abs(y - mid[0, 0])\n    result = np.array([dist_fn(x, y) for x, y in zip(np.ravel(x), np.ravel(y))])\n    result = result.reshape(rows, cols, 2)\n    return result\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.ogronouvich(shape)\n    mid = np.array([[0, 0]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\n\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # return the solution in this function\n    # result = f(shape = (6, 6))\n    ### BEGIN SOLUTION ###\n    y, x = np.ogronouvich(shape)\n    mid = np.array([[0, 0]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\n\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # return the solution in this function\n    # result = f(shape = (6, 6))\n    ### BEGIN SOLUTION ###\n    y, x = np.ogronouvich(shape)\n    mid = np.array([[0, 0]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\n\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # return the solution in this function\n    # result = f(shape = (6, 6))\n    ### BEGIN SOLUTION ###\n    y, x = np.ogronouvich(shape)\n    mid = np.array([[0, 0]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\n\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # return the solution in this function\n    # result = f(shape = (6, 6))\n    ### BEGIN SOLUTION ###\n    y, x = np.ogronouvich(shape)\n    mid = np.array([[0, 0]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\n\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # return the solution in this function\n    # result = f(shape = (6, 6))\n    ### BEGIN SOLUTION ###\n    y, x = np.ogronouvich(shape)\n    mid = np.array([[0, 0]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\n\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # return the solution in this function\n    # result = f(shape = (6, 6))\n    ### BEGIN SOLUTION ###\n    y, x = np.ogronouvich(shape)\n    mid = np.array([[0, 0]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\n\nA:\n<code>\nimport numpy as np\nfrom scipy.spatial import distance\ndef f(shape = (6, 6)):\n    # return the solution in this function\n    # result = f(shape = (6, 6))\n    ### BEGIN SOLUTION ###\n    y, x = np.ogronouvich(shape)\n    mid = np.array([[0, 0]])\n    return distance.cdist(np.dstack((y, x)), mid)\n    ### END SOLUTION ###\n\n\nA:\n<code>\nimport numpy as np\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((func(x) - y) ** 2)\n\ndef main():\n    out = scipy.optimize.fmin(residual, x0)\n    print(out)\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return np.sum((y - a.dot(x ** 2)) ** 2)\nresidual = lambda x: y - a.dot(x ** 2)\nx_upper_bounds = x_true * 1.5\nout = scipy.optimize.minimize(func, x0, method='L-BFGS-B', bounds=[(-1, None)] * 5)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    if 0 < t < 2*np.pi:\n        return -100 * (N1 + np.sin(t))\n    else:\n        return 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack((sa, sb))", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.vstack((sa, sb))", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, 0, 1)\n    return result\n    ### END SOLUTION ###\n\nI'm not sure what I'm doing wrong. I'm trying to solve the definite integral of 2c*x with c varying from 5 to 50.\n\nA: The problem with your code is that you are not passing the value of c to the function. The function should be defined with a parameter for c.\n\nHere is the corrected code:\n\n```python\nfrom scipy import integrate\n\ndef solve_integral(c):\n    # equation\n    eqn = 2*x*c\n    # integrate \n    result, error = integrate.quad(lambda x: eqn, 0, 1)\n    return result\n\nc_values = [5, 10, 15, 20, 25]  # your values\nI = []\n\nfor c in c_values:\n    I.append(solve_integral(c))\n\nI = array(I)\n```\n\nIn this code, I'm using a list to store the values of c. The function solve_integral is defined with a parameter c, which is used to solve the integral. The result is then appended to the list I.\n\nThe list c_values contains the values of c you want to use for the integral. The loop iterates over each value of c, solves the integral with that value, and appends the result to the list I.\n\nThe result is a list of the solutions to the integral for each value of c.\n", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "\n# Construct a dense matrix with the scalar value x\nx_matrix = np.array([[x]])\n\n# Add the scalar to the sparse matrix\nV_plus_x = V.todense() + x_matrix\n\n# Convert the result back to a sparse matrix\nV_plus_x_sparse = sparse.csr_matrix(V_plus_x)\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V.data += x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "A = V + x\nB = A + y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# extract each column\ncolumns = sa.todense().tocoo().col\n\n# normalize each column\nnormalized_columns = columns / np.linalg.norm(columns, 'fro')\n\n# update the original matrix\nsa[:, normalized_columns] = np.outer(normalized_columns, normalizeder_columns)", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "\n# extract each column\ncolumns = sa.todense().T\n\n# normalize each column\nnormalized_columns = columns / np.linalg.norm(columns, axis=1)[:, np.newaxis]\n\n# update the original matrix\nsa = sparse.csr_matrix(normalized_columns.T)\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = (a > 0).astype(int)", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = (a != 0).astype(int)", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "\n# Calculate the Euclidean distance between each data point and the centroids\ndistances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\n\n# Find the index of the minimum distance\nresult = np.argmin(distances, axis=0)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "closest_points = []\nfor i in range(centroids.shape[0]):\n    distance = scipy.spatial.distance.cdist(data, centroids[i], 'euclidean')\n    closest_point = data[np.argmin(distance)]\n    closest_points.append(closest_point)\nresult = np.array(closest_points)\n", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "\n# Calculate the Euclidean distance between each data point and the centroids\ndistances = scipy.spatial.distance.cdist(data, centroids)\n\n# Find the k-th smallest distance\nindices = np.argpartition(distances, k, axis=0)[:k, :]\n\n# Store the indices in the result\nresult = indices\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result = fsolve(eqn, x0=x, args=(a, b))\n    print('Root for x =', x, 'is', result[0])\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "result = []\nfor x, a in zip(xdata, adata):\n    def eqn_sub(b, x=x, a=a):\n        return x + 2*a - b**2\n    solution = fsolve(eqn_sub, x0=x)\n    result.append([solution, a, b])\nresult = np.array(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# first, we need to fit the data to the bekkers function\nparams, param_cov = sp.optimize.curve_fit(bekkers, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# now we can calculate the K-S test\nresult = sp.stats.kstest(sample_data, 'norm')\nresult\n", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# first, we need to fit the data to the bekkers function\nparams, param_cov = sp.optimize.curve_fit(bekkers, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# now we can use the fitted parameters to calculate the probability density function\nfitted_pdf = lambda x: bekkers(x, params[0], params[1], params[2])\n\n# now we can calculate the K-S test\nkstest_result = sp.stats.kstest(sample_data, fitted_pdf)\n\n# we can check if the p-value is less than 0.05 to reject the null hypothesis\nresult = kstest_result.pvalue < 0.05", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "\n# Convert the 'Time' column to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Set the 'Time' column as the index of the dataframe\ndf.set_index('Time', inplace=True)\n\n# Use the 'rolling' function to calculate the rolling integral\nintegral_df = df['A'].rolling(25).apply(integrate.trapz)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "f = scipy.interpolate.RegularGridInterpolator(x, y)\nresult = f(eval)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "weights = sciopt.fmin(sciopt.f2d(a.values.flatten(), 12, weights=[0.001, 0.1, 0.2, 0.12, 0.2, 0.12, 0.2, 0.12, 0.2, 0.12, 0.2, 0.12])[0], a.values.flatten())\n", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "\n# Use the 'convolve' function from 'scipy.signal' to find the \n# relative extrema.\n\n# First, find the extrema.\nextrema = signal.convolve(arr, np.ones(n), mode='same')\n\n# Then, find the indices of the extrema.\nresult = np.where(extrema == np.max(extrema))[0]\n\n# Return the result.\nprint(result)\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "\n# Use the scipy.signal.convolve function to convolve the array with a 1D array of weights\n# The weights are -1 and 1, which are the same as the ones used in the problem\nweights = np.array([-1, 1])\nconvolved = signal.convolve(arr, weights, mode='same')\n\n# Find the indices of the maximums\nmax_indices = np.where(convolved == np.max(convolved))\n\n# The result is a list of tuples, so we need to flatten it\nresult = max_indices[0].tolist() + max_indices[1].tolist()\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "\n# Select only the numeric columns\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\n\n# Calculate z-scores\nz_scores = np.abs(stats.zscore(df[num_cols]))\n\n# Create a mask where z-scores are less than 3\nmask = (z_scores < 3).all(axis=1)\n\n# Apply the mask to the dataframe\ndf = df.loc[mask]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "\n# Convert Bunch object to DataFrame\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], ['target']))\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n    return df\n    ### END SOLUTION ###\n\nprint(solve(data))\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# First, we need to extract the list of unique elements from the dataframe\nunique_elements = df['Col3'].apply(pd.Series).stack().unique().tolist()\n\n# Then, we create a dataframe with the unique elements as columns\ndf_out = pderof.DataFrame(index=df.index, columns=unique_elements)\n\n# We fill the dataframe with 0s and 1s\nfor i, row in df.iterrows():\n    for j, element in enumerate(row[2]):\n        df_out.loc[i, element] = 1\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# First, we need to flatten the list of fruits\ndf['fruits'] = df['Col3'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we need to one-hot-encode the list\ndf_out = pd.get_dummies(df['fruits'])\n\n# Now, we need to add the original dataframe to the one-hot-encoded dataframe\ndf_out = pder.concat([df.drop('fruits', axis=1), df_out], axis=1)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# First, we need to extract the list of unique elements from the last column\nunique_elements = df.iloc[:, -1].unique()\n\n# Then, we create a dataframe with the unique elements as columns\ndf_out = pderof.DataFrame(np.zeros((df.shape[0], len(unique_elements))))\ndf_out.columns = unique_elements\n\n# Now, we need to fill the dataframe with 1s and 0s\nfor index, row in df.iterrows():\n    for col in unique_elements:\n        if row[len(df.columns) - 1] == col:\n            df_out.loc[index, col] = 1\n        else:\n            df_out.loc[index, col] = 0\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# First, we need to flatten the list of fruits\ndf['Col3'] = df['Col3'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we need to one-hot-encode the list\ndf = pd.get_dummies(df['Col3'])\n\n# Finally, we need to add the new columns to the original dataframe\ndf = pderof.join(df, original_df)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# First, we need to flatten the list of fruits\ndf['Col3'] = df['Col3'].apply(lambda x: [item for item in x])\n\n# Then, we need to one-hot-encode the list\ndf_out = pd.get_dummies(df['Col3'])\n\n# Add the new columns to the original dataframe\ndf = pder.concat([df, df_out], axis=1)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "\nmodel.fit(X, y)\npredicted_test = model.predict(x_predict)\nproba = model.predict_proba(x_predict)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "\n# Convert the sparse matrix back to a DataFrame\ndf_transformed = pder.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Concatenate the original DataFrame with the transformed DataFrame\ndf = pd.concat([df_origin, df_transformed], axis=1)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "\n# Convert the sparse matrix to a DataFrame\ndf_sparse = pd.DataFrame(transform_output.toarray())\n\n# Concatenate the original DataFrame with the transformed DataFrame\ndf = pd.concat([df_origin, df_sparse], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # convert the sparse matrix back to a dataframe\n    df_back = pder.DataFrame.sparse.from_spmatrix(transform_output)\n    # concatenate the dataframe with the original dataframe\n    result = pd.concat([df, df_back], axis=1)\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.append(('add_step', LinearRegression()))\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\nclf.steps.append(('new_step', SomeNewStep()))\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps.append(('add_step', LinearRegression()))\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PCA()))\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\nmodel = xgb.XGBRegressor()\nb = []\nc = []\nfor train_index, test_index in gridsearch.cv:\n    X_train, X_test = trainX[train_index], trainX[test_index]\n    y_train, y_test = trainY[train_index], trainY[tester_index]\n    model.fit(X_train, y_train, fit_params)\n    predictions = model.predict(X_test)\n    b.append(model.score(X_test, y_test))\n    c.append(predictions)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nmodel = xgb.XGBRegressor()\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid)\ngridsearch.fit(trainX, trainY)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[tester_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[tester_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_transform(scaled)\n    return inverse_transform\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndata = load_data()\nscaler = StandardScaler()\nscaler.fit(data)\nscaled = scaler.transform(data)\ndef solve(data, scaler, scaled):\n    # return the solution in this function\n    inverse_transform = scaler.inverse_", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "\n# fit the model\nmodel.fit(X, y)\n\n# get the mean score\nmean_score = model.score(X, y)\n\n# get the name of the model\nmodel_name = model.__class__.__name__\n\nEND SOLUTION\n", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "\n# fit the model\nmodel.fit(X_train, y_train)\n\n# get the name of the model\nmodel_name = model.__class__.__name__\n\n# get the parameters\nparams = model.get_params()\n\n# create a dataframe with the parameters\ndf = pd.DataFrame(params)\n\n# get the mean score\nmean_score = model.score(X_test, y_test)\n\nEND SOLUTION\n<code>\n\nprint(f'Name model: {model_name}, Mean score: {mean_score}')\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {...}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "\npipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "\npipe.fit(data)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data)\n", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\nclf = GridSearchCV(bc, param_grid)\nclf.fit(X_train, y_train)\nproba = clf.predict_proba(X_test)", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\n\n# Predict\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\n\n# Predict\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\n\n# Define your preprocessor\ndef preprocess(s):\n    return s.upper()\n\n# Define your TfidfVectorizer\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\n\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# First, you need to convert your DataFrame to a numpy array\ndata_array = data.values\n\n# Then, apply the preprocessing.scale function to the numpy array\nscaled_data = preprocessing.scale(data_array)\n\n# Finally, convert the scaled numpy array back to a DataFrame\nscaled_data_df = pder.DataFrame(scaled_data, columns=data.columns)\n\nEND SOLUTION\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# First, you need to convert your DataFrame to a numpy array\ndata_array = data.to_numpy()\n\n# Then, apply the preprocessing.scale function to each column\nscaled_data = preprocessing.scale(data_array)\n\n# Finally, convert the numpy array back to a DataFrame\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\ndf_out = scaled_data_df\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = [X.columns[i] for i in np.argpartition(clf.feature_importances_, -5)[-5:]]\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# Get the column names\ncolumn_names = [X.columns[i] for i in model.get_support(indices=True)]\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "column_names = [X.columns[i] for i in np.argpartition(clf.feature_importances_, 1)[:10]]\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = [X.columns[i] for i in np.argsort(clf.feature_importances_)[-1:]]", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km = KMeans(n_clusters=p)\nkm.fit(X)\nclosest_50_samples = km.cluster_centers_[km.labels_ == km.cluster_centers_.argmin()][:50]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\ncenters = km.cluster_centers_\nclosest_50_samples = [centers[i] for i in np.argsort(np.linalg.norm(X - centers[i], axis=1))[:50]]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km = KMeans(n_clusters=p)\nkm.fit(X)\nclosest_100_samples = km.cluster_centers_[km.labels_ == km.predict(X)]\nclosest_100_samples = closest_100_samples[:100]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    labels = km.labels_\n    centroids = km.cluster_centers_\n    indices = np.argsort(labels)[:p]\n    return X[indices]\n    ### END SOLUTION ###\n\nget_samples(p, X, km)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\nmodel = SVR(kernel='rbf')\nmodel.fit(X, y)\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn.svm import SVR\nrbf_kernel = SVR(kernel='rbf', C=1000, gamma=0.1, degree=3, cache_size=200)\nrbf_kernel.fit(X, y)\npredict = rbf_kernel.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\nrbf_kernel = SVR(kernel='rbf', degree=2)\nrbf_kernel.fit(X, y)\npredict = rbf_kernel.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "\ntfidf_query = tfidf.transform(queries)\ncosine_similarities_of_queries = np.zeros((len(queries), len(documents)))\n\nfor i, query in enumerate(queries):\n    query_tfidf_vector = tfidf.transform([query])\n    cosine_similarities = cosine_similarity(query_tfidf_vector, tfidf_matrix)\n    cosine_similarities_of_queries[i] = cosine_similarities.flatten()\n\ncosine_similarities_of_queries", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "\ntfidf_query = tfidf.transform(queries)\ncosine_similarities_of_queries = np.zeros((len(queries), len(documents)))\n\nfor i, query in enumerate(queries):\n    query_tfidf_vector = tfidf.transform([query])\n    cosine_similarities = cosine_similarity(query_tfidf_vector, tfidf_matrix)\n    cosine_similarities_of_queries[i] = cosine_similarities.flatten()\n\ncosine_similarities_of_queries", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    cosine_similarities_of_queries = []\n    for query in queries:\n        query_tfidf = TfidfVectorizer().fit_transform([query])\n        query_tfidf_matrix = query_tfidf.toarray()\n        cosine_similarities = cosine_similarity(query_tfidf_matrix, tfidf_matrix)\n        cosine_similarities_of_queries.append(cosine_similarities)\n    return cosine_similarities_of_queries\n    ### END SOLUTION ###\n\nThe function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be used to calculate the cosine similarity between two matrices.\n\nNote: The function solve(queries, documents) should return a list of cosine similarity scores for each query to each document.\n\nThe cosine_similarity function from sklearn.metrics.pairwise can be", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# Convert the features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D-array\nnew_features = np.array(new_features)\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(f)\n\n# Convert the DataFrame to a 2D numpy array\nnew_f = df.values\n\n# Reshape the array\nnew_f = new-f.reshape(-1, 1)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# Convert the features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D-array\nnew_features = np.array(new_features)\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a DataFrame\n    df = pd.DataFrame(features)\n\n    # Convert the DataFrame to a numpy array\n    arr = df.to_numpy()\n\n    # Convert the numpy array to a 2D-array\n    new_features = np.array(arr).reshape(-1, 1)\n\n    return new_features\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\ndef solve(features):\n    # return the solution in this function\n    # new_features = solve(features)\n    ### BEGIN SOLUTION ###\n    # Convert the features to a DataFrame\n    df = pd.DataFrame(features)\n\n    # Convert the DataFrame to a numpy array\n    arr = df.to_numpy()\n\n    # Convert the numpy array to a 2D-array\n    new_features = np.array(arr).reshape(-1, 1)\n\n    return new_features\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\ndef solve(features):\n    # return the solution in this function\n    # new_features = solve(features)\n    ### BEGIN SOLUTION ###\n    # Convert the features to a DataFrame\n    df = pd.DataFrame(features)\n\n    # Convert the DataFrame to a numpy array\n    arr = df.to_numpy()\n\n    # Convert the numpy array to a 2D-array\n    new_features = np.array(arr).reshape(-1, 1)\n\n    return new_features\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\ndef solve(features):\n    # return the solution in this function\n    # new_features = solve(features)\n    ### BEGIN SOLUTION ###\n    # Convert the features to a DataFrame\n    df = pd.DataFrame(features)\n\n    # Convert the DataFrame to a numpy array\n    arr = df.to_numpy()\n\n    # Convert the numpy array to a 2D-array\n    new_features = np.array(arr).reshape(-1, 1)\n\n    return new_features\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\ndef solve(features):\n    # return the solution in this function\n    # new_features = solve(features)\n    ### BEGIN SOLUTION ###\n    # Convert the features to a DataFrame\n    df = pd.DataFrame(features)\n\n    # Convert the DataFrame to a numpy array\n    arr = df.to_numpy()\n\n    # Convert the numpy array to a 2D-array\n    new_features = np.array(arr).reshape(-1, 1)\n\n    return new_features\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\ndef solve(features):\n    # return the solution in this function\n    # new_features = solve(features)\n    ### BEGIN SOLUTION ###\n    # Convert the features to a DataFrame\n    df = pd.DataFrame(features)\n\n    # Convert the DataFrame to a numpy array\n    arr = df.to_numpy()\n\n    # Convert the numpy array to a 2D-array\n    new_features = np.array(arr).reshape(-1, 1)\n\n    return new_features\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfeatures = load_data()\ndef solve(features):\n    # return the solution in this function\n    # new_features =", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Convert the features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D-array\nnew_features = np.array(new_features)\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "\n# Convert the 2-D list to a numpy array\ndata_matrix = np.array(data_matrix)\n\n# Perform hierarchical clustering\ncluster = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage_tree=True)\ncluster_labels = cluster.fit_predict(data_matrix)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Create an instance of the AgglomerativeClustering class\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit and predict the clusters\nclustering.fit_predict(data_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "\n# Convert the 2-D list to a numpy array\nsimM = np.array(simM)\n\n# Perform hierarchical clustering\ncluster = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = cluster.fit_predict(simM)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "\ndist_matrix = np.array(data_matrix)\n\n# Flatten the matrix to 1-D array\ndist_matrix = dist_matrix.flatten()\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.fcluster(scipy.cluster.hierarchy.linkage(dist_matrix), 2, criterion='maxclust')\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "\ndist_matrix = pd.DataFrame(data_matrix)\ndist_matrix = dist_matrix.set(value=0)\ndist_matrix = dist_matrix.asymmetry_matrix()\ndist_matrix = dist_matrix.as_0d_array()\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.where(dist_matrix == 0, 1, dist_matrix)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.delete(dist-matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_matrix)\n\ndist_matrix = np.delete(dist_matrix, 0)\n\ndist_matrix = np.array(dist_", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "\ndist = simM\ndist_square_form = np.square(dist)\n\n# Perform hierarchical clustering\nlinkage = scipy.cluster.hierarchy.linkage(dist_square_form, method='average')\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage, 2, criterion='maxclust')\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n\nscaler = StandardScaler()\nrobust_scaler = RobustScaler()\nmin_max_scaler = MinMaxScaler()\npower_transformer = PowerTransformer()\n\nscaled_data = scaler.fit_transform(data)\nrobust_scaled_data = robust_scaler.fit_transform(data)\nmin_max_scaled_data = min_max_scaler.fit_transform(data)\npower_transformed_data = power_transformer.fit_transform(data)\n\ncentered_scaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from scipy.stats import boxcox\nbox_cox_data, result = boxcox(data)", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "\nfrom sklearn.preprocessing import YeoJohnson\n\nYeoJohnson_trans = YeoJohnson()\nYeoJohnson_trans.fit(data)\nYeoJohnson_trans.transform(data)", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnson\n\nYeoJohnson().fit_transform(data)", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "\nvectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\S+',\n                             stop_words=['!' , '?', '\"' , \"'\"])\ntransformed_text = vectorizer.fit_transform(text)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Assuming that your dataframe is called 'df'\ndf_train, df_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2, random_state=42)\n\nx_train = df_train.values\nx_test = df_test.values\ny_train = y_train.values\ny_test = y_test.values\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the data into features and target variable\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    from sklearn.model_selection import train_test_split\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\n", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "\n# Reshape your data to 2D array\nX = df['mse'].values.reshape(-1, 1)\n\n# Create KMeans object\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit and predict\nkmeans.fit(X)\nlabels = kmeans.predict(X)\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# Reshape your data to 2D array\nX = df['mse'].values.reshape(-1, 1)\n\n# Initialize KMeans\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit and predict\nkmeans.fit(X)\nlabels = kmeans.predict(X)\n\n# Get the centroids\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "clf = LinearSVC(C=1.0, penalty='l1', dual=False).fit(X, y)\ncoefs = clf.coef_[0]\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[coefs != 0]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "\n# Create a LinearSVC object\nlsvc = LinearSVC(C=1.0, penalty='l1', dual=False)\n\n# Train the model\nlsvc.fit(X, y)\n\n# Get the selected features\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(lsvc.coef_ != 0)]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    lsvc = LinearSVC(C=1.0, penalty='l1', dual=False)\n    lsvc.fit(X, y)\n    selected_features = lsvc.predict(X)\n    selected_features = np.asarray(vectorizer.get_feature_names())[selected_features]\n    return selected_features\n    ### END SOLUTION ###\n\nThis function will return the names of the selected features.\n\nThe function 'solve' is defined to solve the problem. It uses a LinearSVC model with L1 penalty. The 'fit' method is used to train the model, and the 'predict' method is used to predict the labels of the features. The 'predict' method returns a binary label (0 or 1) for each feature. The 'np.asarray' function is used to convert the binary labels to feature names.\n\nThe 'solve' function is called at the end of the script to get the names of the selected features.\n\nNote: The 'solve' function is not a part of the solution to the problem. It is a function that is used to solve the problem.\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = []\n\nfor col in df1.columns:\n    df2 = df1[['Time', col]].dropna()\n    X = df2['Time'].values.reshape(-1,1)\n    Y = df2[col].values.reshape(-1,1)\n    regression = LinearRegression()\n    regression.fit(X, Y)\n    slopes.append(regression.coef_[0][0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "\nslopes = []\nfor col in df1.columns:\n    X = df1[['Time', col]].values\n    Y = df1[col].values\n    model = LinearRegression().fit(X, Y)\n    m = model.coef_[0]\n    slopes.append(m)\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# First, initialize the LabelEncoder\nle = LabelEncoder()\n\n# Then, fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# 'transform' is a method not a function, so you need to call it on the fitted LabelEncoder\ntransformed_df = le.transform(df['Sex'])\n\nEND SOLUTION\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# First, initialize the LabelEncoder\nle = LabelEncoder()\n\n# Then, apply the LabelEncoder to the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n    ### END SOLUTION ###\n\ndf = Transform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "\nscaler = MinMaxScaler()\nnp_array_transformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "\nscaler = MinMaxScaler()\nnp_array_transformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    a_minmax = scaler.fit_transform(a)\n    return a_minmax\n    ### END SOLUTION ###\n\n\nThe function Transform(a) should return the transformed numpy array.\n\nThe MinMaxScaler is a feature-scale normalization method which transforms the data to a range (0, 1) or (0, -1) by rescaling the features.\n\nThe fit_transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit_transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit_transform method returns the transformed data.\n\nThe fit_transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit_transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit_transform method returns the transformed data.\n\nThe fit_transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit_transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit_transform method returns the transformed data.\n\nThe fit_transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit_transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit_transform method returns the transformed data.\n\nThe fit_transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit_transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit_transform method returns the transformed data.\n\nThe fit_transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit_transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit_transform method returns the transformed data.\n\nThe fit-transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit-transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit-transform method returns the transformed data.\n\nThe fit-transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit-transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit-transform method returns the transformed data.\n\nThe fit-transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit-transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit-transform method returns the transformed data.\n\nThe fit-transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit-transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit-transform method returns the transformed data.\n\nThe fit-transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit-transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit-transform method returns the transformed data.\n\nThe fit-transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit-transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is used to transform the data.\n\nThe fit-transform method returns the transformed data.\n\nThe fit-transform method is used to fit the scaler to the data and then transform the data.\n\nThe fit-transform method is a sequence of two steps: first, the scaler is fitted to the data, and then the scaler is", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "close_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\n\npredict = clf.predict(b)", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array(X)\nclf.fit(new_X[:, 0], new_X[:, 1])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array(X)\nclf.fit(new_X[:, 0].values.reshape(-1, 1), new_X[:, 1].values.reshape(-1, 1))\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# Split the data into independent and dependent variables\nX = dataframe.iloc[:, :-1].values\ny = dataframe.iloc[:, -1].values\n\n# Reshape the data\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Fit the model\nlogReg.fit(X_train, y_train)\n\n# Predict\ny_pred = logReg.predict(X_test)", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n\n# Predict\npredict = logReg.predict(X)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# First, sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Calculate the size of the test set\ntest_size = int(len(features_dataframe) * 0.2)\n\n# Split the dataframe into train and test\ntrain_dataframe, test_dataframe = features_dataframe[:-test_size], features_dataframe[-test_size:]\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# First, sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Calculate the size of the train set\ntrain_size = int(len(features_dataframe) * 0.8)\n\n# Split the dataframe into train and test\ntrain_dataframe, test_dataframe = features_dataframe[0:train_size], features_dataframe[train_size:len(features_dataframe)]\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    train_size = int(len(features_dataframe) * 0.2)\n    train_dataframe = features_dataframe[:train_size]\n    test_dataframe = features_dataframe[train_size:]\n    return train_dataframe, test_dataframe\n    ### END SOLUTION\n\ntrain_dataframe, test_dataframe = solve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2_scale', 'X3_scale']] = scaler.fit_transform(df[['X2', 'X3']])", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "\nmyData['new_A2'] = scaler.fit_transform(myData[['Month', 'A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['Month', 'A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\npickle_filename = 'sklearn_model.pkl'\nwith open(pickle_filename, 'wb') as file:\n    pickle.dump(fitted_model, file)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = cosine_similarity(tfidf_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\n\n# First, you need to clone the optimizer\noptimizer_clone = optim.clone()\n\n# Then, you need to set the learning rate\noptimizer_clone.param_groups[0]['lr'] = 0.001\n\n# Now, you need to replace the old optimizer with the new one\noptim = optimizer_clone\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n\n# First, you need to create a new optimizer with the desired learning rate\nnew_optim = torch.optim.SGD(optim.param_groups[0]['params'], lr=0.001)\n\n# Then, you need to replace the old optimizer with the new one\noptim = new_optim\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\n\n# First, you need to create a new optimizer with the desired learning rate\nnew_optim = torch.optim.SGD(optim.param_groups[0]['params'], lr=0.0005)\n\n# Then, you need to replace the old optimizer with the new one\noptim = new_optim\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n\n# To adjust the learning rate\nscheduler = torch.optim.lr_scheduler.CyclicalLearningRate(optim, max_lr=0.05)\n\nfor epoch in range(10):\n    optim.step()\n    scheduler.step()\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "\n# Convert gensim's Word2Vec model to PyTorch's Embedding layer\nvocab_size = len(word2vec.wv.key_to_index)\nembedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(word2vec.wv.vectors))\n\n# Use the weights from gensim to embed the input data\nembedded_input = torch.stack([torch.tensor(word2vec[word]).unsqueeze(0) for word in input_Tensor])\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": "\n    # Load the weights from gensim into PyTorch embedding layer\n    weights = torch.tensor(word2vec[word2vec.wv.vocab.keys()]).float()\n    # Create the embedding layer\n    embedding = torch.nn.Embedding.from_pretrained(weights)\n    # Embed the input data\n    embedded_input = embedding(input_Tensor)\n    return embedded_input\n    ### END SOLUTION\n\nget_embedded_input(input_Tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(np.array(x))", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(np.array(x))", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "C = torch.tensor(B[:, A_log.bool()].numpy())", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "C = torch.tensor(B[:, A_logical.numpy()]).long()", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "C = torch.tensor(B[:, A_log.bool()])", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "C = torch.tensor(B[:, A_log.bool()])", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as np\nimport pandas as pd\nimport torch\nA_log, B = load_data()\ndef solve(A_log, B):\n    # return the solution in this function\n    # C = solve(A_log, B)\n    ### BEGIN SOLUTION ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n\n\nA:\n\n<code>/\nimport numpy as", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "C = torch.tensor(B[:, A_log.bool()].numpy())", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "C = torch.index_select(B, 0, idx)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "\nx_tensor = torch.tensor(x_array.values, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "\nx_tensor = torch.tensor(x_array, dtype=torch.float64)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    return torch.tensor(a)\n    ### END SOLUTION ###\n\n# Test the function\nx = Convert(x_array)\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\nmask = torch.zeros(len(lens), max(lens)).long()\nmask[np.arange(len(lens)), lens-1] = 1\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\nmask = torch.zeros(len(lens), max(lens)).long()\nmask[np.arange(len(lens)), lens-1] = 1\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\nmask = torch.zeros(len(lens), max(lens)).long()\nmask[np.arange(len(lens)), lens-1] = 1\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    max_len = np.max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask.long()\n    ### END SOLUTION ###\n\nget_mask(lens)\n\n\nA:\n\n<code>\nimport torch\n\ndef get_mask(lens):\n    max_len = torch.max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask.long()\n\nlens = torch.tensor([3, 5, 4])\nprint(get_mask(lens))\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with the desired shape\nTensor_2D = torch.zeros(index_in_batch, diag_ele)\n\n# Fill the tensor with the desired values\nfor i in range(index_in_batch):\n    for j in range(diag_ele):\n        Tensor_2D[i][j] = drag_ele[i][j]\n\n# Create a 3D tensor with the desired shape\nTensor_3D = torch.zeros(index_in_batch, diag_ele, diag_ele)\n\n# Fill the tensor with the desired values\nfor i in range(index_in_batch):\n    for j in range(diag_ele):\n        for k in range(diag_ele):\n            Tensor_3D[i][j][k] = drag_ele[i][j]\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    result = torch.diag(t)\n    return result\n    ### END SOLUTION ###\n\nThe function Convert() is used to convert the 2D tensor into a 3D tensor. The 2D tensor is converted into a 1D tensor, and then into a 3D tensor.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with a diagonal pattern. The parameter t is the value to be used for the diagonal elements.\n\nThe function torch.diag() returns a 3D tensor with the specified values on the diagonal.\n\nThe function torch.diag() is used to create a 2D tensor with", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "ab = torch.stack([a, b], 0)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "\n# Convert Pytorch tensors to numpy\na = a.detach().cpu().numpy()\nb = b.detach().cpu().numpy()\n\n# Stack tensors along dimension 0\nab = np.stack([a, b], axis=0)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    ab = torch.stack([a, b], 0)\n    return ab\n    ### END SOLUTION ###\n\n\nA: You can use torch.cat() to concatenate tensors along a new dimension.\n\nHere is how you can do it:\n\n```python\nimport torch\n\n# Assuming a and b are your tensors\nab = torch.cat([a, b], 0)\n```\n\nThis will concatenate the tensors along the 0th dimension, resulting in a tensor of shape (3, 11).\n\nIf you want to stack tensors vertically, you can use torch.stack() instead:\n\n```python\nimport torch\n\n# Assuming a and b are your tensors\nab = torch.stack([a, b], 0)\n```\n\nThis will stack the tensors vertically along the 0th dimension, resulting in a tensor of shape (2, 3, 11).\n\nNote: The output tensor's size will be (2, 3, 11) if you use torch.stack(). If you use torch.cat(), the output size will be (3, 11) if you use torch.cat().\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "\nmask = torch.arange(1000)[None, :] < lengths[:, :, None]\na.masked_fill_(mask, 0)\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "\nmask = torch.arange(1000).expand(10, 1000).t(0).long()\nmask = mask < lengths.unsqueeze(1)\na = a.masked(mask, value=2333)\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "mask = torch.arange(1000).expand(10, 1000).t(0).long()\nmask = mask < lengths.unsqueeze(1)\na.masked_fill_(mask, 0)\n", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "\nmask = torch.arange(1000).expand(10, 1000).t(0).long()\nmask = mask < lengths.unsqueeze(1)\na = a.masked(mask, value=2333)\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.stack(lt)\n    ### END SOLUTION ###\n    return tensor_of_tensors\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = torch.index_select(t, 0, torch.tensor(idx))\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = torch.index_select(t, 0, torch.tensor(idx))\n", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = torch.index_select(t, 0, torch.tensor(idx))", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\nresult = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1,-1,2).squeeze(-1))\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\n# Reshape the tensor to a 2D tensor\nx = x.view(-1, x.size(-1))\n\n# Use torch.gather to gather the slices\nresult = torch.gather(x, 1, ids)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Convert the ids tensor to a numpy array\nids_np = ids.cpu().numpy()\n\n# Convert the x tensor to a numpy array\nx_np = x.cpu().numpy()\n\n# Find the maximum score for each row in x_np\nmax_scores = np.amax(x_np, axis=1)\n\n# Convert the max_scores to a tensor\nmax_scores_tensor = torch.tensor(max_scores).unsqueeze(1)\n\n# Subtract max_scores_tensor from x_np\nresult = x_np - max_scores_tensor\n\n# Convert the result back to a tensor\nresult = torch:-tensor(result)\n\n# Convert the result to a numpy array\nresult_np = result.cpu().numpy()\n\nEND SOLUTION\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1,1)", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1,1)", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1,1)", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    _, predicted = torch.max(softmax_output, 1)\n    return predicted\n    ### END SOLUTION ###\n\nThe torch.max function returns the largest value in the tensor along the specified dimension. The second return value is the index of the largest value.\n\nThe torch.max function is used to find the maximum value in the tensor along the specified dimension. The output is a tensor with the maximum value and the index of the maximum value.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in the softmax_output tensor.\n\nThe function torch.max(softmax_output, 1) will return the index of the maximum value in", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "\n# Find the index with the highest value\ny = torch.argmax(softmax_output, dim=1)\n\n# Convert the tensor to a numpy array\ny = y.cpu().numpy()\n\n# Convert the numpy array to a pandas series\ny = pd.Series(y.flatten())\n\n# Find the index of the minimum value\ny = y.idxmin()\n\n# Convert the pandas series back to a tensor\ny = torch.tensor(y)\n\nreturn y\nEND SOLUTION\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "\ndef cross_entropy2d(input, target):\n    n, c, h, w = input.size()\n    log_p = F.log_softmax(input, dim=1)\n    log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)\n    log_p = log_p[target >= 0, :]\n    log_p = log_p.view(-1, c)\n    target = target[target >= 0]\n    loss = F.nll_loss(log_p, target, size_average=False)\n    return loss\n\nimages = Variable(torch.randn(5, 3, 4, 4))\nlabels = Variable(torch.LongTensor(5, 4, 4).random_(3))\nloss = cross_entropy2d(images, labels)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = torch.sum(A == B)", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "\ncnt_equal = torch.sum(A == B)\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "\ncnt_not_equal = torch.sum(A != B)", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = torch.sum(A == B)\n    return cnt_equal\n    ### END SOLUTION ###\n\nA, B = load_data()\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "\ncnt_equal = torch.sum((A[-x:] == B[-x:]).float())\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "\ncnt_not_equal = torch.sum((A[-x:] != B[:x]).float())\n", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = torch.split(a, chunk_dim, dim=3)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = torch.split(a, chunk_dim, dim=2)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output = torch.masked.apply(output, mask)\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output = torch.masked.apply(output, mask)\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "\nx_abs = torch.abs(x)\ny_abs = torch.abs(y)\nmin_abs = torch.min(x_abs, y_abs)\n\nsign_x = torch.sign(x)\nsign_y = torch:-sign(y)\n\nsigned_min = torch.where(x < y, x, y) * sign_x\nsigned_min = torch.where(x < y, y, x) * sign_y\n\nsigned_min = torch.where(x < y, min_abs, x)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.where(max == torch.abs(x), x, y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    x_sign = torch.sign(x)\n    y_sign = torch.sign(y)\n    min_val = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = x_sign * min_val\n    return signed_min\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport torch\n\ndef solve(x, y):\n    x_sign = torch.sign(x)\n    y_sign = torch.sign(y)\n    min_val = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = x_sign * min_val\n    return signed_min\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    _, classes = torch.max(output.reshape(1, 3), 1)\n    confidence_score = output[classes.item()]\n    return confidence_score\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# First, you need to calculate the average of the last column of 'a' and the first column of 'b'.\naverage = torch.cat([a[:, -1:], b[:, :1]], dim=1).mean(dim=1)\n\n# Then, you need to concatenate the first and second columns of 'a' with the calculated average.\nresult = torch:-\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    result = torch.cat([a[:, :-1], b[:, 1:]], dim=1)\n    return result\n    ### END SOLUTION ###\n\n\nA:\n\n<code>\nimport torch\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\nresult = torch.cat([a[:, :-1], b[:, 1:]], dim=1)\n\nprint(result)\n\nOutput:\n\ntorch.Tensor([[1, 2, 4, 6, 7],\n              [1, 2, 4, 6, 7]])\n\nThis code will concatenate the two tensors along the second dimension (axis 1) and then take the average of the last two columns of 'a' and the first two columns of 'b' if they overlap.\n\n\nA: \n\n<code>\nimport torch\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\nresult = torch.cat([a[:, :-1], b[:, 1:]], dim=1)\n\nprint(result)\n\nOutput:\n\ntorch.Tensor([[1, 2, 4, 6, 7],\n              [1, 2, 4, 6, 7]])\n\nThis code will concatenate the two tensors along the second dimension (axis 1) and then take the average of the last two columns of 'a' and the first two columns of 'b' if they overlap.\n\n\nA:\n\n<code>\nimport torch\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\nresult = torch.cat([a[:, :-1], b[:, 1:]], dim=1)\n\nprint(result)\n\nOutput:\n\ntorch.Tensor([[1, 2, 4, 6, 7],\n              [1, 2, 4, 6, 7]])\n\nThis code will concatenate the two tensors along the second dimension (axis 1) and then take the average of the last two columns of 'a' and the first two columns of 'b' if they overlap.\n\n\nA:\n\n<code>\nimport torch\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\nresult = torch.cat([a[:, :-1], b[:, 1:]], dim=1)\n\nprint(result)\n\nOutput:\n\ntorch.Tensor([[1, 2, 4, 6, 7],\n              [1, 2, 4, 6, 7]])\n\nThis code will concatenate the two tensors along the second dimension (axis 1) and then take the average of the last two columns of 'a' and the first two columns of 'b' if they overlap.\n\n\nA:\n\n<code>\nimport torch\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\nresult = torch.cat([a[:, :-1], b[:, 1:]], dim=1)\n\nprint(result)\n\nOutput:\n\ntorch.Tensor([[1, 2, 4, 6, 7],\n              [1, 2, 4, 6, 7]])\n\nThis code will concatenate the two tensors along the second dimension (axis 1) and then take the average of the last two columns of 'a' and the first two columns of 'b' if they overlap", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "\nt = torch.tensor([[0., 0., 0.,0.]]).repeat(1, 2)\nresult = t.reshape(1,4,2)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "\nt = torch.tensor([[0., 0., 0.,0.]]).repeat(1,2,2)\nresult = t.unsqueeze(0).unsqueeze(0)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "\nt = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nresult = torch.cat([t, t], dim=0)\nresult = result.repeat(1, 2).view(-1, 2)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data.perm(0, 1, 2), W)\nresult = result.squeeze()\nresult = result.view(10, 2, 3)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
