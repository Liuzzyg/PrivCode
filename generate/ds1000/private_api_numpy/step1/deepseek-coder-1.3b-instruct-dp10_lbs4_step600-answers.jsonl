{"id": 0, "code": "result = df.iloc[List]", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.iloc[List]\nprint(result)\nprint(np.sum(result['Type'] != df['Type']))\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 2)\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[x])\n\n# Create a new column with the values that have at least two appearances\nnew_values = value_counts.apply(lambda x: x.index[", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x) >= 3)\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    for col in df.columns:\n        if col in ['Qu1', 'Qu2', 'Qu3']:\n            counts = df[col].value_counts()\n            for key, value in counts.items():\n                if value >= 2:\n                    df[col] = df[col].replace(key, 'other')\n    return df\n\nprint(f(example_df))\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the value counts of each column\nvalue_counts = df.apply(lambda x:", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "# Count the number of times each value appears in each column\ncounts = df.apply(lambda x: pd.value_counts(x))\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n\n# Create a new column with the counts\ncounts['count'] = counts.apply(lambda x: x.values.sum())\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "result = df.drop_duplicates(subset='url', keep='last')\nresult", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "def df_to_dict(df):\n    result = {}\n    for name, v1, v2 in zip(df['name'], df['v1'], df['v2']):\n        if name not in result:\n            result[name] = {}\n        result[name][v1] = {}\n        result[name][v1][v2] = df[df['name'] == name][v2].values[0]\n    return result\n\nresult = df_to_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone information\n    # Finally, use the replace() function to remove the timezone information\n    ### END SOLUTION ###\n    return df.apply(lambda x: x.replace('-06:00', ''))\n\nprint(f(example_df))\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "# Convert the message column to a list of dictionaries\nmessage_list = [json.loads(i) for i in df['message']]\n\n# Create a new dataframe from the list of dictionaries\nnew_df = pd.DataFrame(message_list)\n\n# Create a new dataframe with the same columns as the original dataframe\nresult = pd.DataFrame(columns=df.columns)\n\n# Iterate over the original dataframe and append the values to the new dataframe\nfor index, row in df.iterrows():\n    result = result.append(row)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "for product in products:\n    df.loc[df['product'] == product, 'score'] *= 10\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "for product in products:\n    df.loc[df['product'] != product, 'score'] *= 10", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "for product in products:\n    df.loc[df['product'].isin(product), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "for product in products:\n    df.loc[df['product'] == product, 'score'] = (df.loc[df['product'] == product, 'score'] - df.loc[df['product'] == product, 'score'].min()) / (df.loc[df['product'] == product, 'score'].max() - df.loc[df['product'] == product, 'score'].min())", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.apply(lambda row: 'A' if row['A'] == 1 and row['B'] == 0 and row['C'] == 0 and row['D'] == 0 else 'B' if row['A'] == 0 and row['B'] == 1 and row['C'] == 0 and row['D'] == 0 else 'C' if row['A'] == 0 and row['B'] == 0 and row['C'] == 1 and row['D'] == 0 else 'D', axis=1)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = df.apply(lambda row: 'A' if row['A'] == 0 and row['B'] == 1 and row['C'] == 1 and row['D'] == 1 else 'B' if row['A'] == 1 and row['B'] == 0 and row['C'] == 1 and row['D'] == 1 else 'C' if row['A'] == 1 and row['B'] == 1 and row['C'] == 0 and row['D'] == 1 else 'D', axis=1)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "df['category'] = df.apply(lambda row: list(row.index[row == 1]), axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.to_period('M')\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%b-%Y'))\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.to_period('M')\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-01')\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill(2))\ndf['Date'] = df['Date'].apply(lambda x: str(x.year) + '-' + x.month.str.zfill(2) + '-' + str(x.day).zfill", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.columns = ['#1', '#2']\nshifted_df.iloc[0] = df.iloc[0]\nshifted_df.iloc[-1] = df.iloc[-1]\nprint(shifted_df)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.columns = ['#1', '#2']\nshifted_df.iloc[0] = df.iloc[0]\nshifted_df.iloc[-1] = df.iloc[-1]\nprint(shifted_df)\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "shifted_df = df.shift(1, axis=0)\nshifted_df.iloc[0] = df.iloc[0]\nshifted_df.iloc[-1] = df.iloc[-1]\nprint(shifted_df)\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "# shift the first row of the first column down 1 row\ndf['#1'] = df['#1'].shift(1)\n\n# shift the last row of the first column to the first row, first column\ndf['#2'] = df['#2'].shift(1)\n\n# remove the first row of the first column\ndf = df[1:]\n\n# remove the last row of the first column\ndf = df[:-1]\n\n# calculate R^2 values\nr2 = df['#1'].values[:-1]**2 + df['#2'].values[:-1]**2\n\n# find the index of the minimum R^2 value\nmin_r2_index = np.argmin(r2)\n\n# get the row with the minimum R^2 value\nmin_r2_row = df.iloc[min_r2_index]\n\n# print the row with the minimum R^2 value\nprint(min_r2_row)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "for col in df.columns:\n    df.rename(columns={col: col + 'X'}, inplace=True)", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "for col in df.columns:\n    df[col] = df[col] + 'X'\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "# Create a dictionary with the new column names\nnew_cols = {'HeaderAX': 'HeaderA', 'HeaderBX': 'HeaderB', 'HeaderCX': 'HeaderC', 'HeaderX': 'HeaderX'}\n\n# Use the dictionary to rename the columns\ndf = df.rename(columns=new_cols)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "# Get the column names that contain 'val'\nval_columns = [col for col in df.columns if 'val' in col]\n\n# Get the grouped mean for each of the value columns\nresult = df.groupby('group').agg({col: 'mean' for col in val_columns})\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "def sum_other_columns(df):\n    other_columns = [col for col in df.columns if col not in ['group', 'group_color']]\n    return df[other_columns].sum()\n\nresult = sum_other_columns(df)\nprint(result)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "def aggregate_values(df):\n    # Create a dictionary of aggregation functions\n    aggregation_functions = {\"val1\": \"sum\", \"val2\": \"mean\", \"val42\": \"mean\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {\"val3\": \"sum\", \"val4\": \"sum\"}\n\n    # Create a dictionary of aggregation functions for the other value columns\n   ", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "def meanAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].mean(axis=axis)\n\nresult = meanAdvance(row_list, column_list, axis=0)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "def sumAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list, axis=0)\nprint(result)\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "def sumAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list, axis=0)\nresult = result.sort_values(ascending=False).drop_duplicates()\nresult", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.value_counts()\nresult = result.sort_values(ascending=False)\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "result = df.value_counts()\nresult", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0,1]]\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0,1]]\nresult.columns = df.columns\nresult", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "result = df.apply(lambda x: np.where(x.notnull(), x, np.nan), axis=1)\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "result = df.apply(lambda x: np.where(x.isnull(), x.values.tolist(), x.values.tolist()), axis=1)\nresult", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "result = df.where(df.isnull(), np.nan)\nresult", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "# Find rows with value less than the threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# Sum the values of these rows\nsum_values = rows_less_than_thresh['value'].sum()\n# Create a new row with the sum of the values\nnew_row = pd.Series({'value': sum_values})\n# Replace the rows with value less than the threshold with the new row\ndf = df.replace(rows_less_than_thresh, new_row)\nresult = df\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "# Find rows with value less than the threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# Find rows with value greater than the threshold\nrows_greater_than_thresh = df[df['value'] >= thresh]\n# Calculate the average value of the rows greater than the threshold\navg_value = rows_greater_than_thresh['value'].mean()\n# Create a new row with the average value\nnew_row = pd.DataFrame({'value': [avg_value]}, index=[thresh])\n# Combine the rows with value less than the threshold with the new row\nresult = pd.concat([rows_less_than_thresh, new_row])\n# Reset the index\nresult = result.reset_index(drop=True)\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "# find the rows in the section\nrows_in_section = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\n# calculate the average value of the rows in the section\naverage_value = rows_in_section['value'].mean()\n# replace the rows in the section with the average value\nresult = df.replace(rows_in_section, average_value)\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "result = df.copy()\nfor col in df.columns:\n    result[f'inv_{col}'] = df[col].map(lambda x: 1/x)\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "result = df.assign(**{f'exp_{col}': lambda x: x.apply(lambda x: x**(1/x)) for col in df.columns})", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "result = df.copy()\nresult[\"inv_\" + df.columns.values.tolist()[0]] = 1 / df[df.columns.values.tolist()[0]]\nresult[\"inv_\" + df.columns.values.tolist()[1]] = 1 / df[df.columns.values.tolist()[1]]\nresult[\"inv_\" + df.columns.values.tolist()[2]] = 1 / df[df.columns.values.tolist()[2]]\nresult", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = df[col].apply(sigmoid)\n\nresult = df", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "# Get the index location of each respective column minimum\nmin_idx = df.idxmin()\n\n# Get the last occurrence of the column-wise maximum\nmax_idx = df.apply(lambda x: np.argmax(x), axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get the last occurrence of the column-wise maximum\nlast_max_idx = np.max(last_max_idx, axis=1) + min_idx.values\n\n# Get", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "# Get the index location of each respective column minimum\nmin_idx = df.idxmin()\n\n# Get the index location of the first occurrence of the column-wise maximum\nmax_idx = df.apply(lambda x: np.argmax(x), axis=1) + min_idx\n\n# Get the index location of the first occurrence of the column-wise maximum\nresult = max_idx\nresult", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "# Find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Fill the new data frame with 0 for the val column\nnew_df['val'] = new_df['dt'].apply(lambda x: 0 if x not in df['dt'].values else df[df['dt'] == x]['val'].values[0])\n\n# Join the new data frame with the original data frame\nresult = pd.merge(new_df, df, on='dt')\nresult", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "# Find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Fill the new data frame with 0 for the val column\nnew_df['val'] = new_df['dt'].apply(lambda x: 0)\n\n# Join the original data frame with the new data frame\nresult = pd.merge(new_df, df, on='dt')\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "# Find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Fill the new data frame with 233 for the val column\nnew_df['val'] = 233\n\n# Join the original data frame with the new data frame\nresult = pd.merge(new_df, df, on='dt')\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum date to the maximum date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Find the maximum val for each user\nmax_val = df.groupby('user')['val'].max()\n\n# Join the new data frame with the maximum val for each user\nresult = new_df.join(max_val.reset_index())\nresult", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "# Find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from min_date to max_date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Join the new data frame with the original data frame on the 'user' column\nresult = new_df.merge(df, on='user')\n\n# Fill the maximum val for each user\nresult['val'] = result.groupby('user')['val'].transform('max')\n\n# Convert the date column to the correct format\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "# Create a unique ID for each name\nunique_id = df['name'].apply(lambda x: x + 1)\n\n# Create a new column 'id' with unique IDs\ndf['id'] = unique_id\n\n# Replace the 'name' column with the 'id' column\ndf['name'] = df['id']\n\n# Drop the 'id' column\ndf = df.drop(['id'], axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "# Create a unique ID for each value in column 'a'\nunique_id = df['a'].drop_duplicates().reset_index(drop=True)\n\n# Create a new column 'a' with unique IDs\ndf['a'] = unique_id[df['a'] - 1]\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: x + 1)\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "# Create a unique ID for each name\nunique_id = df['name'].apply(lambda x: x + '_' + str(df['a'].values[df['name'] == x][0]))\n\n# Create a new column 'ID' with unique ID\ndf['ID'] = unique_id\n\n# Replace 'a' with unique ID\ndf['a'] = df['ID'] + '_' + df['a'].astype(str)\n\n# Drop the 'ID' column\ndf = df.drop(['ID'], axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.pivot_table(index='user', columns='date', values='value', fill_value=0)\ndf = df.reset_index()\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'someBool': 'date'})\ndf = df.rename(columns={'someBool': '", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value')\ndf = df.rename(columns=lambda x: x.replace('/', ''))\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'user'})\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.pivot_table(index='user', columns='date', values='value', fill_value=None)\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'date', 'value': 'value'})\ndf = df.sort_values('date')\n", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df.c > 0.5][columns].to_numpy()", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df['c'] > 0.45][columns].values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    # Use the following line to convert the result to a numpy array\n    result = df[df.c > 0.5][columns].to_numpy()\n    ### END SOLUTION\n    return result\n\n# Create a DataFrame\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# Call the function\nresult = f(df, columns=['b', 'e'])\nprint(result)\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    ### END SOLUTION\n    return df[df[columns[0]] > 0.5][columns]\n\n# create a dataframe\ndf = pd.DataFrame({'a': [0.945686, 0.919359, 0.976163, 0.459039],\n                   'b': [0.000710, 0.667057, 0.621712, 0.788318],\n                   'c': [0.909158, 0.462478, 0.208423, 0.309892],\n                   'd': [0.892892, 0.008204, 0.980471, 0.100539],\n                   'e': [0.326670, 0.473096, 0.048334, 0.753992]})\n\n# call the function\nresult = f(df, ['b', 'e'])\nprint(result)\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df[df[columns[0]] > 0.5][columns]\n    ### END SOLUTION\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint f(df, ['b', 'e'])\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndef find_overlap(df, X):\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].apply(lambda x: x.date())\n    df['date'] = df['date'].apply(lambda x: x + pd.DateOffset(days=X))\n    df['date'] = df['date'].apply(lambda x: x.strftime('%m/%d/%Y'))\n    df = df.sort_values('date')\n    df = df.drop_duplicates('date')\n    return df\n\nresult = find_overlap(df, X)\nprint(result)\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef find_overlap(df, X):\n    dates = df['date'].values\n    dates_sort = np.sort(dates)\n    dates_sort_unique = np.unique(dates_sort)\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique <= X]\n    return dates_sort_unique\n\nresult = find_overlap(df, X)\nprint(result)\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef find_overlapping_dates(df, X):\n    dates = df['date'].values\n    dates_sort = np.sort(dates)\n    dates_sort_unique = np.unique(dates_sort)\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_sort]\n    dates_sort_unique = dates_sort_unique[dates_sort_unique != dates_", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "result = df.groupby(df.index // 3).mean()\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "result = df.groupby(df.index // 3).mean()\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n# Create a new column 'col1_bin'\ndf['col1_bin'] = df['col1'].groupby(df['col1'].astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).astype(str).", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "result = df.iloc[::-1].reset_index(drop=True).assign(col1=lambda x: x.iloc[::-1].mean())\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column for the sum of the first 3 rows\ndf['col1_sum'] = df['col1'].iloc[0:3].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['col1_avg'] = df['col1'].iloc[0:2].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['col1_sum_last'] = df['col1'].iloc[-3:].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['col1_avg_last'] = df['col1'].iloc[-2:].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['col1_sum_first'] = df['col1'].iloc[0:3].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['col1_avg_first'] = df['col1'].iloc[0:2].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['col1_sum_last'] = df['col1'].iloc[-3:].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['col1_avg_last'] = df['col1'].iloc[-2:].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['col1_sum_first'] = df['col1'].iloc[0:3].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['col1_avg_first'] = df['col1'].iloc[0:2].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['col1_sum_last'] = df['col1'].iloc[-3:].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['col1_avg_last'] = df['col1'].iloc[-2:].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['col1_sum_first'] = df['col1'].iloc[0:3].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['col1_avg_first'] = df['col1'].iloc[0:2].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['col1_sum_last'] = df['col1'].iloc[-3:].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['col1_avg_last'] = df['col1'].iloc[-2:].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['col1_sum_first'] = df['col1'].iloc[0:3].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['col1_avg_first'] = df['col1'].iloc[0:2].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['col1_sum_last'] = df['col1'].iloc[-3:].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['col1_avg_last'] = df['col1'].iloc[-2:].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['col1_sum_first'] = df['col1'].iloc[0:3].sum()\n\n# Create a new column for the average of the first 2 rows\ndf['col1_avg_first'] = df['col1'].iloc[0:2].mean()\n\n# Create a new column for the sum of the last 3 rows\ndf['col1_sum_last'] = df['col1'].iloc[-3:].sum()\n\n# Create a new column for the average of the last 2 rows\ndf['col1_avg_last'] = df['col1'].iloc[-2:].mean()\n\n# Create a new column for the sum of the first 3 rows\ndf['col1_sum_first'] = df['col1'].iloc[0:3].sum()\n\n# Create", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Get the last 3 rows\nlast_3_rows = df[-3:]\n\n# Get the sum of last 3 rows\nlast_3_sum = last_3_rows['col1'].sum()\n\n# Get the average of last 3 rows\nlast_3_avg = last_3_rows['col1'].mean()\n\n# Get the first 2 rows\nfirst_2_rows = df[:2]\n\n# Get the sum of first 2 rows\nfirst_2_sum = first_2_rows['col1'].sum()\n\n# Get the average of first 2 rows\nfirst_2_avg = first_2_rows['col1'].mean()\n\n# Create a new dataframe with the sums and averages\nnew_df = pd.DataFrame({'col1':[last_3_sum, first_2_sum]})\n\n# Add the average of first 2 rows to the new dataframe\nnew_df['col1'] += first_2_avg\n\n# Get the last 3 rows\nlast_3_rows = df[-3:]\n\n# Get the sum of last 3 rows\nlast_3_sum = last_3_rows['col1'].sum()\n\n# Get the average of last 3 rows\nlast_3_avg = last_3_rows['col1'].mean()\n\n# Create a new dataframe with the sums and averages\nnew_df_2 = pd.DataFrame({'col1':[last_3_sum, last_3_avg]})\n\n# Add the average of last 3 rows to the new dataframe\nnew_df_2['col1'] += last_3_avg\n\n# Get the first 2 rows\nfirst_2_rows = df[:2]\n\n# Get the sum of first 2 rows\nfirst_2_sum = first_2_rows['col1'].sum()\n\n# Get the average of first 2 rows\nfirst_2_avg = first_2_rows['col1'].mean()\n\n# Create a new dataframe with the sums and averages\nnew_df_3 = pd.DataFrame({'col1':[first_2_sum, first_2_avg]})\n\n# Add the average of first 2 rows to the new dataframe\nnew_df_3['col1'] += first_2_avg\n\n# Get the last 3 rows\nlast_3_rows = df[-3:]\n\n# Get the sum of last 3 rows\nlast_3_sum = last_3_rows['col1'].sum()\n\n# Get the average of last 3 rows\nlast_3_avg = last_3_rows['col1'].mean()\n\n# Create a new dataframe with the sums and averages\nnew_df_4 = pd.DataFrame({'col1':[last_3_sum, last_3_avg]})\n\n# Add the average of last 3 rows to the new dataframe\nnew_df_4['col1'] += last_3_avg\n\n# Get the first 2 rows\nfirst_2_rows = df[:2]\n\n# Get the sum of first 2 rows\nfirst_2_sum = first_2_rows['col1'].sum()\n\n# Get the average of first 2 rows\nfirst_2_avg = first_2_rows['col1'].mean()\n\n# Create a new dataframe with the sums and averages\nnew_df_5 = pd.DataFrame({'col1':[first_2_sum, first_2_avg]})\n\n# Add the average of first 2 rows to the new dataframe\nnew_df_5['col1'] += first_2_avg\n\n# Get the last 3 rows\nlast_3_rows = df[-3:]\n\n# Get the sum of last 3 rows\nlast_3_sum = last_3_rows['col1'].sum()\n\n# Get the average of last 3 rows\nlast_3_avg = last_3_rows['col1'].mean()\n\n# Create a new dataframe with the sums and averages\nnew_df_6 = pd.DataFrame({'col1':[last_3_sum, last_3_avg]})\n\n# Add the average of last 3 rows to the new dataframe\nnew_df_6['col1'] += last_3_avg\n\n#", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(df.loc[0, 'A'])\ndf = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(df.loc[0, 'A']).fillna(df.loc[-1, 'A'])\ndf = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['duration'] = df['duration'].str.split(' ', 1).str[0]\ndf['time'] = df['duration'].str.split(' ', 1).str[1]\ndf['number'] = df['duration'].str.split(' ', 1).str[0]\ndf['time_days'] = df['duration'].str.split(' ', 1).str[1]\ndf['time_days'] = df['time_days'].apply(lambda x: x.replace('year', 365).replace('day', 1).replace('week', 7).replace('month', 30))\ndf", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['time_day'] = df['time'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|day)', '(365|30|7|1)')\ndf['time_day'] = df['time_day'].replace('(year|month|week|", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    # replace the duration with the number and time\n    df['duration'] = df.duration.replace(r'\\d.*', r'\\d', regex=True, inplace=True)\n    df['time'] = df.duration.replace(r'\\.w.+', r'\\w.+', regex=True, inplace=True)\n    # create a new column time_days\n    df['time_days'] = df.duration.replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['duration'] = df['duration'].str.split(' ', 1).str[0]\ndf['time'] = df['duration'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['number'] = df['number'].astype(int)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number']\nprint(df)\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.where(np.all(np.array([df1[column] == df2[column] for column in columns_check_list]), axis=0))", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = np.all(np.where((df1[columns_check_list] == df2[columns_check_list]), 1, 0), axis=1)", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Create a new column with the date, x and y values\n    df['date_x_y'] = df['date'].apply(lambda x: [x, df['x'][x], df['y'][x]])\n    # Return the new column\n    return df['date_x_y']\n\n# Test the function\ndf = pd.DataFrame({'x': [100, 90, 80], 'y': [7, 8, 9], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    # parse date index using pd.to_datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # swap the two levels\n    df = df.set_index(['id', 'date'])\n    # return the solution in this function\n    return df\n    ### END SOLUTION ###\n\n# test the function\ndf = pd.DataFrame({'x': ['abc'], 'y': [100], 'date': ['3/1/1994']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df_melt = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf_melt = df_melt.pivot_table(index=['Country', 'year'], columns='Variable', values='value')\ndf_melt = df_melt.reset_index()\ndf_melt.columns = ['Country', 'year', 'var1', 'var2']\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df_melt = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf_melt = df_melt.sort_values(['Country', 'year'], ascending=[True, False])\ndf_melt = df_melt.pivot_table(index=['Country'], columns=['Variable'], values='value', fill_value=0)\ndf_melt = df_melt.reset_index()\ndf_melt = df_melt.rename(columns={'Country': 'Variable'})\ndf_melt = df_melt.sort_values(['Variable', 'year'], ascending=[True, False])\ndf_melt = df_melt.reset_index(drop=True)\ndf_melt", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\n# Create a new column with absolute values of all columns\nabs_values = df.abs()\n\n# Create a new column with all columns with absolute values less than 1\nless_than_one = abs_values < 1\n\n# Filter the data frame with the condition\nfiltered_data = df[less_than_one]\n\n# Print the filtered data\nprint(filtered_data)\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\n# Create a list of column names\ncolumn_names = ['Value_B', 'Value_C', 'Value_D']\n\n# Create a new data frame with only the columns with absolute value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = df[new_df]", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a new column with the absolute values of all columns\nabs_values = df.abs()\n\n# Create a new column with the column names\ncolumn_names = df.columns.to_list()\n\n# Create a new column with the column names that have absolute values more than 1\nfiltered_column_names = [column for column in column_names if abs(abs_values[column]) > 1]\n\n# Create a new data frame with the filtered columns\nfiltered_df = df[filtered_column_names]\n\n# Print the filtered data frame\nprint(filtered_df)\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df = df.replace('&AMP;', '&')", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df = df.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    # replace &AMP; with '&'\n    df = df.replace('&AMP;', '&')\n    return df\n\nf(example_df)\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df = df.replace({'&AMP;': '', '&LT;': '', '&GT;': ''}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df = df.replace('&AMP;', '&')", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: None if x is None else x.split(' ')[1])\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x if validate_single_space_name(x) is None else x.split(' ')[1])\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['name'] = df['name'].apply(validate_single_space_name)\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge_asof(df2, df1, on='Timestamp', direction='forward')\nresult['data'] = df1['data'].values[result['Timestamp'].values]\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1, df2, on='Timestamp', direction='forward')\nresult['data'] = df2['stuff']\nresult['stuff'] = df2['stuff']\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= 50, df['col1'], np.max(df[['col1', 'col2', 'col3']]))", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = np.where(df['col2'] > 50, df['col1'], np.sum(df[['col1', 'col2', 'col3']]))\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndef find_errors(df):\n    errors = []\n    for index, row in df.iterrows():\n        if not pd.isnumeric(row[\"Field1\"]):\n            errors.append(row[\"Field1\"])\n    return errors\n\nerrors = find_errors(df)\nprint(errors)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndef get_integer_values(df):\n    return [x for x in df['Field1'].values if isinstance(x, int)]\n\nresult = get_integer_values(df)\nprint(result)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    # iterate over each row in the dataframe\n    for index, row in df.iterrows():\n        # check if the value is not an integer\n        if not isinstance(row['Field1'], int):\n            # append the value to the list\n            errors.append(row['Field1'])\n    ### END SOLUTION ###\n\nf(example_df)\nprint(errors)\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test, axis=0)", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "# Create a new column 'distance' to store the euclidean distance between each car and its nearest neighbour\ndf['distance'] = np.sqrt(((df['x'] - df['x'].shift(1))**2 + (df['y'] - df['y'].shift(1))**2))\n\n# Group by 'time' and 'car', then find the nearest neighbour for each car\nnearest_neighbour = df.groupby('time')['car'].apply(lambda x: x[x == x.min()])\n\n# Calculate the average of the distances for each time point\naverage_distance = df.groupby('time')['distance'].mean()\n\n# Create a new dataframe with the nearest neighbour and average distance\ndf2 = pd.DataFrame({'car': df['car'], 'nearest_neighbour': nearest_neighbour, 'average_distance': average_distance})\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "# Create a new column 'car_neighbour' that contains the nearest car for each car\ndf['car_neighbour'] = df.groupby('car')['car'].transform('min')\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('car')['car'].transform('max')\n\n# Create a new column 'euclidean_distance' that contains the euclidean distance between each car and its nearest neighbour\ndf['euclidean_distance'] = df.groupby('car')['x'].apply(lambda x: ((x - df['x'].values[df['car_neighbour'] == car])**2).sum())\n\n# Calculate the average of the euclidean distance for each time point\ndf['euclidean_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby('time')['farmost_neighbour'].mean()\n\n# Create a new column 'farmost_neighbour' that contains the farthest car for each car\ndf['farmost_neighbour'] = df.groupby", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf['keywords_all'] = df[cols].apply(lambda x: ', '.join(x.dropna()), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf['keywords_all'] = df[cols].apply(lambda x: '-'.join(x.dropna()), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf['keywords_all'] = df[cols].apply(lambda x: '-'.join(x.dropna()), axis=1)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n\n# Create a new column 'keywords_all'\ndf['keywords_all'] = df[cols].apply(lambda x: \"-\".join(x.dropna()), axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "# Randomly select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Set Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = range(len(sample_df))\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "# Randomly select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change value of ProductId to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = range(len(sample_df))\n\n# Print the result\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "# Create a new column 'Random' with a random number between 0 and 1\ndf['Random'] = np.random.rand(len(df))\n\n# Select the rows where Random is less than 0.20\nselected_rows = df[df['Random'] < 0.20]\n\n# Set the Quantity column to zero\ndf.loc[selected_rows.index, 'Quantity'] = 0\n\n# Keep the indexes of the selected rows\nselected_rows_index = selected_rows.index\n\n# Print the resulting DataFrame\nprint(df)\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    # find the first duplicate row\n    first_duplicate = df[df.duplicated(subset=['col1','col2'], keep='first')]\n    # add a new column to the dataframe with the index of the first duplicate\n    first_duplicate['index_original'] = first_duplicate.index\n    return first_duplicate\n\n# call the function\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "group = df.groupby(['Sp', 'Mt'])\nmin_count = group['count'].min()\nresult = group[group['count'] == min_count]", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in filter_list\")\nprint(result)\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "result = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                  ('A', 'B', 'F'),\n                                  ('A', 'C', 'G'),\n                                  ('A', 'C', 'H'),\n                                  ('A', 'D', 'I'),\n                                  ('A', 'D', 'J')])\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "# Define the value_vars\nvalue_vars = [(col, col, col) for col in df.columns]\n# Use pd.melt to melt the data frame\nresult = pd.melt(df, value_vars=value_vars, var_names=['variable_0', 'variable_1', 'variable_2'])\nresult.columns = ['variable_0', 'variable_1', 'variable_2', 'value']\nresult", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['cumsum'] < 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x, skipna=False))\nresult['v'] = np.where(result['v'].isnull(), np.nan, result['v'])\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x, skipna=False))\nresult['v'] = np.where(result['v'].isnull(), np.nan, result['v'])\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def get_relationship(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-to-one'\n    elif column1.min() == column2.max():\n        return 'one-to-many'\n    elif column1.max() == column2.max():\n        return 'many-to-one'\n    elif column1.min() == column2.min():\n        return 'many-to-many'\n    else:\n        return 'none'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {get_relationship(column1, column2)}')\n\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def get_relationship(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-2-many'\n    elif column1.min() == column2.max():\n        return 'one-2-one'\n    elif column1.max() == column2.max():\n        return 'many-2-many'\n    else:\n        return 'many-2-one'\n\nresult = []\nfor column1, column2 in zip(df.columns[:-1], df.columns[1:]):\n    result.append(f'{column1} {column2} {get_relationship(column1, column2)}')\n\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def get_relationship(df):\n    result = pd.DataFrame(index=df.columns, columns=df.columns)\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 == col2:\n                result.loc[col1, col2] = 'one-to-one'\n            else:\n                if col1 in df[col2].values:\n                    result.loc[col1, col2] = 'one-to-many'\n                else:\n                    result.loc[col1, col2] = 'many-to-one'\n    return result\n\nresult = get_relationship(df)\nprint(result)\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def get_relationship(df):\n    column1_to_column2 = df['Column1'].apply(lambda x: 'one-2-many' if x == 1 else 'many-2-one')\n    column2_to_column3 = df['Column2'].apply(lambda x: 'one-2-many' if x == 4 else 'many-2-one')\n    column3_to_column4 = df['Column3'].apply(lambda x: 'one-2-many' if x == 7 else 'many-2-one')\n    column4_to_column5 = df['Column4'].apply(lambda x: 'one-2-many' if x == 9 else 'many-2-one')\n    return pd.DataFrame({\n        'Column1': column1_to_column2,\n        'Column2': column2_to_column3,\n        'Column3': column3_to_column4,\n        'Column4': column4_to_column5,\n        'Column5': column5_to_column5\n    })\n\nresult = get_relationship(df)\nprint(result)\n", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\nresult = dfiban_uniq\nprint(result)\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World!\")\n\n# write a hello world in python\nprint(\"Hello, World", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "group_has_family = df[df['SibSp'] > 0]\ngroup_no_family = df[df['SibSp'] == 0]\n\ngroup_has_family_mean = group_has_family['Survived'].mean()\ngroup_no_family_mean = group_no_family['Survived'].mean()\n\nresult = pd.DataFrame({'Has Family': [group_has_family_mean],\n                       'No Family': [group_no_family_mean]})\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "group_names = ['Has Family', 'No Family']\ngroup_means = df.groupby(group_names).mean()\nresult = group_means\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "group_names = ['Has Family', 'New Family', 'No Family', 'Old Family']\ngroup_values = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n                (df['SibSp'] == 0) & (df['Parch'] == 0),\n                (df['SibSp'] == 0) & (df['Parch'] == 1),\n                (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.astype(bool) for x in group_values]\ngroup_values = [x.astype(bool) for x in group_values]\n\ngroup_values = [x.ast", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\nprint(df)", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\nprint(df)", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "def stdMeann(x):\n    return np.std(np.mean(x))\n\nresult = pd.Series(df.groupby('a').b.apply(stdMeann))\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "def stdMeann(x):\n    return np.std(np.mean(x))\n\nresult = pd.Series(df.groupby('b').a.apply(stdMeann))\nprint(result)\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x, axis=0)\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = df['b'].apply(softmax)\ndf['min-max'] = df['b'].apply(min_max)\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "# Remove rows and columns with only zeros\ndf = df[df.sum(axis=1) != 0]\ndf = df[df.sum(axis=0) != 0]\nresult = df", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "# Remove rows and columns with sum of 0\nresult = df[df.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "max_value = df.max().max()\nresult = df[df[max_value] != max_value]", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "max_value = df.max().max()\ndf[df == max_value] = 0\nresult = df", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=[True, False])", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "s = s.sort_values()\ndf = s.reset_index(drop=True)\ndf = df.sort_values(by=0)\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str) == 's']", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "group = df.groupby(['Sp', 'Mt'])\nmin_count = group['count'].min()\nresult = group[group['count'] == min_count]", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": " ###\n    # Create a new column in the DataFrame with the value from the dict\n    df['Date'] = df['Member'].map(dict)\n    # Fill NaN with the value from the dict\n    df['Date'] = df['Date'].fillna(dict)\n    ### END SOLUTION ###\n    return df\n\nresult = f(example_dict, example_df)\nprint(result)\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(np.nan)\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Year', 'Month']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Year', 'Month']).Date.transform('count')\ndf['Count_y'] = df.groupby('Year').Date.transform('count')\nprint(df)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val']).Date.transform('count')\nprint(df)\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([(df['Date'].dt.year, df['Date'].dt.month)).size()\ndf['Count_y'] = df.groupby([(df['Date'].dt.year, df['Date'].dt.month, df['Date'].dt.day)).size()\ndf['Count_w'] = df.groupby([(df['Date'].dt.year, df['Date'].dt.month, df['Date'].dt.day, df['Date'].dt.weekday())]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date')['B'].apply(lambda x: x.sum())\nresult2 = df.groupby('Date')['C'].apply(lambda x: x.sum())\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "result1 = df[df['B'] % 2 == 0].groupby('Date').sum()\nresult2 = df[df['B'] % 2 != 0].groupby('Date').sum()\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = np.sum(result['D'])\nresult['E'] = np.mean(result['E'])\nresult", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['mean'] = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = np.sum(result['D'])\nresult['E'] = np.mean(result['E'])\nresult", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult['D'] = np.max(result['D'])\nresult['E'] = np.min(result['E'])\nresult", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "import dask.dataframe as dd\n\n# Convert the dataframe to a dask dataframe\ndask_df = dd.from_pandas(df, npartitions=1)\n\n# Split the column 'var1' into multiple rows\ndask_df = dask_df.assign(var1=dask_df['var1'].str.split(','))\n\n# Flatten the dask dataframe\nresult = dask_df.flatten()\n\n# Convert the result back to a pandas dataframe\nresult = result.to_pandas()\n\nprint(result)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "import dask.dataframe as dd\n\n# Convert the dataframe to a dask dataframe\ndask_df = dd.from_pandas(df, npartitions=1)\n\n# Split the column 'var2' into multiple rows\ndask_df = dask_df.assign(var2=dask_df['var2'].str.split(','))\n\n# Convert the dask dataframe back to a pandas dataframe\nresult = dask_df.to_pandas()\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\n# Convert the dataframe to a dask dataframe\ndask_df = dd.from_pandas(df, npartitions=1)\n\n# Split the column into multiple rows\nresult = dask_df.assign(var2=dask_df['var2'].str.split('-'))\n\n# Convert the result back to a pandas dataframe\nresult = result.to_pandas()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if string[i].isalpha():\n            continue\n        else:\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df['fips'] = df['row'].str.split(' ', 1).str[0]\ndf['row'] = df['row'].str.split(' ', 1).str[1]\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df['fips'] = df['row'].str.split(' ', 1).str[0]\ndf['row'] = df['row'].str.split(' ', 1).str[1]\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df['fips'] = df['row'].str.split(' ', 1).str[0]\ndf['medi'] = df['row'].str.split(' ', 1).str[1]\ndf['row'] = df['row'].str.split(' ', 1).str[2]\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "df = df.set_index('Name')\ndf = df.replace(0, np.nan)\ndf = df.fillna(df.mean())\ndf = df.cumsum()\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\n# calculate the cumulative average for each row from end to head\nfor col in df.columns[1:]:\n    df[col] = df[col].rolling(window_size=1).mean()\n\n# ignore if the value is zero\ndf = df.where(df[1:] == 0, 0)\n\n# calculate the cumulative average for each row from end to head\nfor col in df.columns[1:]:\n    df[col] = df[col].rolling(window_size=1).mean()\n\n# ignore if the value is zero\ndf = df.where(df[1:] == 0, 0)\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    # calculate the average for each row, ignore zero values\n    result = df.apply(lambda x: x.mean(), axis=1)\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "# calculate the cumulative average for each row from end to head\nfor col in df.columns[1:]:\n    df[col] = df[col].rolling(window_size=1).mean()\n\n# ignore if the value is zero\ndf = df.where(df[1:] == 0, 0)\n\n# calculate the cumulative average for each row from end to head\nfor col in df.columns[1:]:\n    df[col] = df[col].rolling(window_size=1).mean()\n\n# ignore if the value is zero\ndf = df.where(df[1:] == 0, 0)\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)) > 0\ndf['label'] = df['label'].apply(lambda x: 1 if x else 0)\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\n# Create a new column 'label'\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n\n# Set label for the first row to 1\ndf.loc[0, 'label'] = 1\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: x[0] if x[0] > 0 else 0 if x[0] == 0 else 1)\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_datetime(df.departure_time) - pd.to_datetime(df.arrival_time)\ndf['Duration'] = df.Duration.apply(lambda x: str(x).split(' ')[0])\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = (df['departure_time'].astype('datetime64[ns]') - df['arrival_time'].astype('datetime64[ns]')) / 1000\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "# Convert the arrival and departure times to datetime\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n\n# Calculate the duration in seconds\ndf['Duration'] = (df['departure_time'] - df['arrival_time']).apply(lambda x: x.total_seconds())\n\n# Convert the duration to seconds\ndf['Duration'] = df['Duration'].apply(lambda x: x.total_seconds())\n\n# Convert the arrival and departure times to the desired format\ndf['arrival_time'] = df['arrival_time'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\ndf['departure_time'] = df['departure_time'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size()", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size()", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df.groupby('key1')['key2'].apply(lambda x: x.endswith('e')).value_counts()\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.max(axis=0)\nmin_result = df.min(axis=0)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df['value'].mode()\nmedian_result = df['value'].median()\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(99 <= df['closing_price']) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df[~(99 <= df['closing_price'] <= 101)]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "min_diff = df.groupby(\"item\")[\"diff\"].min()\nmin_otherstuff = df.groupby(\"item\")[\"otherstuff\"].min()\nresult = df[df[\"diff\"] == min_diff]\nresult = result[result[\"otherstuff\"] == min_otherstuff]\n", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[1]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[1]\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "# calculate the number of NaN values\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# calculate the number of NaN values to fill\nnan_to_fill = nan_count // 2\n\n# fill the first nan_to_fill values with 0 and the last nan_to_fill values with 1\ndf['Column_x'] = np.where(np.isnan(df['Column_x']), 0, 1)\n\n# fill the rest of the NaN values with the mode of the column\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0])\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "# calculate the number of NaN values\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# calculate the number of NaN values to fill\nnan_to_fill = nan_count * 0.3\n\n# calculate the number of NaN values to fill in the first 30%\nnan_to_fill_first_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the middle 30%\nnan_to_fill_middle_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill * 0.3\n\n# calculate the number of NaN values to fill in the last 30%\nnan_to_fill", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0)\nnum_one = np.sum(df['Column_x'] == 1)\n\n# calculate the number of 0's and 1's in the column\nnum_zero = np.sum(df['Column_x'] == 0", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "a_b = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0])], columns=['one', 'two'])\nresult = a_b", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0], c.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0], c.iloc[1,0])], columns=['one', 'two'])\nresult", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "# Create a new DataFrame with the same shape as a and b\na_b = pd.DataFrame(np.zeros((a.shape[0], a.shape[1] + b.shape[1])), columns=a.columns + b.columns)\n\n# Iterate over the rows of a and b\nfor i in range(a.shape[0]):\n    # Iterate over the columns of a and b\n    for j in range(a.shape[1]):\n        # Assign the value from a and b to the corresponding position\n        a_b.iloc[i, j] = (a.iloc[i, j], b.iloc[i, j])\n\n# Fill the remaining positions with np.nan\na_b.iloc[:, a.shape[1]:] = np.nan\n\n# Print the result\nprint(a_b)", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins, labels=[1, 1, 1, 1]))\nresult = groups.value_counts()\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins, labels=False))\nresult = groups.value_counts()\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins, labels=False))\nresult = groups.value_counts()\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', '))).to_frame('text')\n", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame('text')\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df['text'].to_list()\nresult = ', '.join(result)\nresult = result.replace(' ', ',')\nresult = result.replace('.', ',')\nresult = result.replace(',', ',')\nresult = result.replace(', ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult = result.replace(',, ', ',')\nresult =", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].to_series().str.split(', ', 1).apply(lambda x: x[0])\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].values.reshape(-1, 1).tolist()[0]\nresult = \"-\".join(result)\nprint(result)\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.concat([df1, df2], axis=0)\nresult['city'] = result['city'].fillna(result['district'])\nresult['district'] = result['district'].fillna(result['city'])\nresult['date'] = result['date'].fillna(result['district'])\nresult['value'] = result['value'].fillna(result['date'])\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "# merge df1 and df2 on 'id'\nmerged = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with the last date\nmerged['date'] = merged['date'].fillna(merged['date'].dropna().max())\n\n# fill NaN with the last value\nmerged['value'] = merged['value'].fillna(merged['value'].dropna().max())\n\n# fill NaN with the last city and district\nmerged['city'] = merged['city'].fillna(merged['city'].dropna().max())\nmerged['district'] = merged['district'].fillna(merged['district'].dropna().max())\n\n# sort by date and id\nmerged = merged.sort_values(['date', 'id'])\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# fill NaN with the last value\nmerged['value'] = merged['value'].fillna(merged['value'].dropna().max())\n\n# fill NaN with the last city and district\nmerged['city'] = merged['city'].fillna(merged['city'].dropna().max())\nmerged['district'] = merged['district'].fillna(merged['district'].dropna().max())\n\n# sort by date and id\nmerged = merged.sort_values(['date', 'id'])\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# fill NaN with the last value\nmerged['value'] = merged['value'].fillna(merged['value'].dropna().max())\n\n# fill NaN with the last city and district\nmerged['city'] = merged['city'].fillna(merged['city'].dropna().max())\nmerged['district'] = merged['district'].fillna(merged['district'].dropna().max())\n\n# sort by date and id\nmerged = merged.sort_values(['date', 'id'])\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# fill NaN with the last value\nmerged['value'] = merged['value'].fillna(merged['value'].dropna().max())\n\n# fill NaN with the last city and district\nmerged['city'] = merged['city'].fillna(merged['city'].dropna().max())\nmerged['district'] = merged['district'].fillna(merged['district'].dropna().max())\n\n# sort by date and id\nmerged = merged.sort_values(['date', 'id'])\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# fill NaN with the last value\nmerged['value'] = merged['value'].fillna(merged['value'].dropna().max())\n\n# fill NaN with the last city and district\nmerged['city'] = merged['city'].fillna(merged['city'].dropna().max())\nmerged['district'] = merged['district'].fillna(merged['district'].dropna().max())\n\n# sort by date and id\nmerged = merged.sort_values(['date', 'id'])\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# fill NaN with the last value\nmerged['value'] = merged['value'].fillna(merged['value'].dropna().max())\n\n# fill NaN with the last city and district\nmerged['city'] = merged['city'].fillna(merged['city'].dropna().max())\nmerged['district'] = merged['district'].fillna(merged['district'].dropna().max())\n\n# sort by date and id\nmerged = merged.sort_values(['date', 'id'])\n\n# reset index\nmerged = merged.reset_index(drop=True)\n\n# fill NaN with the last value\nmerged['value'] = merged['value'].fillna(merged['value'].dropna().max())\n\n# fill NaN with the last city and district\nmerged['city'] = merged['city'].fillna(merged['city'].dropna().max())\nmerged['district'] = merged['district'].fillna(merged['district'].dropna().max())\n\n# sort by date and id\nmerged = merged.sort_values(['date', 'id'])", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "# merge df1 and df2 on 'id'\nmerged = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# group by 'id' and 'date' and fill NaN with NaN\nmerged = merged.groupby(['id', 'date']).first().fillna(np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged = merged.fillna(value=np.nan)\n\n# sort by 'id' and 'date'\nmerged = merged.sort_values(['id', 'date'])", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['B'] = result['B_y']\nresult['B_x'] = result['B_x']\nresult = result[['A', 'B']]\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['B'] = result['B_y']\nresult['B_x'] = result['B_x']\nresult = result[['A', 'B']]\nresult", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "# Create a new column 'dulplicated' and fill it with False\nC['dulplicated'] = False\n\n# Merge C and D on the 'A' column\nmerged = pd.merge(C, D, on='A', how='outer')\n\n# Replace the values in the 'B_y' column with the values from 'B_x' where 'A' is present in the same row in C\nmerged['B_y'] = merged['B_y'].where(merged['A'].isin(C['A']), merged['B_x'])\n\n# Replace the 'B_x' column with the 'B_y' column\nmerged['B_x'] = merged['B_y']\n\n# Drop the 'B_y' column\nmerged = merged.drop(columns='B_y')\n\n# Rename the 'B_x' column to 'B'\nmerged = merged.rename(columns={'B_x': 'B'})\n\n# Set the 'dulplicated' column to True where 'A' is present in C\nmerged['dulplicated'] = merged['A'].isin(C['A'])\n\n# Print the result\nprint(merged)\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist())\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).reset_index(drop=True)\nresult['amount-time-tuple'] = result['amount'].apply(lambda x: tuple(x))\nresult['amount-time-tuple'] = result['amount-time-tuple'].apply(lambda x: sorted(x, key=lambda y: y[1]))\nresult['amount-time-tuple'] = result['amount-time-tuple'].apply(lambda x: list(x))\nresult", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).reset_index(drop=True)\nresult = result.sort_values(['time', 'amount'], ascending=[True, True])\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "df_concatenated = series.apply(lambda x: pd.DataFrame(x.reshape(-1, 1), index=series.index, columns=[f'file{i}' for i in range(1, len(series) + 1)]))\ndf = df_concatenated.reset_index(drop=True)\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "df_concatenated = pd.DataFrame(series.values.tolist(), index=series.index, columns=series.index)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.columns[df.columns.str.contains(s)].to_string(index=False)\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "# Find the column names that contain the string 'spike'\ncolumns = df.columns[df.columns.str.contains(s)]\n\n# Create a new dataframe with the column names\nnew_df = pd.DataFrame(columns, columns=[s])\n\n# Rename the column names\nnew_df.columns = [f'{s}{i}' for i in range(1, len(new_df.columns) + 1)]\n\n# Print the new dataframe\nprint(new_df)\n\n# Access the column with the new dataframe\nresult = new_df[s]\nprint(result)\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "result = df['codes'].apply(lambda x: pd.Series(x, index=range(len(x))))\nresult = result.fillna(np.nan)\nresult", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "result = df['codes'].apply(lambda x: pd.Series(x, index=range(1, len(x) + 1)))\nresult = result.fillna(np.nan)\nresult", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "# Convert the list of codes to a list of lists\ncodes = df['codes'].to_list()\n\n# Create a new dataframe with the codes as columns\nnew_df = pd.DataFrame(codes, columns=[f'code_{i}' for i in range(1, len(codes) + 1)])\n\n# Fill the NaN values with NaN\nnew_df = new_df.fillna(np.nan)\n\n# Sort the dataframe by the first column\nnew_df = new_df.sort_values(by=[new_df.columns[0]])\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "result = df['col1'].apply(lambda x: x[0])\nresult = result.tolist()\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x[::-1]))).values.tolist()\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x)))\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "# Convert the time to minutes\ndf['Time'] = (df['Time'].astype('datetime64[m]') - df['Time'].min()) / 60\n# Group by minute and average the values\ndf = df.groupby(df['Time'].astype('int'))['Value'].mean().reset_index()\n# Convert the time back to datetime\ndf['Time'] = df['Time'].apply(lambda x: pd.to_datetime(str(x)) + pd.to_timedelta(x, unit='m'))\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate(method='linear')\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype('datetime64[m]') - df['Time'].min()) / 60\n# Group by minute and average the values\ndf = df.groupby(df['Time'].astype('int'))['Value'].mean().reset_index()\n# Convert the time back to datetime\ndf['Time'] = df['Time'].apply(lambda x: pd.to_datetime(str(x)) + pd.to_timedelta(x, unit='m'))\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate(method='linear')\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype('datetime64[m]') - df['Time'].min()) / 60\n# Group by minute and average the values\ndf = df.groupby(df['Time'].astype('int'))['Value'].mean().reset_index()\n# Convert the time back to datetime\ndf['Time'] = df['Time'].apply(lambda x: pd.to_datetime(str(x)) + pd.to_timedelta(x, unit='m'))\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate(method='linear')\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype('datetime64[m]') - df['Time'].min()) / 60\n# Group by minute and average the values\ndf = df.groupby(df['Time'].astype('int'))['Value'].mean().reset_index()\n# Convert the time back to datetime\ndf['Time'] = df['Time'].apply(lambda x: pd.to_datetime(str(x)) + pd.to_timedelta(x, unit='m'))\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate(method='linear')\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype('datetime64[m]') - df['Time'].min()) / 60\n# Group by minute and average the values\ndf = df.groupby(df['Time'].astype('int'))['Value'].mean().reset_index()\n# Convert the time back to datetime\ndf['Time'] = df['Time'].apply(lambda x: pd.to_datetime(str(x)) + pd.to_timedelta(x, unit='m'))\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate(method='linear')\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype('datetime64[m]') - df['Time'].min()) / 60\n# Group by minute and average the values\ndf = df.groupby(df['Time'].astype('int'))['Value'].mean().reset_index()\n# Convert the time back to datetime\ndf['Time'] = df['Time'].apply(lambda x: pd.to_datetime(str(x)) + pd.to_timedelta(x, unit='m'))\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate(method='linear')\n# Convert the time to minutes\ndf['Time'] = (df['Time'].astype('datetime64[m]') - df['Time'].min()) / 60\n# Group by minute and average the values\ndf = df.groupby(df['Time'].astype('int'))['Value'].mean().reset_index()\n# Convert the time back to datetime\ndf['Time'] = df['Time'].apply(lambda x: pd.to_datetime(str(x)) + pd.to_timedelta(x", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "# Convert the time to minutes\ndf['Time'] = (df['Time'].astype('datetime64[m]') - df['Time'].min()) / 60\n# Group by time and value\ngrouped = df.groupby('Time').mean()\n# Create a new column with the binning of the time\ngrouped['Time'] = grouped['Time'].apply(lambda x: x.floor())\n# Sum the values in each bin\ngrouped = grouped.groupby('Time').sum()\n# Interpolate the values\ngrouped = grouped.interpolate(method='linear')\n# Print the result\nprint(grouped)\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[np.isnan(df.iloc[0]).values]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.loc[0, 'c2'] == df.loc[8, 'c2']", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = [col for col in df.columns if not np.all(np.isnan(df[col]))]", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(np.nan, np.nan) if np.isnan(df.iloc[0, i]) and np.isnan(df.iloc[8, i]) else (df.iloc[0, i], df.iloc[8, i]) for i in range(10)]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().stack().", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.assign(A_0=lambda x: x['A'][0],\n               B_0=lambda x: x['B'][0],\n               C_0=lambda x: x['C'][0],\n               D_0=lambda x: x['D'][0],\n               E_0=lambda x: x['E'][0],\n               A_1=lambda x: x['A'][1],\n               B_1=lambda x: x['B'][1],\n               C_1=lambda x: x['C'][1],\n               D_1=lambda x: x['D'][1],\n               E_1=lambda x: x['E'][1],\n               A_2=lambda x: x['A'][2],\n               B_2=lambda x: x['B'][2],\n               C_2=lambda x: x['C'][2],\n               D_2=lambda x: x['D'][2],\n               E_2=lambda x: x['E'][2])\n\ndf = df.assign(A_0=lambda x: x['A'][0],\n               B_0=lambda x: x['B'][0],\n               C_0=lambda x: x['C'][0],\n               D_0=lambda x: x['D'][0],\n               E_0=lambda x: x['E'][0],\n               A_1=lambda x: x['A'][1],\n               B_1=lambda x: x['B'][1],\n               C_1=lambda x: x['C'][1],\n               D_1=lambda x: x['D'][1],\n               E_1=lambda x: x['E'][1],\n               A_2=lambda x: x['A'][2],\n               B_2=lambda x: x['B'][2],\n               C_2=lambda x: x['C'][2],\n               D_2=lambda x: x['D'][2],\n               E_2=lambda x: x['E'][2])\n\ndf = df.assign(A_0=lambda x: x['A'][0],\n               B_0=lambda x: x['B'][0],\n               C_0=lambda x: x['C'][0],\n               D_0=lambda x: x['D'][0],\n               E_0=lambda x: x['E'][0],\n               A_1=lambda x: x['A'][1],\n               B_1=lambda x: x['B'][1],\n               C_1=lambda x: x['C'][1],\n               D_1=lambda x: x['D'][1],\n               E_1=lambda x: x['E'][1],\n               A_2=lambda x: x['A'][2],\n               B_2=lambda x: x['B'][2],\n               C_2=lambda x: x['C'][2],\n               D_2=lambda x: x['D'][2],\n               E_2=lambda x: x['E'][2])\n\ndf = df.assign(A_0=lambda x: x['A'][0],\n               B_0=lambda x: x['B'][0],\n               C_0=lambda x: x['C'][0],\n               D_0=lambda x: x['D'][0],\n               E_0=lambda x: x['E'][0],\n               A_1=lambda x: x['A'][1],\n               B_1=lambda x: x['B'][1],\n               C_1=lambda x: x['C'][1],\n               D_1=lambda x: x['D'][1],\n               E_1=lambda x: x['E'][1],\n               A_2=lambda x: x['A'][2],\n               B_2=lambda x: x['B'][2],\n               C_2=lambda x: x['C'][2],\n               D_2=lambda x: x['D'][2],\n               E_2=lambda x: x['E'][2])\n\ndf = df.assign(A_0=lambda x: x['A'][0],\n               B_0=lambda x: x['B'][0],\n               C_", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "def avg(columns):\n    return np.mean(df[columns].values, axis=1)\n\ndf['Avg'] = avg(list_of_my_columns)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "for column in list_of_my_columns:\n    df[column] = df[column].mean()\n\n# Get Min, Max and Median\nfor column in list_of_my_columns:\n    df[column+'_Min'] = df[column].min()\n    df[column+'_Max'] = df[column].max()\n    df[column+'_Median'] = df[column].median()\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by='time')", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(['time', 'VIM'], ascending=[True, True])", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "# Create a new column 'Date' with the date only\ndf['Date'] = df['Date'].dt.date\n\n# Create a new column 'Date_only' with the date only\ndf['Date_only'] = df['Date'].dt.date\n\n# Delete the dates '2020-02-17' and '2020-02-18'\ndf = df[df['Date_only'] != '2020-02-17']\ndf = df[df['Date_only'] != '2020-02-18']\n\n# Set the index back to 'Date'\ndf.set_index('Date', inplace=True)\nresult = df", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Create a new column 'Day' to get the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Day_of_week' to get the day of the week\ndf['Day_of_week'] = df['Date'].dt.dayofweek\n\n# Create a new column 'Date_day' to get the date part\ndf['Date_day'] = df['Date'].dt.date\n\n# Create a new column 'Date_day_of_week' to get the date part of the day of the week\ndf['Date_day_of_week'] = df['Date'].dt.dayofweek\n\n# Create a new column 'Date_day_of_week_name' to get the date part of the day of the week\ndf['Date_day_of_week_name'] = df['Date'].dt.day_name()\n\n# Create a new column 'Date_day_of_week_name_day' to get the date part of the day of the week\ndf['Date_day_of_week_name_day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Date_day_of_week_name_day_of_week' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week'] = df['Date'].dt.dayofweek\n\n# Create a new column 'Date_day_of_week_name_day_of_week_name' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week_name'] = df['Date'].dt.day_name()\n\n# Create a new column 'Date_day_of_week_name_day_of_week_name_day' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week_name_day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Date_day_of_week_name_day_of_week_name_day_of_week' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week_name_day_of_week'] = df['Date'].dt.dayofweek\n\n# Create a new column 'Date_day_of_week_name_day_of_week_name_day_of_week_name' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week_name_day_of_week_name'] = df['Date'].dt.day_name()\n\n# Create a new column 'Date_day_of_week_name_day_of_week_name_day_of_week_name_day' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week_name_day_of_week_name_day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Date_day_of_week_name_day_of_week_name_day_of_week_name_day_of_week' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week_name_day_of_week_name_day_of_week'] = df['Date'].dt.dayofweek\n\n# Create a new column 'Date_day_of_week_name_day_of_week_name_day_of_week_name_day_of_week_name' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week_name_day_of_week_name_day_of_week_name'] = df['Date'].dt.day_name()\n\n# Create a new column 'Date_day_of_week_name_day_of_week_name_day_of_week_name_day_of_week_name_day' to get the date part of the day of the week\ndf['Date_day_of_week_name_day_of_week_name_day_of_week_name_day_of_week_name_day'] = df['Date'].dt.day_name()\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr[np.abs(corr) > 0.3]", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr.where(corr > 0.3, 0)", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "last_column = df.columns[-1]\ndf.rename(columns={last_column: 'Test'}, inplace=True)", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "result = df.rename(columns={df.columns[0]: 'Test'})", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "frequent = df.apply(lambda x: x.value_counts().index[0])\nfreq_count = df.apply(lambda x: x.value_counts().values[0])\n\ndf['frequent'] = frequent\ndf['freq_count'] = freq_count\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "frequent = df.apply(lambda x: x.value_counts().index[0])\nfreq_count = df.apply(lambda x: x.value_counts().values[0])\n\ndf['frequent'] = frequent\ndf['freq_count'] = freq_count\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "# Create a new column 'frequent'\ndf['frequent'] = df.apply(lambda row: list(row.value_counts().index[row.value_counts().values == row.value_counts().values.max()]), axis=1)\n\n# Create a new column 'freq_count'\ndf['freq_count'] = df.apply(lambda row: len(row.value_counts()), axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "res = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nres", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "res = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, on='EntityNum', left_on_in_a='EntityNum', right_on_in_b='EntityNum')\nresult = result[['EntityNum', 'foo', 'a_col']]\nresult", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', left_on_in_a='a_col', right_on_in_b='b_col')\nresult = result[['EntityNum', 'foo', 'b_col']]\nresult", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = np.nan_to_num(x, nan=np.nan)\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = np.array([[1400, 1500, 1600], [1800, 1700]])\nresult = result.tolist()", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((len(a), 4))\nb[0, 0] = 1\nb[1, 1] = 1\nb[2, 3] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 1] = 1\nb[0, 0] = 1\nb[1, 1] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1\nb[2, 2] = 1\nb[0, 1] = 1\nb[1, 0] = 1\nb[2, 0] = 1\nb[0, 2] = 1\nb[1, 3] = 1\nb[2, 3] = 1\nb[0, 3] = 1\nb[1, 2] = 1", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((len(a), 4))\nb[np.arange(len(a)), a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((len(a), len(a)))\nfor i in range(len(a)):\n    b[i, a[i]] = 1\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)\nb = np.where(a > 0, 1, 0)\nb = np.where(a == 0, 1, 0)\nb = np.where(a < 0, 1, 0)", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i, j] = a[i, j]\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.array(A).reshape(nrow, -1)\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "def vec2matrix(A, ncol):\n    return np.array(A).reshape(-1, ncol)\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    return np.array(A).reshape(-1, ncol)\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "def shift(a, shift):\n    return np.roll(a, shift, axis=1)\n\nresult = shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=1)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "max_value = np.max(a)\nmax_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nresult", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "min_index = np.unravel_index(np.argmin(a, axis=None), a.shape, order='F')\nresult = np.ravel_index(min_index, a.shape, order='F')\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a), a.shape)\n    return max_value, max_index\n    ### END SOLUTION ###\n\nresult = f(example_a)\nprint(result)\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "# find the maximum value in the array\nmax_value = np.max(a)\n# find the position of the maximum value\nmax_position = np.unravel_index(np.argmax(a), a.shape)\nresult = max_position\nprint(result)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "z = np.any(np.isnan(a), axis=0)\na = np.delete(a, z, axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = np.delete(a, np.where(np.isnan(a)))", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.zeros((3,3))\nresult[0,0] = a[0][0]\nresult[0,1] = a[0][1]\nresult[0,2] = a[0][2]\nresult[1,0] = a[1][0]\nresult[1,1] = a[1][1]\nresult[1,2] = a[1][2]\nresult[2,0] = a[2][0]\nresult[2,1] = a[2][1]\nresult[2,2] = a[2][2]\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.take(a, permutation, axis=1)", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.take(a, permutation, axis=0)", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravel_index(np.argmin(a), a.shape)\nresult = min_index\nprint(result)", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = (max_index[0], max_index[1])\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "min_index = np.unravel_index(np.argmin(a, axis=0), a.shape)\nresult = np.array([[0, 1], [1, 0]])\nresult", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.degrees(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.degrees(degree))", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * 180 / np.pi\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)\nprint(a)\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(a, 2))\nprint(f(a, 3))\nprint(f(a, 4))\nprint(f(a, 5))\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (int(result[0]), int(result[1]))\nprint(result)", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    # Use numpy's floor_divide function to divide the numerator by the denominator\n    # and return the result as a tuple\n    return np.floor_divide(numerator, denominator)\n\n# Test the function\nprint(f(98, 42))  # Output: (7, 3)\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = (numerator / denominator, denominator / numerator)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = np.array([a, b, c]).mean(axis=0)", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(a, np.maximum(b, c))", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.shape[0]) - np.arange(a.shape[0])\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a.flatten())", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.diag(a)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.diag(np.arange(a.shape[0]))", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = np.array([x for x in X.flatten()])", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.array([], dtype=object)\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result = np.append(result, X[i, j])\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = np.array([x for x in np.ravel(X)])\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty((X.shape[0],), dtype=object)\nfor i in range(X.shape[0]):\n    result[i] = X[i, :]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.fromstring(mystr, dtype=int, sep='')\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "# find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a.T != 0, axis=1))\n\n# find the maximum set of linearly independent vectors\nresult = np.where(np.sum(a.T != 0, axis=1) == max_independent_vectors)\nresult = np.array([result[0][i] for i in range(len(result[0]))])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) == 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if np.sum(a[result[i]]) != 0])\nresult = np.array([result[i] for i in range(len(result)) if", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "# calculate the t-statistic\nt_stat = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_stat, np.size(a))\nprint(p_value)", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "# calculate the t-statistic\nt_stat = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_stat, np.size(a))\nprint(p_value)", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the t-statistic\nt_stat = np.sqrt(anobs*bvar/(anobs*anobs + bnobs*bvar) + bnobs*anobs*bvar/(anobs*anobs + bnobs*bvar) + bnobs*anobs*anobs*bvar/(anobs*anobs + bnobs*bvar))\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_stat, anobs + bnobs)\np_value", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.asarray([[1,1,2], [1,1,3], [0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0]])", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint c", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint c", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint c", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)\nprint result", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "del_col = np.array([1, 2, 4, 5])\ndel_col = np.array([i for i in del_col if i < a.shape[1]])\na = np.delete(a, del_col, axis=1)\nresult = a", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a_l = a.tolist()\n    a_l.insert(pos, element)\n    a = np.asarray(a_l)\n    return a\n    ### END SOLUTION ###\n\nprint(f(a=example_a, pos=2, element=66))\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.all(a[1:] == a[:-1], axis=1))", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.equal(a[0], a[1:]))", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.all(a == a[0], axis=1))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    # calculate the 2D integral over (cosx)^4 + (siny)^2\n    # use Simpson's rule\n    # result = np.sum(np.sum(np.square(np.cos(x))**4 + np.square(np.sin(y))**2, axis=0))\n    ### END SOLUTION ###\n    return result\n\n# call the function\nf(x, y)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "# compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow, high = np.min(grades), np.max(grades)\nwhile high - low > threshold:\n    high = np.max(grades[grades < high])\n    low = np.min(grades[grades > low])\n    if high - low < threshold:\n        break\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "randomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 0] = 1\nrandomLabel[randomLabel == 1] = 0\nrandomLabel = np.random.randint(", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = np.array(a)", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a) + np.arange(len(a))", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.sort(a)[::-1][:N]", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "a = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\ndef patch_2x2(a):\n    return a[:-1, :-1], a[:-1, 1:], a[1:, :-1], a[1:, 1:]\n\nresult = patch_2x2(a)\nprint(result)\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "def sliding_window(a, window_size):\n    shape = a.shape\n    shape_out = shape[:-1] + (shape[-1]-window_size[0]+1, shape[-1]-window_size[1]+1)\n    shape_out = shape_out.reshape(-1, 2)\n    shape_out = shape_out.reshape(-1, 2, 2)\n    shape_out = shape_out.reshape(-1, 2, 2, 2)\n    return shape_out", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "a = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\ndef patch_2x2(a):\n    return a[:-1, :-1], a[:-1, 1:], a[1:, :-1], a[1:, 1:]\n\nresult = patch_2x2(a)\nprint(result)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\npatch_shape = a.shape[:-1]\npatch_shape = patch_shape + (patch_shape[-1] - patch_shape[-1] % patch_size) % patch_size\n\nresult = np.array([a[i:i+patch_shape[0], j:j+patch_shape[1]] for i in range(0, a.shape[0], patch_shape[0]) for j in range(0, a.shape[1], patch_shape[1])])\n\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\nprint(result)\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\npatch_shape = a.shape[:-1]\npatch_shape = patch_shape + (patch_shape[-1] - patch_shape[-1] % patch_size) % patch_size\n\nresult = np.array([a[i:i+patch_shape[0], j:j+patch_shape[1]] for i in range(0, a.shape[0], patch_shape[0]) for j in range(0, a.shape[1], patch_shape[1])])\n\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.fromstring(string, dtype=np.float64)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.log(np.random.uniform(min, max, n))", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.log(np.random.uniform(np.exp(min), np.exp(max), n))", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.log(np.random.uniform(min, max, n))\n    ### END SOLUTION ###\n\nprint(f(min=1, max=np.e, n=10000))\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, index[0], index[1], index[2])", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "def sub2ind(dims, index):\n    return np.asscalar(np.prod(dims[:-1])) + index[0] * np.prod(dims[1:]) + index[1] * np.prod(dims[2:]) + index[2]\n\nresult = sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df.values.dtype)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.sum(a[accmap])\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0], axis=0)", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.min(a[index])", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        z[i, j] = elementwise_function(x[i, j], y[i, j])", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, probabilit)\nprint(result)", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "a = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\na = np.pad(a, ((0, 0), (0, 0)), mode='constant')\nresult = a[low_index:high_index, low_index:high_index]\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = np.delete(x, np.where(x < 0))\n", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.iscomplex(x)]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_max = np.max(bin_data, axis=1)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "# calculate the bin size\nbin_size = len(data[0]) // bin_size\n\n# calculate the bin data\nbin_data = np.array([data[i:i+bin_size] for i in range(0, len(data), bin_size)])\n\n# calculate the mean of each bin\nbin_data_mean = np.mean(bin_data, axis=1)\n\n# discard the first few elements of each row when misalignment occurs\nbin_data_mean = bin_data_mean[bin_size:]\n\n# discard the last partition if it is not the same size\nif len(bin_data_mean[-1]) != bin_size:\n    bin_data_mean = bin_data_mean[:-1]\n\nprint(bin_data_mean)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (x - x_min) * (x - x_max)", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    x_min = np.clip(x_min, 0, 1)\n    x_max = np.clip(x_max, 0, 1)\n    x = np.clip(x, x_min, x_max)\n    return np.where(x < x_min, x_min, np.where(x > x_max, x_max, x))\n\n# Test the function\nprint(smoothclamp(x, x_min, x_max, N))", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "# Create a circular correlation\nresult = np.correlate(a, b, mode='full')\nprint(result)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = np.array([[np.random.randint(10, size=(4, 15, 5)) for _ in range(5)] for _ in range(4)])", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(15)])", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.unpackbits(np.uint8(a))\nresult = np.reshape(result, (len(a), m))\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:m]\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "def convert_to_binary(a, m):\n    binary_array = np.unpackbits(np.uint8(a))\n    binary_array = binary_array[:m]\n    return binary_array\n\nresult = convert_to_binary(a, m)\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc_start = \u03bc - 3*\u03c3\n\u03bc_end = \u03bc + 3*\u03c3\nresult = (\u03bc_start, \u03bc_end)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc_minus_2\u03c3 = \u03bc - 2*\u03c3\n\u03bc_plus_2\u03c3 = \u03bc + 2*\u03c3\nresult = (\u03bc_minus_2\u03c3, \u03bc_plus_2\u03c3)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std = np.std(a)\n    start = mean - 3*std\n    end = mean + 3*std\n    return (start, end)\n    ### END SOLUTION ###\n\nresult = f(a)\nprint(result)\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "# calculate the 2nd standard deviation\nstd_dev = np.std(a, ddof=1)\n# calculate the mean of the array\nmean = np.mean(a)\n# calculate the 2nd standard deviation interval\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\n# detect outliers\nresult = (a < lower_bound) | (a > upper_bound)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data, percentile)\nprint(prob)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[0, 1] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.argmax(a, axis=1)\nmask = np.where(mask == np.argmax(a, axis=1), True, False)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.min(a, axis=1)\nmask = np.where(mask == 0, False, True)\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "result = np.corrcoef(post, distance)[0, 1]", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.matmul(X, np.transpose(X, (1, 0)))", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((Y.shape[0], Y.shape[1]))\nfor i in range(Y.shape[1]):\n    X += Y[:, i, :, i]\n", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = np.any(a == number)", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\n# Create a set from B to speed up the search\nB_set = set(B)\n\n# Create a new array with the values from A that are not in B\nC = np.array([x for x in A if x not in B_set])\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "A = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\nC = A[np.in1d(A, B)]", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "# Create a boolean array that is True for elements in B that are in A\nboolean_array = np.in1d(A, B)\n# Use the boolean array to index A\nC = A[boolean_array]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.max(a) - np.min(a) + 1 - rankdata(a)", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.max(rankdata(a)) - np.min(rankdata(a)) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np.min(result) + 1\nresult = np.max(result) - np", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    return np.max(a) - np.min(a) + np.max(a) - np.min(a)\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:, second, third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.linalg.norm(X, 1)\nresult = X / l1.reshape(5,1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.array([LA.norm(v,ord=2) for v in X])\nresult = np.array([x/np.linalg.norm(x) for x in X])\nprint(result)", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = np.array([v/x for v in X])\nprint(result)", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = [a['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "# calculate distance between all points\ndistances = np.sqrt(np.sum((a[:, None, :] - a[None, :, :])**2, axis=-1))\n# replace the last dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the first dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances = np.transpose(distances)\n# replace the first dimension with the distance\ndistances = np.reshape(distances, (a.shape[0], a.shape[0]))\n# replace the last dimension with the distance\ndistances =", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "result = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if i == j:\n            result[i, j] = 0\n        else:\n            result[i, j] = np.linalg.norm(a[i] - a[j])\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "result = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if i <= j:\n            result[i, j] = np.linalg.norm(a[i] - a[j])\n        else:\n            result[j, i] = result[i, j]\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "a = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\na = np.unique(a)\nresult = a[a!=0]\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "# remove adjacent (before removing) duplicate non-zero value\na = np.where((a[:-1] != 0) & (a[1:] != 0), a, 0)\n\n# remove all the zero value\na = np.where(a != 0, a, 0)\n\n# remove all the zero value after removing adjacent (before removing) duplicate non-zero value\na = np.where((a[:-1] != 0) & (a[1:] != 0), a, 0)\n\nresult = a", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "lat_lon_val = np.stack((lat, lon, val), axis=1)\ndf = pd.DataFrame(lat_lon_val, columns=['lat', 'lon', 'val'])\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    # Create a pandas dataframe\n    df = pd.DataFrame({'lat': lat[0], 'lon': lon[0], 'val': val[0]})\n\n    # Create a new dataframe with the first row of each array\n    new_df = pd.DataFrame({'lat': lat[0], 'lon': lon[0], 'val': val[0]})\n\n    # Iterate over the other arrays\n    for i in range(1, len(lat)):\n        # Create a new dataframe with the first row of each array\n        new_row = pd.DataFrame({'lat': lat[i], 'lon': lon[i], 'val': val[i]})\n\n        # Append the new row to the new dataframe\n        new_df = new_df.append(new_row)\n\n    return new_df\n\n# Call the function\nf(lat, lon, val)\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "lat_lon_val = np.stack((lat, lon, val), axis=1)\nmax_values = np.max(lat_lon_val, axis=1)\nmax_values = np.repeat(max_values, lat_lon_val.shape[1])\nmax_values = np.reshape(max_values, (lat_lon_val.shape[0], -1))\ndf = pd.DataFrame({'lat': lat_lon_val[:, 0], 'lon': lat_lon_val[:, 1], 'val': lat_lon_val[:, 2], 'maximum': max_values})\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, shape[-1] - size[1] + 1)\n    shape = shape[0] * shape[1]\n    shape = shape.reshape(-1, 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, shape[-1] - size[1] + 1)\n    shape = shape[0] * shape[1]\n    shape = shape.reshape(-1, 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape.shape[0], 1)\n    shape = shape.reshape(shape", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    # compute the mean of the array of complex numbers\n    mean = np.mean(a)\n    # compute the mean of the imaginary part of the array\n    mean_imag = np.mean(a.imag)\n    # compute the mean of the real part of the array\n    mean_real = np.mean(a.real)\n    # compute the mean of the absolute value of the array\n    mean_abs = np.mean(np.abs(a))\n    # compute the mean of the absolute value of the imaginary part of the array\n    mean_abs_imag = np.mean(np.abs(a.imag))\n    # compute the mean of the absolute value of the real part of the array\n    mean_abs_real = np.mean(np.abs(a.real))\n    # compute the mean of the absolute value of the absolute value of the array\n    mean_abs_abs = np.mean(np.abs(np.abs(a)))\n    # compute the mean of the absolute value of the absolute value of the imaginary part of the array\n    mean_abs_abs_imag = np.mean(np.abs(np.abs(a.imag)))\n    # compute the mean of the absolute value of the absolute value of the real part of the array\n    mean_abs_abs_real = np.mean(np.abs(np.abs(a.real)))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the array\n    mean_abs_abs_abs = np.mean(np.abs(np.abs(np.abs(a))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the imaginary part of the array\n    mean_abs_abs_abs_imag = np.mean(np.abs(np.abs(np.abs(a.imag))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the real part of the array\n    mean_abs_abs_abs_real = np.mean(np.abs(np.abs(np.abs(a.real))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the array\n    mean_abs_abs_abs_abs = np.mean(np.abs(np.abs(np.abs(np.abs(a)))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the imaginary part of the array\n    mean_abs_abs_abs_abs_imag = np.mean(np.abs(np.abs(np.abs(np.abs(a.imag)))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the real part of the array\n    mean_abs_abs_abs_abs_real = np.mean(np.abs(np.abs(np.abs(np.abs(a.real)))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the array\n    mean_abs_abs_abs_abs_abs = np.mean(np.abs(np.abs(np.abs(np.abs(np.abs(a))))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the imaginary part of the array\n    mean_abs_abs_abs_abs_abs_imag = np.mean(np.abs(np.abs(np.abs(np.abs(np.abs(a.imag))))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the real part of the array\n    mean_abs_abs_abs_abs_abs_real = np.mean(np.abs(np.abs(np.abs(np.abs(np.abs(a.real))))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the array\n    mean_abs_abs_abs_abs_abs_abs = np.mean(np.abs(np.abs(np.abs(np.abs(np.abs(np.abs(a)))))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the imaginary part of the array\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:,-1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "print(c in CNTS)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "print(c in CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.diag(i)\nprint(i)", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[1:, 1:] = 0", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\nresult = np.linspace(start_date, end_date, n)\nresult = pd.DatetimeIndex(result)\nprint(result)", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No index found\")\nelse:\n    print(result)", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "# find the indices of (a, b)\nindices = np.where((x == a) & (y == b))\nresult = indices[0]\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "# calculate the coefficients a, b and c\na, b, c = np.linalg.solve(np.vstack([x, np.ones(len(x))]).T, y)\nresult = np.array([a, b, c])\nresult = result[np.argsort(np.abs(result))]\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "# calculate the coefficients\ncoeffs = np.polyfit(x, y, degree)\n# sort the coefficients in descending order\nresult = np.sort(coeffs)[::-1]\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = [0,1,2,3]\ntemp_df = pd.DataFrame([[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]])\n\nresult = np.array([[x-y for x, y in zip(row, temp_arr)] for row in temp_df])\nprint(result)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\nprint(result)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "mask = arr < -10\nmask2 = arr >= 15\nmask3 = mask ^ mask2\n\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr = np.where(arr < n1, 0, np.where(arr >= n2, 30, arr + 5))", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(s1 != s2)[0].shape[0]", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "result = np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.all(np.array_equal(a[0], a[1])) and np.array_equal(a[0], a[2])", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = np.all(np.isnan(a))", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "a = np.ones((41, 13))\nshape = (93, 13)\na = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = a", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "a = np.ones((41, 12))\nshape = (93, 13)\na = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = a", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "a = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\na = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')\nresult = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape you want to pad to\n    max_shape = np.max(shape)\n    max_arr_shape = np.max(arr.shape)\n\n    # Find the maximum shape of the original array and the shape", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "def zeropad(a, shape):\n    shape_a = a.shape\n    shape_diff = np.array(shape_a) - np.array(shape)\n    shape_diff = shape_diff[shape_diff < 0]\n    shape_diff = shape_diff[shape_diff > 0]\n    shape_diff = shape_diff.tolist()\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n    shape_diff = shape_diff[0]\n   ", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=2)", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=2)", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = np.where(df['a'] > 1, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "def remove_periphery(im):\n    rows, cols = im.shape\n    for i in range(rows):\n        if np.all(im[i,:] == 0):\n            im[i,:] = np.zeros(cols)\n        if np.all(im[i,:] == 0):\n            im[i,:] = np.zeros(cols)\n    return im\n\nresult = remove_periphery(im)\nprint(result)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "# Find the position of the first non-zero value in each row and column\nrow_pos = np.argmax(np.any(A, axis=1), axis=1)\ncol_pos = np.argmax(np.any(A, axis=0), axis=1)\n\n# Find the position of the last non-zero value in each row and column\nrow_end = np.argmax(np.any(A[row_pos:], axis=1), axis=1) + row_pos\ncol_end = np.argmax(np.any(A[:, col_pos:], axis=0), axis=1) + col_pos\n\n# Create a new array with the positions of the first and last non-zero value in each row and column\nresult = np.array([[row_pos, row_end], [col_pos, col_end]])\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "def remove_peripheral_zeros(im):\n    rows, cols = im.shape\n    for i in range(rows):\n        if np.all(im[i,:] == 0):\n            im[i,:] = 0\n        if np.all(im[i,:] == 0):\n            im[i,:] = 0\n    for i in range(cols):\n        if np.all(im[:,i] == 0):\n            im[:,i] = 0\n        if np.all(im[:,i] == 0):\n            im[:,i] = 0\n    return im\n\nresult = remove_peripheral_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "def remove_periphery(im):\n    rows, cols = im.shape\n    for i in range(rows):\n        if np.all(im[i,:] == 0):\n            im[i,:] = np.zeros(cols)\n        if np.all(im[i,:] == 0):\n            im[i,:] = np.zeros(cols)\n    return im\n\nresult = remove_periphery(im)\nprint(result)\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='x-y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "line_styles = ['-', '--', '-.', ':']\nfor line_style in line_styles:\n    plt.plot(x, np.random.randn(10), line_style)\n\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "line_styles = ['-', '--', '-.', ':']\nfor line_style in line_styles:\n    plt.plot(x, np.random.randn(10), line_style)\n\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, marker='d')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'd', markersize=10, linewidth=2, color='blue')\nplt.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, 'r')\nplt.plot([2, 4], [0, 0], 'r')\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = np.linspace(0, 2, 100)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1, 2])\ny = np.array([0, 2, 4])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    height=5,\n    aspect=1,\n)\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style('darkgrid')\nsns.set_style('white')\nsns.set_style('dark')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\ns", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "sns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize=20)\nplt.title('cos(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.setp(l, facecolor='r', alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"black\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.setp(l, color=\"r\")\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "x_rotated = np.rot90(x, 1)\nplt.plot(x_rotated, np.cos(x_rotated), label=\"cos\")\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "x_rotated = np.rot90(x, 1)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.linspace(0, 2 * np.pi, 10)\nplt.plot(x_ticks, np.cos(x_ticks), label=\"sin\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim(0, 2 * np.pi)\nplt.ylim(-1, 1)\nplt.xticks(x_ticks)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper left')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticks(g.get_xticks(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# find the longest line in myTitle\nlongest_line = max(len(x), len(y), len(myTitle))\n# divide myTitle into multiple lines\nmyTitle_lines = np.array_split(myTitle, longest_line)\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = np.sort(y)\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.arange(0, 1.5, 0.01)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'o-')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of y = 2x')\nplt.grid(True)\nplt.tick_params(axis='both', which='major', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='major', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.tick_params(axis='both', which='minor', label", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.ticklabel_format(style='plain')\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n\n# Create a lineplot with a dashed line\nline_dashed = sns.lineplot(x=x, y=y, color='black', linestyle='-')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_title('sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_title('cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y2')\naxs[1].legend()\n\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "plt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(x, y1, label='sin(x)')\nplt.title('sin(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y2, label='cos(x)')\nplt.title('cos(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel(\"\")\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelleft=False, labelright=False, labelbottom=False)\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4, 1))\nplt.grid(axis='x', which='both', linestyle='-', linewidth=0.5, color='black')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.grid(True, which='both')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.xticks(np.arange(1, 2, 0.1))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "# plt.subplots_adjust(top=0.95, bottom=0.05, left=0.05, right=0.95, hspan=0.05, wspan=0.05)\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlabel(\"Column Labels\")\nax.set_ylabel(\"Row Labels\")\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Y')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y over X')\nplt.legend()\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of y over x')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y, label=\"Y\")\nplt.ylabel(\"Y\")\nplt.xlabel(\"X\")\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "# Create a new dataframe with only the columns 'total_bill' and 'tip'\ntips_new = tips[['total_bill', 'tip']]\n\n# Create a new dataframe with the distribution plot\ndist_plot = sns.distplot(tips_new['total_bill'], kde=False, color='green')\n\n# Create a new dataframe with the line and scatter plot\nline_plot = sns.lineplot(data=tips_new, x='total_bill', y='tip', color='blue')\n\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "# Create a new dataframe with the regression line\nregression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram_regression_line = pd.DataFrame({'total_bill': tips['total_bill'],\n                                         'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_regression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                                        'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_regression_line_regression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_regression_line_regression_line_regression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_regression_line_regression_line_regression_line_regression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_regression_line_regression_line_regression_line_regression_line_regression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_regression_line_regression_line_regression_line_regression_line_regression_line_regression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                                                'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram_regression_line_regression_line_regression_line_regression_line_regression_line_regression_line_regression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_b", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "# Create a joint regression plot\nsns.jointplot(data=tips, x='total_bill', y='tip', kind='reg')\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 5))\nfor i, celltype in enumerate(df[\"celltype\"]):\n    plt.bar(i, df[\"s1\"][i], label=celltype, color=\"blue\")\n    plt.bar(i, df[\"s2\"][i], label=celltype, color=\"red\")\n\nplt.xlabel(\"celltype\")\nplt.ylabel(\"s1 and s2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 5))\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X', color='r')\nplt.ylabel('Y', color='r')\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.legend(color='r')\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.plot(x, y, 'r')\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'o-')\nplt.xlabel('x', fontsize=10)\nplt.ylabel('y', fontsize=10)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--')\n# plt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, cmap='hot', aspect='auto')\nplt.xlabel(xlabels, rotation=90)\nplt.ylabel(ylabels, rotation=0)\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "# copy the code of the above plot and edit it to have legend for all three cruves in the two subplots\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\n\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "# create a new data frame with a, b and c\ndf = pd.DataFrame({'a': a, 'b': b, 'c': c})\n\n# create scatter plot\nplt.scatter(df['a'], df['b'])\n\n# annotate each data point with correspond numbers in c\nfor i, (x, y) in enumerate(zip(a, b)):\n    plt.annotate(str(c[i]), (x, y), textcoords=\"data\")\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"y over x\")\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.title(\"y over x\")\nplt.legend(title=\"Legend\", title_fontsize=14, title_fontweight=\"bold\")\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, bins=10, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d = np.repeat(np.repeat(x, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d_2 = np.repeat(np.repeat(y, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d_3 = np.repeat(np.repeat(x, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d_4 = np.repeat(np.repeat(y, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d_5 = np.repeat(np.repeat(x, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d_6 = np.repeat(np.repeat(y, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d_7 = np.repeat(np.repeat(x, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d_8 = np.repeat(np.repeat(y, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (10, 10)\narray_2d_9 = np.repeat(np.repeat(x, 10), 10)\n\n# Create a 2D array with the same shape as the original 2D array.\n# The first axis of the array is the number of rows, and the second axis is the number of columns.\n# The value in the array is the same as the original array.\n# The shape of the array is (1", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 5))\nplt.hist(x, bins=10, alpha=0.5, label='x')\nplt.hist(y, bins=10, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "x = np.linspace(0, 5, 100)\ny = a * x + b\n\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "# create a new figure with two subplots\nfig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\n# create a colormap for x and y\ncmap = plt.get_cmap('viridis')\n\n# create a colorbar for x and y\ncbar = plt.colorbar(cmap, ax=axs[0])\ncbar.set_label('x')\n\n# create a colorbar for x and y\ncbar = plt.colorbar(cmap, ax=axs[1])\ncbar.set_label('y')\n\n# plot x and y on the two subplots\naxs[0].imshow(x, cmap=cmap)\naxs[1].imshow(y, cmap=cmap)\n\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.title('Plot of two columns in x')\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "plt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(x, y, label='Y')\nplt.title('Y and Z')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(a, z, label='Z')\nplt.title('Y and Z')\nplt.legend()\n\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "# Create a line plot for points\nplt.plot(points)\n\n# Set the log scale for the y-axis\nplt.loglog()\n\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "for line, color in zip(lines, c):\n    plt.plot(*line, color=color)\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.loglog(x, y, basex=10, basey=10)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-log plot')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "# Calculate the total sum of data\ntotal_sum = np.sum(data)\n\n# Create a new array with the data renormalized to sum up to 1\ndata_normalized = [x / total_sum for x in data]\n\n# Create a histogram of the data\nplt.hist(data_normalized, bins=10, edgecolor='black')\n\n# Set the y tick labels as percentage\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels as the original data\nplt.xticks(np.arange(0, 1.1, 0.1))\nplt.xlabel('Data')\n\n# Set the title of the plot\nplt.title('Histogram of Data')\n\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', alpha=0.5, linestyle='-')\nplt.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label=\"y\")\naxs[0].plot(a, z, label=\"a\")\naxs[0].legend()\naxs[1].plot(x, z, label=\"z\")\naxs[1].legend()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 10))\n\n# Plot the first subplot\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", ax=axs[0])\n\n# Plot the second subplot\nsns.regplot(data=df, x=\"flipper_length_mm\", y=\"bill_length_mm\", ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xlabel(\"Second\")\nax.set_xlim(1, 10)\nax.set_xticks(range(1, 10))\nax.set_xlabel(\"Second\")\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(range(0, 10, 2), np.arange(2, 10, 2))\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.tick_params(axis='y', which='both', left=False, right=False, top=False, bottom=False, labelleft=False, labelright=False, labeltop=False, labelbottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=False, bottom=False, left=False, right=False, top=", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(np.arange(2010, 2020), alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(xmin=0, ymin=0, xmax=0, ymax=0)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(xmin=0, ymin=0, xmax=0, ymax=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\naxs.plot(x, y, label='y')\naxs.plot(x, y, label='y')\naxs.set_title('Figure')\naxs.legend()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df[\"Type A\"], df[\"Type B\"], label=\"Type A\")\nplt.xlabel(\"Type A\")\nplt.ylabel(\"Type B\")\nplt.title(\"Line Chart of Type A and Type B\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='|')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[0], y[-1]], '--', color='k')\nplt.plot([x[0], x[-1]], [y[", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, s=100, hatch='v')\nplt.scatter(x, y, s=100, hatch='*')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "xlim = [1, 5]\nylim = [1, 4]\n\nplt.figure(figsize=(10, 10))\nplt.imshow(data, extent=[xlim[0], xlim[1], ylim[0], ylim[1]])\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.figure(figsize=(10, 5))\nplt.plot([3], [0], 'r', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 5))\nplt.bar(labels, height, color=\"blue\")\nplt.title(\"Bar plot\")\nplt.xlabel(\"Labels\")\nplt.ylabel(\"Height\")\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.figure(figsize=(10, 10))\nplt.pie(data, labels=l, wedge_width=0.4)\nplt.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b-')\nplt.grid(True, color='black', alpha=0.25)\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.minorticks_on()\nplt.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title(\"Pie Chart\")\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title(\"Pie Chart\")\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5)\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='r', alpha=0.5, edgecolor='r')\nplt.plot(x, y, 'r--', marker='o', marker_size=100, marker_edgecolor='r', marker_facecolor='", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Create a figure with a single axis\nfig, ax = plt.subplots(1, 1)\n\n# Create the bars\nbar1 = ax.bar(np.arange(3), blue_bar, color='b', label='Blue bars')\nbar2 = ax.bar(np.arange(3) + 0.5, orange_bar, color='orange', label='Orange bars')\n\n# Set the labels and title\nax.set_xlabel('Bar number')\nax.set_ylabel('Height')\nax.set_title('Bar plot')\n\n# Set the legend\nax.legend()\n\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('y over x')\nplt.grid(True)\nplt.tick_params(axis='x', which='both', labelsize=10)\nplt.tick_params(axis='y', which='both', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(data=df, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"species\", sharey=False, kind=\"bar\", aspect=\"auto\")\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(10, 10))\nplt.plot([0.5], [0.5], 'ro')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0, 1], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0, 1], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r\"$\\phi$\", fontsize=16, fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"Line\")\nplt.legend(handlelength=0.3)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc=\"upper left\")\nplt.plot(x, y, marker=\"*\", label=\"Line\")\nplt.legend(loc=\"upper left\")\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, cmap='hot')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(1, figsize=(10, 5))\nplt.title(\"Figure 1\", bold=True)\nplt.plot(x, y)\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y, 'r')\nplt.invert_xaxis()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\n\n# Create a scatter plot\nplt.scatter(x, y, alpha=0.5)\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolor='black')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax, x, y in zip(axs.flat, x, y):\n    ax.plot(x, y)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('y over x')\n\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('x')\nplt.ylabel('frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 5))\nplt.errorbar(x, y, yerr=error, fmt='o', color='blue', label='y')\nplt.fill_between(x, y-error, y+error, color='blue', alpha=0.2, label='y error')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Error bars')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 0], 'w')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "for i in range(len(box_position)):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], fmt='o', color=c[i])\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\nax1.plot(x, y, label='Y')\nax1.set_title('Y')\nax2.plot(z, ax2.get_ylim(), label='Z')\nax2.set_title('Z')\nax2.set_ylim(ax1.get_ylim())\nax2.set_ylabel('Z')\nax2.set_xlabel('X')\nax2.legend()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor ax, x, y in zip(axs.flatten(), x, y):\n    ax.plot(x, y)\n    ax.set_xticks(x)\n    ax.set_yticks(y)\n    ax.set_xlabel(x)\n    ax.set_ylabel(y)\n    ax.set_title(f\"x={x}, y={y}\")\n\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1, figsize=(8, 8))\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nplt.table(\n    df,\n    collabels=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    rowlabels=[\"penguin1\", \"penguin2\", \"penguin3\", \"penguin4\", \"penguin5\", \"penguin6\"],\n    celltext=np.array([[\"penguin1\", \"penguin2\", \"penguin3\"], [\"penguin4\", \"penguin5\", \"penguin6\"]]),\n    cellcolor=\"white\",\n    bbox=[0, 0, 1, 1],\n)\nplt.table(\n    df,\n    collabels=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    rowlabels=[\"penguin1\", \"penguin2\", \"penguin3\", \"penguin4\", \"penguin5\", \"penguin6\"],\n    celltext=np.array([[\"penguin1\", \"penguin2\", \"penguin3\"], [\"penguin4\", \"penguin5\", \"penguin6\"]]),\n    cellcolor=\"white\",\n    bbox=[0, 0, 1, 1],\n)\nplt.table(\n    df,\n    collabels=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    rowlabels=[\"penguin1\", \"penguin2\", \"penguin3\", \"penguin4\", \"penguin5\", \"penguin6\"],\n    celltext=np.array([[\"penguin1\", \"penguin2\", \"penguin3\"], [\"penguin4\", \"penguin5\", \"penguin6\"]]),\n    cellcolor=\"white\",\n    bbox=[0, 0, 1, 1],\n)\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.legend()\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.grid(True)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\ndf[\"group\"] = df[\"kind\"].apply(lambda x: \"Fat\" if x == \"Fat\" else \"No Fat\")\n\n# Create a new column \"group\" by using the \"kind\" column\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"exercise\"]\n\n# Create a new column \"Exercise Time\" by concatenating \"time\" and \"exercise\"\ndf[\"Exercise Time\"] = df[\"time\"].astype(str) + \" \" + df[\"", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter = df[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot = df_scatter[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot = df_scatter_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot = df_scatter_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot = df_scatter_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot = df_scatter_plot_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot_plot = df_scatter_plot_plot_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot_plot_plot = df_scatter_plot_plot_plot_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot_plot_plot_plot = df_scatter_plot_plot_plot_plot_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot_plot_plot_plot_plot = df_scatter_plot_plot_plot_plot_plot_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot = df_scatter_plot_plot_plot_plot_plot_plot_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot = df_scatter_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot = df_scatter_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot[[\"time\", \"pulse\", \"kind\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\", \"pulse\", \"kind\", and \"diet\"\ndf_scatter_plot_plot_plot_plot_plot_plot_plot_plot_plot_plot", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label=\"y\")\nplt.legend(loc=\"upper right\")\nplt.show()\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 5))\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "sns.stripplot(data=df, x=\"sex\", y=\"bill_length_mm\", hue=\"species\", palette=\"deep\")\nplt.legend(loc=\"upper left\")\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, gap=0, aspect=0.05, share_all=True)\n\n# Create a FaceGrid object for each row in \"b\"\nfor i, row in enumerate(df[\"b\"]):\n    facegrid.add_facegrid(\n        sns.pointplot(x=\"a\", y=\"c\", hue=row, data=df, aspect=0.05),\n        facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid.facegrid", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_zlim([0, 1])\nax.set_view_angle(30, 50)\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y, label=\"y\")\nplt.plot(x, y, label=\"x\")\nplt.axis(['x', 'y'])\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.legend()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "# Create a gridspec with nrow rows and ncol columns\n# Use the gridspec to create the subplots\n# Use the gridspec to plot the x in each subplot as an image\n# Use the gridspec to remove the space between each subplot and make the subplot adjacent to each other\n# Use the gridspec to remove the axis ticks from each subplot\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nx = tf.cast(x, tf.int32)\nprint(x)\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nx = tf.cast(x, tf.int32)\nprint(x)\n", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    # Create a 2D tensor with the same shape as the input labels\n    result = tf.zeros_like(labels)\n    # Set the value of the first column to 1\n    result[:, 0] = 1\n    # Set the value of the last column to 1\n    result[:, -1] = 1\n    # Set the value of the other columns to 0\n    result[:, 1:-1] = 0\n    # Return the result\n    return result\n\n# Call the function\nresult = f(example_labels)\nprint(result)\n## END SOLUTION ###\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return [[i, i+1, i+2]]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return tf.concat([input, input + 1, input + 2], axis=0)\n    ### END SOLUTION ###\n\n# Create a dataset from the input\nds = tf.data.Dataset.from_tensor_slices(example_input)\n\n# Map the function to the dataset\nds = ds.map(f)\n\n# Create a session to run the dataset\nwith tf.compat.v1.Session() as sess:\n    # Run the dataset and get the result\n    result = sess.run(ds)\n\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\nmask = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(mask == 1, 1, 0)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0, result)\nresult = tf.where(mask == 1, 1, result)\nresult = tf.where(mask == 0, 0", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\nmask = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0, 1)\nresult = tf.where(mask == 0, 0, 1)\nresult = tf.where(mask == 1, 0", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [0, 8-lengths.shape[0]]], 'CONSTANT')\nmask = tf.cast(lengths_padded > 0, tf.float32)\nresult = tf.where(lengths_padded > 0, tf.ones_like(lengths_padded), tf.zeros_like(lengths_padded))\nresult = tf.where(lengths_padded > 0, tf.ones_like(lengths_padded), tf.zeros_like(lengths_padded))\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths),", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\nmask = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = tf.where(lengths_padded == 0, 1, 0)\nresult = t", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\nresult = tf.stack([a, b], axis=1)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    # Use tf.stack to combine the elements of a and b in a cartesian product\n    # Then use tf.reshape to convert the result to a 2D tensor\n    # Finally use tf.transpose to transpose the result\n    # result = tf.stack([tf.stack([a,b]) for a in example_a for b in example_b])\n    # result = tf.reshape(result, (-1, 2))\n    # result = tf.transpose(result)\n    ### END SOLUTION ###\n    return result\n\nprint(f(a=example_a,b=example_b))\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "a = tf.reshape(a, (50, 100, 512))", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "a = tf.reshape(a, (50, 100, 1, 512))", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "a = tf.reshape(a, (1, 50, 100, 1, 512))", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.reciprocal(A)", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "# calculate the square of the difference between the two embeddings\ndiff_square = tf.square(a - b)\n\n# reduce the square by row\nresult = tf.reduce_sum(diff_square, axis=1)\n\n# calculate the square root of the sum of the square of the difference\nresult = tf.sqrt(tf.reduce_sum(diff_square))\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "# calculate the square of the difference between the two embeddings\ndiff = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_square, axis=0)\n\n# calculate the square difference\ndiff_square = tf.square(a - b)\n\n# reduce the square difference by column\nresult = tf.reduce_sum(diff_", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    # calculate the square of the difference between A and B\n    square_diff = tf.square(tf.subtract(A, B))\n    # reduce the square difference by row\n    result = tf.reduce_sum(square_diff, axis=1)\n    # return the result\n    return result\n\n# call the function\nresult = f(A=example_a,B=example_b)\nprint(result)\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = x[y,z]\nresult = tf.gather(x, y, axis=1)\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = x[row, col]\nresult = tf.gather(x, row, col)\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    return tf.gather(x, y, axis=1)\n    ### END SOLUTION ###\n\nresult = f(example_x, example_y, example_z)\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "result = tf.math.reduce_sum(A[:, :, None] * B[:, None, :], axis=2)", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "result = tf.math.reduce_sum(A[:, :, None] * B[:, None, :], axis=2)", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "x = tf.constant([\n    b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a'])\n\nresult = tf.strings.as_string(x)\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    # Convert the byte list to string list\n    result = tf.strings.as_string(x)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "x_nonzero = tf.where(x != 0, x, tf.zeros_like(x))\nx_nonzero_sum = tf.reduce_sum(x_nonzero, axis=-1, keepdims=True)\nresult = tf.where(x_nonzero_sum != 0, x_nonzero / x_nonzero_sum, tf.zeros_like(x))\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "x_padded = tf.pad(x, [[0, 0], [0, 0], [0, 0], [1, 1]])\nx_padded = tf.where(x_padded != 0, x_padded, tf.zeros_like(x_padded))\nresult = tf.reduce_sum(x_padded, axis=-1) / tf.reduce_sum(x_padded, axis=-1)\nresult = tf.where(x_padded != 0, result, tf.zeros_like(result))\nresult = tf.reshape(result, x.shape[:-1])\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    # find the non-zero elements\n    non_zero_elements = tf.where(x != 0, x, tf.zeros_like(x))\n    # calculate the average\n    average = tf.reduce_sum(non_zero_elements, axis=-1) / tf.reduce_sum(non_zero_elements, axis=-1)\n    return average\n\nresult = f(example_x)\nprint(result)\n## END SOLUTION\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\n# Create two 2D tensor with random values\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Calculate the sum of the matrix multiplication of A and B\nresult = tf.reduce_sum(tf.matmul(A, B))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "# Convert the scores tensor to the index of the maximum value in each row\nmax_index = tf.argmax(a, axis=1)\n\n# Convert the maximum index to the same shape as the original scores tensor\nmax_index = tf.reshape(max_index, [-1])\n\n# Compare the maximum index with the actual scores\nresult = tf.equal(max_index, tf.argmax(a, axis=1))\n\n# Count the number of same values\nresult = tf.reduce_sum(result)\n\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "# Convert the scores tensor to the index of the maximum value in each column\nmax_index = tf.argmax(a, axis=1)\n\n# Convert the maximum index to the index of the maximum value in each row\nresult = tf.argmax(max_index, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    # Find the maximum value in each row\n    max_values = tf.argmax(a, axis=1)\n    # Convert the maximum value to the index of the maximum value in each row\n    result = tf.argmax(max_values, axis=1)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_a))\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1) + 1", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n\n# Save the model\nmodel.save('my_model')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "seed_x = 10\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform([10], minval=1, maxval=4, dtype=tf.int32)\nprint(result)", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "seed_x = 10\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform([114], minval=2, maxval=5, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": " ###\n    tf.random.set_seed(seed_x)\n    random_int = tf.random.uniform([10], minval=1, maxval=4, dtype=tf.int32)\n    return random_int\n\n# call the function\nresult = f(seed_x=10)\nprint(result)\n## END SOLUTION ##\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "print(tf.__version__)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "# Fit the polynomial of order 2\nresult = np.polyfit(x, y, 2)\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the polynomial of order 1\npoly1 = np.polyfit(x, y, 1)\n# Fit the polynomial of order 2\npoly2 = np.polyfit(x, y, 2)\n# Fit the polynomial of order 3\npoly3 = np.polyfit(x, y, 3)\n# Fit the polynomial of order 4\npoly4 = np.polyfit(x, y, 4)\n# Fit the polynomial of order 5\npoly5 = np.polyfit(x, y, 5)\n\n# Create a list of the results\nresults = [poly1, poly2, poly3, poly4, poly5]\n\n# Print the results\nprint(results)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, A, B, C):\n    return A * np.exp(B * x) + C\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array([result[0], result[1], result[2]])\nprint(result)", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + (3*a - b - c)**2 + sin(b) + cos(b) + 4)\n\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.zdist.pdf(z_scores, loc=0, scale=1)", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = scipy.stats.zdist.pdf(z_scores, mu, sigma)", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = scipy.stats.zscore(p_values)", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)\nprint(result)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(total, mu, stddev)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    return sA * sB\n    ### END SOLUTION ###\n\nresult = f(example_sA, example_sB)\nprint(result)\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1],", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1].min(), points[:, 1].max(), 100),\n                       np.linspace(points[:, 2].min(), points[:, 2].max(), 100))\n\n# Create a 3D grid from the points\nx, y, z = np.meshgrid(np.linspace(points[:, 0].min(), points[:, 0].max(), 100),\n                       np.linspace(points[:, 1", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, reshape=False)\nxrot = x0 + np.cos(np.radians(angle)) * (y0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - x0)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = np.diagonal(M)", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # Use scipy's kstest function to test the hypothesis that the points are uniformly chosen from the range 0 to T.\n    # The result should be a KStest result.\n    ### END SOLUTION ###\n    return stats.kstest(times, \"uniform\")\n\nprint(f())", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, \"uniform\")\nprint(result) ", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = c1.concatenate(c2)", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = c1.concatenate(c2)", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "# calculate the distance between points\ndist = scipy.spatial.distance.cdist(points1, points2)\n# find the minimum distance\nmin_dist = np.min(dist)\n# find the points with the minimum distance\nmin_points = points2[np.argmin(dist, axis=1)]\n# find the points that are not used\nused_points = np.array([point for point in points2 if point not in min_points])\n# assign points to the points that are not used\nresult = np.array([point for point in points2 if point not in used_points])\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "# calculate the Manhattan distance between points\ndistances = scipy.spatial.distance.cityblock(points1, points2)\n# find the minimum distance\nmin_distance = np.min(distances)\n# find the points that are not used\nunused_points = np.array([i for i in range(N) if not np.any(distances == i)])\n# assign points to unused points\nresult = np.array([unused_points[i] for i in np.argmin(distances, axis=1)])\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b = b.toarray()\nb[np.arange(b.shape[0]), np.arange(b.shape[1])] = 0\nb = sparse.csr_matrix(b)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "# Create a mask of the image where the value is greater than the threshold\nmask = img > threshold\n\n# Use ndimage.label function to find the regions of the image\nlabel_image = ndimage.label(mask)\n\n# Count the number of regions\nnum_regions = np.max(label_image)\n\nresult = num_regions<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "region_count = np.sum(img < threshold)", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use numpy to count the number of regions of cells which value exceeds a given threshold\n    # You can use numpy's binary filter to count the number of regions\n    # The binary filter will return a 2D array where each element is 1 if the value is greater than the threshold, and 0 otherwise\n    # Then you can use numpy's sum function to count the number of 1's in the binary array\n    # The result will be the number of regions of cells which value exceeds the threshold\n    ### END SOLUTION ###\n    return result\n\n# Call the function with the example image\nresult = f(example_img)\nprint(result)\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the center of mass of such regions\ncenter_of_mass = np.mean(np.where(mask), axis=1)\n\n# Find the distance between the center of mass of such regions and the top left corner\ndistance = np.sqrt((center_of_mass[0] - 0)**2 + (center_of_mass[1] - 0)**2)\n\nresult = distance<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T - np.diag(np.diag(M))\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    # make the matrix symmetric\n    sA = lil_matrix(sA)\n    sA = sA + sA.T\n    ### END SOLUTION ###\n    return sA\n\n# call the function\nf(example_sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "# erode the square by 1\neroded = scipy.ndimage.erode(square, 1)\n# dilate the eroded square by 1\ndilated = scipy.ndimage.dilate(eroded, 1)\n# remove the isolated single cells\nresult = np.where(dilated == 0, 0, square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask of the square\nmask = np.zeros_like(square)\nmask[10:-10, 10:-10] = 1\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask, 1)\n\n# Erase the square with the mask\nsquare[mask == 0] = 0\n\n# Dilate the mask again to remove the small holes\nmask = scipy.ndimage.binary_dilation(mask, 2)\n\n# Erase the square with the mask\nsquare[mask == 0] = 0\n\n# Print the square\nprint(square)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col)\nstandard_deviation = np.std(col)", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_value = np.max(col)\nmin_value = np.min(col)\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "# Convert the sparse matrix to a dense array\ndarr = np.array(sA.toarray())\n\n# Get the median and mode\nmedian = np.median(darr)\nmode = np.mode(darr)\n\nprint(median)\nprint(mode)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "popt, pcov = curve_fit(fourier1, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 2D numpy array\narray_2d = np.array(example_array)\n# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.pdist(array_2d, 'euclidean')\n# Convert distances tometres\ndistances_in_metres = distances * 1000\n# Convert distances to a 2D array with the form \"from ID, to ID, distance\"\nresult = np.array([(i, j, distances_in_metres[i, j]) for i in range(distances_in_metres.shape[0]) for j in range(distances_in_metres.shape[1])])\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Convert the array to a 1D array\narray_1d = np.ravel(example_array)\n# Calculate pairwise Manhattan distances\ndistances = scipy.spatial.distance.pdist(array_1d, 'manhattan')\n# Convert the distances to a 2D array\ndistances_2d = np.reshape(distances, (len(array_1d), len(array_1d)))\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distance = np.min(distances_2d)\n# Print the minimum distance\nprint(min_distance)\n# Print the distances between all possible combinations of regions\nprint(distances_2d)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate pairwise Euclidean distances between all regions\n    distances = scipy.spatial.distance.pdist(example_array, 'euclidean')\n    # Convert distances tometres\n    distances_in_metres = distances * 1000\n    # Convert distances to a two-dimensional array\n    distances_array = np.array(distances_in_metres).reshape(len(distances_in_metres), len(distances_in_metres))\n    # Find the minimum distance separating the nearest edges of each raster patch\n    min_distance = np.min(distances_array)\n    # Return the result\n    return min_distance\n    ### END SOLUTION ###\n\n# Call the function\nmin_distance = f(example_array)\nprint(min_distance)\n# Print the result\nprint(min_distance)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(x.shape[1]):\n    tck = interpolate.splrep(x[:, i], y[:, i], k = 2, s = 4)\n    result[i] = interpolate.splev(x_val, tck, der = 0)\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array([38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0])\nx2 = np.array([39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8])\nx3 = np.array([34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0])\nx4 = np.array([34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8])\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x1, x2, x3, x4)\nprint(statistic, critical_values, significance_level)", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nresult = ss.anderson_ksamp(x1, x2)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(df['A']) # keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = pd.rolling_apply(df['B'], 3, lambda x: tau1(x))\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nresult = is_csr_matrix_only_zeroes(sa)", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n\nresult = is_lil_matrix_only_zeroes(sa)", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(a[0], a[1], a[2])", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n    ### END SOLUTION ###\n\nprint(f(pre_course_scores, during_course_scores))\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "kurtosis_result = np.mean(np.power(a, 3)) / np.power(np.mean(np.power(a, 3)), 3)", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=True)\nprint(kurtosis_result)", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "x_new = np.linspace(-1, 1, 10)\ny_new = np.linspace(-2, 0, 10)\nx_new, y_new = np.meshgrid(x_new, y_new)\nx_new = x_new.flatten()\ny_new = y_new.flatten()\nresult = scipy.interpolate.interp2d(x_new, y_new, z, kind='cubic')\nresult(x_new, y_new)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a 2D array with the values of s and t\n    s_t = np.array([s, t]).T\n    # Create a 2D array with the values of z\n    z_values = np.array([z, z]).T\n    # Create a 2D array with the values of s and t\n    s_t_values = np.array([s, t]).T\n    # Create a 2D array with the values of z\n    z_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values_values = np.array([z, z]).T\n    # Create a 2D array with the values of z\n    z_values_values_values_values_values_values_values_values_values_values_values_values_values_", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list of points that are not in the voronoi regions\nextraPointsNotInVoronoi = [point for point in extraPoints if point not in vor.regions]\n# Create a list of points that are in the voronoi regions\nextraPointsInVoronoi = [point for point in extraPoints if point in vor.regions]\n# Create a list", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "# find the voronoi region for each extra point\nregion_indices = []\nfor point in extraPoints:\n    region_indices.append(vor.point_to_region(point))\nregion_indices = np.array(region_indices)\nprint(region_indices)\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "# Find the maximum size of the vectors\nmax_size = max([len(v) for v in vectors])\n\n# Create a sparse matrix with zeros\nresult = sparse.csc_matrix((max_size, len(vectors)), dtype=np.float64)\n\n# Fill the sparse matrix with the vectors\nfor i, v in enumerate(vectors):\n    result[i, :len(v)] = v\n\n# Fill the rest of the matrix with zeros\nresult[i+1, :len(vectors[i+1])] = vectors[i+1]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, origin=[0, 1])", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = np.array([M[row[0], column[0]], M[row[1], column[1]]])", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((x_new.size, 10, 10))\nfor i in range(x.size):\n    for j in range(x.size):\n        f = scipy.interpolate.interp1d(x, array[:, i, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u,o2,x)\nprint(prob)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(x = 2.5, u = 1, o2 = 3))\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a 2D DCT matrix\ndct_matrix = np.zeros((N, N))\n\n# Create a 2D DCT matrix with a 2D DCT matrix\ndct_matrix[0, 0] = 1\ndct_matrix[0, 1] = 0.5\ndct_matrix[0, 2] = 0.5\ndct_matrix[1, 0] = 0.5\ndct_matrix[1, 1] = 0.5\ndct_matrix[1, 2] = 0.5\ndct_matrix[2, 0] = 0.5\ndct_matrix[2, 1] = 0.5\ndct_matrix[2, 2] = 0.5\n\n# Create a 2D DCT matrix with a 2D DCT matrix\ndct_matrix_2 = np.zeros((N, N))\ndct_matrix_2[0, 0] = 1\ndct_matrix_2[0, 1] = 0.5\ndct_matrix_2[0, 2] = 0.5\ndct_matrix_2[1, 0] = 0.5\ndct_matrix_2[1, 1] = 0.5\ndct_matrix_2[1, 2] = 0.5\ndct_matrix_2[2, 0] = 0.5\ndct_matrix_2[2, 1] = 0.5\ndct_matrix_2[2, 2] = 0.5\n\n# Create a 2D DCT matrix with a 2D DCT matrix\ndct_matrix_3 = np.zeros((N, N))\ndct_matrix_3[0, 0] = 1\ndct_matrix_3[0, 1] = 0.5\ndct_matrix_3[0, 2] = 0.5\ndct_matrix_3[1, 0] = 0.5\ndct_matrix_3[1, 1] = 0.5\ndct_matrix_3[1, 2] = 0.5\ndct_matrix_3[2, 0] = 0.5\ndct_matrix_3[2, 1] = 0.5\ndct_matrix_3[2, 2] = 0.5\n\n# Create a 2D DCT matrix with a 2D DCT matrix\ndct_matrix_4 = np.zeros((N, N))\ndct_matrix_4[0, 0] = 1\ndct_matrix_4[0, 1] = 0.5\ndct_matrix_4[0, 2] = 0.5\ndct_matrix_4[1, 0] = 0.5\ndct_matrix_4[1, 1] = 0.5\ndct_matrix_4[1, 2] = 0.5\ndct_matrix_4[2, 0] = 0.5\ndct_matrix_4[2, 1] = 0.5\ndct_matrix_4[2, 2] = 0.5\n\n# Create a 2D DCT matrix with a 2D DCT matrix\ndct_matrix_5 = np.zeros((N, N))\ndct_matrix_5[0, 0] = 1\ndct_matrix_5[0, 1] = 0.5\ndct_matrix_5[0, 2] = 0.5\ndct_matrix_5[1, 0] = 0.5\ndct_matrix_5[1, 1] = 0.5\ndct_matrix_5[1, 2] = 0.5\ndct_matrix_5[2, 0] = 0.5\ndct_matrix_5[2, 1] = 0.5\ndct_matrix_5[2, 2] = 0.5\n\n# Create a 2D DCT matrix with a 2D DCT matrix\ndct_matrix_6 = np.zeros((N, N))\ndct_matrix_6[0, 0] = 1\ndct_matrix_6[0, 1] = 0.5\ndct_matrix_6[0, 2] = 0.5\ndct_matrix_6[1, 0] = 0", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "result = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result) ", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "result = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        result[i, j] = scipy.stats.binom.pmf(j, i, p)\nresult[i, j] = scipy.stats.binom.pmf(j, i, p)", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x, axis=0), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x, axis=0))\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=1)\n# create a new data frame with zscore and data\nresult = pd.DataFrame(data=np.c_[df.values, zscore],\n                      columns=np.append(df.columns, 'zscore'))\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=0)\n# round zscore to 3 decimal places\nzscore = np.round(zscore, 3)\n# create a new dataframe with zscore and original data\nresult = pd.DataFrame(np.c_[df.values, zscore], columns=np.concatenate((df.columns, ['zscore'])))\nresult", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y = np.arange(shape[0])\nx = np.arange(shape[1])\nmid = np.array([[0, 0], [shape[0]-1, shape[1]-1]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\nmid = np.array([[0, 0], [shape[0]-1, shape[1]-1]])\nresult = distance.cdist(np.dstack((y, x)), mid)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n    center = np.array([(shape[0] - 1) / 2, (shape[1] - 1) / 2])\n    return distance.cdist(np.dstack((y, x)), center)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint result", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return np.dot(a, x ** 2)\n\ndef residual(pars, a, y):\n    x = pars['x']\n    model = func(x, a)\n    return (y - model) ** 2\n\ndef main():\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = scipy.optimize.minimize(residual, fit_params, args=(a, y))\n    print(out.x)\n\nif __name__ == '__main__':\n    main()\nEND SOLUTION\n<code>\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x, a):\n    return np.dot(a, x ** 2)\n\ndef residual(pars, a, y):\n    x = pars.valuesdict()['x']\n    model = func(x, a)\n    return (y - model) ** 2\n\nout = scipy.optimize.minimize(residual, x0, args=(a, y), method='L-BFGS-B', bounds=[x_lower_bounds])\nprint(out.x)", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1\n\ndef dN1_dt_time_varying(t, N1):\n    if t < 2*np.pi:\n        return -100 * N1\n    else:\n        return 200 * N1\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n\nsol_time_varying = scipy.integrate.solve_ivp(fun=dN1_dt_time_varying, t_span=time_span, y0=[N0,])\nresult_time_varying = sol_time_varying.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "def fn(x):\n    return 2*x*c\n\nresult, error = scipy.integrate.quad(fn, low, high)\nI.append(result)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    def eqn(x):\n        return 2*x*c\n    result, error = scipy.integrate.quad(eqn, low, high)\n    return result\n\n# calculate the solution for each value of c\nc = [5, 10, 15]\nresult = [f(c=c[n]) for n in range(len(c))]\nprint(result)\n<code>\n# END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.to_dok()\nV += x", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "x = x.reshape(1, 1)\nV = V + x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "x = x + V.toarray()[0,0]\ny = y + V.toarray()[0,0]\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].toarray()\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    column /= length\n    # update the original column of the matrix\n    sa[:, col] = column\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    column /= length\n    # update the original column of the matrix\n    sa[:, col] = column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = np.where(a > 0, 1, 0)", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = np.where(a == 0, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest element to each centroid\nresult = np.argmin(distances, axis=1)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the minimum distance for each cluster\nmin_dist_index = scipy.spatial.distance.argmin(distances, axis=1)\n# extract the closest point to each cluster\nresult = data[min_dist_index]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the k-th closest element to each centroid\nclosest_elements = np.argpartition(distances, k, axis=0)[:k, :]\n# find the index of the k-th closest element in original data for each cluster\nresult = np.argwhere(closest_elements == np.arange(data.shape[0]))\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result = fsolve(lambda x, a: eqn(x, a, b), x0=x)\n    print(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for i in range(xdata.shape[0]):\n    result = fsolve(lambda x, a: eqn(x, a, xdata[i]), x0=xdata[i], args=(adata[i],))\n    result = np.sort(result)\n    print(result)", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# calculate the estimated parameters\nestimated_a, estimated_m, estimated_d = 1,1,1\n\n# calculate the estimated distribution\nestimated_distribution = bekkers(range_start, estimated_a, estimated_m, estimated_d)\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n\n# calculate the estimated distribution function\nestimated_distribution_function = sp.stats.distribution.pdf\n", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# estimate the parameters of the function\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n\n# calculate the estimated function\nestimated_function = bekkers(sample_data, estimated_a, estimated_m, estimated_d)\n\n# calculate the K-S test\nkstest_result = stats.kstest(estimated_function, 't')\n\n# check the p-value at 95% confidence level\np_value = kstest_result.pvalue\n\n# check whether the K-S test can reject the null hypothesis\nif p_value < 0.05:\n    result = True\nelse:\n    result = False\nprint(result)", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def rolling_integral(df):\n    return integrate.trapz(df['A'], df['Time'])\n\nrolling_integral(df)", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "x = np.array([(2,2), (1,2), (2,3), (3,2), (2,1)])\ny = np.array([5,7,8,10,3])\neval = np.array([(2.7, 2.3)])\n\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "# calculate the number of categories\nn_categories = a.max() + 1\n\n# calculate the weights\nweights = np.ones(n_categories) / n_categories\n\n# calculate the maximum likelihood estimator\nmax_likelihood_estimator = sciopt.maximum_likelihood_estimator(a.values, weights)\n\n# print the maximum likelihood estimator\nprint(max_likelihood_estimator)", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "# find the absolute values of the array\nabs_arr = np.abs(arr)\n# find the absolute values of the neighbouring n elements\nabs_neighbour_n = np.abs(arr[n:])\n# find the absolute values of the neighbouring n elements forwards\nabs_neighbour_n_forward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards\nabs_neighbour_n_backward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n# find the absolute values of the neighbouring n elements forwards and backwards\nabs_neighbour_n_forward_backward = np.abs(arr[n:n+2])\n# find the absolute values of the neighbouring n elements backwards and forwards\nabs_neighbour_n_backward_forward = np.abs(arr[n-2:n])\n#", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "# find the absolute values of the elements in the array\nabs_arr = np.abs(arr)\n# find the absolute values of the elements in the array forwards and backwards\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backwards\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backwards\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backwards\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backwards\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concatenate((abs_arr[:, 0], abs_arr[:, -1]))\nabs_arr_backward = np.concatenate((abs_arr[:, 0], abs_arr[:, 1]))\n# find the absolute values of the elements in the array forwards and backward\nabs_arr_forward = np.concaten", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "# Filter out the numerical columns\nnum_cols = df.select_dtypes(include=np.number).columns\n\n# Calculate z-score for each column\nz_scores = np.abs(stats.zscore(df[num_cols]))\n\n# Create a mask to remove outlying values\nmask = (z_scores < 3).all(axis=1)\n\n# Remove outlying values from the dataframe\ndf = df[mask]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\nprint(data1)", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                      columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\nprint(data1)", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    # Convert the data from Bunch to DataFrame\n    data_df = pd.DataFrame(data.data, columns=data.feature_names)\n    data_df['target'] = data.target\n    return data_df\n    ### END SOLUTION ###\n\n# Test the function\nprint(solve(data))\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "# Create a new dataframe with unique elements from the column\nunique_elements = np.unique(df['Col3'])\n\n# Create a new dataframe with zeros\nnew_df = pd.DataFrame(np.zeros((len(unique_elements), len(df))), columns=unique_elements, index=df.index)\n\n# Iterate over the unique elements and set the value to 1\nfor i, element in enumerate(unique_elements):\n    new_df[element] = df['Col3'].apply(lambda x: 1 if x == element else 0)\n\n# Drop the original column\nnew_df = new_df.drop(columns='Col3')\n\n# Rename the columns\nnew_df = new_df.rename(columns=lambda x: x.replace(' ', '_'))\n\n# Rename the index\nnew_df = new_df.rename(index=lambda x: x.replace(' ', '_'))\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "# Convert the list of strings to a one-hot-encoded array\none_hot_encoded = pd.get_dummies(df['Col3'].apply(lambda x: np.array(x).astype(str)))\n\n# Create a new dataframe with one-hot-encoded columns\ndf_out = pd.concat([df, one_hot_encoded], axis=1)\n\n# Remove the original column\ndf_out = df_out.drop(['Col3'], axis=1)\n\n# Print the new dataframe\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "# Get unique elements from last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# One-hot-encode the last column\nfor i, element in enumerate(unique_elements):\n    new_df[element] = np.where(df.iloc[:, -1] == element, 1, 0)\n\n# Remove last column\nnew_df = new_df.drop(columns=[unique_elements[-1]])\n\n# Add original columns to new dataframe\nnew_df[unique_elements] = df\n\n# Print new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "# Get the unique values in the last column\nunique_values = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with the unique values as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_values.shape[0])), columns=unique_values)\n\n# Iterate over the rows of the original dataframe\nfor i in range(df.shape[0]):\n    # Get the value of the last column\n    last_column_value = df.iloc[i, -1]\n    \n    # Iterate over the unique values in the last column\n    for j in range(unique_values.shape[0]):\n        # If the value in the last column is equal to the unique value, set the value to 1\n        if last_column_value == unique_values[j]:\n            new_df.iloc[i, j] = 1\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "# Get the unique values in the last column\nunique_values = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with the unique values as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_values.shape[0])), columns=unique_values)\n\n# Iterate over the rows of the original dataframe\nfor i in range(df.shape[0]):\n    # Get the value of the last column\n    last_column_value = df.iloc[i, -1]\n    \n    # Iterate over the unique values in the last column\n    for j in range(unique_values.shape[0]):\n        # If the value in the last column is equal to the unique value, set the value to 1\n        if last_column_value == unique_values[j]:\n            new_df.iloc[i, j] = 1\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "# convert transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# merge transform_output and df_origin\nmerged_data = pd.concat([df_origin, transform_output], axis=1)\n\n# convert merged_data back to a DataFrame\nmerged_data = pd.DataFrame(merged_data.toarray())\n\n# print the merged data\nprint(merged_data)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "transform_output = transform_output.toarray()\ntransform_output = csr_matrix(transform_output)\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.DataFrame(transform_output.toarray())\ntransform_output = pd.Data", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # transform the transform_output back to a dataframe\n    transform_output = pd.DataFrame(transform_output.toarray())\n    # merge the transform_output back to the original dataframe\n    result = pd.concat([df, transform_output], axis=1)\n    return result\n    ### END SOLUTION ###\n\n# call the function\nresult = solve(df_origin, transform_output)\nprint(result)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete any step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\n# delete the first step\ndel clf.steps[0]\n# insert a new step\nclf.steps.append(('new_step', SVC()))\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a new step\nnew_estimator = ('reduce_dim', PCA())\nclf.steps.append(new_estimator)\n\n# Insert a new step at a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures())\nclf.steps.append((new_estimator[0], new_estimator[1]))\n\n# Insert a new step with a specific position\nnew_estimator = ('svm', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Insert a new step with a specific name\nnew_estimator = ('poly', PolynomialFeatures", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\n# Insert a new step\nnew_estimator = ('new_estimator', SVC())\nclf.steps.append(new_estimator)\n\n# Insert a new step at a specific position\nnew_estimator = ('new_estimator', SVC())\nposition = 1\nclf.steps.insert(position, new_estimator)\n\n# Delete a step\nposition = 0\ndel clf.steps[position]\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\n# Insert ('t1919810', PCA()) right before 'svdm'\nclf.steps.insert(1, ('t1919810', PCA()))\n\n# Insert ('t1919810', PolynomialFeatures()) right before 'svm'\nclf.steps.insert(2, ('t1919810', PolynomialFeatures()))\n\n# Insert ('t1919810', SVC()) right before 'reduce_dim'\nclf.steps.insert(0, ('t1919810', SVC()))\n\n# Insert ('t1919810', PolynomialFeatures()) right before 'svm'\nclf.steps.insert(2, ('t1919810', PolynomialFeatures()))\n\n# Insert ('t1919810', SVC()) right before 'reduce_dim'\nclf.steps.insert(0, ('t1919810', SVC()))\n\n# Insert ('t1919810', PolynomialFeatures()) right before 'svm'\nclf.steps.insert(2, ('t1919810', PolynomialFeatures()))\n\n# Insert ('t1919810', SVC()) right before 'reduce_dim'\nclf.steps.insert(0, ('t1919810', SVC()))\n\n# Insert ('t1919810', PolynomialFeatures()) right before 'svm'\nclf.steps.insert(2, ('t1919810', PolynomialFeatures()))\n\n# Insert ('t1919810', SVC()) right before 'reduce_dim'\nclf.steps.insert(0, ('t1919810', SVC()))\n\n# Insert ('t1919810', PolynomialFeatures()) right before 'svm'\nclf!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# Define the parameters to be searched\nparamGrid = {\n    \"early_stopping_rounds\": [10, 20, 30, 40],\n    \"learning_rate\": [0.01, 0.001, 0.0001],\n    \"max_depth\": [3, 4, 5],\n    \"n_estimators\": [100, 200, 300]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the time series split\ntime_series_split = TimeSeriesSplit(n_splits=5)\n\n# Define the grid search\ngrid_search = GridSearchCV(model, paramGrid, cv=time_series_split.get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n\n# Fit the grid search\ngrid_search.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n# Get the best score\nbest_score = grid_search.best_score_\n\n# Get the best iteration\nbest_iteration = grid_search.best_iteration_\n\n# Get the best estimator\nbest_estimator = grid_search.best_estimator_\n\n# Get the best score of the best estimator\nbest_estimator_score = best_estimator.score(testX, testY)\n\n# Print the best score\nprint(\"Best score: \", best_score)\n\n# Print the best estimator\nprint(\"Best estimator: \", best_estimator)\n\n# Print the best estimator score\nprint(\"Best estimator score: \", best_estimator_score)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"learning_rate\" : [0.01, 0.1, 0.001],\n    \"max_depth\" : [3, 4, 5],\n    \"n_estimators\" : [100, 200, 300],\n    \"alpha\" : [0.01, 0.1, 0.001],\n    \"lambda\" : [1, 2, 3]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the time series split\ntime_split = TimeSeriesSplit(n_splits=3)\n\n# Define the grid search\ngrid_search = GridSearchCV(model, paramGrid, verbose=1, cv=time_split.get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n\n# Fit the grid search\ngrid_search.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n# Get the score\nscore = grid_search.score(testX, testY)\n\n# Get the prediction\nprediction = grid_search.predict(testX)\n\n# Print the score and prediction\nprint(\"Score: \", score)\nprint(\"Prediction: \", prediction)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "for train_index, test_index in cv:\n    logreg.fit(X[train_index], y[train_index])\n    proba.append(logreg.predict_proba(X[test_index]))\nproba = np.array(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_predicted = scaler.inverse_transform(scaled[:, 0])\n    # check score\n    score = np.mean((t_predicted - data[:, 0])**2)\n    return t_predicted, score\n\nt_predicted, score = solve(scaled, scaler, scaled)\nprint(score)\nprint(t_predicted)\n", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = scores.mean()\nprint(f'Name model: {model_name}, Mean score: {mean_score}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = np.mean(scores)\nprint(f'Name model: {model_name}, Mean score: {mean_score}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\n# Define the parameter grid\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\n\n# Define the GridSearchCV\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n# Use the best parameters to create a new BaggingClassifier\nnew_bc = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=best_params['base_estimator__max_depth']),\n                            n_estimators=20,\n                            max_samples=best_params['max_samples'],\n                            max_features=0.5)\n\n# Fit the new BaggingClassifier\nnew_bc.fit(X_train, y_train)\n\n# Get the probabilities\nproba = new_bc.predict_proba(X_test)", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef solve():\n    tfidf = TfidfVectorizer(preprocessor=preprocess)\n    tfidf.fit([\"This is a test\"])\n    print(tfidf.transform([\"This is a test\"]))\n\nsolve()\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# example variable\ntext = \"This is a test.\"\n\n# create a data frame\ndf = pd.DataFrame([text], columns=[\"text\"])\n\n# apply tfidf vectorizer\ntfidf.fit(df[\"text\"])\n\n# print the tfidf vectorizer\nprint(tfidf.get_feature_names())\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "# Apply preprocessing.scale to the data\ndata_scaled = preprocessing.scale(data)\n\n# Create a new DataFrame from the scaled data\ndf_out = pd.DataFrame(data_scaled, columns=data.columns)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "# scale the data\nscaled_data = preprocessing.scale(data)\n\n# create a new DataFrame with the scaled data\ndf_out = pd.DataFrame(scaled_data, columns=data.columns)\n\n# print the new DataFrame\nprint(df_out)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.steps[1][1].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.get_params()['model']['alpha']\n", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "# Get the feature names from the model\nfeature_names = X.columns\n# Create a DataFrame with the feature names and their importance\nfeature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': clf.feature_importances_})\n# Save the feature names and their importance to a new CSV file\nfeature_importance_df.to_csv('feature_importance.csv', index=False)\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "# get feature names\nfeature_names = X.columns\n\n# get feature importances\nimportances = clf.feature_importances_\n\n# create a new dataframe with feature names and importances\nfeature_importances_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n\n# sort feature importances in descending order\nfeature_importances_df = feature_importances_df.sort_values('importance', ascending=False)\n\n# get feature names from sorted dataframe\ncolumn_names = feature_importances_df['feature'].tolist()\n\nprint(column_names)\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns.tolist()\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    return centers[p]\n    ### END SOLUTION ###\n\n# call the function\nsamples = get_samples(p, X, km)\nprint(samples)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"rbf\")\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"rbf\")\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR object\nsvm = SVR(kernel='poly', degree=2)\n\n# fit the model\nsvm.fit(X, y)\n\n# predict the output\npredict = svm.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"poly\", degree=2)\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarities(query, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    matrix = np.zeros((len(documents), len(documents)))\n    for i, doc in enumerate(documents):\n        matrix[i, :] = np.dot(query_tfidf, tfidf.transform([doc]))\n    return matrix\n\ncosine_similarities_of_queries = np.zeros((len(queries), len(documents)))\nfor i, query in enumerate(queries):\n    cosine_similarities_of_queries[i, :] = get_cosine_similarities(query, documents)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarities(query, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    matrix = np.zeros((len(documents), len(documents)))\n    for i, doc in enumerate(documents):\n        matrix[i, :] = np.dot(query_tfidf, tfidf.transform([doc]))\n    return matrix\n\ncosine_similarities_of_queries = get_cosine_similarities(queries, documents)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_matrix = tfidf.transform(documents)\n    tfidf_matrix = tfidf_matrix.toarray()\n    tfidf_matrix = np.array(tfidf_matrix)\n    tfidf_matrix = tfidf_matrix.tolist()\n    tfidf_matrix = np.array(tfidf_matrix)\n    tfidf_matrix = tfidf_matrix.tolist()\n    tfidf_matrix = np.array(tfidf_matrix)\n    tfidf_matrix = tfidf_matrix.tolist()\n    return tfidf_matrix\n    ### END SOLUTION ###\n\n# Test the function\nsolve(queries, documents)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "# Convert the features to a 2D array\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([[1 if feature in feature_list else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "f = np.array(f)\nnew_f = np.zeros((f.shape[0], f.shape[1]))\nfor i in range(f.shape[0]):\n    for j in range(f.shape[1]):\n        new_f[i, j] = f[i, j]\nnew_f = new_f.astype(int)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f == 0, 0, 1)\nnew_f = np.where(new_f == 1, 1, 0)\nnew_f = np.where(new_f", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "# Convert the features to a 2D array\nnew_features = np.array([[0, 0, 0, 1, 1, 1],\n                         [1, 0, 1, 0, 0, 0],\n                         [0, 0, 1, 1, 1, 1]])\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the feature values to one-hot encoding\n    one_hot_encoded_features = np.zeros((len(features_array), len(features_array[0])))\n    for i in range(len(features_array)):\n        for j in range(len(features_array[i])):\n            one_hot_encoded_features[i, j] = features_array[i, j]\n    # Convert the one-hot encoded features to a 2D array\n    one_hot_encoded_features_array = np.array(one_hot_encoded_features)\n    # Return the one-hot encoded features array\n    return one_hot_encoded_features_array\n    ### END SOLUTION ###\n\n# Test the function\nfeatures = [\n    ['f1', 'f2', 'f3'],\n    ['f2', 'f4', 'f5', 'f6'],\n    ['f1', 'f2']\n]\nprint(solve(features))\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "# Convert the list of features to a 2D array\nnew_features = np.array(features).astype(int)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.array([np.array(feature).astype(int) for feature in features])\n\n# Convert the ", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the data matrix\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2)\n\n# Fit the clustering model to the data\nclustering.fit(data_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from the data\ndistance_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2, distance_threshold=0.1)\n\n# Fit the clustering model to the distance matrix\nclustering.fit(distance_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the 3-d array\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2)\n\n# Fit the clustering model\nclustering.fit(simM)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage\n\n# calculate the linkage matrix\nlinkage_matrix = linkage(data_matrix, method='average')\n\n# get the labels\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maxclusters')\ncluster_labels = cluster_labels.tolist()\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage\n\n# calculate the linkage matrix\nlinkage_matrix = linkage(data_matrix, method='average')\n\n# get the labels of the clusters\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maxclusters')\ncluster_labels = cluster_labels.tolist()\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a 2-d array from the similarity matrix\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.linkage(simM, method='average')\n\n# Print the cluster labels\nprint(cluster_labels)", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import YeoJohnson\n\ntransformer = YeoJohnson()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnson\n\n# Create YeoJohnson object\ntransformer = YeoJohnson()\n\n# Apply transformation to data\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer()\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\ntransformed_text = np.array(transformed_text, dtype=np.int32)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop([-1], axis=1), dataset[-1], test_size=0.2, random_state=42)\n\n# define x and y\nx_train = x_train.values\ny_train = y_train.values\nx_test = x_test.values\ny_test = y_test.values\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features and the target\nx = data.drop(columns=[data.columns[-1]])\ny = data[data.columns[-1]]\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset[:, :-1], dataset[:, -1], test_size=0.3, random_state=42)\n\n# define x and y\nx_train = x_train.values\ny_train = y_train.values\nx_test = x_test.values\ny_test = y_test.values\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": " ###\n    # split the dataset into training and testing sets\n    train_size = int(0.8 * len(data))\n    train_set = data[:train_size]\n    test_set = data[train_size:]\n\n    # define x and y\n    x_train = train_set[:, :-1]\n    y_train = train_set[:, -1]\n    x_test = test_set[:, :-1]\n    y_test = test_set[:, -1]\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION ###\n\n# call the function\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "mse_values = df['mse'].values\nlabels = KMeans(n_clusters=2, n_init=10).fit(mse_values.reshape(-1, 1)).predict(mse_values.reshape(-1, 1))\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "f1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2, n_init=10).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "# Create a LinearSVC object\nsvm = LinearSVC(penalty='l1', loss='l1', max_iter=10000, random_state=42)\n# Fit the model\nsvm.fit(X, y)\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[svm.get_support()]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "# Create a linear svm model\nsvm = LinearSVC(penalty='l1', loss='l1', max_iter=10000)\n\n# Fit the model\nsvm.fit(X, y)\n\n# Get the feature names\nfeature_names = np.asarray(vectorizer.get_feature_names())\n\n# Get the selected feature names\nselected_feature_names = feature_names[svm.get_support()]\n\nprint(selected_feature_names)\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    # Use LinearSVC with penalty='l1'\n    svc = LinearSVC(penalty='l1', loss='l1', max_iter=10000, random_state=42)\n    # Fit the model\n    svc.fit(X, y)\n    # Get the feature names\n    feature_names = np.asarray(vectorizer.get_feature_names())[svc.get_support()]\n    return feature_names\n    ### END SOLUTION ###\n\n# Test the solution\nsolve(corpus, y, vectorizer, X)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nprint(feature_names)\nprint(X.toarray())\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "series = np.array([]) # blank list to append results\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] # remove NaN values for each column to apply sklearn function\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y) # either this or the next line\n    m = slope.coef_[0]\n    series= np.concatenate((series, m), axis = 0)\nslopes = series\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "for col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    series= np.concatenate((series, m), axis = 0)\nslopes = series\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column\ndf['Sex'] = le.transform(df['Sex'])\n\n# Print the transformed data\nprint(df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column into a new column 'Sex_encoded'\ndf['Sex_encoded'] = le.transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ### END SOLUTION ###\n    return df\n\ntransformed_df = Transform(df)\nprint(transformed_df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION ###\n\n# Test the function\nnp_array = np.array([[1, 2], [3, 4]])\nprint(Transform(np_array))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close, ma50, ma100, ma200])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array([['dsa', '2'], ['sato', '3']])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:,:-1].astype(float)\ny = dataframe.iloc[:,-1]\nlogReg = LogisticRegression()\nlogReg.fit(X,y)\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the dataframe to numpy array\nX = dataframe.astype(float).values\n# Convert the class to numpy array\ny = dataframe.iloc[:, -1].values\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model\nlogReg.fit(X[:None], y)\n# Predict the class of the new animal\nnew_animal = np.array([[3.6, 600, 7, 3.35, 40, 20000, 975, 0, 0, 0, 0, \"Primary Scavenger\"]])\npredict = logReg.predict(new_animal)\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "train_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n\ntrain_dataframe = train_dataframe.sort_values(\"date\")\ntest_dataframe = test_dataframe.sort_values(\"date\")\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\ntrain_dataframe = train_dataframe.drop([\"date\"], axis=1)\ntest_dataframe = test_dataframe.drop([\"date\"], axis=1)\n\ntrain_dataframe = train_dataframe.values\ntest_dataframe = test_dataframe.values\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\ntrain_dataframe = np.array(train_dataframe)\ntest_dataframe = np.array(test_dataframe)\n\ntrain_dataframe = np.reshape(train_dataframe, (train_dataframe.shape[0], -1))\ntest_dataframe = np.reshape(test_dataframe, (test_dataframe.shape[0], -1))\n\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "train_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n\ntrain_dataframe = train_dataframe.sort([\"date\"])\ntest_dataframe = test_dataframe.sort([\"date\"])\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": " ###\n    # sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(\"date\")\n    # calculate the number of days in the dataframe\n    num_days = len(features_dataframe)\n    # calculate the number of days to be used for the test set\n    test_days = num_days * 0.8\n    # calculate the number of days to be used for the train set\n    train_days = num_days - test_days\n    # sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(\"date\")\n    # split the dataframe into train and test set\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=test_days)\n    return train_dataframe, test_dataframe\n    ### END SOLUTION ###\n\n# call the function\nsolve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2', 'X3']] = scaler.fit_transform(df[['X2', 'X3']])\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "myData['new_A2'] = scaler.fit_transform(myData[['A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = GridSearch_fitted.fit(X_train, y_train)\nfull_results.best_score_\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = GridSearch_fitted.fit(X_train, y_train)\nfull_results.best_score_\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport sklearn\nfrom sklearn.ensemble import IsolationForest\n\n# Load the data\ndata = np.load('data.npy')\n\n# Fit the model\nif not hasattr(fitted_model, 'weights_'):\n    model = IsolationForest()\n    model.fit(data)\n    # Save the model\n    np.save('sklearn_model', model)\nelse:\n    # Load the model\n    model = np.load('sklearn_model.npy')\n\n# Use the model for predict\npredict_result = model.predict(data)\nprint(predict_result)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "# tokenize each description\ndf['description'] = df['description'].apply(lambda x: x.split())\n\n# transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(df['description'])\n\n# calculate cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = 1 - sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix)\n\n# print the result\nprint(cosine_similarity_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\n# Create a new optimizer with a new learning rate\nnew_lr = 0.001\nnew_optimizer = torch.optim.SGD(optim.parameters(), lr=new_lr)\n\n# Set the new optimizer as the current optimizer\noptim = new_optimizer\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n# Define the optimizer\noptimizer = torch.optim.SGD(optim.parameters(), lr=0.01)\n\n# Define the learning rate schedule\nlr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n\n# Update the learning rate\nlr_schedule.step()\n\n# Check the new learning rate\nprint(lr_schedule.get_lr())", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\n# Create a new optimizer with a new learning rate\nnew_lr = 0.0005\nnew_optimizer = torch.optim.SGD(optim.parameters(), lr=new_lr)\n\n# Set the new optimizer as the current optimizer\noptim = new_optimizer\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define the optimizer\noptimizer = torch.optim.SGD(optim.parameters(), lr=0.005)\n\n# Define the learning rate schedule\nlr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n\n# Update the learning rate\nlr_schedule.step()\n\n# Check the new learning rate\nprint(lr_schedule.get_lr())", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# convert the word2vec model to a numpy array\nembedding_matrix = word2vec.wv.vectors\n# convert the numpy array to a PyTorch tensor\nembedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n# create a embedding layer with the embedding tensor\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_tensor)\n# embed the input data using the embedding layer\nembedded_input = embedding_layer(input_Tensor)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    embedding_weights = torch.tensor(word2vec.wv.vectors)\n    embedding_weights = embedding_weights.reshape(embedding_weights.shape[0], -1)\n    embedding_weights = embedding_weights.to(input_Tensor.device)\n    embedding_weights = embedding_weights.unsqueeze(0)\n    embedding_weights = embedding_weights.repeat(input_Tensor.shape[0], 1)\n    embedding_weights = embedding_weights.reshape(input_Tensor.shape[0], -1)\n    return embedding_weights\n    ### END SOLUTION ###\n\n# Test the function\ninput_Tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(get_embedded_input(input_Tensor))\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "x = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "x = torch.rand(6,6)\npx = pd.DataFrame(x.numpy())\nprint(px)", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "A_log = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log]\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "A_logical = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical]\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "A_log = torch.ByteTensor([1, 1, 0])\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log]\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "A_log = torch.ByteTensor([0, 1, 0])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log]\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    ### END SOLUTION ###\n    return C\n\ndef load_data():\n    A_log = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_log, B\n\nsolve(A_log, B)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "A_log = torch.ByteTensor([0, 0, 1])\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log]\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "# Convert the index to a numpy array\nidx = idx.numpy()\n# Use numpy indexing to select the columns\nC = B[np.arange(B.shape[0])[idx]]", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    # Convert the numpy array to a torch tensor\n    t = torch.tensor(a, dtype=torch.float16)\n    return t\n    ### END SOLUTION ###\n\n# Test the function\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=np.object)\n\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "lens = np.array([3, 5, 4])\nmask = np.zeros((len(lens), max(lens)))\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\nmask = torch.from_numpy(mask)\nprint(mask)\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "lens = np.array([1, 9, 3, 5])\nmask = np.zeros((len(lens), 10))\nmask[np.arange(len(lens)), lens - 1] = 1\nmask = torch.from_numpy(mask)\nprint(mask)\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "lens = np.array([3, 5, 4])\nmask = np.zeros((len(lens), max(lens)))\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\nmask = torch.from_numpy(mask)\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    mask = np.zeros((len(lens), max(lens)))\n    for i, l in enumerate(lens):\n        mask[i, :l] = 1\n    return mask\n    ### END SOLUTION ###\n\n# Test the function\nlens = [3, 5, 4]\nprint(get_mask(lens))\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "# Create a 2D tensor with diagonal elements\ndiag_ele = torch.ones(2, 2)\n\n# Create a 2D tensor with index_in_batch * diag_ele\nindex_in_batch = torch.arange(10)\nindex_in_batch = index_in_batch.unsqueeze(1)\n\n# Create a 2D tensor with index_in_batch * diag_ele\nindex_in_batch_2D = index_in_batch * diag_ele\n\n# Create a 3D tensor with index_in_batch_2D\nindex_in_batch_3D = index_in_batch_2D.unsqueeze(2)\n\n# Print the 3D tensor\nprint(index_in_batch_3D)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    # Create a 3D tensor with the same shape as the input tensor\n    result = torch.zeros_like(t)\n    # Use torch.diag() to create a diagonal matrix\n    diagonal_matrix = torch.diag(t)\n    # Use torch.matmul() to multiply the diagonal matrix with the input tensor\n    result = torch.matmul(diagonal_matrix, t)\n    ### END SOLUTION ###\n    return result\n\n# Test the function\nt = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(Convert(t))\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\nab = torch.stack((a, b), 0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = torch.tensor(a)\nb = torch.tensor(b)\nab = torch.stack((a, b), 0)\nprint(ab.shape)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    # stack the tensors a and b along the first dimension\n    # then stack the result with a and b along the second dimension\n    # ### END SOLUTION ###\n    return torch.stack((a, b), 0)\n\n# call the function with a and b as input\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[7, 8, 9]])\n\n# print the result\nprint(solve(a, b))\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[ : , lengths : , : ] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[ : , lengths : , : ] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    return torch.tensor(lt)\n    ### END SOLUTION ###\n\n# Call the function\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = torch.tensor([0.3, -0.5, 0.2])", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = torch.tensor([t[i] for i in idx])", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = torch.tensor([-0.2, 0.1, -0.4])", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "result = x[np.arange(x.shape[0]), ids]", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "result = x[np.arange(x.shape[0]), np.arange(x.shape[1]), ids]", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask with the indices of the selected elements\nmask = np.where(x[:, 0] == 1, 1, 0)\n\n# Gather the selected slices\nresult = np.where(mask == 1, x[mask == 1, 1], 0)\n\n# Convert the result to a tensor\nresult = torch.tensor(result)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "# Convert the softmax output to a numpy array\nsoftmax_output = softmax_output.numpy()\n\n# Find the maximum value in each row\nmax_values = np.max(softmax_output, axis=1)\n\n# Create a tensor with the maximum value\nmax_tensor = torch.tensor(max_values, dtype=torch.int64)\n\n# Create a tensor with the class index of the maximum value\nmax_class = torch.argmax(softmax_output, axis=1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n# Create a tensor with the class index of the maximum value\nmax_class = max_class.unsqueeze(1)\n\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "# Convert the softmax output to a numpy array\nsoftmax_output = softmax_output.numpy()\n\n# Find the maximum value in each row\nmax_values = np.max(softmax_output, axis=1)\n\n# Create a tensor with the maximum value\nmax_tensor = torch.tensor(max_values, dtype=torch.int32)\n\n# Create a tensor with the class index of the maximum value\nmax_class = torch.argmax(softmax_output, axis=1)\n\n# Create a tensor with the maximum value and the class index\nmax_tensor = torch.stack([max_tensor, max_class], axis=1)\n\n# Convert the tensor to a numpy array\nmax_tensor = max_tensor.numpy()\n\n# Print the result\nprint(max_tensor)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "# Convert the softmax output to a numpy array\nsoftmax_output = softmax_output.numpy()\n\n# Find the index of the minimum value in each row\ny = np.argmin(softmax_output, axis=1)\n\n# Convert the numpy array to a tensor\ny = torch.from_numpy(y)\n\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    # Find the maximum value in each row\n    max_values = np.argmax(softmax_output, axis=1)\n    # Create a tensor with the maximum value\n    max_tensor = torch.tensor(max_values, dtype=torch.int64)\n    # Return the tensor\n    return max_tensor\n    ### END SOLUTION ###\n\n# Test the function\nsoftmax_output = np.array([[0.2, 0.1, 0.7], [0.6, 0.2, 0.2], [0.1, 0.8, 0.1]])\nprint(solve(softmax_output))\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "# Find the maximum value in each row\nmax_values = torch.max(softmax_output, 1)\n# Find the index of the maximum value\nmax_index = torch.argmax(softmax_output, 1)\n# Create a tensor with the maximum value and its index\ny = torch.stack([max_values, max_index], 1)\nreturn y", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "# Convert labels to one-hot encoding\nlabels = F.one_hot(labels, num_classes=7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 3, 2, 1).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7)\n# Convert labels to one-hot encoding\nlabels = labels.permute(0, 2, 1, 3).contiguous().view(-1, 7", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = np.sum(A == B)", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = np.sum(A == B)", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = np.sum(A != B)", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    return np.sum(A == B)\n    ### END SOLUTION ###\n\n# Test the function\nA = np.random.rand(1000, 1)\nB = np.random.rand(1000, 1)\nprint(Count(A, B))\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "x = A.shape[0]\ncnt_equal = np.sum(A[-x:] == B[-x:])", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = np.sum(A[-x:] != B[-x:])", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\n# Create a tensor with shape (1, 3, 10, 40, 1)\na = torch.randn(1, 3, 10, 40, 1)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split = torch.chunk(a, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_2 = torch.chunk(a_split, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_3 = torch.chunk(a_split_2, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_4 = torch.chunk(a_split_3, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_5 = torch.chunk(a_split_4, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_6 = torch.chunk(a_split_5, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_7 = torch.chunk(a_split_6, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_8 = torch.chunk(a_split_7, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_9 = torch.chunk(a_split_8, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_10 = torch.chunk(a_split_9, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_11 = torch.chunk(a_split_10, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_12 = torch.chunk(a_split_11, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_13 = torch.chunk(a_split_12, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_14 = torch.chunk(a_split_13, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_15 = torch.chunk(a_split_14, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_16 = torch.chunk(a_split_15, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_17 = torch.chunk(a_split_16, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_18 = torch.chunk(a_split_17, chunk_dim, dim=3)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\n# Create a tensor with shape (1, 3, 10, 40, 1)\na = torch.randn(1, 3, 10, 40, 1)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split = torch.chunk(a, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_2 = torch.chunk(a_split, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_3 = torch.chunk(a_split_2, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_4 = torch.chunk(a_split_3, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_5 = torch.chunk(a_split_4, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_6 = torch.chunk(a_split_5, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_7 = torch.chunk(a_split_6, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_8 = torch.chunk(a_split_7, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_9 = torch.chunk(a_split_8, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_10 = torch.chunk(a_split_9, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_11 = torch.chunk(a_split_10, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_12 = torch.chunk(a_split_11, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_13 = torch.chunk(a_split_12, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_14 = torch.chunk(a_split_13, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_15 = torch.chunk(a_split_14, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_16 = torch.chunk(a_split_15, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_17 = torch.chunk(a_split_16, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_18 = torch.chunk(a_split_17, chunk_dim, dim=2)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\na_split_", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output = torch.where(mask == 1, clean_input_spectrogram, output)", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output = torch.where(mask == 0, clean_input_spectrogram, output)", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x))\nmin_y = torch.min(torch.abs(y))\nmin_x_y = torch.min(min_x, min_y)\nsigned_min = torch.where(min_x < min_y, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\nsigned_max = torch.where(x > y, sign_x * max_x, sign_y * max_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x), torch.abs(y))\n    min_y = torch.min(torch.abs(x), torch.abs(y))\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, min_x_sign, min_y_sign)\n    return min_x_sign\n\n# solve(x, y)\n## END SOLUTION ##\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\nprint(f\"Confidence score: {confidence_score}, Class name: {class_name}\")\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "a = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\na = a[:, :-1]\nb = b[:, 1:]\n\nresult = torch.stack([a, b], dim=1)\n\nresult = result.mean(dim=1)\n\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a[:, :-1]\n    b = b[:, 1:]\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(b.shape[0], 1)\n    b = b.repeat(a.shape[0], 1)\n    a = a.reshape(-", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "t = torch.arange(8).reshape(1,4,2).float()\nnew = torch.tensor([[0., 0., 0.,0.]])\nr = torch.stack([t,new])\nprint(r)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "t = torch.tensor([[1, 2], [3, 4]])\nnew = torch.tensor([[0, 0, 0, 0]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
