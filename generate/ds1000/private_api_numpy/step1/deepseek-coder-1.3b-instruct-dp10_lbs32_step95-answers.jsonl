{"id": 0, "code": "\nresult = df.reindex(List)\nresult", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "\nresult = df.loc[List]\nresult['Type'] = df['Type'].values[List]\nresult", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a mask for the rows where the value in column Qu1 is not in the value_counts() of column Qu1\nmask = df['Qu1'].map(df['Qu1'].value_counts()) < 2\n\n# Apply the mask to the column Qu1\ndf.loc[mask, 'Qu1'] = 'other'\n\n# Create a mask for the rows where the value in column Qu2 is not in the value_counts() of column Qu2\nmask = df['Qu2'].map(df['Qu2'].value_counts()) < 2\n\n# Apply the mask to the column Qu2\ndf.loc[mask, 'Qu2'] = 'other'\n\n# Create a mask for the rows where the value in column Qu3 is not in the value_counts() of column Qu3\nmask = df['Qu3'].map(df['Qu3'].value_counts()) < 2\n\n# Apply the mask to the column Qu3\ndf.loc[mask, 'Qu3'] = 'other'\n\nresult = df\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a mask for the values in Qu1 that have at least 3 occurrences\nmask = df['Qu1'].apply(lambda x: pd.value_counts(df.Qu1) >= x)\n\n# Apply the mask to the Qu1 column\ndf['Qu1'] = df['Qu1'].where(mask, 'other')\n\n# Create a mask for the values in Qu2 that have at least 3 occurrences\nmask = df['Qu2'].apply(lambda x: pd.value_counts(df.Qu2) >= x)\n\n# Apply the mask to the Qu2 column\ndf['Qu2'] = df['Qu2'].where(mask, 'other')\n\n# Create a mask for the values in Qu3 that have at least 3 occurrences\nmask = df['Qu3'].apply(lambda x: pd.value_counts(df.Qu3) >= x)\n\n# Apply the mask to the Qu3 column\ndf['Qu3'] = df['Qu3'].where(mask, 'other')\n\nresult = df\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "\n    for col in df.columns:\n        counts = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: 'other' if x in counts[counts >= 2].index else x)\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a mask for the values in Qu1 that have at least 3 occurrences\nmask = df['Qu1'].apply(lambda x: pd.value_counts(df.Qu1) >= x)\n\n# Apply the mask to the Qu1 column\ndf['Qu1'] = df['Qu1'].where(mask, 'other')\n\n# Create a mask for the values in Qu2 and Qu3 that have at least 2 occurrences\nmask = df['Qu2'].apply(lambda x: pd.value_counts(df.Qu2) >= x) | df['Qu3'].apply(lambda x: pd.value_counts(df.Qu3) >= x)\n\n# Apply the mask to the Qu2 and Qu3 column\ndf['Qu2'] = df['Qu2'].where(mask, 'sausage')\ndf['Qu3'] = df['Qu3'].where(mask, 'sausage')\n\nresult = df\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a dictionary to map the values\nmap_dict = {'apple': 'other', 'potato': 'other', 'cheese': 'other', 'banana': 'other', 'egg': 'egg'}\n\n# Apply the map dictionary to the columns\ndf['Qu1'] = df['Qu1'].map(map_dict)\ndf['Qu2'] = df['Qu2'].map(map_dict)\ndf['Qu3'] = df['Qu3'].map(map_dict)\n\n# Create a dictionary to map the values for the second column\nmap_dict_2 = {'banana': 'other', 'apple': 'other', 'sausage': 'other'}\n\n# Apply the map dictionary to the columns\ndf['Qu2'] = df['Qu2'].map(map_dict_2)\ndf['Qu3'] = df['Qu3'].map(map_dict_2)\n\n# Set the values in the columns to 'other' where the value is not in the map dictionary\ndf['Qu1'] = df['Qu1'].map(lambda x: 'other' if x not in map_dict.keys() else x)\ndf['Qu2'] = df['Qu2'].map(lambda x: 'other' if x not in map_dict_2.keys() else x)\ndf['Qu3'] = df['Qu3'].map(lambda x: 'other' if x not in map_dict_2.keys() else x)\n\nresult = df\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "\ndf['keep_if_dup'] = df['keep_if_dup'].map({'Yes': True, 'No': False})\nresult = df.drop_duplicates(subset='url', keep='first')\nresult = result[['id', 'url', 'keep_if_dup']]\nresult['keep_if_dup'] = result['keep_if_dup'].map({True: 'Yes', False: 'No'})\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "\nresult = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\ndf['keep_if_dup'] = df['keep_if_dup'].map({'Yes': True, 'No': False})\nresult = df.drop_duplicates(subset='url', keep='last')\nresult = result[result['keep_if_dup']]\nresult = result[['id', 'url', 'keep_if_dup']]\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\nresult = df.groupby('name')['v1', 'v2'].apply(lambda x: {k: {y: v} for k, y, v in zip(x[0], x[1], x[2])}).to_dict()\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": "\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\ndf = df.sort_values('datetime')\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# First, we need to parse the message to extract the key-value pairs\ndf['job'] = df['message'].apply(lambda x: x.split(': ')[1].split(', ')[0].split(' ', 1)[1] if 'job' in x else None)\ndf['money'] = df['message'].apply(lambda x: x.split(': ')[1].split(', ')[1].split(' ', 1)[1] if 'money' in x else None)\ndf['wife'] = df['message'].apply(lambda x: x.split(': ')[1].split(', ')[2].split(' ', 1)[1] if 'wife' in x else None)\ndf['group'] = df['message'].apply(lambda x: x.split(': ')[1].split(', ')[3].split(' ', 1)[1] if 'group' in x else None)\ndf['kids'] = df['message'].apply(lambda x: x.split(': ')[1].split(', ')[4].split(' ', 1)[1] if 'kids' in x else None)\n\n# Then, we drop the message column and rename the columns to the expected names\ndf = df.drop('message', axis=1)\ndf = df.rename(columns={'job': 'job', 'money': 'money', 'wife': 'wife', 'group': 'group', 'kids': 'kids'})\n\n# Finally, we print the result\nprint(df)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df['score'] * 10\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10 if df['product'].isin(products) else df['score']", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "\ndf['score'] = df['score'] * 10\n\nfor product_range in products:\n    df.loc[df['product'].between(product_range[0], product_range[1]), 'score'] = df.loc[df['product'].between(product_range[0], product_range[1]), 'score'] * 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\ndf['score'] = df['product'].apply(lambda x: 1 if x in products else df['score'].min() if x == products[0] else df['score'].max())\ndf['score'] = df['score'] / df['score'].max()\ndf", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.idxmax(axis=1)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "\ndf['category'] = df.idxmax(axis=1)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col]], axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf['#1'] = df['#1'].fillna(df['#1'].iloc[0])\ndf['#2'] = df['#2'].fillna(df['#2'].iloc[0])\n\nprint(df)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf['#1'] = df['#1'].fillna(df['#1'].iloc[0])\ndf['#2'] = df['#2'].fillna(df['#2'].iloc[0])\ndf = df.iloc[:-1, :]\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\n\ndf = df.iloc[1:]\n\nprint(df)\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\ndf['#1'] = df['#1'].shift(-1)\n\n# shift the last row of the first column up 1 row\ndf['#2'] = df['#2'].shift(1)\n\n# drop the first row of the first column\ndf = df.drop(df.index[0])\n\n# drop the last row of the first column\ndf = df.drop(df.index[-1])\n\ndf", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'X' for col in df.columns]", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col for col in df.columns]", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "\n# Create a list of all column names that end with 'X'\nx_columns = [col for col in df.columns if col.endswith('X')]\n\n# Create a list of all column names that don't end with 'X'\nnon_x_columns = [col for col in df.columns if not col.endswith('X')]\n\n# Create a dictionary to map the new column names\nnew_columns = {**{col: col + 'X' for col in x_columns}, **{col: 'XHeader' + col for col in non_x_columns}}\n\n# Rename the columns\ndf.rename(columns=new_columns, inplace=True)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "\ndef mean_columns(df):\n    return df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\n\nresult = mean_columns(df)\nprint(result)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' in col})\nresult['group_color'] = df.groupby('group')['group_color'].first()\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndef aggregate_columns(df):\n    df['val1'] = df.groupby('group')['val1'].transform('sum')\n    df['val2'] = df.groupby('group')['val2'].transform('mean')\n    df['val42'] = df.groupby('group')['val42'].transform('mean')\n    return df\n\nresult = aggregate_columns(df)\nresult\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "\nresult = df.loc[row_list, column_list].mean()", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "\nresult = df.loc[row_list, column_list].sum()", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "\nresult = df.loc[row_list, column_list].sum()\nresult = result.sort_values(ascending=False).drop_duplicates().index", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "\nresult = df.apply(lambda x: pd.Series(x.value_counts()), axis=0)\nresult = result.fillna(0)\nresult = result.astype(float)\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "\nresult = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "\n# Get the value counts for each column\nresult = df.apply(lambda x: x.value_counts())\n\n# Convert the result to a string\nresult = result.astype(str)\n\n# Remove the 'Name:' part from the result\nresult = result.replace('Name: ', '')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "df = df.iloc[[0, 1]]\nresult = df.merge(df.iloc[[2, 3]], left_index=True, right_index=True, how='outer')\nresult.columns = ['Sample type', 'Concentration', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\nresult", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0, 1]].append(df.iloc[[2, 3, 4, 5]])\nresult.columns = ['Nanonose', 'Unnamed: 1', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\nresult", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "\ndf['0'] = df['0'].replace(np.nan, 1)\ndf['1'] = df['1'].replace(np.nan, 2)\ndf['2'] = df['2'].replace(np.nan, 3)\n\ndf = df.sort_values(['0','1','2'])\ndf = df.reset_index(drop=True)\n\nresult = df\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "\ndf['0'] = df['0'].replace({np.nan: df['0'].mean()})\ndf['1'] = df['1'].replace({np.nan: df['1'].mean()})\ndf['2'] = df['2'].replace({np.nan: df['2'].mean()})\n\nresult = df\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "\ndf = df.where(df.notnull(), df.mean())\nresult = df.where(df.isnull(), df.mean())\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# Create a mask for rows where value is less than threshold\nmask = df['value'] < thresh\n\n# Apply the mask to the dataframe\ndf.loc[mask] = df.loc[mask].sum()\n\n# Set the index back to 'lab'\ndf.set_index('lab', inplace=True)\n\n# Set the index back to 'lab'\nresult = df\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\n# Select rows where value is less than threshold\ndf_below_thresh = df[df['value'] < thresh]\n\n# Calculate the average of the selected rows\navg_value = df_below_thresh.mean()\n\n# Create a new row with the average value\nnew_row = pd.Series({'value': avg_value})\n\n# Append the new row to the original dataframe\ndf = df.append(new_row)\n\n# Reset the index\ndf = df.reset_index()\n\n# Set the index back to 'lab'\ndf = df.set_index('lab')\n\nresult = df\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# Create a mask for the rows to be replaced\nmask = (df['value'] >= section_left) & (df['value'] <= section_right)\n\n# Replace the rows with the average of the values in the section\ndf.loc['X'] = df.loc[mask].mean()\n\n# Remove the rows to be replaced\ndf = df.loc[~mask]\n\n# Set the index back to the original lab column\ndf = df.reset_index()\n\nresult = df\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\nresult = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nfor col in df.columns:\n    result[f\"exp_{col}\"] = df[col]**2\n\nprint(result)\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\ndf['inv_A'] = 1 / df['A']\ndf['inv_B'] = 1 / df['B']\n\nresult = df\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndf = df.applymap(sigmoid)\ndf.columns = [f'sigmoid_{col}' for col in df.columns]\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the last occurrence of the column-wise maximum\nmax_idx = df.idxmax()\n\n# Get the index of the last occurrence of the column-wise maximum up to the minimum\nresult = max_idx[min_idx]\n\nprint(result)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the first occurrence of the column-wise maximum\nmax_idx = df.idxmax()\n\n# Get the index of the first occurrence of the column-wise maximum down to the minimum\nresult = max_idx[min_idx]\n\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "\n# Create a pivot table with the minimum and maximum dates for each user\npivot = df.pivot_table(index='user', columns='dt', aggfunc={'val': 'min', 'dt': 'min', 'user': 'first'})\n\n# Expand the pivot table with all dates and fill with 0\nresult = pd.pivot_table(df, index='user', columns='dt', aggfunc={'val': 'min', 'dt': 'min', 'user': 'first'}).reset_index().fillna(0)\n\n# Rename the columns\nresult.columns = ['user', 'dt', 'val']\n\n# Convert the dt column to datetime\nresult['dt'] = pd.to_datetime(result['dt'])\n\n# Sort the result\nresult = result.sort_values(['user', 'dt'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "\n# Create a new dataframe with all the dates from the minimum to the maximum date\nnew_df = pd.DataFrame({'user': df['user'].unique(), 'dt': pd.date_range(df['dt'].min(), df['dt'].max())})\n\n# Merge the new dataframe with the original dataframe\nresult = pd.merge(new_df, df, on=['user', 'dt'], how='left')\n\n# Fill the NaN values with 0\nresult = result.fillna(0)\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "\n# Create a new dataframe with the minimum and maximum dates\ndf_minmax = pd.DataFrame({'user': ['a','b'], 'dt': [df['dt'].min(), df['dt'].max()]})\n\n# Expand the dataframe with all dates\ndf_expanded = pd.merge(df_minmax, df, on='user', how='outer')\n\n# Fill the missing values with 233\ndf_expanded['val'] = df_expanded['val'].fillna(233)\n\n# Sort the dataframe by user and dt\ndf_expanded = df_expanded.sort_values(['user', 'dt'])\n\n# Set the dt column to datetime\ndf_expanded['dt'] = pd.to_datetime(df_expanded['dt'])\n\n# Set the user column to categorical\ndf_expanded['user'] = df_expanded['user'].astype('category')\n\n# Set the val column to integer\ndf_expanded['val'] = df_expanded['val'].astype(int)\n\n# Set the index to user and dt\ndf_expanded.set_index(['user', 'dt'], inplace=True)\n\nresult = df_expanded\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "\n# Create a pivot table with the minimum and maximum dates for each user\npivot = df.pivot_table(index='user', columns='dt', aggfunc={'val': 'min', 'dt': 'max'})\n\n# Reset the index and rename the columns\npivot = pivot.reset_index().rename(columns={'dt': 'max_dt', 'val': 'max_val'})\n\n# Merge the pivot table with the original data frame\nresult = pd.merge(df, pivot, on=['user', 'max_dt'], how='left')\n\n# Fill the missing values with the maximum value for the user\nresult['val'] = result.groupby('user')['val'].transform(max)\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "df['dt'] = df.groupby('user')['dt'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('max')\nresult = df.pivot(index='dt', columns='user', values='val')\nresult.index = result.index.strftime('%d-%b-%Y')\nresult", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "\ndf['name'] = df['name'].astype('category').cat.codes\ndf['name'] += 1\nresult = df\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "\ndf['a'] = df['a'].astype(str) + df['name'].astype(str)\ndf['a'] = df['a'].astype(int) + 1\n\nresult = df\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "\n    df['name'] = df['name'].astype('category').cat.codes\n    df['name'] += 1\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\n# Create a new column 'ID' with a unique ID for each name\ndf['ID'] = df['name'].factorize()[0]\n\n# Combine 'ID' and 'a' into a new column 'combined'\ndf['combined'] = df['ID'] + df['a']\n\n# Sort the dataframe by 'combined'\ndf = df.sort_values('combined')\n\n# Reset the index of the dataframe\ndf = df.reset_index(drop=True)\n\n# Drop the 'combined' column\ndf = df.drop(columns='combined')\n\n# Rename the 'ID' column to 'b'\ndf = df.rename(columns={'ID': 'b'})\n\n# Rename the 'a' column to 'c'\ndf = df.rename(columns={'a': 'c'})\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.sort_values(['user', 'date'])\ndf", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value', fill_value=0)\ndf = df.pivot_table(index='user', columns='02/12/15', values='value', fill_value=0)\ndf = df.pivot_table(index='user', columns='someBool', values='value', fill_value=0)\ndf = df.reset_index()\ndf = df.rename(columns={'user': 'others'})\ndf = df.rename(columns={'01/12/15': 'value'})\ndf = df.rename(columns={'02/12/15': 'value'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'value': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': '01/12/15'})\ndf = df.rename(columns={'01/12/15': '02/12/15'})\ndf = df.rename(columns={'02/12/15': 'someBool'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'value': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': '01/12/15'})\ndf = df.rename(columns={'01/12/15': '02/12/15'})\ndf = df.rename(columns={'02/12/15': 'someBool'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'value': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': '01/12/15'})\ndf = df.rename(columns={'01/12/15': '02/12/15'})\ndf = df.rename(columns={'02/12/15': 'someBool'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'value': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': '01/12/15'})\ndf = df.rename(columns={'01/12/15': '02/12/15'})\ndf = df.rename(columns={'02/12/15': 'someBool'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'value': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': '01/12/15'})\ndf = df.rename(columns={'01/12/15': '02/12/15'})\ndf = df.rename(columns={'02/12/15': 'someBool'})\ndf = df.rename(columns={'someBool': 'value'})\ndf = df.rename(columns={'value': 'others'})\ndf = df.rename(columns={'others': 'value'})\ndf = df.rename(columns={'value': 'user'})\ndf = df.rename(columns={'user': '01/12/15'})\ndf = df.", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.sort_values(['user', 'date'])\ndf", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "\ndf_sub = df[df['c'] > 0.5][columns]\nresult = df_sub.values", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "\ndf_sub = df.loc[df['c'] > 0.45, columns]\nresult = df_sub.values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    df_sub = df.loc[df['c'] > 0.5, columns]\n    df_sub['sum'] = df_sub[columns[0]] + df_sub[columns[1]]\n    return df_sub\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\n# Convert the date column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column that subtracts the date from the current date\ndf['date_diff'] = df['date'] - df['date'].shift(1)\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number of days between the current date and the previous date\ndf['days_diff'] = df['date_diff'].dt.days\n\n# Create a new column that counts the number", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column that calculates the difference in days between the current date and the date of the previous row\ndf['difference'] = df['date'].diff()\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days'] = (df['date'].diff().dt.days + 1)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff()\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_first'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_first'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n\n# Create a new column that calculates the number of days between the current date and the date of the previous row\ndf['days_since_last'] = df['days'].diff().fillna(0)\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\n# Convert the date column to datetime format\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the dataframe by date\ndf = df.sort_values('date')\n\n# Create a new column 'week' which represents the week number of the date\ndf['week'] = df['date'].dt.week\n\n# Create a new column 'week_end' which represents the end of the week\ndf['week_end'] = df['date'].dt.weekday\n\n# Create a new column 'week_start' which represents the start of the week\ndf['week_start'] = df['date'].dt.weekday\n\n# Create a new column 'week_start_date' which represents the start date of the week\ndf['week_start_date'] = df['date'].dt.date\n\n# Create a new column 'week_end_date' which represents the end date of the week\ndf['week_end_date'] = df['date'].dt.date\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_end_date_str' which represents the end date of the week in string format\ndf['week_end_date_str'] = df['week_end_date'].dt.strftime('%m-%d-%Y')\n\n# Create a new column 'week_start_date_str' which represents the start date of the week in string format\ndf['week_start_date_str'] = df['week_start_date'].dt.strftime('%m", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df['col1']\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\ndf['col1'] = df['col1'].rolling(3).mean().reset_index(0, drop=True)\nresult = df", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\ndf['col1'] = df['col1'].groupby(df.index // 4).sum()\nresult = df['col1'].values", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df['col1']\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column with the cumulative sum of the first 3 rows\ndf['cumulative_sum'] = df.iloc[:3].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['cumulative_avg'] = df.iloc[-2:].mean()\n\n# Create a new column with the cumulative sum of the last 3 rows\ndf['cumulative_sum_last'] = df.iloc[-3:].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['cumulative_avg_first'] = df.iloc[:2].mean()\n\n# Create a new column with the cumulative sum of the first 4 rows\ndf['cumulative_sum_first'] = df.iloc[:4].sum()\n\n# Create a new column with the average of the last 3 rows\ndf['cumulative_avg_last'] = df.iloc[-3:].mean()\n\n# Create a new column with the cumulative sum of the first 5 rows\ndf['cumulative_sum_first_last'] = df.iloc[:5].sum()\n\n# Create a new column with the average of the last 4 rows\ndf['cumulative_avg_last_first'] = df.iloc[-4:].mean()\n\n# Create a new column with the cumulative sum of the first 6 rows\ndf['cumulative_sum_first_last_first'] = df.iloc[:6].sum()\n\n# Create a new column with the average of the last 5 rows\ndf['cumulative_avg_last_first_first'] = df.iloc[-5:].mean()\n\n# Create a new column with the cumulative sum of the first 7 rows\ndf['cumulative_sum_first_last_first_first'] = df.iloc[:7].sum()\n\n# Create a new column with the average of the last 6 rows\ndf['cumulative_avg_last_first_first_first'] = df.iloc[-6:].mean()\n\n# Create a new column with the cumulative sum of the first 8 rows\ndf['cumulative_sum_first_last_first_first_first'] = df.iloc[:8].sum()\n\n# Create a new column with the average of the last 7 rows\ndf['cumulative_avg_last_first_first_first_first'] = df.iloc[-7:].mean()\n\n# Create a new column with the cumulative sum of the first 9 rows\ndf['cumulative_sum_first_last_first_first_first_first'] = df.iloc[:9].sum()\n\n# Create a new column with the average of the last 8 rows\ndf['cumulative_avg_last_first_first_first_first_first'] = df.iloc[-8:].mean()\n\n# Create a new column with the cumulative sum of the first 10 rows\ndf['cumulative_sum_first_last_first_first_first_first_first'] = df.iloc[:10].sum()\n\n# Create a new column with the average of the last 9 rows\ndf['cumulative_avg_last_first_first_first_first_first_first'] = df.iloc[-9:].mean()\n\n# Create a new column with the cumulative sum of the first 11 rows\ndf['cumulative_sum_first_last_first_first_first_first_first_first'] = df.iloc[:11].sum()\n\n# Create a new column with the average of the last 10 rows\ndf['cumulative_avg_last_first_first_first_first_first_first_first'] = df.iloc[-10:].mean()\n\n# Create a new column with the cumulative sum of the first 12 rows\ndf['cumulative_sum_first_last_first_first_first_first_first_first_first'] = df.iloc[:12].sum()\n\n# Create a new column with the average of the last 11 rows\ndf['cumulative_avg_last_first_first_first_first_first_first_first_first'] = df.iloc[-11:].mean()\n\n# Create a new column with the cumulative sum of the first 1", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the cumulative sum of the dataframe\ndf['cumulative_sum'] = df['col1'].cumsum()\n\n# Create a new column with the cumulative sum of the dataframe from the end\ndf['cumulative_sum_end'] = df['cumulative_sum'].iloc[-3:]\n\n# Create a new column with the cumulative sum of the dataframe from the head\ndf['cumulative_sum_head'] = df['cumulative_sum'].iloc[:3]\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end'] = df['cumulative_sum_end'].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head'] = df['cumulative_sum_head'].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end'] = df['cumulative_sum_end'].sum()\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head'] = df['cumulative_sum_head'].sum()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end_2'] = df['cumulative_sum_end'].iloc[-2:].sum()\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head_2'] = df['cumulative_sum_head'].iloc[:2].sum()\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end_2'] = df['cumulative_sum_end'].iloc[-2:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head_2'] = df['cumulative_sum_head'].iloc[:2].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end_3'] = df['cumulative_sum_end'].iloc[-3:].sum()\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head_3'] = df['cumulative_sum_head'].iloc[:3].sum()\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end_3'] = df['cumulative_sum_end'].iloc[-3:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head_3'] = df['cumulative_sum_head'].iloc[:3].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end_4'] = df['cumulative_sum_end'].iloc[-4:].sum()\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head_4'] = df['cumulative_sum_head'].iloc[:4].sum()\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end_4'] = df['cumulative_sum_end'].iloc[-4:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head_4'] = df['cumulative_sum_head'].iloc[:4].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end_5'] = df['cumulative_sum_end'].iloc[-5:].sum()\n\n# Create a new column with the sum of the dataframe from the head\ndf['sum_head_5'] = df['cumulative_sum_head'].iloc[:5].sum()\n\n# Create a new column with the average of the dataframe from the end\ndf['avg_end_5'] = df['cumulative_sum_end'].iloc[-5:].mean()\n\n# Create a new column with the average of the dataframe from the head\ndf['avg_head_5'] = df['cumulative_sum_head'].iloc[:5].mean()\n\n# Create a new column with the sum of the dataframe from the end\ndf['sum_end_6'] = df['cumulative", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].ffill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].bfill()\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill').fillna(method='bfill')\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum().mask(df['A'] == 0))\ndf['A'] = df['A'].fillna(df['A'].ne(0).cumsum())\ndf", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_days'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].astype(int)\ndf", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "\n    df['time'] = df['duration'].str.split(' ').str[0]\n    df['number'] = df['duration'].str.split(' ').str[1]\n    df['time_days'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1].astype(int)\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number']\ndf\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.all(df1[columns_check_list] == df2[columns_check_list], axis=1)", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "\nresult = [all(getattr(df1, column) == getattr(df2, column) for column in columns_check_list) for _ in range(len(df1))]\nresult", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='date')\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='datetime')\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "\n    df['date'] = pd.to_datetime(df['date'])\n    return df[['date', 'x', 'y']].values\n    ### END SOLUTION\n\ndf = pd.DataFrame({'id': ['abc', 'abc', 'abc'],\n                   'date': ['3/1/1994', '9/1/1994', '3/1/1995'],\n                   'x': [100, 90, 80],\n                   'y': [7, 8, 9]})\n\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.swaplevel(0, 1)\n    return df\n    ### END SOLUTION\n\ndf = pd.DataFrame({'x': ['abc'], 'y': [100, 90, 80], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df_long = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf_long['year'] = df_long['year'].astype(int)\ndf_long = df_long.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf_long.columns.name = None\ndf_long", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df_long = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf_long['year'] = df_long['year'].astype(int)\ndf_long = df_long.sort_values(['Country', 'year', 'Variable'], ascending=[True, True, True])\ndf_long = df_long.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf_long.columns.name = None\ndf_long = df_long[df_long['Variable'] != 'value']\ndf_long = df_long.rename(columns={'Country': 'Country', 'year': 'year', 'Variable': 'var1', 'value': 'var2'})\ndf_long", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf = df.loc[abs(df['Value_B']) < 1]\ndf = df.loc[abs(df['Value_C']) < 1]\ndf = df.loc[abs(df['Value_D']) < 1]\n\ndf = df.loc[abs(df['Value_A']) < 1]\ndf", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\ndf = df[(abs(df['Value_B']) > 1) | (abs(df['Value_C']) > 1) | (abs(df['Value_D']) > 1)]", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\ndf = df[(abs(df['Value_B']) > 1) | (abs(df['Value_C']) > 1) | (abs(df['Value_D']) > 1)]", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df['A'] = df['A'].str.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "\n    df.replace({'&AMP;': '&'}, regex=True, inplace=True)\n    return df\n\nf()\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df['A'] = df['A'].str.replace('&AMP;', '&').str.replace('&LT;', '<').str.replace('&GT;', '>')\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "\ndef validate_single_space_name(name: str) -> str:\n    if ' ' in name:\n        return name.split(' ')[0]\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: validate_single_space_name(x) if ' ' in x else None)\n\ndf", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x if validate_single_space_name(x) else None)\n\ndf\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^(.*) (.*)$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return match_obj.groups()\n    else:\n        return None\n\ndf['first_name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(validate_single_space_name))\n\ndf = df.drop(columns=['name'])\n\ndf", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = df2.merge(df1, on='Timestamp', how='left')\nresult['data'] = result['data'].fillna(result['stuff'])\nresult = result.drop(['stuff'], axis=1)\nresult = result.sort_values('Timestamp')\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1['Timestamp'], df2['Timestamp'], on='Timestamp', direction='nearest')\nresult['data'] = df2['stuff']\nresult = result.sort_values('Timestamp')\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= row['col3'] else row['col1'].max(), axis=1)\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.isnumeric(str(x)) else \"and\")\n\ndf['Field1'] = df['", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: [int(i) for i in str(x).split() if str(i).isnumeric()])\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) > 0 else None)\n\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "\n    result = []\n    for value in df['Field1']:\n        if not isinstance(value, int):\n            result.append(value)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "df = df.drop(test)\nresult = df", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\n# Calculate the pairwise distances between cars\ndf['distance'] = ((df['x'] - df['x'].shift())**2 + (df['y'] - df['y'].shift())**2).sqrt()\n\n# Find the nearest neighbour for each car\ndf['nearest_neighbour'] = df.groupby('car')['distance'].transform('min')\n\n# Calculate the average of the distances for each frame\ndf['average_distance'] = df.groupby('time')['distance'].mean()\n\ndf\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\n# First, we need to calculate the euclidean distance for each pair of cars\ndf['euclidean_distance'] = df.groupby('time')['car'].apply(lambda x: ((x - x.shift())**2).sum())\n\n# Then, we need to find the car with the highest euclidean distance for each time\ndf['farmost_neighbour'] = df.groupby('time')['euclidean_distance'].transform('idxmin')\n\n# Finally, we calculate the average of the distances for each time\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\ndf\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: ', '.join(x.dropna().values), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values), axis=1)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndf['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values[::-1]), axis=1)\ndf", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change the value of the Quantity column to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index[~df.index.isin(sample_df.index)]\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change the value of the ProductId column of these rows to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index[~df.index.isin(sample_df.index)]\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# First, calculate the number of rows to select for each user\nnum_rows = df['UserId'].value_counts()\n\n# Then, select the 20% of rows for each user\ndf_selected = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n\n# Set the Quantity column to zero\ndf_selected['Quantity'] = 0\n\n# Keep the indexes of the altered rows\ndf_selected.index = df.index[df.index.isin(df_selected.index)]\n\n# Print the resulting DataFrame\nprint(df_selected)\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "\n# First, create a boolean mask of duplicates\nmask = df.duplicated(subset=['col1','col2'], keep=False)\n\n# Then, create a new column 'index_original' that contains the original index of the first duplicate\ndf['index_original'] = df.loc[mask, 'col1'].index\n\n# Finally, select the rows of the original dataframe that are duplicates\nresult = df.loc[mask]\n\nEND SOLUTION\n!result\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\nresult = df.loc[df['index_original'] == True]\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "\n    df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\n    df['index_original'] = df['index_original'].apply(lambda x: df.index[x])\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "\n# First, create a mask to identify the duplicates\nmask = df.duplicated(subset=['col1','col2', '3col'], keep=False)\n\n# Then, create a new column 'index_original' that contains the index of the first duplicate\ndf['index_original'] = mask.where(mask, -1)\n\n# Finally, replace the indices of the duplicates with the index of the first duplicate\ndf['index_original'] = df['index_original'].replace(-1, df.index[0])\n\n# Now, df has a new column 'index_original' that contains the index of the first duplicate\nresult = df\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "\n# First, create a boolean mask for the duplicates\nmask = df.duplicated(subset=['col1','col2'], keep=False)\n\n# Then, create a new column 'index_original' that contains the index of the last duplicate\ndf['index_original'] = df.loc[mask, 'val'].drop_duplicates().index.values\n\n# Finally, select the rows with duplicates\nresult = df.loc[mask]\n\nprint(result)\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum counts\nresult = df.join(max_counts.set_index(['Sp', 'Mt'])['count'], on=['Sp', 'Mt'])\n\n# Finally, filter the result to only include rows where count equals the maximum count\nresult = result[result['count'] == result['count'].max()]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "\n# First, find the maximum count in each group\nmax_count = df.groupby(['Sp','Mt'])['count'].max()\n\n# Then, find the rows in the original DataFrame that have the maximum count in each group\nresult = df[df['count'] == max_count]\n\n# Finally, reset the index of the result\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\n# First, find the minimum count for each group\nmin_counts = df.groupby(['Sp', 'Mt'])['count'].min()\n\n# Then, find the rows where count equals the minimum count\nresult = df[df['count'] == min_counts]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# First, find the maximum count for each group\nmax_count = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum count dataframe on the 'Sp' and 'Value' columns\nresult = df.join(max_count.set_index(['Sp','Value']), on=['Sp','Value'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count\nresult = result[result['count'] == result['count'].max()]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "\n# Create a list of tuples where each tuple contains the first column level, the second column level, and the third column level\nvalue_vars = [('A', 'B', 'E'),\n              ('A', 'B', 'F'),\n              ('A', 'C', 'G'),\n              ('A', 'C', 'H'),\n              ('A', 'D', 'I'),\n              ('A', 'D', 'J')]\n\n# Use the list of tuples to melt the data frame\nresult = pd.melt(df, value_vars=value_vars)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "\n# Define the tuples of column levels\ntuples = [(a, b, c) for a in df.columns[0] for b in df.columns[1] for c in df.columns[2]]\n\n# Use pd.melt to melt the data frame\nresult = pd.melt(df, id_vars=tuples, value_vars=tuples)\n\n# Rename the columns\nresult.columns = ['variable_0', 'variable_1', 'variable_2', 'value']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] >= 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "\nresult = df.groupby('r')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index()\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "\ndef relationship(df):\n    result = []\n    for c1 in df.columns:\n        for c2 in df.columns:\n            if c1 != c2:\n                if df[c1].nunique() == 1:\n                    result.append(f'{c1} {c2} one-to-many')\n                elif df[c2].nunique() == 1:\n                    result.append(f'{c1} {c2} many-to-one')\n                else:\n                    result.append(f'{c1} {c2} many-to-many')\n    return result\n\nresult = relationship(df)\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "\ndef relationship(df):\n    relationships = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                relationships.append(f'{col1} {col2} {[\"one-2-one\", \"one-2-many\", \"many-2-one\", \"many-2-many\"][df[col1].nunique() > 1 and df[col2].nunique() > 1]}')\n    return relationships\n\nresult = relationship(df)\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "\ndef relationship(x, y):\n    if x.nunique() == 1:\n        return 'one-to-many'\n    elif y.nunique() == 1:\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\nresult = df.apply(relationship, axis=0)\nresult", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "\ndef relationship(x, y):\n    if x.nunique() == 1:\n        if y.nunique() == 1:\n            return 'one-2-one'\n        else:\n            return 'one-2-many'\n    else:\n        if y.nunique() == 1:\n            return 'many-2-one'\n        else:\n            return 'many-2-many'\n\n\nresult = df.apply(lambda x: relationship(x, df[x.name]), axis=1)\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "\n# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n\n# sort the dataframe by bank account\ndfiban_uniq = dfiban_uniq.sort_values(by='bank', ascending=False)\n\nresult = dfiban_uniq\nresult\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\nprint(\"Hello, World!\")\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\ndf['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\nresult = df.groupby('Family')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndf['Family'] = (df['SibSp'] == 1) & (df['Parch'] == 1)\ndf.loc[df['Family'] == 1, 'Family'] = 'Has Family'\ndf.loc[df['Family'] == 0, 'Family'] = 'No Family'\ndf.loc[df['Family'] == 2, 'Family'] = 'New Family'\ndf.loc[df['Family'] == 3, 'Family'] = 'Old Family'\n\nresult = df.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\nresult", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A')).reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list(), names=['Caps', 'Lower'])", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "\ndef std_mean(x):\n    return np.std(np.mean(x))\n\nresult = df.groupby('a')['b'].apply(std_mean).reset_index(name='mean')\nresult['std'] = df.groupby('a')['b'].apply(std_mean)\n\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "\ndef std_mean(x):\n    return np.std(np.mean(x))\n\nresult = df.groupby('b')['a'].apply(std_mean).reset_index(name='mean')\nresult['std'] = df.groupby('b')['a'].apply(std_mean)\n\nprint(result)\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "\nimport numpy as np\n\n# calculate softmax\ndf['softmax'] = np.exp(df['b']) / np.exp(df['b']).sum()\n\n# calculate min-max\ndf['min-max'] = (df['b'] - df['b'].min()) / (df['b'].max() - df['b'].min())\n\ndf", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "\n# First, we need to find the rows and columns that only contain zeros.\n# Then, we can remove these rows and columns from the dataframe.\n\n# Find the rows and columns that only contain zeros.\nrows_with_zeros = df.loc[:, df.eq(0).all()].index\ncolumns_with_zeros = df.loc[df.eq(0).all(), :].columns\n\n# Remove the rows and columns that only contain zeros.\nresult = df.drop(rows_with_zeros, axis=0).drop(columns_with_zeros, axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "df = df.loc[:, (df.sum() != 0)]\nresult = df.loc[(df.sum() != 0)]\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "\n# Create a mask where the maximum value in each row is 2\nmask = df.max(axis=1) == 2\n\n# Apply the mask to the dataframe\nresult = df[mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "\n# Create a mask where the maximum value in each row is 2\nmask = df.max(axis=1) == 2\n\n# Apply the mask to the dataframe\nresult = df.where(mask)\n\nprint(result)\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=[True, False])\nresult", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.reset_index().sort_values(by=['index', 0]).reset_index(drop=True)\ndf['index'] = df['index'].astype(str)\ndf['index'] = df['index'].str.replace('tf', '')\ndf['index'] = df['index'].str.replace('p', '')\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('1', '')\ndf['index'] = df['index'].str.replace('0', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('-', '')\ndf['index'] = df['index'].str.replace('e', '')\ndf['index'] = df['index'].str.replace('n', '')\ndf['index'] = df['index'].str.replace('u', '')\ndf['index'] = df['index'].str.replace('t', '')\ndf['index'] = df['index'].str.replace('d', '')\ndf['index'] = df['index'].str.replace('o', '')\ndf['index'] = df['index'].str.replace('a', '')\ndf['index'] = df['index'].str.replace('i', '')\ndf['index'] = df['index'].str.replace('g', '')\ndf['index'] = df['index'].str.replace('r', '')\ndf['index'] = df['index'].str.replace('y', '')\ndf['index'] = df['index'].str.replace('v', '')\ndf['index'] = df['index'].str.replace('s', '')\ndf['index'] = df['index'].str.replace('m', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "\nresult = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str) == df['A']]", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp', 'Mt'])['count'].max()\n\n# Then, find the rows where count equals the maximum count\nresult = df[df['count'] == max_counts]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "\n# First, find the maximum count in each group\nmax_count = df.groupby(['Sp','Mt'])['count'].max()\n\n# Then, find the rows in the original DataFrame that have the maximum count in each group\nresult = df[df['count'] == max_count]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\n# First, find the minimum count for each group\nmin_counts = df.groupby(['Sp', 'Mt'])['count'].min()\n\n# Then, find the rows where count equals the minimum count\nresult = df[df['count'] == min_counts]\n\n# Finally, reset the index\nresult = result.reset_index(drop=True)\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# First, find the maximum count for each group\nmax_count = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, join the original dataframe with the maximum count dataframe on the 'Sp' and 'Value' columns\nresult = df.join(max_count.set_index(['Sp','Value']), on=['Sp','Value'])\n\n# Finally, filter the result to only include rows where the count is equal to the maximum count\nresult = result[result['count'] == result['count'].max()]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'].fillna(pd.to_datetime('17/8/1926'), inplace=True)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    df['Date'] = df['Member'].map(dict)\n    ### END SOLUTION\n    return df\n\nresult = f(example_dict, example_df)\nprint(result)\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val']).Val.transform('count')\ndf", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_w'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val']).Val.transform('count')\ndf", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "\n# Create a new column 'B_zero' and 'C_zero' to store the count of zero values in column B and C\ndf['B_zero'] = df['B'].apply(lambda x: x == 0)\ndf['C_zero'] = df['C'].apply(lambda x: x == 0)\n\n# Create a new column 'B_non_zero' and 'C_non_zero' to store the count of non-zero values in column B and C\ndf['B_non_zero'] = df['B'].apply(lambda x: x != 0)\ndf['C_non_zero'] = df['C'].apply(lambda x: x != 0)\n\n# Print the result\nprint(df[['Date', 'B_zero', 'C_zero']])\nprint(df[['Date', 'B_non_zero', 'C_non_zero']])\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "\n# Create a new column 'Date' with the date format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create a new column 'Even' with the count of even numbers\ndf['Even'] = df['B'].apply(lambda x: x % 2 == 0)\n\n# Create a new column 'Odd' with the count of odd numbers\ndf['Odd'] = df['B'].apply(lambda x: x % 2 != 0)\n\n# Group by 'Date' and count the even and odd numbers\nresult1 = df.groupby('Date')['Even'].sum()\nresult2 = df.groupby('Date')['Odd'].sum()\n\nprint(result1)\nprint(result2)\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.mean)\nresult", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.sum)\nresult['mean'] = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.mean)\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "\nresult = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split(',').str[0].repeat(df['var2'].str.len())).compute()\n\nresult", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split(',').str[0].repeat(df['var2'].str.len())).compute()\n\nresult", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=2)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split('-').str[0]).compute()\n\nresult", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf['fips'] = df['fips'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "\ndf[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf['row'] = df['row'].str.split(' ', 2, expand=True)[2]\ndf = df[['fips', 'medi', 'row']]\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndf = df.set_index('Name').replace(0, 1).cumsum() / df.set_index('Name').replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df.replace(0, 1).cumsum() / df.replace(0, 1)\ndf = df.replace(0, 1).cumprod()\ndf = df", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndf['2001'] = df['2001'].replace(0, 1).cumprod() / df['2001'].replace(0, 1).cumprod()\ndf['2002'] = df['2002'].replace(0, 1).cumprod() / df['2002'].replace(0, 1).cumprod()\ndf['2003'] = df['2003'].replace(0, 1).cumprod() / df['2003'].replace(0, 1).cumprod()\ndf['2004'] = df['2004'].replace(0, 1).cumprod() / df['2004'].replace(0, 1).cumprod()\ndf['2005'] = df['2005'].replace(0, 1).cumprod() / df['2005'].replace(0, 1).cumprod()\ndf['2006'] = df['2006'].replace(0, 1).cumprod() / df['2006'].replace(0, 1).cumprod()\n\ndf", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "\n    df = df.set_index('Name')\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n    df = df.replace(0, 1)\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndf['2001'] = df['2001'].replace(0, 1)\ndf['2002'] = df['2002'].replace(0, 1)\ndf['2003'] = df['2003'].replace(0, 1)\ndf['2004'] = df['2004'].replace(0, 1)\ndf['2005'] = df['2005'].replace(0, 1)\ndf['2006'] = df['2006'].replace(0, 1)\n\ndf['2001'] = df['2001'].cumprod()\ndf['2002'] = df['2002'].cumprod()\ndf['2003'] = df['2003'].cumprod()\ndf['2004'] = df['2004'].cumprod()\ndf['2005'] = df['2005'].cumprod()\ndf['2006'] = df['2006'].cumprod()\n\ndf = df.replace(0, 1)\n\ndf", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)).apply(lambda x: 1 if x > 0 else 0)\ndf['label'] = df['label'].replace({0: 0, 1: 1})\ndf", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['label'].iloc[0] = 1\n\ndf\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: -1 if x < 0 else (1 if x > 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_timedelta(df['departure_time'].sub(df['arrival_time']))\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = pd.to_timedelta(df['Duration'])\ndf", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = pd.to_datetime(df['departure_time']) - pd.to_datetime(df['arrival_time'])\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'] - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "\nresult = df[df['key2'] == 'one'].groupby('key1').size()", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "\nresult = df[df['key2'].str.endswith('e')].groupby('key1').size()\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.idxmax()\nmin_result = df.idxmin()\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df['value'].mode()[0]\nmedian_result = df['value'].median()", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "\ndf = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "\ndf = df[(df['closing_price'] < 100) | (df['closing_price'] > 101)]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "df1 = df.groupby(\"item\", as_index=False)[\"diff\"].min()\nresult = df[df.groupby(\"item\")[\"diff\"].transform(\"min\") == df[\"diff\"]]", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df['SOURCE_NAME']\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n\n# Calculate the number of NaN values to be filled\nfill_count = nan_count // 2\n\n# Create a mask for the first half of the NaN values\nmask = df['Column_x'].isna()\n\n# Fill the first half of the NaN values with 0\ndf.loc[mask[:fill_count], 'Column_x'] = 0\n\n# Create a mask for the second half of the NaN values\nmask = df['Column_x'].isna()\n\n# Fill the second half of the NaN values with 1\ndf.loc[mask[fill_count:], 'Column_x'] = 1\n\ndf", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n\n# Calculate the number of values to replace\nreplace_count = int(nan_count * 0.3)\n\n# Create a mask for the first 30% of NaN values\nmask = df['Column_x'].isna()\n\n# Create a mask for the middle 30% of NaN values\nmask[nan_count - replace_count:nan_count] = False\n\n# Create a mask for the last 30% of NaN values\nmask[nan_count:] = False\n\n# Replace the NaN values with the appropriate values\ndf['Column_x'] = df['Column_x'].where(mask)\n\n# Fill the NaN values with the mode of the column\ndf['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0])\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# First, create a mask for the rows where the value is not NaN\nmask = df['Column_x'].notna()\n\n# Then, create a series of 0 and 1 with the same length as the original series\nseries = pd.Series([0, 1] * (len(df) // 2))\n\n# Finally, replace the NaN values in the original series with the series\ndf['Column_x'] = df['Column_x'].where(mask, series)\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame(list(zip(a['one'], b['one'])), columns=['one', 'two'])\nresult['two'] = list(zip(a['two'], b['two']))\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.DataFrame([(a.iloc[0, 0], b.iloc[0, 0], c.iloc[0, 0]), (a.iloc[1, 0], b.iloc[1, 0], c.iloc[1, 0])], columns=['one', 'two'])\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\n# Create a new dataframe with the same columns as a and b\nresult = pd.DataFrame(columns=['one', 'two'])\n\n# Iterate over the rows of a and b\nfor i in range(len(a)):\n    # Create a tuple from the corresponding elements in a and b\n    tuple_elements = (a.iloc[i, 0], b.iloc[i, 0])\n    # Append the tuple to the result dataframe\n    result = result.append(pd.Series(tuple_elements, index=['one', 'two']), ignore_index=True)\n\n# Fill the vacancy with np.nan\nresult = result.fillna(np.nan)\n\nprint(result)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "\nresult = df.assign(text=lambda x: ', '.join(x['text']))\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "\nresult = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame()\nresult.columns = ['text']\nresult", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "\nresult = df['text'].apply(lambda x: ', '.join(x.split(', '))).reset_index(drop=True)\nresult['text'] = result['text'].str.strip()\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].str.cat(sep=', ')\nresult", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "\nresult = df['text'].str.cat(sep='-')\nprint(result)\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "\nresult = pd.merge(df1, df2, on=['id', 'city', 'district'], how='outer')\nresult['date'] = result['date_x']\nresult = result.drop(['date_x'], axis=1)\nresult.columns = ['id', 'city', 'district', 'date', 'value']\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\ndf1['date'] = df1['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\ndf2['date'] = df2['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n\nresult = pd.concat([df1, df2], axis=0)\nresult = result.groupby(['id', 'city', 'district', 'date']).last().reset_index()\nresult['date'] = result['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\nresult", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\ndf2['city'] = df2['id'].map(df1.set_index('id')['city'])\ndf2['district'] = df2['id'].map(df1.set_index('id')['district'])\ndf2['date'] = df2['id'].map(df1.set_index('id')['date'])\n\ndf2 = df2.sort_values(['id', 'date'])\n\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values(['id', 'date'])\n\nresult\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer').sort_index()\n", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer').fillna(D['B'].values)", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "\n# Create a new column 'duplicated' in C that indicates if the values in 'A' are the same in C and D\nC['duplicated'] = C['A'].apply(lambda x: x in D['A'].values)\n\n# Merge C and D on 'A' and keep the duplicated column\nresult = pd.merge(C, D, on='A', how='outer').drop('A', axis=1)\n\n# Fill the NaN values in the duplicated column with False\nresult['duplicated'] = result['duplicated'].fillna(False)\n\n# Rename the duplicated column to 'dulplicated'\nresult.rename(columns={'duplicated': 'dulplicated'}, inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "\nresult = df.groupby('user')[['time', 'amount']].apply(list).reset_index()\nresult['time'] = result['time'].apply(lambda x: [(x[0], x[1])])\nresult['amount'] = result['amount'].apply(lambda x: [(x[0], x[1])])\nresult = result[['user', 'time', 'amount']]\nresult\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "\ndf['amount-time'] = list(zip(df['time'], df['amount']))\nresult = df.groupby('user')['amount-time'].apply(list).reset_index()\nresult.columns = ['user', 'amount-time']\nresult['amount-time'] = result['amount-time'].apply(lambda x: [tuple(i) for i in x])\nresult\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "\n# First, we need to create a new column that contains the time and amount as a tuple\ndf['amount-time-tuple'] = list(zip(df['time'], df['amount']))\n\n# Then, we group by 'user' and apply the 'tuple' function to the new column\nresult = df.groupby('user')['amount-time-tuple'].apply(list)\n\n# Finally, we sort the result\nresult = result.sort_values(by='amount-time-tuple')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\ndf = pd.DataFrame(series.values.reshape(-1, 1), index=series.index, columns=['column'])\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'file'})\ndf = df.set_index(['file'])\ndf_concatenated = df.transpose()\ndf_concatenated = df_concatenated.reset_index()\ndf_concatenated = df_concatenated.rename(columns={'index': 'column'})\ndf_concatenated = df_concatenated.set_index(['column'])\ndf_concatenated = df_concatenated.transpose()\ndf_concatenated", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\ndf = pd.DataFrame(series.values.tolist(), index=series.index, columns=['0', '1', '2', '3'])\ndf.columns = ['name'] + df.columns.tolist()\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'name'})\ndf", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "\nresult = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "\n# Find the column name that contains the string 'spike'\nfor col in df.columns:\n    if s in col:\n        result = col\n        break\n\n# Access the column with the result\ndf[result]", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "\n# Find the column names that contain the string 'spike'\ncolumns = [col for col in df.columns if s in col]\n\n# Create a new dataframe with the columns that contain 'spike'\nresult = df[columns]\n\n# Rename the columns\nresult.columns = [col.replace('-', '') for col in result.columns]\n\nprint(result)\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# First, we need to flatten the list of codes into a single series\nflattened_codes = df['codes'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a new dataframe with the flattened codes as columns\ndf_result = pd.DataFrame(flattened_codes.tolist(), columns=['code_' + str(i) for i in range(len(flattened_codes))])\n\n# Finally, we replace the original codes column with the new dataframe\ndf['codes'] = df_result\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# First, we need to flatten the list of codes into a single series\nflattened_codes = df['codes'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a new dataframe with the flattened codes as columns\ndf_result = pd.DataFrame(flattened_codes.tolist(), columns=['code_' + str(i) for i in range(1, len(flattened_codes) + 1)])\n\n# Finally, we replace the original codes column with the new dataframe\ndf['codes'] = df_result\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# First, we need to flatten the list of codes into a single series\nflat_codes = df['codes'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a new dataframe with the flattened list of codes\ndf_flat = pd.DataFrame(flat_codes.tolist(), columns=['code_' + str(i) for i in range(1, len(flat_codes) + 1)])\n\n# Now, we can merge the original dataframe with the flattened dataframe\nresult = pd.merge(df, df_flat, left_index=True, right_index=True)\n\n# Finally, we can drop the original codes column\nresult.drop('codes', axis=1, inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\nimport ast\n\ndf['col1'] = df['col1'].apply(lambda x: ast.literal_eval(x)[0])\nresult = df['col1'].tolist()\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\nimport numpy as np\n\ndf['col1'] = df['col1'].apply(lambda x: ', '.join(map(str, np.flip(x))))\nresult = df['col1'].values[0]\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\nimport numpy as np\n\nids = df['col1'].apply(lambda x: ', '.join(map(str, x)))\n\nresult = ids.values.tolist()\n\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "df['Time'] = df['Time'].apply(lambda x: x.floor('2min'))\ndf = df.groupby('Time').mean()\ndf['Time'] = df.index.to_series().apply(lambda x: x.floor('2min'))\ndf = df.reset_index()\ndf", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "\n# Convert the time to minutes\ndf['Time'] = df['Time'].map(lambda x: x.replace(minute=x.minute // 3 * 3))\n\n# Group by the time and sum the values\ndf = df.groupby('Time').sum()\n\n# Reset the index\ndf = df.reset_index()\n\n# Rename the columns\ndf.columns = ['Time', 'Value']\n\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate()\n\ndf\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\ndf", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\ndf", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[~filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[df.isna().all(axis=1)]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "\nresult = df.iloc[0].ne(df.iloc[8]).index[df.iloc[0].ne(df.iloc[8])]\n", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[df.isnull().all(axis=1)].tolist()", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "\nresult = [(x, y) for x, y in zip(df.iloc[0], df.iloc[8]) if x != y or (x == np.nan and y != np.nan) or (x != np.nan and y == np.nan)]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.stack().reset_index()\ndf.columns = [f'A_{i}' for i in df.level_0] + [f'B_{i}' for i in df.level_0] + [f'C_{i}' for i in df.level_0] + [f'D_{i}' for i in df.level_0] + [f'E_{i}' for i in df.level_0]\ndf = df.groupby(df.level_0).first().reset_index()\ndf = df.set_index(['A', 'B', 'C', 'D', 'E']).stack().reset_index()\ndf.columns = [f'A_{i}' for i in df.level_0] + [f'B_{i}' for i in df.level_0] + [f'C_{i}' for i in df.level_0] + [f'D_{i}' for i in df.level_0] + [f'E_{i}' for i in df.level_0]\ndf = df.groupby(df.level_0).first().reset_index()\ndf = df.set_index(['A', 'B', 'C', 'D', 'E']).stack().reset_index()\ndf.columns = [f'A_{i}' for i in df.level_0] + [f'B_{i}' for i in df.level_0] + [f'C_{i}' for i in df.level_0] + [f'D_{i}' for i in df.level_0] + [f'E_{i}' for i in df.level_0]\ndf = df.groupby(df.level_0).first().reset_index()\ndf = df.set_index(['A', 'B', 'C', 'D', 'E']).stack().reset_index()\ndf.columns = [f'A_{i}' for i in df.level_0] + [f'B_{i}' for i in df.level_0] + [f'C_{i}' for i in df.level_0] + [f'D_{i}' for i in df.level_0] + [f'E_{i}' for i in df.level_0]\ndf = df.groupby(df.level_0).first().reset_index()\ndf = df.set_index(['A', 'B', 'C', 'D', 'E']).stack().reset_index()\ndf.columns = [f'A_{i}' for i in df.level_0] + [f'B_{i}' for i in df.level_0] + [f'C_{i}' for i in df.level_0] + [f'D_{i}' for i in df.level_0] + [f'E_{i}' for i in df.level_0]\ndf = df.groupby(df.level_0).first().reset_index()\ndf = df.set_index(['A', 'B', 'C', 'D', 'E']).stack().reset_index()\ndf.columns = [f'A_{i}' for i in df.level_0] + [f'B_{i}' for i in df.level_0] + [f'C_{i}' for i in df.level_0] + [f'D_{i}' for i in df.level_0] + [f'E_{i}' for i in df.level_0]\ndf = df.groupby(df.level_0).first().reset_index()\ndf = df.set_index(['A', 'B', 'C', 'D', 'E']).stack().reset_index()\ndf.columns = [f'A_{i}' for i in df.level_0] + [f'B_{i}' for i in df.level_0] + [f'C_{i}' for i in df.level_0] + [f'D_{i}' for i in df.level_0] + [f'E_{i}' for i in df.level_0]\ndf = df.groupby(df.level_0).first().reset_index()\ndf = df.set_index(['A', 'B', 'C', 'D', 'E']).stack().reset_index()\ndf.columns = [f'A_{i}' for i in df.level_0] + [f'B_{i}' for i in df.level_0] + [f", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.stack().reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'+str(x) if x%2==0 else 'B_'+str(x) for x in df.levels[0]]\ndf = df.groupby(df.levels[0]).first()\ndf = df.reset_index()\ndf.columns = ['A_'", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "\nresult = df.sort_values(by=['treatment','time'])\nresult.reset_index(drop=True, inplace=True)\nresult\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "\nresult = df.sort_values(['time', 'VIM'])\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "\n# Create a new dataframe with the dates you want to keep\ndates_to_keep = pd.DataFrame({'Date': ['2020-02-17', '2020-02-18']})\ndates_to_keep['Date'] = pd.to_datetime(dates_to_keep['Date'])\n\n# Create a mask to keep the rows with the dates you want to keep\nmask = df['Date'].between(dates_to_keep['Date'].min(), dates_to_keep['Date'].max())\n\n# Apply the mask to the original dataframe\nresult = df[mask]\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "\n# Create a new column 'Day' that contains the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Filter the dataframe to keep only the rows where the day is not 'Friday'\nresult = df[df['Day'] != 'Friday']\n\n# Reset the index\nresult.reset_index(inplace=True)\n\n# Convert the 'Date' column to the original format\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "\n# Create a mask for the correlation matrix where the value is above 0.3\nmask = (corr > 0.3).stack()\n\n# Create a new DataFrame where the mask is True\nresult = corr.where(mask, 0)\n\n# Reset the index and column names\nresult = result.reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "\nresult = corr.stack().reset_index()\nresult.columns = ['col1', 'col2', 'value']\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc[result['value'] > 0.3]\nresult = result.loc[result['col1'] <= result['col2']]\nresult = result.loc[result['col2'] <= result['col1']]\nresult = result.loc[result['value'] == 1.0]\nresult = result.loc[result['col1'] != result['col2']]\nresult = result.loc", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.rename(columns={df.columns[0]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\n# Create a new column 'frequent' that indicates the most frequent value in each row\ndf['frequent'] = df.apply(lambda row: max(row.values), axis=1)\n\n# Create a new column 'freq_count' that counts the number of times the most frequent value occurs in each row\ndf['freq_count'] = df.apply(lambda row: row.values.tolist().count(row.frequent), axis=1)\n\ndf", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\n# Create a new column 'frequent' that contains the most frequent value in each row\ndf['frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\n\n# Create a new column 'freq_count' that contains the count of the most frequent value in each row\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n\ndf", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\n# Create a new column 'frequent'\ndf['frequent'] = df.apply(lambda row: list(row.value_counts().index[row.value_counts() == row.value_counts().max()]), axis=1)\n\n# Create a new column 'freq_count'\ndf['freq_count'] = df['frequent'].apply(lambda x: len(x))\n\ndf", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "\ndf['bar'] = df['bar'].replace('NULL', np.nan)\ndf['bar'] = df['bar'].fillna(df['bar'].mean())\n\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\n", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "\n# Convert 'bar' column to numeric if it's not NULL\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce')\n\n# Fill NULL values with 0\ndf['bar'].fillna(0, inplace=True)\n\n# Group by 'id1' and 'id2', and calculate the mean of 'foo' and 'bar'\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\n\n# Reset the index\nresult = result.reset_index()\n\n# Convert 'bar' column to float if it's not NULL\nresult['bar'] = result['bar'].apply(lambda x: x if x != 0 else float('nan'))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = df_c[['EntityNum', 'foo', 'a_col']]\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = df_c[['EntityNum', 'foo', 'b_col']]", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[~np.isnan(x)]", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = x.tolist()\nresult = [i for i in result if i is not np.nan]\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.eye(max(a), dtype=int)[a-1]", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((len(a), 4))\nb[:, a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((len(a), a.max() + 1))\nb[:, a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((len(a), 3))\nb[:, a.astype(int)] = 1", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nb[a[:, 0] - 1, a[:, 1] - 1] = 1", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.reshape(A, (-1, ncol))", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=1)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=0)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape)\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_indices = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    return max_value, max_indices\n    ### END SOLUTION ###\n\nresult = f(example_a)\nprint(result)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2)[-1], a.shape)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "import numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t\t[1., 2., 3., 9]])\n\nz = np.any(np.isnan(a), axis=0)\n\na = a[:, ~z]\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "import numpy as np\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \nresult = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravel_index(np.argmin(a, axis=None), a.shape)\nresult = min_index\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a, axis=None), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape)\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.deg2rad(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "cos_value = np.cos(np.deg2rad(degree))\nresult = cos_value\nprint(result)\n", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.degrees(np.arcsin(value))\nprint(result)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "result = np.pad(A, (0, length - A.shape[0]), 'constant')", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "result = np.pad(A, (0, length - A.shape[0]), 'constant')", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "print(np.power(a, power))\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nresult = f(example_a, 5)\nprint(result)\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (result.astype(int), 1)\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.array([numerator, denominator]).astype(int) / np.gcd.reduce(np.array([numerator, denominator]))\n    return tuple(result)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = (numerator / denominator, denominator / numerator) if denominator != 0 else (np.nan, np.nan)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.where(a > b, a, b)\nresult = np.where(result > c, result, c)\nprint(result)\n", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.flatten()[-1::-1])", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a.flatten()[-1::-1])", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.diag(a.flatten())[::-1]", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(a.shape[0]), np.diag(a.shape[1] - 1, k=1)), axis=1)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = [x for x in np.ravel(X)]\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\nresult = np.array(result)\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\nprint(result)", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "import numpy as np\n\ndef get_maximal_independent_vectors(a):\n    # Get the transpose of the matrix\n    a_t = a.T\n\n    # Compute the rank of the matrix\n    rank = np.linalg.matrix_rank(a_t)\n\n    # If the rank is less than the number of columns in the matrix,\n    # then the matrix is not full rank, so there are more linearly independent vectors\n    if rank < a.shape[1]:\n        # Compute the column with the maximum sum\n        max_column = np.argmax(np.sum(a, axis=0))\n\n        # Get the column with the maximum sum\n        result = a[:, max_column]\n\n    else:\n        # If the matrix is full rank, then all vectors are linearly independent\n        result = np.ones(a.shape[1])\n\n    return result\n\n# Test the function\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\nresult = get_maximal_independent_vectors(a)\nprint(result)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]\n", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b, equal_var=True, nan_policy='omit')\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "def weighted_t_test(a, b, n):\n    # calculate the weights\n    weights = np.concatenate([np.ones(n), np.ones(n)])\n    # calculate the t-values\n    t_values = np.abs(scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')[0])\n    # calculate the p-values\n    p_values = np.array([scipy.stats.t.pdf(t_values[i], df=len(a) + len(b) - 2) for i in range(len(t_values))])\n    # calculate the adjusted p-values\n    p_values_adjusted = p_values / np.sum(p_values)\n    return p_values_adjusted\n\np_value = weighted_t_test(a, b, 40)\nprint(p_value)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the standard deviation\nstd_dev = np.sqrt(np.sum([anobs, bnobs]) * (avar + bvar))\n\n# calculate the t-statistic\nt_statistic = (bmean - amean) / std_dev\n\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_statistic, anobs + bnobs)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.setdiff1d(np.setdiff1d(A.flatten(), B.flatten()), B.flatten())\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)[:, ::-1]", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.array_equal(a[0], a[1:]), axis=1)", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.array_equal(a[0], a[1:]), axis=0)", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.array_equal(a[0], a[1]))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.trapz(np.trapz(z, y, axis=0), x)", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    return (np.cos(x))**4 + (np.sin(y))**2\n    ### END SOLUTION ###\n\n# create a grid of x and y values\nx, y = np.meshgrid(example_x, example_y)\n\n# calculate the integral\nresult = np.trapz(f(x, y), x, y)\n\nprint(result)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(data):\n    # sort the data\n    x = np.sort(data)\n    # calculate the number of data points\n    n = x.shape[0]\n    # calculate the x values\n    x_values = np.arange(1, n + 1) / n\n    # calculate the y values\n    y_values = np.arange(1, n + 1) / n\n    # return the x values and y values\n    return x_values, y_values", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\n# compute the ECDF\necdf_values = ecdf(grades)\n\n# find the longest interval [low, high) that satisfies ECDF(x) < threshold\nlow, high = 0, 1\nwhile ecdf_values(high) < threshold:\n  low = high\n  high *= 2\n\n# print the result\nprint(low, high)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.choice([0, 1], size=size, p=[one_ratio, 1-one_ratio])", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\nresult = result.tolist()\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[-N:][::-1]\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "\ndef extract_patches(a, patch_size):\n    h, w = a.shape\n    patches = []\n    for i in range(0, h - patch_size[0] + 1, patch_size[0]):\n        for j in range(0, w - patch_size[1] + 1, patch_size[1]):\n            patches.append(a[i:i + patch_size[0], j:j + patch_size[1]])\n    return patches\n\npatch_size = (2, 2)\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "\ndef sliding_window(a, window_size):\n    shape = a.shape\n    strides = a.strides\n    result = np.lib.stride_tricks.as_strided(a, shape=(shape[0]-window_size[0]+1, shape[1]-window_size[1]+1, window_size[0], window_size[1]), strides=(strides[0], strides[1], a.itemsize, a.itemsize))\n    return result\n\nwindow_size = (2, 2)\nresult = sliding_window(a, window_size)\nprint(result)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "\ndef extract_patches(a, patch_size):\n    h, w = a.shape\n    patches = []\n    for i in range(0, h - patch_size[0] + 1, patch_size[0]):\n        for j in range(0, w - patch_size[1] + 1, patch_size[1]):\n            patches.append(a[i:i + patch_size[0], j:j + patch_size[1]])\n    return patches\n\npatch_size = (2, 2)\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "\n# calculate the number of patches\nnum_patches = (a.shape[0] - patch_size + 1) * (a.shape[1] - patch_size + 1)\n\n# create a list to store the patches\npatches = []\n\n# loop over the array\nfor i in range(num_patches):\n    # calculate the start and end indices for the patch\n    start_row = i // (a.shape[1] - patch_size + 1)\n    end_row = start_row + patch_size\n    start_col = i % (a.shape[1] - patch_size + 1)\n    end_col = start_col + patch_size\n    \n    # extract the patch and append it to the list\n    patches.append(a[start_row:end_row, start_col:end_col])\n\n# convert the list to a numpy array and assign it to the result variable\nresult = np.array(patches)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.pad(a, ((0, h - a.shape[0]), (0, w - a.shape[1])), mode='constant')\nresult = result.reshape((h, w))\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "\n# calculate the number of patches\nnum_patches = (a.shape[0] - patch_size + 1) * (a.shape[1] - patch_size + 1)\n\n# create a list to store the patches\npatches = []\n\n# loop over the array\nfor i in range(num_patches):\n    # calculate the start and end indices for the patch\n    start_row = i // (a.shape[1] - patch_size + 1)\n    end_row = start_row + patch_size\n    start_col = i % (a.shape[1] - patch_size + 1)\n    end_col = start_col + patch_size\n    \n    # extract the patch and append it to the list\n    patches.append(a[start_row:end_row, start_col:end_col])\n\n# convert the list to a numpy array and assign it to the result variable\nresult = np.array(patches)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "import numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(string)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.random.uniform(min, max, n)\nresult = np.log(result)\n", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.random.loguniform(min, max, n)", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.random.loguniform(min, max, n)\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.array([])\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(dims, index)\n", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = np.sub2ind(a.shape, index)\nprint(result)", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.array([np.sum(a[accmap == i]) for i in np.unique(accmap)])", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.maximum.at(a, index, a)\nprint(result)\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.cumsum(a[accmap >= 0])", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = a[index]\nresult = np.min(result)\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.vectorize(elementwise_function)(x, y)\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, p=probabilit)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "result = np.pad(a, ((low_index, high_index), (low_index, high_index)), 'constant')[low_index:, low_index:]", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.iscomplex(x)]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_max = [np.max(bin) for bin in bin_data]\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.array_split(data, np.arange(bin_size, len(data) + 1, bin_size))\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    t = (x - x_min) / (x_max - x_min)\n    t = np.clip(t, 0, 1)\n    return t * t * (3 - 2 * t)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0, 1)\n    t = t**N\n    return t * (1 - t)\n\nprint(smoothclamp(x, x_min, x_max, N))\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\n# Create a circular correlation\nresult = np.correlate(a, b, mode='full')\n\n# Remove the zero-padding\nresult = result[len(a):]\n\nprint(result)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.groupby(['major', 'timestamp']).apply(lambda x: x.values.reshape(4, 15, 5)).values", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = df.groupby('major').apply(lambda x: x.values.reshape(1, 4, 5)).values", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.array([list(np.unpackbits(np.uint8(num))) for num in a])\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.array([list(np.unpackbits(np.uint8(num))) for num in a])\nresult = np.pad(result, (0, m - result.shape[1]), 'constant')\nresult = result.astype(int)\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "\n# Convert the array to binary and pad it to the length of m\nbinary_array = np.array([format(num, '0' + str(m) + 'b') for num in a])\n\n# Convert the binary array to a numpy array of the right length\nbinary_array = np.array([list(map(int, bin_num)) for bin_num in binary_array])\n\n# Compute the exclusive OR of all the rows\nresult = np.bitwise_xor.reduce(binary_array, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\nresult = (\u03bc - 3*\u03c3, \u03bc + 3*\u03c3)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\nresult = (\u03bc-2*\u03c3, \u03bc+2*\u03c3)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    third_std_dev = np.std(a, ddof=1)\n    start = mean - 3 * third_std_dev\n    end = mean + 3 * third_std_dev\n    return (start, end)\n    ### END SOLUTION ###\n\nresult = f(a)\nprint(result)\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "mean = np.mean(a)\nstd_dev = np.std(a)\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\nresult = (a >= lower_bound) & (a <= upper_bound)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.compressed(), percentile)\nprint(prob)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[:, 0] = 0", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.argmax(a, axis=1)\nmask = mask.reshape(-1, 1)\nmask = (mask == np.arange(a.shape[1]))\nmask = mask.astype(bool)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "import scipy.stats\n\nresult = scipy.stats.pearsonr(post, distance)\nprint(result)\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ij,ij->ij', X, X)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((M, N))\nfor i in range(M):\n    X[i] = np.sum(Y[i], axis=0)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "C = A[~np.isin(A, B)]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[np.in1d(A, B)]", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "\n# Create a boolean array that indicates whether each element in A is in B\nmask = np.isin(A, B)\n\n# Use the mask to index A\nC = A[mask]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = rankdata(a, method='max') - 1", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.array(len(a) - rankdata(a)) - 1", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    result = np.array(len(a) - rankdata(a))\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack([x_dists, y_dists])", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.linalg.norm(X, ord=1, axis=1)\nresult = X / l1[:, np.newaxis]\nprint(result)", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.linalg.norm(X, axis=1, ord=2)\nresult = X / x[:, np.newaxis]", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = X / x[:, np.newaxis]\nprint(result)\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].str.contains(target)\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "import scipy.spatial\n\ndistance_matrix = scipy.spatial.distance.cdist(a, a, metric='euclidean')\nresult = distance_matrix", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "import scipy.spatial\n\nresult = scipy.spatial.distance.cdist(a, a, metric='euclidean')", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "import scipy.spatial\n\ndistance_matrix = scipy.spatial.distance.cdist(a, a, metric='euclidean')\nresult = distance_matrix\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA)\nprint(AVG)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# Remove the adjacent (before removing) duplicate non-zero value\na = a[a != 0]\n\n# Remove the zero value\na = a[a != 0]\n\nresult = a\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# Create a mask to exclude adjacent duplicates and zero values\nmask = np.concatenate(([True], (a[:-1] != a[1:]) & (a[:-1] != 0) & (a[1:] != 0), [True]))\n\n# Apply the mask to the array\nresult = a[mask]", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf = df.reset_index(drop=True)\ndf = df.reindex(columns=['lat', 'lon', 'val'])\ndf", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    ### END SOLUTION\n    return df\n\ndf = f(lat, lon, val)\nprint(df)\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df.max(axis=1)\ndf = df.reset_index(drop=True)\ndf = df.reindex(columns=['lat', 'lon', 'val', 'maximum'])\ndf", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "\ndef rolling_window(a, size):\n    shape = a.shape\n    dim = len(shape)\n    strides = a.strides\n    strides = strides + strides\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "\ndef rolling_window(a, size):\n    shape = a.shape\n    dim = len(shape)\n    strides = a.strides\n    strides = strides + strides\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [1])))\n    sub_strides = tuple(np.concatenate(([1], strides, [1])))\n    sub_shape = tuple(np.concatenate(([1], shape, [", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:,-1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = c in CNTS\nprint(result)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = any(np.array_equal(c, cnt) for cnt in CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.eye(4, dtype=np.float64)\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[a != 0] = 0\n", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_datetime = pd.to_datetime(start)\nend_datetime = pd.to_datetime(end)\ndelta = end_datetime - start_datetime\nresult = pd.date_range(start_datetime, end_datetime, periods=n)\nresult = result.to_series()\nresult", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No such index\")\nelse:\n    print(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))[0]\nif indices.size > 0:\n    result = indices\nelse:\n    result = np.array([])\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "\n# calculate the coefficients a, b and c using the numpy.polyfit function\nresult = np.polyfit(x, y, 2)\n\n# print the coefficients in the order of highest to lowest\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "\n# Create a matrix with the x values as columns\nX = np.column_stack((np.ones(len(x)), np.array(x)))\n\n# Calculate the coefficients using the formula for the least squares solution\ncoefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n\n# Sort the coefficients in descending order\nresult = np.sort(coefficients)[::-1]\n\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = [0, 1, 2, 3]\ndf = df - temp_arr\nprint(df)", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "\nmask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr >= -10\narr[mask] = arr[mask] + 5\n\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr_temp = arr.copy()\nmask = arr_temp < n1\nmask2 = arr_temp < n2\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narry[~mask2] = 30 \n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "print np.nonzero(np.logical_not(np.isclose(s1, s2)))[0].shape[0]\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = all(np.array_equal(a[0], a[1]), np.array_equal(a[0], a[2]))", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all() for arr in a)", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "\n# calculate the number of rows and columns to pad\nrows_pad = shape[0] - a.shape[0]\ncols_pad = shape[1] - a.shape[1]\n\n# zero pad the array\nresult = np.pad(a, ((0, rows_pad), (0, cols_pad)))\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "\n# Calculate the maximum shape\nmax_shape = np.array(shape)\n\n# Calculate the maximum shape minus the original shape\npadding = np.array(shape) - np.array(a.shape)\n\n# Zero pad the array\nresult = np.pad(a, padding, mode='constant', constant_values=0)\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "\n# Calculate the number of rows and columns needed to pad the array\nrows_needed = max(shape[0], a.shape[0])\ncols_needed = max(shape[1], a.shape[1])\n\n# Create a new array with the desired shape and fill it with the element\nresult = np.full(shape, element)\n\n# Pad the original array to the new shape\nresult[:a.shape[0], :a.shape[1]] = a\nresult[-rows_needed:, :a.shape[1]] = a[:a.shape[0], :]\nresult[:a.shape[0], -cols_needed:] = a[:a.shape[0], -cols_needed:]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # Calculate the difference between the shape of the original array and the shape of the padded array\n    diff = np.array(shape) - np.array(arr.shape)\n    # Use the numpy `numpy.pad` function to pad the array\n    result = np.pad(arr, pad_width=diff, mode='constant', constant_values=0)\n    ### END SOLUTION ###\n    return result\n\n# Test the function with the example array\nprint(f())\n", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "\n# calculate the difference between the shape and the original shape\ndifference = tuple(np.array(shape) - np.array(a.shape))\n\n# calculate the number of rows and columns to pad\npad_rows = difference[0]\npad_cols = difference[1]\n\n# zero pad the array\nresult = np.pad(a, ((0, pad_rows), (0, pad_cols)), mode='constant')\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1]), b[:, None, None]])", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1]), b[:, None, None]])", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "\nresult = df.loc[df['a'].between(1, 4), 'b'].values\nresult = np.where(result.size > 0, result, np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "\n# Create a mask where the image is not all zeros\nmask = np.any(im, axis=1)\n\n# Create a new array where the mask is True\nresult = im[mask]\n\n# Reshape the result to the desired shape\nresult = result.reshape(-1, 2)\n\nprint(result)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "\n# Find the indices of the non-zero elements\nindices = np.where(A != 0)\n\n# Get the minimum and maximum indices\nmin_index = np.min(indices, axis=1)\nmax_index = np.max(indices, axis=1)\n\n# Truncate the array\nresult = A[min_index[0]:max_index[0]+1, min_index[1]:max_index[1]+1", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=1)\n\n# Create a mask where the image is not entirely zero\nmask = np.any(im != 0, axis=0)\n\n# Apply the mask to the image\nresult = im[mask]\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "\n# Create a mask where the image is not all zero\nmask = im != 0\n\n# Create a new array where the mask is applied\nresult = im[mask]\n\n# Remove the first and last rows and columns\nresult = result[1:-1, 1:-1]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.figure(figsize=(10, 6))\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'r-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'go-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'b*-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'kp-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c^-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'mh-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c--')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm.-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.figure(figsize=(10, 6))\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'r-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'go-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'b*-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'kp-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c^-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'mh-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c--')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm.-')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'y:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'c:')\n\n# draw a line with random y\nplt.plot(x, np.random.rand(10), 'm:')\n\n# draw a line with random y\nplt.plot(", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, marker='d', linestyle='-', color='blue')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, marker='d', linestyle='-', linewidth=2, markersize=10)\nplt.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, 'r--')\nplt.plot(x, 'r--', range(2, 4))\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = 2 * x + 1\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line from (0,0) to (1,2)')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    height=5,\n    aspect=1.5,\n)\n\n# add a legend to the plot\nplt.legend(title=\"Gender\")\n\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize=20)\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), range(10), \"k-\", lw=5, markersize=30)\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), x, 'r-', lw=5, markersize=30)\nplt.plot(range(10), y, 'r-', lw=5, markersize=30)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.array([0, 2, 4, 6, 8, 10])\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc=\"upper right\")\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.imshow(H, cmap='hot')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# split the title into words\nwords = myTitle.split(' ')\n# get the longest word\nlongest_word = max(words, key=len)\n# fit the longest word into multiple lines\nmultiple_lines = ' '.join(words[:words.index(longest_word)])\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = -y\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5])\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.plot(x, y)\nplt.xticks(np.arange(min(x), max(x) + 1, 1))\nplt.yticks(np.arange(min(y), max(y) + 1, 1))\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%.0f'))\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax.plot(x, y, linestyle='dashed')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y2')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_title('sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_title('cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks([])\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4))\nplt.grid(axis='x')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 5))\nplt.xticks(np.arange(1, 3))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5)\nplt.grid(axis='x', which='both', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), constrained_layout=True)\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xticks([])\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.gca().set_xticks(np.arange(0, 10, 2))\nplt.gca().set_xticks(np.arange(0, 10, 2), minorticks=True)\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks([])\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().tick_params(axis='y')\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green', scatter_kws={'color':'blue'})\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green')\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"reg\")\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 6))\n\n# Create a bar plot of s1 and s2\nplt.bar(df[\"celltype\"], df[\"s1\"], label=\"s1\")\nplt.bar(df[\"celltype\"], df[\"s2\"], label=\"s2\")\n\n# Set the x-axis tick labels to rotate 45 degrees\nplt.xticks(rotation=45)\n\n# Set the xlabel to be the celltype\nplt.xlabel(\"celltype\")\n\n# Set the ylabel to be the value of s1 and s2\nplt.ylabel(\"value\")\n\n# Set the title of the plot\nplt.title(\"Bar plot of s1 and s2\")\n\n# Add a legend to the plot\nplt.legend()\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel('X', color='red')\nplt.ylabel('Y', color='red')\nplt.xticks(color='red')\nplt.yticks(color='red')\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X', color='red')\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.xticks(x, rotation=90)\nplt.tick_params(axis='both', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "\n# create a figure and a set of subplots\nfig, ax = plt.subplots()\n\n# draw vertical lines at the given coordinates\nax.axvline(0.22058956, color='r')\nax.axvline(0.33088437, color='g')\nax.axvline(2.20589566, color='b')\n\n# set the labels and title\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Vertical Lines at x = 0.22058956, 0.33088437, 2.20589566')\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, cmap='hot', interpolation='nearest')\nplt.xticks(range(len(xlabels)), xlabels, rotation=90)\nplt.yticks(range(len(ylabels)), ylabels)\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax.plot(time, temp, \"-\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "plt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\nplt.show()\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.figure(figsize=(10, 6))\nplt.scatter(b, a, c=c, cmap='viridis')\nplt.xlabel('b')\nplt.ylabel('a')\nplt.title('Scatter plot of a over b')\nplt.colorbar()\n\n# annotate each data point with correspond numbers in c\nfor i, j, k in zip(b, a, c):\n    plt.annotate(k, (j, i))\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend', title_fontsize='12', label_fontsize='12', loc='best')\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, alpha=0.6, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(2, 3, figsize=(10, 10))\n\n# Make the first subplot three times wider than the second subplot but they should have the same height.\naxs[0, 0].plot(x, y)\naxs[0, 1].plot(x, y)\naxs[0, 2].plot(x, y)\naxs[1, 0].plot(x, y)\naxs[1, 1].plot(x, y)\naxs[1, 2].plot(x, y)\n\n# Set the labels for the axes\naxs[0, 0].set_xlabel('x')\naxs[0, 1].set_xlabel('x')\naxs[0, 2].set_xlabel('x')\naxs[1, 0].set_xlabel('x')\naxs[1, 1].set_xlabel('x')\naxs[1, 2].set_xlabel('x')\n\naxs[0, 0].set_ylabel('y')\naxs[0, 1].set_ylabel('y')\naxs[0, 2].set_ylabel('y')\naxs[1, 0].set_ylabel('y')\naxs[1, 1].set_ylabel('y')\naxs[1, 2].set_ylabel('y')\n\n# Set the titles for the axes\naxs[0, 0].set_title('Subplot 1')\naxs[0, 1].set_title('Subplot 2')\naxs[0, 2].set_title('Subplot 3')\naxs[1, 0].set_title('Subplot 4')\naxs[1, 1].set_title('Subplot 5')\naxs[1, 2].set_title('Subplot 6')\n\n# Set the legend for the axes\naxs[0, 0].legend()\naxs[0, 1].legend()\naxs[0, 2].legend()\naxs[1, 0].legend()\naxs[1, 1].legend()\naxs[1, 2].legend()\n\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Add a legend to the chart\nplt.legend()\n\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist([x, y], bins=10, histtype='bar', label=['x', 'y'])\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Grouped Histogram of x and y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.plot([a, c], [b, d])\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, axs = plt.subplots(2, 1)\n\n# create a colormap with x and y\ncmap = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_y = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_x = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n# create a colormap with y and x\ncmap_yx = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_xy = plt.get_cmap('viridis')\n\n#", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[0].plot(a, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].plot(x, z, label='Y')\naxs[1].plot(a, y, label='Z')\naxs[1].set_title('Y and Z')\n\n# Set the labels for the x and y axes\naxs[0].set_xlabel('X')\naxs[0].set_ylabel('Y')\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\n\n# Set the labels for the z axes\naxs[0].set_zlabel('Z')\naxs[1].set_zlabel('Y')\n\n# Set the legend for the two subplots\naxs[0].legend()\naxs[1].legend()\n\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.figure(figsize=(10, 6))\nplt.plot([x[0] for x in points], [x[1] for x in points])\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line plot of points')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.plot(x, y)\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "plt.figure(figsize=(5, 5))\nfor line in lines:\n    plt.plot(*zip(*line), c=c[lines.index(line)])\n\n# Add a title to the plot\nplt.title(\"Line Segments\")\n\n# Add a legend to the plot\nplt.legend([f\"Line {i}\" for i in range(len(lines))])\n\n# Display the plot\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o-')\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-Log Plot')\nplt.grid(True)\nplt.xticks(np.arange(0, 1000, 100))\nplt.yticks(np.arange(0, 1000, 100))\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\n\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "\n# Calculate the cumulative sum of the data\ncumulative_sum = np.cumsum(data)\n\n# Normalize the data to sum up to 1\nnormalized_data = data / cumulative_sum[-1]\n\n# Create a histogram of the normalized data\nplt.hist(normalized_data, bins=len(data), edgecolor='black')\n\n# Set the y tick labels as percentage\nplt.yticks(np.arange(0, 1.1, 0.1))\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.ylabel('Percentage')\n\n# Set the x tick labels as the data values\nplt.xticks(range(1, len(data) + 1))\n\n# Set the x tick labels as the data values\nplt.xticks(range(1, len(data) + 1), data)\n\n# Set the x label as 'Data values'\nplt.xlabel('Data values')\n\n# Set the title as 'Histogram of normalized data'\nplt.title('Histogram of normalized data')\n\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, markeredgecolor='blue', markerfacecolor='blue', alpha=0.5)\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, a, label='a')\naxs[0].legend()\n\naxs[1].plot(z, z, label='z')\naxs[1].legend()\n\nfig.legend()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=False, sharey=False, figsize=(10, 10))\n\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(range(0, 10, 2), [2.1, 3, 7.6])\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\nplt.tight_layout()\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.gca().set_xticks(x, minor=True)\nplt.gca().xaxis.tick_top()\nplt.gca().xaxis.set_minor_locator(plt.MultipleLocator(1))\nplt.gca().yaxis.set_minor_locator(plt.MultipleLocator(1))\nplt.gca().yaxis.tick_right()\nplt.gca().yaxis.set_major_locator(plt.MultipleLocator(1))\nplt.gca().yaxis.tick_left()\nplt.gca().yaxis.set_major_formatter(plt.FormatStrFormatter('%.1f'))\nplt.gca().yaxis.set_minor_formatter(plt.NullFormatter())\nplt.gca().yaxis.set_label_coords(-0.1, 1.02)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0.1, -0.1)\nplt.gca().yaxis.set_label_coords(-0.1, -0.1)\nplt.gca().xaxis.set_label_coords(-0", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(x, alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0.05)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n\naxs.plot(x, y, label='y')\naxs.plot(x, y, label='y')\n\naxs.set_title('Figure')\naxs.legend()\n\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.figure(figsize=(10, 5))\nplt.plot(df['Type A'], df['Type B'])\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Line Chart of Type A and Type B')\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='||')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none', hatch='|')\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*', hatch='//')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*' + '|')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, extent=(1, 5, 1, 4))\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(y, x, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.figure(figsize=(10, 5))\nplt.plot([3], [0], 'o-', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 10))\nplt.pie(height, labels=labels, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedgeprops={'width': 0.4})\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle=':')\nplt.grid(which='major', linestyle='-')\nplt.show()\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.title(\"Activity Distribution\")\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.title(\"Activity Distribution\")\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='none', markeredgecolor='blue', markeredgewidth=2)\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "plt.figure(figsize=(10, 5))\nplt.bar(np.arange(3), blue_bar, color='b', align='center')\nplt.bar(np.arange(3) + 0.2, orange_bar, color='orange', align='center')\nplt.xlabel('Bar')\nplt.ylabel('Value')\nplt.title('Side-by-side bars')\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[1].plot(x, z, label='z')\naxs[1].plot(a, y, label='y')\n\n# Add a legend to the first subplot\naxs[0].legend()\n\n# Add a legend to the second subplot\naxs[1].legend()\n\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.xticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "\ng = sns.catplot(x=\"bill_length_mm\", y=\"species\", hue=\"sex\", data=df, height=5, aspect=0.9, kind=\"bar\", share=False)\n\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(5, 5))\nplt.plot([0.5, 0.5, 0.5 + 0.2, 0.5], [0.5, 0.5 + 0.2, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5 + 0.2, 0.5], 'k')\nplt.plot([0.5, 0.5 + 0.2, 0.5 + 0.2, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5 + 0.2, 0.5 + 0.2], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], 'k')\nplt.plot([0.5, 0.5, 0.5, 0", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(loc=\"center right\", bbox_to_anchor=(0.3, 0.5), frameon=False)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc=\"upper right\")\nplt.plot(x, y, marker=\"*\", label=\"Line\")\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, cmap='viridis')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.plot(x, y)\nplt.title('Figure 1', fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.gca().invert_xaxis()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n\n# Create a scatter plot\nplt.scatter(x, y, c='blue')\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolors='black')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\nplt.show()\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o-')\nplt.errorbar(x, y, yerr=error, fmt='o-', capsize=5)\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 1], 'w-')\nplt.plot([0, 1], [0, 0], 'w-')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[0].legend()\n\naxs[1].plot(z, a, label='Z')\naxs[1].set_title('Z')\naxs[1].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs.flat:\n    ax.plot(x, y)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1)\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 6))\nsns.heatmap(df, cmap=\"viridis\", cbar=False, bbox=[0, 0, 1, 1])\nplt.show()\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(x)\nplt.yticks(y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(x)\nplt.yticks(y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks([])\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "\n# Create a figure and a set of subplots\nfig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\n# Create a scatter plot for \"time\" and \"pulse\" with \"kind\" as the hue and \"diet\" as the color\naxs[0].scatter(df['time'], df['pulse'], hue=df['kind'], c=df['diet'])\naxs[0].set_title(\"Group: Fat\")\n\n# Create a scatter plot for \"time\" and \"pulse\" with \"kind\" as the hue and \"diet\" as the color\naxs[1].scatter(df['time'], df['pulse'], hue=df['kind'], c=df['diet'])\naxs[1].set_title(\"Group: No Fat\")\n\n# Change the layout of the subplots\nplt.tight_layout()\n\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "plt.figure(figsize=(10, 6))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.9, kind=\"point\")\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Scatter Plot of Exercise Time vs Pulse by Kind and Diet\")\nplt.show()\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "plt.figure(figsize=(10, 5))\n\n# First subplot\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Time vs Pulse\")\n\n# Second subplot\nplt.subplot(1, 2, 2)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Time vs Pulse\")\nplt.ylabel(\"Pulse\")\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='upper right', bbox_to_anchor=(1.05, 1), borderaxespad=0)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "fig, ax = plt.subplots(3, figsize=(10, 10))\n\nax[0].plot(t, a, label='sin(t)')\nax[0].set_xlabel('t')\nax[0].set_ylabel('a')\nax[0].legend()\n\nax[1].plot(t, b, label='cos(t)')\nax[1].set_xlabel('t')\nax[1].set_ylabel('b')\nax[1].legend()\n\nax[2].plot(t, c, label='a + b')\nax[2].set_xlabel('t')\nax[2].set_ylabel('c')\nax[2].legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=0.2, edgecolor=\"black\")\nplt.remove()\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))\n\ngrid = sns.FaceGrid(axes, margin_fill=False, fig=fig)\n\nfor ax, b in zip(grid.axes.flatten(), df[\"b\"].unique()):\n    subset = df[df[\"b\"] == b]\n    ax.scatter(subset[\"a\"], subset[\"c\"])\n    ax.set_xticks(np.arange(min(subset[\"a\"]), max(subset[\"a\"]), 2))\n    ax.set_xticklabels(np.arange(min(subset[\"a\"]), max(subset[\"a\"]), 2))\n\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azim=100, elev=50)\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[0, 1], height_ratios=[1, 0])\naxes = []\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.axis('off')\n    axes.append(ax)\n\n# Plot the image in each subplot\nfor ax, x_ in zip(axes, x.flatten()):\n    ax.imshow(x_)\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nprint(x.numpy())", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\n# Convert the labels to one-hot encoding\nlabels_one_hot = tf.one_hot(labels, depth=10, dtype=tf.int32)\n\n# Calculate the sum of each row\nresult = tf.reduce_sum(labels_one_hot, axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\n# Convert the labels to one-hot encoding\nlabels = tf.one_hot(labels, depth=10, dtype=tf.int32)\n\n# Calculate the sum of each class\nresult = tf.reduce_sum(labels, axis=0)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "\n# Convert the labels to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a tensor of zeros with the same shape as the labels tensor\nresult = tf.zeros_like(labels_tensor)\n\n# Set the values of the result tensor to the corresponding values in the labels tensor\nresult = tf.tensor_scatter_nd_update(result, tf.expand_dims(labels_tensor, 1), 1)\n\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    result = tf.one_hot(labels, depth=10, dtype=tf.int32)\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\n# Convert the labels to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a tensor of ones with the same shape as the labels tensor\nones_tensor = tf.ones_like(labels_tensor)\n\n# Create a tensor of zeros with the same shape as the labels tensor\nzeros_tensor = tf.zeros_like(labels_tensor)\n\n# Create a tensor of ones with the same shape as the labels tensor\nresult = tf.where(labels_tensor == 0, ones_tensor, zeros_tensor)\n\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.concat([[i], [i+1], [i+2]], axis=0)\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return [i for input_value in input for i in input_value]\n    ### END SOLUTION ###\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(f)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n\n# Convert the mask to integers\nmask = tf.cast(mask, tf.int32)\n\n# Create the padded mask\nresult = tf.pad(mask, [[0, 0], [1, 1]])\n\n# Convert the result to a tensor of 1s and 0s\nresult = tf.where(result == 0, tf.constant(1.), tf.constant(0.))\n\nprint(result)\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n\n# Convert the mask to a tensor of 1s and 0s\nmask = tf.cast(mask, tf.float32)\n\n# Create a tensor of 1s with the same shape as the lengths tensor\nones = tf.ones_like(lengths, dtype=tf.float32)\n\n# Create the result tensor by multiplying the mask and the ones\nresult = tf.multiply(mask, ones)\n\n# Print the result tensor\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": "\n    max_length = tf.constant(8)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\n    result = tf.cast(mask, tf.float32)\n    return result\n\nresult = f(example_lengths)\nprint(result)\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\n# Create a tensor of ones with the same shape as the lengths tensor\nones = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nzeros = tf.zeros_like(lengths)\n\n# Create a tensor of ones with the same shape as the lengths tensor\npadding = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, zeros)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones with the same shape as the lengths tensor\nresult = tf.where(lengths > 0, ones, padding)\n\n# Create a tensor of ones", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([tf.repeat(a, len(b))] + [tf.repeat(b, len(a))])", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "\n    return tf.stack([tf.repeat(a, len(b))]*len(a))\n\nresult = f(example_a, example_b)\nprint(result)\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "\nresult = tf.squeeze(a, [2])\nresult = tf.reshape(result, (50, 100, 512))\n\nprint(result)\n", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.expand_dims(a, axis=2)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "result = tf.expand_dims(a, axis=1)\nresult = tf.expand_dims(result, axis=2)\nresult = tf.reshape(result, (1, 50, 100, 1, 512))\nresult = tf.squeeze(result)\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\n# Calculate the L2 distance element-wise\ndifference = tf.square(tf.sub(a, b))\n\n# Calculate the sum of the squares\nsum_of_squares = tf.reduce_sum(difference, axis=0)\n\n# Calculate the L2 distance\nl2_distance = tf.sqrt(sum_of_squares)\n\n# Print the result\nprint(l2_distance)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "\n# Calculate the square of the difference\ndifference = tf.square(a - b)\n\n# Reduce the square difference by column\nresult = tf.reduce_sum(difference, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    return tf.square(tf.sub(A, B))\n\nresult = f()\nprint(result)\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "\nresult = tf.gather_nd(x, tf.stack([y, z], axis=1))\n\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = tf.gather_nd(x, tf.stack([row, col], axis=1))\nresult = m.numpy()\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "\n    return x[y,z]\n\nresult = tf.map_fn(f, (example_x, example_y, example_z), dtype=tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\nresult = tf.einsum('ijk,ik->ij', A, B)\n\nprint(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "\nresult = tf.einsum('ijk,ilk->ijl', A, B)\nresult = tf.reduce_sum(result, axis=-1)\n\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\nx = tf.constant(x)\nresult = tf.strings.unicode_transcode(x, \"UTF-8\")\n\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    result = tf.strings.unicode_transcode(x, \"UTF-8\")\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "\n# Get the mask of non-zero entries\nmask = tf.cast(tf.math.greater(x, 0), tf.float32)\n\n# Compute the average\nresult = tf.math.reduce_sum(x * mask, axis=-1) / tf.math.reduce_sum(mask, axis=-1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = tf.not_equal(x, 0)\nresult = tf.reduce_sum(x, axis=-2, keepdims=True) / tf.reduce_sum(mask, axis=-2, keepdims=True)\nresult = tf.where(mask, result, tf.zeros_like(result))\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    mask = tf.cast(tf.math.reduce_any(x != 0, axis=-1), tf.float32)\n    return tf.reduce_sum(x, axis=-1) / mask\n    ### END SOLUTION\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\nimport tensorflow as tf\n\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\n\nwith tf.compat.v1.Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\n# Get the indices of the maximum values in each row\nindices = tf.argmax(a, axis=1)\n\n# Convert the indices to a tensor of shape [100]\nresult = tf.reshape(indices, [-1])\n\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\n# Get the indices of the maximum values in each column\nindices = tf.argmax(a, axis=1)\n\n# Convert the indices to a tensor of the same shape as the original tensor\nresult = tf.tensor_scatter_nd_update(tf.ones_like(a), indices, tf.range(10))\n\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    return tf.argmax(a, axis=1)\n\nresult = f(example_a)\nprint(result)\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save(\"my_model\")\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\n# Set the seed\ntf.random.set_seed(seed_x)\n\n# Generate 10 random integers from a uniform random variable\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "\n# Define the seed\ntf.random.set_seed(seed_x)\n\n# Define the range of values\nrange_values = [2, 3, 4, 5]\n\n# Generate the tensor\nresult = tf.constant(range_values)\n\n# Print the tensor\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    tensor = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n    return tensor\n\nresult = f(seed_x=10)\nprint(result)\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "print(tf.__version__)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "coeff = np.polyfit(x, y, 1)\nresult = coeff\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "coeff = np.polyfit(x, y, 2)\nresult = coeff\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + (3*a - b - c)**2 + sin(b) + cos(b) + 4)\n\nresult = optimize.minimize(f, initial_guess)\nresult.x", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.norm.sf(z_scores)", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [scipy.stats.norm.sf(z) for z in z_scores]\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = np.array([scipy.stats.norm.ppf(1 - p_value) for p_value in p_values])", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm([stddev], loc=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa.dot(sb)", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    return sA * sB\n\nresult = f(example_sA, example_sB)\nprint(result)\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "\n# Create a 3D grid from the points\nx, y, z = np.mgrid[0:3:100j, 0:3:100j, 0:3:100j]\n\n# Create a LinearNDInterpolator from the points\ninterpolator = scipy.interpolate.LinearNDInterpolator((x, y, z), V)\n\n# Get the interpolated value for the request point\nresult = interpolator(request)\n\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "\n# Create a 3D grid from the points\nx, y, z = np.mgrid[np.min(points[:, 0]):np.max(points[:, 0]):100j,\n                    np.min(points[:, 1]):np.max(points[:, 1]):100j,\n                    np.min(points[:, 2]):np.max(points[:, 2]):100j]\n\n# Interpolate the moisture V values onto the grid\ninterpolator = scipy.interpolate.LinearNDInterpolator(points[:, :3], V)\nresult = interpolator(x, y, z)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, mode='reflect')\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = M.diagonal()", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, 'uniform')\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # Compute the K-S test statistic\n    D = np.abs(stats.norm.ppf(1 - (1 - rate) / 2) - stats.norm.ppf(1 - (1 - T) / 2))\n    # Compute the p-value\n    p_value = stats.norm.sf(D)\n    ### END SOLUTION ###\n    return p_value\nresult = f(example_times, example_rate, example_T)\nprint(result)\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "# compute the empirical distribution of the times\nempirical_distribution = np.histogram(times, bins=int(T/10), range=(0, T))[0]\n\n# compute the theoretical distribution\ntheoretical_distribution = stats.uniform.pdf(np.linspace(0, T, int(T/10)), 0, T)\n\n# compute the K-S test\nresult = stats.kstest(empirical_distribution, theoretical_distribution)[1] > 0.05\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.hstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "\nimport numpy as np\n\n# Concatenate the two matrices vertically\nFeature = np.concatenate((c1.toarray(), c2.toarray()), axis=1)\n\n# Convert the concatenated matrix back to a sparse matrix\nFeature = sparse.csr_matrix(Feature)\n", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "distances = scipy.spatial.distance.cdist(points1, points2)\nresult = scipy.optimize.linear_sum_assignment(distances)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "distances = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\nresult = scipy.optimize.linear_sum_assignment(distances)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\nb = sparse.csr_matrix(b.toarray())\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "\n# Create a binary mask where the values are greater than the threshold\nmask = img > threshold\n\n# Use the ndimage.label function to create a region labeling of the mask\nlabels = ndimage.label(mask)\n\n# The number of regions is the number of unique labels\nnum_regions = len(np.unique(labels))\n\nresult = num_regions\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "mask = img < threshold\nresult = np.sum(mask)", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use the numpy function np.greater_equal to count the number of regions of cells which value exceeds a given threshold\n    result = np.greater_equal(img, threshold).sum()\n    ### END SOLUTION ###\n    return result\nprint(f(img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\n# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the coordinates of the regions\ncoords = np.argwhere(mask)\n\n# Calculate the center of mass of the regions\ncenter_of_mass = np.mean(coords, axis=0)\n\n# Calculate the distance between the center of mass of the regions and the top left corner\ndistance = center_of_mass - np.array([0, 0])\n\n# Append the distance to the result list\nresult.append(distance)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T - np.diag(M.diagonal())\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    sA = (sA + sA.T) / 2\n    sA = sA + sA.T\n    ### END SOLUTION ###\n    return sA\n\nsA = f(example_sA)\nprint(sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\n# Create a mask where the square is surrounded by 1's\nmask = np.zeros(square.shape, dtype=bool)\nmask[1:-1, 1:-1] = 1\n\n# Erode the mask\nmask = scipy.ndimage.binary_erosion(mask)\n\n# Dilate the mask\nmask = scipy.ndimage.binary_dilation(mask)\n\n# Invert the mask to get the isolated cells\nmask = np.logical_not(mask)\n\n# Apply the mask to the square\nsquare = np.where(mask, 0, square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask where the image is not zero\nmask = square != 0\n\n# Use scipy.ndimage.measurements.label to create a labeled image\nlabeled_image, num_labels = scipy.ndimage.measurements.label(mask)\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0] = 1\n\n# Use scipy.ndimage.measurements.center_of_mass to get the center of mass of each label\ncenters = scipy.ndimage.measurements.center_of_mass(new_image, labeled_image, index=np.arange(1, num_labels + 1))\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0] = 1\n\n# Use scipy.ndimage.measurements.center_of_mass to get the center of mass of each label\ncenters = scipy.ndimage.measurements.center_of_mass(new_image, labeled_image, index=np.arange(1, num_labels + 1))\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0] = 1\n\n# Use scipy.ndimage.measurements.center_of_mass to get the center of mass of each label\ncenters = scipy.ndimage.measurements.center_of_mass(new_image, labeled_image, index=np.arange(1, num_labels + 1))\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0] = 1\n\n# Use scipy.ndimage.measurements.center_of_mass to get the center of mass of each label\ncenters = scipy.ndimage.measurements.center_of_mass(new_image, labeled_image, index=np.arange(1, num_labels + 1))\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0] = 1\n\n# Use scipy.ndimage.measurements.center_of_mass to get the center of mass of each label\ncenters = scipy.ndimage.measurements.center_of_mass(new_image, labeled_image, index=np.arange(1, num_labels + 1))\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0] = 1\n\n# Use scipy.ndimage.measurements.center_of_mass to get the center of mass of each label\ncenters = scipy.ndimage.measurements.center_of_mass(new_image, labeled_image, index=np.arange(1, num_labels + 1))\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0] = 1\n\n# Use scipy.ndimage.measurements.center_of_mass to get the center of mass of each label\ncenters = scipy.ndimage.measurements.center_of_mass(new_image, labeled_image, index=np.arange(1, num_labels + 1))\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0] = 1\n\n# Use scipy.ndimage.measurements.center_of_mass to get the center of mass of each label\ncenters = scipy.ndimage.measurements.center_of_mass(new_image, labeled_image, index=np.arange(1, num_labels + 1))\n\n# Create a new array where the labels are replaced with zeros\nnew_image = np.zeros_like(mask)\nnew_image[labeled_image != 0]", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_value = np.max(col.toarray())\nmin_value = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\nmedian = np.median(col.toarray())\nmode = np.mode(col.toarray())\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *coeffs):\n    return sum(coeff * np.cos(n * np.pi * x / tau) for n, coeff in enumerate(coeffs))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\nflat_array = example_array.flatten()\n\n# Create a 2D array with unique IDs as rows and columns\nunique_ids = np.unique(flat_array)\n\n# Create an empty array to store the distances\ndistances = np.zeros((len(unique_ids), len(unique_ids)))\n\n# Calculate the pairwise Euclidean distances\nfor i in range(len(unique_ids)):\n    for j in range(len(unique_ids)):\n        distances[i, j] = scipy.spatial.distance.cdist([flat_array[flat_array == unique_ids[i]]], [flat_array[flat_array == unique_ids[j]]])[0, 0]\n\n# Convert the distances from cells to meters\ndistances = distances * example_array.mean()\n\n# Print the distances\nprint(distances)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Calculate pairwise Manhattan distances\ndistances = scipy.spatial.distance.cdist(np.arange(example_array.shape[0]), np.arange(example_array.shape[1]), metric='cityblock')\n\n# Create a list to store the result\nresult = []\n\n# Iterate over the array to find the minimum distance between each pair of regions\nfor i in range(example_array.shape[0]):\n    for j in range(example_array.shape[1]):\n        # Find the minimum distance between the current region and the other regions\n        min_distance = np.min(distances[i, :])\n        # Add the result to the list\n        result.append([i, np.argmin(distances[i, :]), min_distance])\n\n# Convert the result to a numpy array\nresult = np.array(result)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate the pairwise Euclidean distances between all regions\n    distances = scipy.spatial.distance.pdist(example_array, metric='euclidean')\n    # Convert the distances from cells to meters\n    distances_in_meters = distances / np.max(example_array)\n    # Convert the distances to a 2D array in the form \"from ID, to ID, distance\"\n    result = np.array(list(zip(np.where(example_array == 0)[0], np.where(example_array == 0)[1], distances_in_meters)))\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_array))\n", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "result = np.empty((5, 100))\nfor i in range(5):\n    tck, u = interpolate.splprep([x[i], x[-1, i]], y[i], s=4)\n    result[i] = interpolate.splev(x_val, tck)\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x = np.array([x1, x2, x3, x4])\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x)\nprint(statistic, critical_values, significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nresult = ss.anderson_ksamp([x1, x2])\nprint(result)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "\ndef tau1(x):\n    y = np.array(df['A']) # keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['AC'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['BC'] = df['B'].rolling(3).apply(lambda x: tau1(x))\n\ndf", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "result = bool(sa.data.sum() == 0)", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "result = bool(sa.count_nonzero()) == 0\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores).pvalue\n", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n    ### END SOLUTION ###\n\np_value = f(pre_course_scores, during_course_scores)\nprint(p_value)\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "import scipy.stats\n\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interpolated = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interpolated(x, y)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a grid of points\n    xi, yi = np.mgrid[-1:1:10j, -2:0:10j]\n    # Compute the function values at the grid points\n    zi = (xi + yi)*np.exp(-6.0 * (xi * xi + yi * yi))\n    # Create a function from the grid points and function values\n    f = scipy.interpolate.interp2d(xi, yi, zi, kind='cubic')\n    # Compute the interpolated values\n    result = f(s, t)\n    ### END SOLUTION ###\n    return result\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "import numpy as np\n\n# Create a list of the indices of the regions occupied by the extra points\nresult = np.array([vor.point_region[point] for point in extraPoints])\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "import numpy as np\n\n# Create a list of indices for the extra points\nextra_indices = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the points in the voronoi diagram\nvor_points = [vor.vertices[i] for i in range(len(vor.vertices))]\n\n# Create a list of the regions of the voronoi diagram\nvor_regions = [vor.point_region[i] for i in range(len(vor.point_region))]\n\n# Create a list of the points in the extra points\nextra_points = [extraPoints[i] for i in range(len(extraPoints))]\n\n# Create a list of the regions of the extra points\nextra_regions = [vor.point_region[i] for i in range(len(extraPoints))]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i in range(len(extraPoints)) if extra_regions[i] in vor_regions]\n\n# Create a list of the indices of the extra points in the extra points\nextra_indices_in_extra = [i for i in range(len(extraPoints)) if extra_regions[i] in extra_regions]\n\n# Create a list of the indices of the extra points in the voronoi diagram\nextra_indices_in_vor = [i for i", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "\n# Find the maximum size among all vectors\nmax_size = max([vector.size for vector in vectors])\n\n# Create a sparse matrix with padded zeros\nresult = sparse.csc_matrix(np.array([vector + [max_vector_size - vector.size] for vector in vectors]))\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=[0, 1])", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[i, j] for i, j in zip(row, column)]\nprint(result)\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, :, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u, o2, x):\n    dev = abs((x - u) / o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer / 2\n    return P\n\nprob = NormalDistro(u, o2, x)\nprint(prob)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n!pip install scipy\n!pip install numpy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install pandas\n!pip install seaborn\n!pip install numpy\n!pip install scipy\n!pip install matplotlib\n!pip install", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "\n# Create a 2D array with random values\nA = np.random.rand(N, N)\n\n# Compute the DCT matrix\ndct_matrix = sf.dctn(A, norm='ortho')\n\n# Store the result in the variable 'result'\nresult = dct_matrix", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "offset = [-1, 0, 1]\nresult = sparse.diags(matrix, offset, shape=(5, 5)).toarray()\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "\ndef binomial_distribution(n, p):\n    return scipy.stats.binom.pmf(range(n + 1), n, p)\n\nresult = binomial_distribution(N, p)\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x), axis=0)\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "df['data'] = df.index\ndf['zscore'] = stats.zscore(df.values, axis=1)\nresult = df\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "df['data'] = df.apply(lambda row: pd.Series({'sample1': row['sample1'], 'sample2': row['sample2'], 'sample3': row['sample3']}), axis=1)\ndf['zscore'] = df.apply(lambda row: pd.Series({'sample1': stats.zscore(row['sample1']), 'sample2': stats.zscore(row['sample2']), 'sample3': stats.zscore(row['sample3'])}), axis=1)\n\ndf = df.drop(['sample1', 'sample2', 'sample3'], axis=1)\n\ndf = df.rename(columns={'sample1': 'data', 'sample2': 'data', 'sample3': 'data'})\n\ndf = df.round({'data': 3, 'zscore': 3})\n\nresult = df\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y = np.arange(shape[0])\nx = np.arange(shape[1])\nmid = np.array([[np.sqrt(i**2 + j**2) for j in x] for i in y])\nresult = distance.cdist(mid, mid)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.ogrid[:shape[0], :shape[1]]\nresult = distance.cdist(np.dstack((x, y)), np.array([[5., 4., 3., 3., 4., 5.], [4., 3., 2., 2., 3., 4.], [3., 2., 1., 1., 2., 3.], [3., 2., 1., 1., 2., 3.], [4., 3., 2., 2., 3., 4.], [5., 4., 3., 3., 4., 5.]]))", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    center = np.array([[shape[0] / 2, shape[1] / 2]])\n    distances = distance.cdist(center, [y, x], 'euclidean')\n    return distances\n    ### END SOLUTION ###\n!f(shape = (6, 6))\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((func(x) - y) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, method='Nelder-Mead')\nprint(out.x)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((y - func(x)) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, method='L-BFGS-B', bounds=[x_lower_bounds])\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) * (t < 2 * np.pi)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.hstack([sa, sb])", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\n# Load the data\nc = np.loadtxt('cooltemp.dat', unpack=True, usecols=[1])\n\n# Initialize an empty array to store the solutions\nI = np.empty(len(c))\n\n# Loop over the values of c\nfor n in range(len(c)):\n    # Define the equation\n    eqn = 2 * c[n] * x\n    # Integrate the equation\n    result, error = scipy.integrate.quad(eqn, 0, 1)\n    # Store the result in the array\n    I[n] = result\n\n# Print the array of solutions\nprint(I)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n\n# create an array of c values\nc_values = array([5, 10, 15, 20])\n\n# create an array to store the solutions\nI = array([])\n\n# loop over the c values\nfor c in c_values:\n    # calculate the solution\n    result = f(c=c)\n    # append the solution to the array\n    I = append(I, result)\n\n# print the solutions\nprint(I)\n", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.reshape(-1, 1)\nV_coords = V.keys()\nx_coords = x.keys()\nV_data = V.data\nx_data = x.data\nV_indices = V.indices\nx_indices = x.indices\nV_shape = V.shape\nx_shape = x.shape\nV_reshaped = V.reshape(-1, 1)\nx_reshaped = x.reshape(-1, 1)\nV_coords_reshaped = V.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nx_coords_reshaped = x.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nV_data_reshaped = V.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nx_data_reshaped = x.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nV_indices_reshaped = V.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nx_indices_reshaped = x.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nV_shape_reshaped = V.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nx_shape_reshaped = x.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nV_reshaped = V.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nx_reshaped = x.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nV_coords_reshaped = V.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nx_coords_reshaped = x.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nV_data_reshaped = V.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nx_data_reshaped = x.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nV_indices_reshaped = V.reshape(-1, 1).reshape(-1, 1, 1).reshape(-1, 1, 1, 1).reshape(-1, 1, 1, 1, 1)\nx_indices", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V += x", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V += x\nV += y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    column = sa[:, col].data\n    length = np.sqrt(np.sum(column**2))\n    sa[:, col] = column / length\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].toarray()\n    # get the column length\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    normalized_column = column / length\n    # update the original column in the sparse matrix\n    sa[:, col] = normalized_column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "\n# Convert the matrix to binary matrix\na = (a > 0).astype(int)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "\n# Convert the matrix to binary matrix\na = (a > 0).astype(int)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argmin(distances, axis=1)\nresult = closest_indices", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# Calculate the distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n\n# Find the indices of the minimum distance for each cluster\nindices = np.argmin(distances, axis=1)\n\n# Extract the closest point to each cluster\nresult = data[indices]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "distances = scipy.spatial.distance.cdist(data, centroids)\nclosest_indices = np.argpartition(distances, k, axis=0)[:k, :]\nresult = closest_indices", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result.append(fsolve(eqn, x0=x, args=(a, b)))\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result.append(fsolve(eqn, x0=x, args=(a,)))\nresult = np.array(result)\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[:, 1:]\nresult = result.tolist()\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Fit the data to the bekkers function\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nkstest_result = sp.stats.kstest(sample_data, bekkers(range_start, *popt))\n\n# Print the result\nprint(kstest_result)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Fit the data to the bekkers function\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nkstest_result = sp.stats.kstest(sample_data, bekkers(range_start, *popt))\n\n# Check the p-value at 95% confidence level\nresult = kstest_result.pvalue < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "\n# Convert the time column to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Set the time column as the index\ndf.set_index('Time', inplace=True)\n\n# Calculate the rolling integral\nintegral_df = df['A'].rolling(window='25S').apply(lambda x: integrate.trapz(x, x))\n\n# Reset the index\nintegral_df.reset_index(inplace=True)\n\n# Print the result\nprint(integral_df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "\n# Create a grid data object\ngrid_data = scipy.interpolate.GridData(x, y, eval)\n\n# Interpolate the value at the evaluation points\nresult = grid_data.evaluate()\n\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "\n# calculate the number of categories\nn_categories = a.nunique()\n\n# calculate the number of observations\nn_observations = a.shape[0]\n\n# calculate the number of observations in each category\nn_observations_in_category = a.value_counts()\n\n# calculate the weights\nweights = n_observations_in_category / n_observations\n\n# print the weights\nprint(weights)\n", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "\n# Create a mask to exclude the first and last elements\nmask = np.ones(arr.shape, dtype=bool)\nmask[0] = False\nmask[-1] = False\n\n# Find the indices of the elements that are less or equal to the neighbouring n elements\nindices = np.where((np.roll(arr, n) <= arr) & mask)[0]\n\n# The result is the indices of the elements that are relative extrema\nresult = indices\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "\n# Create a 2D array with the same shape as the input array\nresult_arr = np.zeros_like(arr)\n\n# Find the indices of the elements that are less or equal to the neighbouring n elements\nindices = np.where((arr[:, :-1] <= arr[:, 1:]) & (arr[:, 1:] <= arr[:, :-1]))\n\n# Store the indices in the result array\nresult_arr[indices] = indices\n\n# Print the result array\nprint(result_arr)\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "df = df[(np.abs(stats.zscore(df.select_dtypes(include=np.number))) < 3).all(axis=1)]\n", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                      columns=data['feature_names'] + ['target'])\n    return df\n\n# Call the function with the data\ndf = solve(data)\nprint(df)\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a list of unique elements in Col3\nunique_elements = df['Col3'].apply(list).apply(lambda x: set(x))\n\n# Create a dataframe with the unique elements as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(unique_elements))), columns=unique_elements)\n\n# Fill the dataframe with 1s where the corresponding element is in the list\ndf_out.loc[df['Col1'], unique_elements] = 1\n\n# Fill the dataframe with 0s where the corresponding element is not in the list\ndf_out.loc[~df['Col1'], unique_elements] = 0\n\ndf_out", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# Create a list of unique names\nunique_names = df['Col3'].apply(lambda x: list(x)).apply(lambda x: [item for sublist in x for item in sublist])\nunique_names = list(set(unique_names))\n\n# Create a dataframe with unique names as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(unique_names))), columns=unique_names)\n\n# Update the dataframe with 1 for the names that exist in the original dataframe\nfor index, row in df.iterrows():\n    for name in row['Col3']:\n        df_out.loc[index, name] = 1\n\ndf_out", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Get the unique elements in the last column\nunique_elements = df['Col4'].apply(lambda x: list(set(x)))\n\n# Create a dataframe with the unique elements as columns\ndf_out = pd.get_dummies(unique_elements)\n\n# Add the original dataframe as a new column\ndf_out['Col4'] = df['Col4']\n\n# Drop the original column\ndf_out.drop('Col4', axis=1, inplace=True)\n\n# Rename the new column to 'Col4'\ndf_out.rename(columns={'Col4': 'Col4'}, inplace=True)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col3'].apply(lambda x: list(set(x)))\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'].apply(lambda x: pd.Series(x)).stack()).sum(level=0)\n\n# Add the original dataframe and the one-hot-encoded dataframe\ndf_out = pd.concat([df.drop('Col3', axis=1), df_out], axis=1)\n\n# Fill the NaN values with 0\ndf_out.fillna(0, inplace=True)\n\n# Reset the index\ndf_out.reset_index(drop=True, inplace=True)\n\nprint(df_out)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col3'].apply(lambda x: list(set(x)))\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'].apply(lambda x: pd.Series(x)).stack()).sum(level=0)\n\n# Add the original dataframe and the one-hot-encoded dataframe\ndf_out = pd.concat([df.drop('Col3', axis=1), df_out], axis=1)\n\n# Fill the NaN values with 0\ndf_out.fillna(0, inplace=True)\n\n# Reset the index\ndf_out.reset_index(drop=True, inplace=True)\n\nprint(df_out)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "\n# Convert the sparse matrix to a DataFrame\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Concatenate the original dataframe and the transformed dataframe\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "\n# Convert the csr_matrix to a pandas DataFrame\ndf_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Merge the original DataFrame and the transformed DataFrame\ndf = pd.concat([df_origin, df_transform_output], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # Convert the sparse matrix to a DataFrame\n    df_transform_output = pd.DataFrame.sparse.from_spmatrix(transform_output)\n    # Concatenate the original DataFrame and the transformed DataFrame\n    result = pd.concat([df, df_transform_output], axis=1)\n    return result\n    ### END SOLUTION ###\n!!solve(df_origin, transform_output)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.pop(1) # remove the second step\nclf.steps.insert(1, ('reduce_poly', PolynomialFeatures())) # insert a new step\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\nclf.steps.append(('new_step', PolynomialFeatures()))\nclf.fit(np.array([[1, 2, 3], [4, 5, 6]]), np.array([1, 2]))\nprint(clf.predict(np.array([[1, 2, 3], [4, 5, 6]])))\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps.append(('new_step', SVC()))\nclf.steps.remove(('reduce_poly', PolynomialFeatures()))\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PCA()))\nclf.steps\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5],\n    'max_depth': [3, 4, 5, 6, 7, 8],\n    'n_estimators': [100, 200, 300, 400, 500]\n}\nb = []\nc = []\nfor i in range(5):\n    gridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX, trainY)\n    b.append(gridsearch.best_score_)\n    c.append(gridsearch.predict(testX))\nprint(b)\nprint(c)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5],\n    'max_depth': [3, 4, 5, 6, 7, 8],\n    'n_estimators': [100, 200, 300, 400, 500]\n}\nfit_params = {\"early_stopping_rounds\": 42,\n             \"eval_metric\" : \"mae\",\n             \"eval_set\" : [[testX, testY]]}\nb, c = gridsearch.fit(trainX, trainY, fit_params=fit_params, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX, trainY)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_predicted = scaler.inverse_transform(scaled['t'])\n    # check score\n    score = check_score(t_predicted)\n    # return the solution in this function\n    return t_predicted, score\n", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = str(model)\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nclf = grid_search.best_estimator_\nproba = clf.predict_proba(X_test)", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n\n# example variable\ntexts = [\"Hello World\", \"Python is cool\", \"Data science is fun\"]\n\n# fit the vectorizer\ntfidf.fit(texts)\n\n# transform the texts\ntfidf_matrix = tfidf.transform(texts)\n\n# print the transformed texts\nprint(tfidf_matrix.toarray())\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Convert the DataFrame to a numpy array\ndata_array = data.values\n\n# Apply the preprocessing.scale function to the numpy array\nscaled_data = preprocessing.scale(data_array)\n\n# Convert the scaled numpy array back to a DataFrame\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Assign the scaled DataFrame to the variable df_out\ndf_out = scaled_data_df\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a numpy array from the DataFrame\narray = data.values\n\n# Apply preprocessing.scale to the numpy array\nscaled_array = preprocessing.scale(array)\n\n# Convert the scaled numpy array back to a DataFrame\ndf_out = pd.DataFrame(scaled_array, columns=data.columns)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = X.columns[model.get_support()]\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "import numpy as np\n\n# get the indices of the selected features\nselected_indices = np.where(model.get_support())[0]\n\n# get the names of the selected features\ncolumn_names = X.columns[selected_indices]\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns[model.get_support()].tolist()\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[km.labels_ == km.labels_[p]]\nclosest_50_samples = closest_50_samples[:50]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    closest_samples = centers[p]\n    return closest_samples\n    ### END SOLUTION ###\n\n# Call the function with the desired p value\nclosest_samples = get_samples(p, X, km)\nprint(closest_samples)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Gaussian kernel\nkernel = 'rbf'\n\n# Create a SVM classifier\nclf = svm.SVC(kernel=kernel)\n\n# Fit the classifier to the training data\nclf.fit(X_train, y_train)\n\n# Predict the labels of the testing data\npredict = clf.predict(X_test)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "\ntfidf_query = tfidf.transform(queries)\ncosine_similarities_of_queries = np.array([np.sqrt(1 - np.sum(np.square(tfidf_query - tfidf_doc))) for tfidf_doc in tfidf.transform(documents)])\ncosine_similarities_of_queries = pd.DataFrame(cosine_similarities_of_queries, index=queries, columns=documents)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "\ntfidf_query = tfidf.transform(queries)\ncosine_similarities_of_queries = np.dot(tfidf_query, tfidf_query.T)", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_query = TfidfVectorizer().fit_transform(queries)\n    cosine_similarities_of_queries = tfidf_query.dot(tfidf.fit_transform(documents)).toarray()\n    return cosine_similarities_of_queries\n    ### END SOLUTION ###\n\n# Call the function with your data\ncosine_similarities_of_queries = solve(queries, documents)\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# Convert the list of lists to a pandas DataFrame\ndf = pd.DataFrame(f)\n\n# Create a binary matrix where each row corresponds to a feature and each column corresponds to a variant\nbinary_matrix = df.apply(lambda x: pd.get_dummies(x, drop_first=True))\n\n# Convert the binary matrix to a numpy array\nnew_f = binary_matrix.to_numpy()\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n# Convert the features to a 2D array\nnew_features = np.array(features).reshape(-1, 1)\n\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n\n    # Create a DataFrame from the array\n    df = pd.DataFrame(features_array, columns=features[0])\n\n    # Convert the DataFrame to a 2D array\n    new_features = df.values\n\n    ### END SOLUTION ###\n    return new_features\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Convert the list of lists to a pandas DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D array with ones for the features that are present and zeros for the features that are not present\nnew_features = np.where(df == 1, 1, 0)\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from the data\ndistances = pd.DataFrame(data_matrix)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data\ncluster_labels = cluster.fit_predict(distances)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the distance matrix\ncluster_labels = cluster.fit_predict(dist_matrix)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from the similarity matrix\ndistances = pd.DataFrame(simM)\n\n# Create an instance of the AgglomerativeClustering class\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the distance matrix\ncluster_labels = cluster.fit_predict(distances)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# Perform hierarchical clustering\ncluster_labels = fcluster(linkage_matrix, 2, criterion='maxclust')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "import scipy.cluster.hierarchy as shc\n\n# Create a linkage matrix\nlinkage_matrix = shc.linkage(data_matrix, method='average')\n\n# Perform hierarchical clustering\ncluster_labels = shc.fcluster(linkage_matrix, 2, criterion='maxclust')\n\ncluster_labels", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix\nlinkage_matrix = linkage(simM, 'ward')\n\n# Perform hierarchical clustering\ncluster_labels = fcluster(linkage_matrix, 2, criterion='maxclust')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import PowerTransformer\n\nbox_cox = PowerTransformer(method='box-cox')\nbox_cox_data = box_cox.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(punctuation='!\"?\"\\'')\ntransformed_text = vectorizer.fit_transform(text)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features (x) and the target (y)\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    from sklearn.model_selection import train_test_split\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION\n!solve(dataset)\n", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "\nmse_values = df['mse'].values\nreshaped_mse_values = mse_values.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2, n_init=10).fit(reshaped_mse_values)\nlabels = kmeans.predict(reshaped_mse_values)\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# Reshape the data\nX = df['mse'].values.reshape(-1, 1)\n\n# Create a KMeans instance with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit the model to the data\nkmeans.fit(X)\n\n# Get the labels\nlabels = kmeans.predict(X)\n\n# Get the centroids\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "\n# Fit the model\nmodel = LinearSVC(penalty='l1', dual=False)\nmodel.fit(X, y)\n\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[model.coef_ != 0]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "selector = LinearSVC(penalty='l1', dual=False).fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selector.coef_ != 0]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    svc = LinearSVC(penalty='l1', dual=False)\n    svc.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\n    return selected_feature_names\n    ### END SOLUTION ###\n\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vocabulary = ['Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX']\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\nfeature_names = np.array(feature_names)\nfeature_names = np.sort(feature_names)\nX = np.array([np.where(feature_names == word, 1, 0) for word in feature_names])\nfeature_names = feature_names.tolist()\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nvectorizer.fit(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = vectorizer.transform(corpus).toarray()\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[['Time', col]].dropna()\n    X, Y = df2['Time'].values.reshape(-1, 1), df2[col].values.reshape(-1, 1)\n    slope = LinearRegression().fit(X, Y)\n    slopes.append(slope.coef_[0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(transformed_df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n    ### END SOLUTION\n\ndf = Transform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION ###\n\nnp_array = Transform(np_array)\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close[-1:], ma50[-1:], ma100[-1:], ma200[-1:]])\npredict", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array(X)\nnew_X[:, 0] = new_X[:, 0].astype(str)\nnew_X[:, 1] = new_X[:, 1].astype(str)\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array(X)\nnew_X = new_X.astype(float)\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array(X)\nnew_X = new_X.astype(float)\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Separating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n\n# Predict\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the dataframe to numpy arrays\nX = dataframe.iloc[:, :-1].astype(float).values\ny = dataframe.iloc[:, -1].values\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model with the data\nlogReg.fit(X, y)\n# Predict the class of the animal\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by=\"date\")\n\n# Calculate the number of days in the dataframe\nnum_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the test set\ntest_days = num_days * 0.8\n\n# Calculate the number of days in the train set\ntrain_days = num_days - test_days\n\n# Create a range of days for the train set\ntrain_range = np.arange(0, train_days)\n\n# Create a range of days for the test set\ntest_range = np.arange(train_days, num_days)\n\n# Create a dataframe for the train set\ntrain_dataframe = features_dataframe.loc[train_range]\n\n# Create a dataframe for the test set\ntest_dataframe = features_dataframe.loc[test_range]\n\n# Sort the train and test dataframes by date\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Calculate the number of days in the dataframe\nnum_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the test set\ntest_days = int(num_days * 0.2)\n\n# Calculate the number of days in the train set\ntrain_days = num_days - test_days\n\n# Create the train and test dataframes\ntrain_dataframe = features_dataframe.iloc[:train_days]\ntest_dataframe = features_dataframe.iloc[train_days:]\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    features_dataframe['date'] = pd.to_datetime(features_dataframe['date'])\n    features_dataframe = features_dataframe.sort_values('date')\n    train_dataframe = features_dataframe.iloc[:int(len(features_dataframe)*0.2)]\n    test_dataframe = features_dataframe.iloc[int(len(features_dataframe)*0.2):]\n    return train_dataframe, test_dataframe\n    ### END SOLUTION\n\nfeatures_dataframe = solve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2', 'X3']] = scaler.fit_transform(df[['X2', 'X3']])\ndf['X2_scale'] = df[['X2', 'X3']].apply(lambda x: x[0]*100, axis=1)\ndf['X3_scale'] = df[['X2', 'X3']].apply(lambda x: x[1]*100, axis=1)\ndf", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "\nmyData['new_A2'] = scaler.fit_transform(myData[['Month', 'A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['Month', 'A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results.sort_values(by='rank_test_score', inplace=True)\nfull_results.reset_index(drop=True, inplace=True)\nfull_results\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values('mean_fit_time')\nfull_results", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport joblib\njoblib.dump(fitted_model, 'sklearn_model')\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix, tfidf_matrix)\ncosine_similarity_matrix", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\noptimizer = optim.param_groups[0]['lr']\noptim.param_groups[0]['lr'] = 0.001\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define a function to update the learning rate\ndef update_learning_rate(optimizer, epoch):\n    if epoch > 10:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.0005\n    else:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.05\n\n# Apply the function to the optimizer\noptim.step()\noptim.zero_grad()\nupdate_learning_rate(optim, epoch)\noptim.step()\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the word2vec model to a PyTorch embedding layer\nembedding = torch.tensor(word2vec.wv.vectors)\n\n# Create a tensor with your input data\ninput_Tensor = torch.tensor(input_Tensor)\n\n# Embed the input data using the pre-trained word2vec weights\nembedded_input = torch.nn.functional.embedding(input_Tensor, embedding)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # Convert the input tensor to a numpy array\n    input_array = input_Tensor.numpy()\n\n    # Get the word2vec weights\n    weights = word2vec.wv.vectors\n\n    # Create a PyTorch embedding layer with the weights\n    embedding_layer = torch.nn.Embedding.from_pretrained(torch.tensor(weights))\n\n    # Embed the input data using the embedding layer\n    embedded_input = embedding_layer(input_array)\n\n    return embedded_input\n    ### END SOLUTION ###\n\nget_embedded_input(input_Tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.numpy())", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "\nA_log = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n\n# Convert ByteTensor to LongTensor\nA_log = A_log.long()\n\n# Use logical indexing to get the columns that correspond to a 1 value in the index vector\nC = B[:, A_log]\n\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "\n# Convert the ByteTensor to a numpy array\nA_logical_np = A_logical.numpy()\n\n# Use numpy slicing to select the columns\nC = B[:, A_logical_np]\n\n# Convert the numpy array back to a torch tensor\nC = torch.from_numpy(C)\n\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "\nC = B[:, A_log.bool()]\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "\nA_log = torch.ByteTensor([0, 1, 0])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n\n# Create a mask where A_log is True\nmask = A_log.unsqueeze(1).expand_as(B)\n\n# Use the mask to index B\nC = B[mask]\n\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    return C\n\nA_log = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = solve(A_log, B)\nprint(C)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "\n# Convert the ByteTensor to a numpy array\nA_log_np = A_log.numpy()\n\n# Use numpy to select the columns\nC = B[:, A_log_np]\n\n# Convert the numpy array back to a torch tensor\nC = torch.from_numpy(C)\n\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Convert the tensor to numpy array\nB_np = B.numpy()\n\n# Use numpy to select the columns\nC_np = B_np[:, idx.numpy()]\n\n# Convert the numpy array back to tensor\nC = torch.from_numpy(C_np)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    return torch.tensor(a, dtype=torch.float16)\n    ### END SOLUTION ###\n\n# Test the function\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\n# Create a tensor of zeros with the same shape as the lens tensor\nmask = torch.zeros_like(lens)\n\n# Fill the mask tensor with ones where the lens tensor is greater than 0\nmask[lens > 0] = 1\n\n# Print the mask tensor\nprint(mask)\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\n# Create a tensor of zeros with the same shape as the lens tensor\nmask = torch.zeros_like(lens)\n\n# Create a tensor of ones with the same shape as the lens tensor\nones = torch.ones_like(lens)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices = torch.arange(lens.numel()).unsqueeze(1).expand(-1, -1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.numel(), 1)\n\n# Create a tensor of indices with the same shape as the lens tensor\nindices_repeated = indices.repeat(lens.", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask.long()\n    ### END SOLUTION ###\n\nget_mask(lens)\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with the same shape as the input tensor\nTensor_2D_diagonal = torch.diag(Tensor_2D)\n\n# Create a 3D tensor with the same shape as the input tensor\nTensor_3D = Tensor_2D_diagonal.unsqueeze(0).unsqueeze(0)\n\n# Print the 3D tensor\nprint(Tensor_3D)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    index_in_batch = t.shape[0]\n    diag_ele = t.shape[1]\n    Matrix = torch.diag(t)\n    result = torch.zeros(index_in_batch, diag_ele)\n    result[:, :] = Matrix[:, :]\n    return result\n    ### END SOLUTION ###\n\n# Test the function\nt = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(Convert(t))\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]])\nab = torch.stack((a, b), 0)\nprint(ab.shape)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "\n# Convert tensors to numpy arrays\na_np = a.cpu().numpy()\nb_np = b.cpu().numpy()\n\n# Stack numpy arrays\nab_np = np.stack((a_np, b_np), axis=0)\n\n# Convert numpy array back to tensor\nab = torch.from_numpy(ab_np)\n\n# Convert tensor to GPU if available\nif torch.cuda.is_available():\n    ab = ab.to('cuda')\n\nprint(ab.shape) # (138, 514)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    a = torch.tensor(a)\n    b = torch.tensor(b)\n    ab = torch.stack((a, b), 0)\n    return ab.numpy()\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "\na[torch.arange(10), lengths, :] = 0\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "\na[torch.arange(10), lengths, :] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[torch.arange(10), lengths - 1, :] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "\na[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.tensor(lt, dtype=torch.float32)\n    return tensor_of_tensors\n    ### END SOLUTION ###\n\n# Call the function with your list of tensors\ntensor_of_tensors = Convert(list_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "\nresult = torch.gather(t, 1, torch.tensor(idx, dtype=torch.long).unsqueeze(-1).unsqueeze(-1))\nresult = result.squeeze()\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "\nresult = t[np.arange(t.shape[0])[:, None], idx - 1]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "\nresult = t[range(len(t)), idx - 1]", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\n# Flatten the tensor to 1D\nids = ids.view(-1)\n\n# Gather the scores based on the ids\nresult = torch.gather(x, 1, ids.unsqueeze(-1).unsqueeze(-1)).squeeze(1)\n\n# Get the maximum score\nmax_score = result.max(1)[0]\n\n# Get the indices of the maximum score\nmax_indices = torch.where(result == max_score)[0]\n\n# Gather the maximum score\nresult = torch.gather(max_score, 1, max_indices.unsqueeze(-1)).squeeze(1)\n\n# Get the indices of the maximum score\nmax_indices = torch.where(max_score == result)[0]\n\n# Gather the maximum score\nresult = torch.gather(x, 1, max_indices.unsqueeze(-1)).squeeze(1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\n# Flatten the tensor x\nx_flat = x.view(-1, 114)\n\n# Compute the maximum scores for each row\nmax_scores = x_flat.max(dim=1)[0]\n\n# Create a mask to select the rows with the maximum scores\nmask = max_scores.unsqueeze(1).expand_as(x_flat)\n\n# Gather the selected rows\nresult = x_flat[mask]\n\n# Reshape the result to the original shape\nresult = result.view(-1, 3, 114)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask where the ids tensor is 1\nmask = ids == 1\n\n# Gather the scores from the x tensor where the mask is True\nselected_scores = x[mask].gather(1, ids.unsqueeze(-1)).squeeze(1)\n\n# Find the maximum score\nmax_score = selected_scores.max()\n\n# Create a mask where the selected scores are equal to the maximum score\nresult_mask = selected_scores == max_score\n\n# Gather the ids where the result mask is True\nresult = ids[result_mask]\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "\n# Get the maximum value from the softmax output\nmax_values = torch.max(softmax_output, 1)[0]\n\n# Create a tensor with the class indices\nclass_indices = torch.argmax(softmax_output, 1)\n\n# Create a tensor with the class indices\ny = class_indices.unsqueeze(1)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "\n_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1, 1)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "\n# Get the maximum value from the softmax output\nmax_values = torch.max(softmax_output, 1)[0]\n\n# Create a tensor of zeros with the same shape as the softmax output\ny = torch.zeros_like(softmax_output)\n\n# Set the values in the tensor to the maximum value\ny.scatter_(0, max_values.unsqueeze(1), 1)\n\n# Print the tensor y\nprint(y)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    _, predicted = torch.max(softmax_output, 1)\n    return predicted.view(-1, 1)\n    ### END SOLUTION ###\n\n# Test the function\nprint(solve(softmax_output))\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "\n# Get the maximum value from the softmax output\nmax_value = torch.max(softmax_output, 1)[0]\n\n# Create a tensor of the same shape as the softmax output with the maximum value\ny = torch.full_like(softmax_output, max_value)\n\n# Create a tensor of the same shape as the softmax output with the indices of the maximum value\nindices = torch.argmax(softmax_output, 1)\n\n# Create a tensor of the same shape as the softmax output with the indices of the maximum value\ny = torch.full_like(softmax_output, indices)\n\n# Subtract the indices from the maximum value to get the class labels\ny = y - indices\n\n# Add 1 to the class labels to get the class labels as 0, 1, 2\ny = y + 1\n\n# Convert the class labels to long tensor\ny = y.long()\n\nreturn y\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "\n# Convert labels to one-hot encoding\nlabels = F.one_hot(labels, num_classes=7).float()\n\n# Calculate cross entropy loss\nloss = F.cross_entropy(input=images, target=labels, reduction='none')\n\n# Average over the batch dimension\nloss = loss.mean()\n\n# Print the loss\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "\ncnt_equal = torch.sum((A == B).float())\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "\ncnt_equal = torch.sum((A == B).float())\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "\ncnt_not_equal = torch.sum((A != B).bool())\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = (A == B).sum().item()\n    ### END SOLUTION ###\n    return cnt_equal\n\nA = torch.randn(1000, 1)\nB = torch.randn(1000, 1)\n\nprint(Count(A, B))\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "\ncnt_equal = torch.sum((A[-x:] == B[-x:]).float())\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "\ncnt_not_equal = torch.sum((A[-x:] != B[-x:]).bool())", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensor_31 = a[:, :, :, i:i + chunk_dim, :]\n    tensors_31.append(tensor_31)\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 10, 31, 1)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensor_31 = a[:, :, i, :, :]\n    tensors_31.append(tensor_31)\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 31, 10, 1)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "\noutput[mask.bool()] = clean_input_spectrogram[mask.bool()]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x))\nmin_y = torch.min(torch.abs(y))\nsigned_min = torch.where(x < y, min_x, min_y) * sign_x\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.where(x >= 0, sign_x, -sign_y) * max\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x))\n    min_y = torch.min(torch.abs(y))\n    signed_min = torch.where(x < 0, -min_x, min_x)\n    signed_min = torch.where(y < 0, -min_y, min_y)\n    return signed_min\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    confidence_score = conf.item()\n    return confidence_score, class_names[classes.item()]\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# Get the last column of a and the first column of b\nlast_a, first_b = a[:, -1:], b[:, 0]\n\n# Compute the average of the last column of a and the first column of b\nresult = torch.cat([last_a, (first_b + last_a) / 2], dim=1)\n\n# Remove the last column of a and the first column of b\nresult = result[:, 1:-1]\n\n# Add the last column of a and the first column of b to the result\nresult = torch.cat([result, last_a, first_b], dim=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a[:, :-1]\n    b = b[:, 1:]\n    result = torch.cat([a, b], dim=1)\n    return result\n    ### END SOLUTION ###\n\n# Test the function\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\nprint(solve(a, b))\n!", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "\n# Create a tensor with the same shape as t, but with all elements set to 0\nresult = torch.zeros_like(t)\n\n# Set the first row of the result tensor to be the same as the first row of t\nresult[:1, :] = t[:1, :]\n\n# Set the second row of the result tensor to be the same as the second row of t\nresult[1:, :] = t[1:, :]\n\n# Set the third row of the result tensor to be the same as the third row of t\nresult[2:, :] = t[2:, :]\n\n# Set the fourth row of the result tensor to be the same as the fourth row of t\nresult[3:, :] = t[3:, :]\n\n# Set the fifth row of the result tensor to be the same as the fifth row of t\nresult[4:, :] = t[4:, :]\n\n# Set the sixth row of the result tensor to be the same as the sixth row of t\nresult[5:, :] = t[5:, :]\n\n# Set the seventh row of the result tensor to be the same as the seventh row of t\nresult[6:, :] = t[6:, :]\n\n# Set the eighth row of the result tensor to be the same as the eighth row of t\nresult[7:, :] = t[7:, :]\n\n# Set the ninth row of the result tensor to be the same as the ninth row of t\nresult[8:, :] = t[8:, :]\n\n# Set the tenth row of the result tensor to be the same as the tenth row of t\nresult[9:, :] = t[9:, :]\n\n# Set the eleventh row of the result tensor to be the same as the eleventh row of t\nresult[10:, :] = t[10:, :]\n\n# Set the twelfth row of the result tensor to be the same as the twelfth row of t\nresult[11:, :] = t[11:, :]\n\n# Set the thirteenth row of the result tensor to be the same as the thirteenth row of t\nresult[12:, :] = t[12:, :]\n\n# Set the fourteenth row of the result tensor to be the same as the fourteenth row of t\nresult[13:, :] = t[13:, :]\n\n# Set the fifteenth row of the result tensor to be the same as the fifteenth row of t\nresult[14:, :] = t[14:, :]\n\n# Set the sixteenth row of the result tensor to be the same as the sixteenth row of t\nresult[15:, :] = t[15:, :]\n\n# Set the seventeenth row of the result tensor to be the same as the seventeenth row of t\nresult[16:, :] = t[16:, :]\n\n# Set the eightteenth row of the result tensor to be the same as the eightteenth row of t\nresult[17:, :] = t[17:, :]\n\n# Set the nineteenth row of the result tensor to be the same as the nineteenth row of t\nresult[18:, :] = t[18:, :]\n\n# Set the twentieth row of the result tensor to be the same as the twentieth row of t\nresult[19:, :] = t[19:, :]\n\n# Set the twenty-first row of the result tensor to be the same as the twenty-first row of t\nresult[20:, :] = t[20:, :]\n\n# Set the twenty-second row of the result tensor to be the same as the twenty-second row of t\nresult[21:, :] = t[21:, :]\n\n# Set the twenty-third row of the result tensor to be the same as the twenty-third row of t\nresult[22:, :] = t[22:, :]\n\n# Set the twenty-fourth row of the result tensor to be the same as the twenty-fourth row of t\nresult[23:, :] = t[23:, :]\n\n# Set the twenty-fifth row of the result tensor to be the same as the twenty-f", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "\n# Create a tensor with the same shape as t, but with all elements set to 0\nnew = torch.zeros_like(t)\n\n# Stack t and new along the first dimension\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "\n# Create a tensor with the shape of (1, 4, 2) and fill it with -1\nt = torch.full((1, 4, 2), -1)\n\n# Create a tensor with the shape of (1, 1, 2) and fill it with the first row of t\nnew = torch.unsqueeze(t[0], 0)\n\n# Stack t and new\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data.view(10, 2 * 3, 1), W.view(1, 1, hid_dim)).squeeze()\nresult = result.view(10, 2, 3)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
