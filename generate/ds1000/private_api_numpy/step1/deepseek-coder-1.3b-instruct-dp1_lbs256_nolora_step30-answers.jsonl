{"id": 0, "code": "result = df.reindex(List)\nprint(result)\n", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "\nresult = df.reindex(List)\nprint(result)\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n\n# First, we create a mask for the rows where the value count is 2 or more\nmask = df.apply(lambda x: pd.Series(x) >= 2, axis=1).any(axis=1)\n\n# Then, we create a dictionary where the keys are the unique values in the column and the values are the new values\nnew_values = {v: 'other' for v in df.Qu1.unique() if df[df.Qu1 == v].shape[0] < 2}\n\n# We replace the values in the dataframe\ndf.Qu1.replace(new_values, inplace=True)\n\n# We repeat the process for Qu2\nnew_values = {v: 'other' for v in df.Qu2.unique() if df[df.Qu2 == v].shape[0] < 2}\ndf.Qu2.replace(new_values, inplace=True)\n\n# We repeat the process for Qu3\nnew_values = {v: 'other' for v in df.Qu3.unique() if df[df.Qu3 == v].shape[0] < 2}\ndf.Qu3.replace(new_values, inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n\n# First, get the value counts for each column\ncounts = df.apply(pd.value_counts)\n\n# Then, create a mask for the rows where the value count is less than 3\nmask = counts < 3\n\n# Finally, replace the values in the dataframe where the mask is True with 'other'\ndf.loc[mask, 'Qu1'] = 'other'\ndf.loc[mask, 'Qu2'] = 'other'\ndf.loc[mask, 'Qu3'] = 'other'\n\nresult = df\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "\n    for col in df.columns:\n        counts = df[col].value_counts()\n        for index, value in counts.items():\n            if value >= 2:\n                df[col] = df[col].replace(index, 'other')\n    return df\n    ### END SOLUTION\n\nprint(f(example_df))\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n\n# First, count the number of occurrences for each column\ncounts = df.count()\n\n# Then, create a mask for the rows where the count is greater or equal to 3\nmask = counts >= 3\n\n# Apply the mask to the dataframe\ndf['Qu1'] = df['Qu1'].mask(mask)\n\n# For the second column, create a mask where the count is greater or equal to 2\nmask = counts >= 2\n\n# Apply the mask to the dataframe\ndf['Qu2'] = df['Qu2'].mask(mask)\ndf['Qu3'] = df['Qu3'].mask(mask)\n\n# For the third column, create a mask where the count is less than 3\nmask = counts < 3\n\n# Apply the mask to the dataframe\ndf['Qu1'] = df['Qu1'].mask(mask)\ndf['Qu3'] = df['Qu3'].mask(mask)\n\n# For the fourth column, create a mask where the count is less than 2\nmask = counts < 2\n\n# Apply the mask to the dataframe\ndf['Qu2'] = df['Qu2'].mask(mask)\n\nresult = df\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n\n# First, we need to find the indices of the values that are not 'apple'\nnot_apple_indices = [i for i in range(len(df)) if df.iloc[i]['Qu1'] != 'apple']\n\n# Then, we create a new column 'Qu1_new' with the new values\ndf['Qu1_new'] = df['Qu1']\n\n# For the 'Qu2' column, we find the indices that are not 'apple' and 'egg'\nnot_apple_egg_indices = [i for i in range(len(df)) if df.iloc[i]['Qu2'] != 'apple' and df.iloc[i]['Qu2'] != 'egg']\n\n# We replace the values in the 'Qu2' column with 'other'\ndf.loc[not_apple_egg_indices, 'Qu2'] = 'other'\n\n# For the 'Qu3' column, we find the indices that are not 'apple' and 'egg'\nnot_apple_egg_indices = [i for i in range(len(df)) if df.iloc[i]['Qu3'] != 'apple' and df.iloc[i]['Qu3'] != 'egg']\n\n# We replace the values in the 'Qu3' column with 'other'\ndf.loc[not_apple_egg_indices, 'Qu3'] = 'other'\n\n# Finally, we drop the old 'Qu1' column\ndf = df.drop('Qu1', axis=1)\n\n# Rename the new 'Qu1' column to 'Qu1_new'\ndf = df.rename(columns={'Qu1_new': 'Qu1'})\n\nresult = df\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "\ndf['keep_if_dup'] = df.groupby('url')['keep_if_dup'].transform(lambda x: 'Yes' if x.nunique()>1 else x.iloc[0])\nresult = df.drop_duplicates(subset='url', keep='first')\n", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "\ndf['drop_if_dup'] = df.groupby('url')['drop_if_dup'].transform(lambda x: 'Yes' if x.nunique()>1 else x.iloc[0])\nresult = df.drop_duplicates(subset=['url', 'drop_if_dup'], keep='first')\n", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\ndf['id'] = df.groupby('url').cumcount() + 1\nresult = df.drop_duplicates(subset=['id', 'url'], keep='last')\nresult = result.drop(columns=['id'])\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndf_melt = pd.melt(df, id_vars=['name'], var_name='v2', value_name='v3')\ndf_grouped = df_melt.groupby(['name', 'v2']).v3.sum().reset_index()\nresult = df_grouped.set_index('name')['v3'].to_dict()\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n    ### END SOLUTION ###\n\nprint(f(example_df))\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# First, we need to parse the message to extract the key-value pairs\npairs = [re.findall(r'\\[(.*?)\\]', i) for i in df['message']]\n\n# Then, we need to convert these pairs into a dictionary\ndict_pairs = [dict(pair) for pair in pairs]\n\n# Now, we can create a new dataframe with these dictionaries\ndf_new = pd.DataFrame(dict_pairs)\n\n# Finally, we can merge the original dataframe with the new one\nresult = pd.merge(df, df_new, on='name')\n\n# We need to rename the columns to match the expected output\nresult.rename(columns={0: 'job', 1: 'money', 2: 'wife', 3: 'group', 4: 'kids'}, inplace=True)\n\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "\ndf['score'] = df.apply(lambda row: row['score'] * 10 if row['product'] in products else row['score'], axis=1)\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "\ndf['score'] = df['product'].apply(lambda x: 10 if x in products else x)\ndf['score'] = df['score'] * 10\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "\nfor product in products:\n    df.loc[df['product'].between(product[0], product[1]), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\ndf['score'] = df.apply(lambda row: 1 if row['product'] in products else row['score'], axis=1)\ndf['score'] = df['score'] / df['score'].max()\ndf.drop('product', axis=1, inplace=True)\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "\ndf['category'] = df.idxmax(axis=1)\ndf\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "\ndf['category'] = df.idxmax(axis=1)\ndf\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\n# First, create a list of the binary columns\nbinary_columns = ['A', 'B', 'C', 'D']\n\n# Then, create a new column that will hold the categories\ndf['category'] = ''\n\n# Iterate over each row\nfor index, row in df.iterrows():\n    # For each row, iterate over each binary column\n    for column in binary_columns:\n        # If the value is 1, add the column to the category\n        if row[column] == 1:\n            df.at[index, 'category'] += ' ' + column\n\n# Remove the leading space from the category column\ndf['category'] = df['category'].str.lstrip()\n\ndf\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf = df[df['Date'].between('17-Jan-2019', '20-Feb-2019')]\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "\ndf_shifted = df.shift(1, axis=1)\ndf_shifted.columns = ['#1', '#2']\ndf_shifted.iloc[0] = df.iloc[0]\ndf_shifted = df_shifted.reindex(df.index)\n\nprint(df_shifted)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "\ndf = df.iloc[:, ::-1].reset_index(drop=True)\ndf.columns = ['#2', '#1']\ndf = df.iloc[1:].reset_index(drop=True)\ndf.columns = ['#2', '#1']\ndf = df.iloc[:, ::-1].reset_index(drop=True)\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "\ndf_shifted = df.shift(1, axis=1)\ndf_shifted.columns = ['#1', '#2']\ndf_shifted.iloc[0] = df.iloc[0]\ndf_shifted.iloc[-1] = df.iloc[-1]\n\ndf_shifted", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n\n# First, we shift the first row of the first column down by 1 row\ndf['#1'] = df['#1'].shift(-1)\n\n# Then, we shift the last row of the first column up by 1 row\ndf['#2'] = df['#2'].shift(1)\n\ndf = df.iloc[:, ::-1]\n\ndf = df.dropna()\n\ndf\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.rename(columns={'HeaderA': 'HeaderAX', 'HeaderB': 'HeaderBX', 'HeaderC': 'HeaderCX'}, inplace=True)\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col if i > 0 else 'XHeader' for i, col in enumerate(df.columns)]\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "\n# Create a list of all column names\nold_cols = df.columns.tolist()\n\n# Create a list of new column names\nnew_cols = ['XHeader' + col if 'X' not in col else col for col in old_cols]\n\n# Rename the columns\ndf.columns = new_cols\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "\ndf.columns = [i for i in df.columns if 'val' in i]\nresult = df.groupby('group').agg({col: 'mean' for col in df.columns})\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "\ndf.columns = [i for i in df.columns if 'val' in i]\nresult = df.groupby('group').agg({col: 'sum' for col in df.columns})\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndf.rename(columns={'val1': 'val1_sum', 'val2': 'val2_mean', 'val42': 'val42_sum'}, inplace=True)\ndf.groupby('group')[['group_color', 'val1_sum', 'val2_mean', 'val42_sum']].agg({'group_color': 'first', 'val1_sum': 'sum', 'val2_mean': 'mean', 'val42_sum': 'sum'})\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "\nresult = df.loc[row_list, column_list].mean()", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "\nresult = df.loc[row_list, column_list].sum()", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "\nresult = df.loc[row_list, column_list].sum()\nresult = result.drop(result.idxmax())\n", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "\n# First, you need to remove the 'null' values from 'temp' column\ndf['temp'] = df['temp'].apply(lambda x: 1 if x != 'null' else 0)\n\n# Then, you can get the value_counts\nresult = df['temp'].value_counts()\n\nprint(result)\n", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "\ndf['null'] = df['temp'].isnull().cumsum()\nresult = df['null'].value_counts()\n", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "\n# First, you need to remove the 'null' values from 'temp' column\ndf['temp'] = df['temp'].apply(lambda x: 'null' if x == 'null' else x)\n\n# Then, you can get the value counts for each column\nresult = pd.DataFrame(df.value_counts(dropna=False))\n\n# Rename the columns\nresult.columns = ['count']\n\n# Rename the index\nresult.index.name = 'column'\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "df = df.iloc[1:].reset_index(drop=True)\ndf.columns = df.loc[0]\ndf = df.iloc[1:].reset_index(drop=True)\ndf.columns = df.loc[0]\nresult = df\n", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "df = df.iloc[2:].reset_index(drop=True)\ndf.columns = df.loc[0]\ndf = df.iloc[1:].reset_index(drop=True)\ndf.columns = df.loc[0]\nresult = df\n", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "\ndf['0'] = df['0'].astype(float)\ndf['1'] = df['1'].astype(float)\ndf['2'] = df['2'].astype(float)\n\ndf = df.fillna(method='ffill').astype(int)\n\nresult = df\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "\ndf = df.apply(lambda x: x.fillna(x.dropna().iloc[-1]), axis=1)\ndf = df.apply(lambda x: x.fillna(x.dropna().iloc[-1]), axis=0)\ndf = df.apply(lambda x: x.fillna(x.dropna().iloc[-1]), axis=1)\ndf = df.apply(lambda x: x.fillna(x.dropna().iloc[-1]), axis=0)\n\nresult = df\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "\ndf['0'] = df['0'].astype(float)\ndf['1'] = df['1'].astype(float)\ndf['2'] = df['2'].astype(float)\n\ndf = df.fillna(method='ffill').dropna()\n\ndf = df.sort_values('0').reset_index(drop=True)\n\nresult = df\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# First, create a mask for rows with value less than threshold\nmask = df['value'] < thresh\n\n# Then, sum the values of these rows\nresult = df.loc[mask, 'value'].sum()\n\n# And finally, replace the original dataframe with the result\ndf.loc[mask, 'value'] = result\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\n# First, we need to find all rows where value is less than threshold\nrows_to_average = df[df['value'] < thresh]\n\n# Then, we calculate the average of these rows\naverage = rows_to_average.mean()\n\n# Finally, we replace these rows with the average\ndf.loc[df['value'] >= thresh, 'value'] = average\n\nresult = df\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# Create a mask for the rows to be replaced\nmask = (df['value'] < section_left) | (df['value'] > section_right)\n\n# Replace the rows with the average of the values in the masked rows\ndf.loc[mask, 'value'] = df.loc[~mask, 'value'].mean()\n\n# Reset the index\ndf = df.reset_index()\n\nresult = df\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\nfor col in df.columns:\n    df[\"inv_\" + col] = 1 / df[col]\n\nresult = df\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndf = pd.concat([df.add(1, fill_value=0), df.add(1, fill_value=0)], axis=1)\ndf.columns = [\"exp_\" + c if c != \"A\" and c != \"B\" else c for c in df.columns]\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\n# create a new dataframe with the inverses\ndf_inv = df.iloc[:, ::-1].drop(df.columns[0], axis=1)\n\n# add the original dataframe and the inverted dataframe\ndf_inv = pd.concat([df, df_inv], axis=1)\n\n# rename the columns\ndf_inv.columns = [f\"inv_{col}\" if i == 0 else col for i, col in enumerate(df.columns)]\n\nresult = df_inv\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndf = df.add_suffix('_sigmoid')\ndf = df.applymap(sigmoid)\n\nresult = df\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "\n# First, find the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Then, find the index of the maximum value in each column\nmax_idx = df.idxmax()\n\n# Now, find the index of the last occurrence of the maximum value\nlast_max_idx = max_idx.loc[min_idx.loc[max_idx.index]]\n\n# Finally, select the corresponding column names\nresult = df.iloc[last_max_idx]\n\nprint(result)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n\n# First, find the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Then, find the index of the first occurrence of the maximum value in each column\nmax_idx = df.idxmax()\n\n# Finally, find the index of the first occurrence of the maximum value in each column\n# that is not in the range of the minimum value\nresult = max_idx[~np.isin(max_idx, min_idx)]\n\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "\n# find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# create a range from the minimum to the maximum date\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# expand the date_range to include all dates\ndf_full = df.set_index('dt').reindex(date_range, fill_value=0).reset_index()\n\n# fill the val column with 0 for missing dates\ndf_full['val'] = df_full['val'].fillna(0)\n\n# reset the index\ndf_full = df_full.reset_index(drop=True)\n\n# rename the columns\ndf_full = df_full.rename(columns={'index': 'dt', 'user': 'user', 'val': 'val'})\n\n# sort the dataframe\ndf_full = df_full.sort_values('dt')\n\nresult = df_full\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a range from the minimum to the maximum date\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Merge the original dataframe with the date_range\ndf = pd.merge(date_range, df, how='outer').fillna(0)\n\n# Sort the dataframe by the date and user\ndf = df.sort_values(['dt', 'user'])\n\n# Set the date as the index\ndf.set_index('dt', inplace=True)\n\n# Pivot the dataframe\nresult = df.pivot(index='dt', columns='user', values='val')\n\n# Reset the index\nresult.reset_index(inplace=True)\n\n# Rename the columns\nresult.columns.name = None\n\n# Fill the NaN values with 0\nresult.fillna(0, inplace=True)\n\nprint(result)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "\n# First, find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a range from the minimum to the maximum date\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Expand the date_range to include all dates\ndf_full = df.set_index('dt').reindex(date_range, fill_value=233)\n\n# Reset the index\ndf_full.reset_index(inplace=True)\n\n# Fill the 'val' column with 233 where it is not filled\ndf_full['val'] = df_full['val'].combine(df_full['val'], lambda x, y: 233 if pd.notnull(x) and pd.notnull(y) else 233)\n\n# Set the 'dt' column back to datetime\ndf_full['dt'] = df_full['dt'].astype('datetime64[D]')\n\n# Set the 'user' column to the original 'user' column\ndf_full['user'] = df_full['user'].astype('object')\n\n# Reset the index\ndf_full.reset_index(inplace=True)\n\n# Rename the columns\ndf_full.rename(columns={'index': 'dt', 'dt': 'date', 'val': 'value'}, inplace=True)\n\nresult = df_full\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "\n# First, find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a range from the minimum to the maximum date\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Merge the original dataframe with the date_range\ndf = pd.merge(date_range, df, how='outer')\n\n# Fill the missing values with the maximum value of 'val' for each 'user'\ndf['val'] = df.groupby('user')['val'].transform('max')\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\nresult = df\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "\n# Find the minimum and maximum date\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Expand the date range to include all dates\ndf_expanded = df.set_index('dt').reindex(date_range, fill_value=0).reset_index()\n\n# Fill in the maximum val of the user for the val column\ndf_expanded['val'] = df_expanded.groupby('user')['val'].transform('max')\n\n# Convert the dt column to the desired format\ndf_expanded['dt'] = df_expanded['dt'].dt.strftime('%d-%b-%Y')\n\n# Reset the index\ndf_expanded = df_expanded.reset_index(drop=True)\n\nresult = df_expanded\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "\n# First, create a dictionary with the unique names as keys and their corresponding IDs as values\nid_dict = {name: id for id, name in enumerate(df['name'].unique(), 1)}\n\n# Then, replace the names in the dataframe with their corresponding IDs\ndf['name'] = df['name'].map(id_dict)\n\nresult = df\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "\n# First, create a new column with unique IDs\ndf['id'] = range(1, len(df) + 1)\n\n# Then, replace 'a' with 'id'\ndf['a'] = df['id']\n\n# Finally, drop the 'id' column\ndf.drop('id', axis=1, inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "\n    df['name'] = df['name'].astype('category').cat.codes\n    return df\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\n# First, create a new column 'ID' with a sequence of numbers starting from 1\ndf['ID'] = range(1, len(df) + 1)\n\n# Then, rename 'name' to 'ID'\ndf.rename(columns={'name': 'ID'}, inplace=True)\n\n# Finally, sort the dataframe by 'ID'\ndf.sort_values('ID', inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.sort_values(['user', 'date'])\ndf", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.melt(id_vars=['user', '02/12/15'], var_name='others', value_name='value')\ndf = df.rename(columns={'user': 'user', '02/12/15': 'date', 'value': 'value2'})\ndf = df.drop(columns=['others'])\ndf = df.rename(columns={'date': '01/12/15', 'value2': 'value'})\ndf = df.sort_values(by=['user', '01/12/15'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.melt(id_vars='user', var_name='date', value_name='value')\ndf['date'] = df['date'].str.split(' ', 1).str[0]\ndf = df.pivot(index='user', columns='date', values='value').reset_index()\ndf.columns.name = None\ndf = df.rename(index=str, column={'user': 'user', '01/12/15': 'date1', '02/12/15': 'date2', 'someBool': 'someBool'})\ndf = df.loc[:, ['user', 'date1', 'date2', 'someBool']]\n", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "\nmask = df['c'] > 0.5\nresult = df.loc[mask, columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "\nmask = df['c'] > 0.45\nresult = df.loc[mask, columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    mask = df['c'] > 0.5\n    return df.loc[mask, columns].assign(sum=lambda x: x[columns[0]] + x[columns[1]])\n    ### END SOLUTION\n\ndf = pd.DataFrame({'a': [0.945686, 0.919359, 0.976163, 0.459039],\n                   'b': [0.000710, 0.667057, 0.621712, 0.788318],\n                   'c': [0.909158, 0.462478, 0.208423, 0.309892],\n                   'd': [0.892892, 0.008204, 0.980471, 0.100539],\n                   'e': [0.326670, 0.473096, 0.048334, 0.753992]})\n\nprint(f(df, ['b', 'e']))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\nf(df)\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\n# Convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the dataframe by 'date'\ndf = df.sort_values('date')\n\n# Create an empty list to store the overlapping rows\noverlapping_rows = []\n\n# Iterate over each row in the dataframe\nfor index, row in df.iterrows():\n    # If the row is not the first row in the dataframe\n    if index != 0:\n        # Get the date of the current row\n        current_date = row['date']\n        # Iterate over each row in the dataframe that is not the current row\n        for i in range(index):\n            # Get the date of the ith row in the dataframe\n            ith_row_date = df.iloc[i]['date']\n            # If the date of the ith row is within X days of the current row's date\n            if (current_date - ith_row_date).days <= X:\n                # Add the ith row to the list of overlapping rows\n                overlapping_rows.append(df.iloc[i])\n\n# Remove the overlapping rows from the dataframe\ndf = df.drop(overlapping_rows)\n\n# Convert the 'date' column back to string\ndf['date'] = df['date'].dt.strftime('%m/%d/%y')\n\nresult = df\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\n# Convert date to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column 'week' that represents the week of the year\ndf['week'] = df['date'].dt.week\n\n# Create a new column 'week_end' that represents the end of the week\ndf['week_end'] = df['date'].dt.week_end\n\n# Create a list of all weeks\nall_weeks = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks ending\nall_weeks_end = list(range(df['week_end'].min(), df['week_end'].max() + 1))\n\n# Create a list of all weeks starting\nall_weeks_start = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n# Create a list of all weeks starting and ending\nall_weeks_start_end = list(range(df['week'].min(), df['week'].max() + 1))\n\n#", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\n# Convert date to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the dataframe by date\ndf = df.sort_values('date')\n\n# Create a new column 'week' that contains the week number of the date\ndf['week'] = df['date'].dt.week\n\n# Create a new column 'week_end' that contains the end of the week\ndf['week_end'] = df['date'].dt.week_end\n\n# Create a new column 'week_start' that contains the start of the week\ndf['week_start'] = df['date'].dt.week_start\n\n# Create a new column 'is_overlap' that is True if the date is within the X weeks of another date\ndf['is_overlap'] = df.groupby('ID')['week'].transform(lambda x: x.isin(x.add(pd.Timedelta(weeks=X))))\n\n# Create a new column 'is_overlap_end' that is True if the date is within the X weeks of another date\ndf['is_overlap_end'] = df.groupby('ID')['week_end'].transform(lambda x: x.isin(x.add(pd.Timedelta(weeks=X))))\n\n# Create a new column 'is_overlap_start' that is True if the date is within the X weeks of another date\ndf['is_overlap_start'] = df.groupby('ID')['week_start'].transform(lambda x: x.isin(x.add(pd.Timedelta(weeks=X))))\n\n# Create a new column 'overlap' that is True if the date is within the X weeks of another date\ndf['overlap'] = df['is_overlap'].astype(bool) | df['is_overlap_end'].astype(bool) | df['is_overlap_start'].astype(bool)\n\n# Remove rows that do not have an overlap\ndf = df[df['overlap']]\n\n# Convert date to string\ndf['date'] = df['date'].dt.strftime('%d-%b-%Y')\n\n# Convert ID to string\ndf['ID'] = df['ID'].astype(str)\n\n# Convert close to float\ndf['close'] = df['close'].astype(float)\n\n# Rename columns\ndf = df.rename(columns={'ID': 'ID', 'date': 'date', 'close': 'close'})\n\nresult = df\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\n\ndf['col1'] = df['col1'].rolling(3).mean()\nresult = df['col1']\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\n\ndf['col1'] = df.groupby(df.index // 3).cumcount() + 1\nresult = df\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n\ndf['col1'] = df.groupby(df.index // 4).cumcount() + 1\nresult = df['col1'].values.tolist()\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\n\ndf['col1'] = df['col1'].rolling(window=3).mean()\nresult = df['col1']\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# First, we need to create a new column that indicates whether the row is the first, second or third\ndf['is_first'] = range(len(df)) % 3\n\n# Then, we group by the new column and perform the desired operations\nresult = df.groupby('is_first')['col1'].apply(lambda x: x.sum() if x.count() == 3 else x.mean()).reset_index(drop=True)\n", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# First, we need to create a new column that indicates whether the row is the first, second or third\ndf['is_first'] = range(len(df)) % 3\n\n# Then, we group by the new column and perform the desired operations\nresult = df.groupby('is_first')['col1'].apply(lambda x: x.sum() if x.count() == 3 else x.mean()).reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "\ndf['A'] = df['A'].mask(df['A'] == 0).ffill(downcast='infer')\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "\ndf['A'] = df['A'].mask(df['A'] == 0).ffill(downcast='infer')\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill').combine(df['A'].fillna(method='bfill'), max)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "\n# Split the duration into number and time\ndf[['number', 'time']] = df['duration'].str.split(' ', 1, expand=True)\n\n# Convert number to integer\ndf['number'] = df['number'].astype(int)\n\n# Convert time to days\ndf['time_days'] = df['time'].replace(['year', 'day', 'week', 'month'], [365, 1, 7, 30])\n\n# Drop the duration column\ndf.drop('duration', axis=1, inplace=True)\n\ndf\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "\ndf['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(['year', 'week', 'day'], [365, 7, 1])\ndf['time_day'] = df['time_day'].astype(int)\ndf\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "\n    df['number'] = df['duration'].str.extract('(\\d+)').astype(int)\n    df['time'] = df['duration'].str.extract('([a-zA-Z]+)')\n    df['time_days'] = df['time'].apply(lambda x: { 'year': 365, 'day': 1, 'week': 7, 'month': 30 }[x])\n    return df\n    ### END SOLUTION\n\nprint(f(example_df))\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "\ndf['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['number'] = df['number'].astype(int)\n\ndf['time_day'] = df['time'].replace(['year', 'week', 'day', 'month'], [365, 7, 1, 1])\ndf['time_day'] *= df['number']\n\ndf = df.drop(columns=['time', 'number'])\n\ndf\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "\nresult = [(df1[column] == df2[column]).all() for column in columns_check_list]\nprint(result)\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "\nresult = [(df1[column] == df2[column]).all() for column in columns_check_list]\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index(['date', 'id'])\n    return df.to_numpy()\n    ### END SOLUTION\n\ndf = pd.DataFrame({'x': [100, 90, 80], 'y': [7, 8, 9], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "\n    df.index = pd.to_datetime(df.index)\n    df = df.swaplevel(0,1)\n    return df\n    ### END SOLUTION\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf = df.rename(columns={'variable': 'year'})\ndf = df.pivot(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf.columns.name = None\ndf = df.sort_values(['Country', 'year'])\ndf", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(['Country', 'year', 'Variable'])\ndf = df.pivot(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf.columns.name = None\ndf = df.fillna(0)\ndf", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "df = df.loc[abs(df.iloc[:, 1:].max(axis=1)) < 1]\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\ndf = df[(abs(df['Value_B']) > 1) | (abs(df['Value_C']) > 1) | (abs(df['Value_D']) > 1)]\n", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\ndf = df.loc[abs(df.filter(regex='Value').abs().max(axis=1) <= 1).any(axis=1)]\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "\ndf['A'] = df['A'].str.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "\n    df.replace(to_replace='&AMP;', \n               pattern='&', \n               inplace=True)\n    return df\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'})\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "\ndf['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "\ndef split_name(name):\n    if ' ' in name:\n        return name.split(' ', 1)\n    else:\n        return [name, None]\n\ndf[['first_name', 'last_name']] = df['name'].apply(split_name)\ndf = df.drop(columns=['name'])\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "\ndf['name'] = df['name'].apply(lambda x: x.split()[0] if ' ' in x else x)\ndf['1_name'] = df['name'].apply(lambda x: x.split()[0] if ' ' in x else x)\ndf['2_name'] = df['name'].apply(lambda x: x.split()[1] if ' ' in x else x)\n\ndf = df.drop(columns=['name'])\n\ndf", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "\ndef split_name(name):\n    if ' ' in name:\n        first_name, *middle_names = name.split(' ')\n        if middle_names:\n            last_name = middle_names[-1]\n            middle_names = ' '.join(middle_names[:-1])\n        else:\n            last_name = ''\n        return first_name, middle_names, last_name\n    else:\n        return name, '', ''\n\ndf[['first name', 'middle_name', 'last_name']] = df['name'].apply(split_name)\ndf = df.drop(columns=['name'])\ndf = df.rename(columns={'first name': 'name', 'middle_name': 'middle_name', 'last_name': 'last_name'})\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge_asof(df1.sort_values('Timestamp'), df2.sort_values('Timestamp'), left_on='Timestamp', right_on='Timestamp', direction='nearest')\nresult = result.merge(df1.sort_index(), on='Timestamp', how='left')\nresult = result.merge(df2.sort_index(), on='Timestamp', how='right')\n", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1['Timestamp'], df2['Timestamp'], direction='nearest')\nresult = result.merge(df1, how='left')\nresult = result.merge(df2, how='left')\n", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where((df['col2'] <=50) & (df['col3'] <=50), df['col1'], df['col3'].max())\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "\ndf['state'] = np.where((df['col2'] > 50) & (df['col3'] > 50), df['col1'], df['col1'] + df['col2'] + df['col3'])\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: [x] if not isinstance(x, int) else [x])\ndf['Field1'] = df['Field1'].apply(lambda x: x[0] if len(x) == 1 else \"and\")\ndf", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndf['Field1'] = df['Field1'].astype(str)\n\nnew_list = []\n\nfor i in df['Field1']:\n    if pd.to_numeric(i, errors='coerce') is not None:\n        new_list.append(int(i))\n    else:\n        new_list.append(i)\n\ndf['Field1'] = new_list\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "\n    result = []\n    for i in range(len(df)):\n        if not df.iloc[i]['Field1'].is_numeric():\n            result.append(df.iloc[i]['Field1'])\n    return result\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum() * 100\ndf['val2'] = df['val2'] / df['val2'].sum() * 100\ndf['val3'] = df['val3'] / df['val3'].sum() * 100\ndf['val4'] = df['val4'] / df['val4'].sum() * 100\n\ndf = df.set_index('cat')\n\ndf", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf = df.set_index('cat')\n\ndf", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "df.drop(test, inplace=True)\nresult = df\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": "\n    return df.loc[test]\n    ### END SOLUTION\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\n# First, calculate the Euclidean distance between each pair of cars\ndf['euclidean_distance'] = ((df['x'] - df['x'].shift())**2 + (df['y'] - df['y'].shift())**2).apply(np.sqrt)\n\n# Then, find the nearest car for each car at each time point\ndf['nearest_neighbour'] = df.groupby('time')['euclidean_distance'].transform('idxmin')\n\n# Finally, calculate the average distance for each time point\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\ndf\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\n# First, let's calculate the pairwise distances between cars\ndf['distance'] = ((df['x'] - df['x'].shift())**2 + (df['y'] - df['y'].shift())**2).apply(np.sqrt)\n\n# Then, let's find the car with the maximum distance for each time point\ndf['farmost_neighbour'] = df.groupby('time')['distance'].transform('idxmax')\n\n# Finally, let's calculate the average distance for each time point\ndf['average_distance'] = df.groupby('time')['distance'].mean()\n\ndf\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "\ndf['keywords_all'] = df.apply(lambda row: ','.join(row.dropna().astype(str).values), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "\ndf['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().astype(str).values), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "\ndf['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().astype(str)), axis=1)\ndf\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndf['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values[::-1]), axis=1)\ndf\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change the value of the Quantity column to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index\n\ndf = df.drop(sample_df.index)\n\ndf = pd.concat([df, sample_df])\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change the value of the ProductId column to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index\n\ndf = df.drop(sample_df.index)\n\ndf = pd.concat([df, sample_df])\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# First, we find the number of rows for each user\nuser_counts = df['UserId'].value_counts()\n\n# Then, we find the number of rows to select for each user\nn_to_select = user_counts * 0.2\n\n# We select the rows for each user\nselected_rows = df.groupby('UserId').apply(lambda x: x.sample(n=n_to_select, random_state=0))\n\n# We change the Quantity to zero\nselected_rows['Quantity'] = 0\n\n# We keep the original indexes\nselected_rows.reset_index(drop=True, inplace=True)\n\ndf = df.drop(selected_rows.index)\n\ndf = pd.concat([selected_rows, df])\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "\n# First, we need to find the first duplicate\nfirst_duplicate_index = df.duplicated(subset=['col1','col2'], keep=False)\n\n# Then, we need to find the index of the first duplicate\nfirst_duplicate_index = first_duplicate_index.idxmax()\n\n# We need to add a new column with the index of the first duplicate\ndf['index_original'] = first_duplicate_index\n\n# Now, we can get the duplicates\nduplicate = df.loc[df.duplicated(subset=['col1','col2'], keep='first')]\n\nresult = duplicate\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "\n# First, we need to find the last occurrence of each row\nlast_indices = df.groupby(['col1','col2']).apply(lambda x: x.index.stop-1 if x.index[-1] == x.index[-2] else x.index[-1]).reset_index(drop=True)\n\n# Then, we merge this with the original dataframe\ndf = pd.concat([df, last_indices.rename('index_original')], axis=1)\n\n# Finally, we drop the duplicates\ndf = df.drop_duplicates(subset=['col1','col2'], keep='last')\n\nresult = df\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "\n    duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df.loc[duplicate_bool == True]\n    duplicate['index_original'] = duplicate_bool.apply(lambda x: x.index[0])\n    return duplicate\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "\n# First, we need to find the first duplicate in each column\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\n\n# Then, we need to find the index of the first duplicate\nduplicate_index = duplicate_bool.idxmax()\n\n# Finally, we need to add a new column with the index of the first duplicate\ndf['index_original'] = duplicate_index\n\nresult = df\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "\n# First, we need to find the last occurrence of each duplicate\nlast_occurrence = df.groupby(['col1','col2'])['val'].transform('last')\n\n# Then, we merge the original dataframe with this last occurrence\ndf = df.drop(columns=['val'])\ndf = df.join(last_occurrence.rename('index_original'))\n\n# Finally, we select the rows where the original index is different from the last occurrence\nresult = df[df['index_original'] != df['val']]\n\nprint(result)\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, merge this with the original DataFrame to get the rows with the maximum count\nresult = df.merge(max_counts, how='inner', on=['Sp', 'Mt'])\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n\n# Then, merge this with the original DataFrame to get the rows with the max count\nresult = pd.merge(df, max_counts, on=['Sp','Mt'], how='inner')\n\n# Finally, drop the intermediate DataFrame\nresult = result.drop('count', axis=1)\n\n# Rename the columns\nresult.rename(columns={'count_x':'count'}, inplace=True)\n\n# Sort the DataFrame by 'Sp' and 'Mt'\nresult = result.sort_values(by=['Sp','Mt'])\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\n# First, find the minimum count for each group\nmin_counts = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\n\n# Then, merge this with the original DataFrame to get the rows with the minimum count\nresult = pd.merge(df, min_counts, on=['Sp', 'Mt'], how='inner')\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, merge this with the original DataFrame to get the rows with the maximum count\nresult = pd.merge(df, max_counts, on=['Sp','Value'])\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df[df['Category'].isin(filter_list)]", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df[~df['Category'].isin(filter_list)]", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "\n# First, we need to create a list of tuples where each tuple contains the first and second column level\n# and the third column level.\n\ncolumn_levels = [('A', 'B', 'E'), ('A', 'B', 'F'), ('A', 'C', 'G'), ('A', 'C', 'H'), ('A', 'D', 'I'), ('A', 'D', 'J')]\n\n# Then, we can use this list to melt the data frame\n\nresult = pd.melt(df, value_vars=column_levels)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "\n# First, create a list of tuples where each tuple contains the first, second, and third column level\ntuples = [(df.columns[i][0], df.columns[i][1], df.columns[i][2]) for i in range(len(df.columns))]\n\n# Then, use pd.melt to melt the data frame\nresult = pd.melt(df, id_vars=tuples)\n\n# Rename the columns\nresult.columns = ['variable', 'value']\n\n# If necessary, rename the value column\nif 'value_' in result.columns:\n    result.rename(columns={'value_': 'value'}, inplace=True)\n\nresult\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df.groupby('id')['val'].cumsum().sub(df['cumsum']).add(df['cumsum'])\ndf", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cumsum().values\ndf['cummax'] = df.groupby('id')['val'].cummax().values\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] >= 0, 0)\ndf", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index(name='v')\n", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "\nresult = df.groupby('r')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index(name='v')\n", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "\nresult = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != x.size else np.nan).reset_index(name='v')\n", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "\ndef relationship(column1, column2):\n    if len(df[column1]) == len(df[column2]):\n        return 'one-to-one'\n    elif len(df[column1]) < len(df[column2]):\n        return 'one-to-many'\n    else:\n        return 'many-to-one'\n\nresult = [f'{column1} {column2} {relationship(column1, column2)}' for column1 in df.columns for column2 in df.columns]\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "\ndef relationship(column1, column2):\n    if len(df[column1]) == len(df[column2]):\n        return 'one-2-one'\n    elif len(df[column1]) < len(df[column2]):\n        return 'one-2-many'\n    else:\n        return 'many-2-many'\n\nresult = [f'{column1} {column2} {relationship(column1, column2)}' for column1 in df.columns for column2 in df.columns]\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "\ndef relationship(x):\n    if x.nunique() == 1:\n        return 'one-to-one'\n    elif x.nunique() == len(x):\n        return 'one-to-many'\n    else:\n        return 'many-to-one'\n\nresult = df.apply(relationship)\nresult.name = 'relationship'\nresult = result.rename(index={0: 'Column1', 1: 'Column2', 2: 'Column3', 3: 'Column4', 4: 'Column5'})\nresult", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "\ndef relationship(x, y):\n    if len(x.unique()) == 1:\n        return 'one-2-one'\n    elif len(x.unique()) == len(y.unique()):\n        return 'one-2-many'\n    elif len(y.unique()) == 1:\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\n\nfor col1 in df.columns:\n    for col2 in df.columns:\n        if col1 != col2:\n            result.loc[col1, col2] = relationship(df[col1], df[col2])\n\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "\n# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n             .applymap(lambda s: s.lower() if type(s) == str else s)\n             .applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n             .drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n\n# sort by bank account\ndfiban_uniq = dfiban_uniq.sort_values(by='bank', ascending=False)\n\nresult = dfiban_uniq\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n\nprint(\"Hello, World\")\n\nThis is a simple program that prints \"Hello, World\" to the console.\n\nThe Python interpreter is a program that reads and interprets the code written in Python.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed as an argument to the \"print\" function.\n\nThe \"print\" function is a built-in function in Python that is used to output the result of a function.\n\nThe string \"Hello, World\" is passed", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\n# First, create a new column 'HasFamily' that is True if 'SibSp' or 'Parch' is greater than 0\ndf['HasFamily'] = df['SibSp'] > 0 | df['Parch'] > 0\n\n# Then, group by 'HasFamily' and calculate the mean of 'Survived'\nresult = df.groupby('HasFamily')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\n# First, create a new column that indicates whether the passenger has family\ndf['HasFamily'] = (df['Survived'] > 0) | (df['Parch'] > 0)\n\n# Then, group by the new column and calculate the mean\nresult = df.groupby('HasFamily')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\nconditions = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n              (df['SibSp'] == 0) & (df['Parch'] == 0),\n              (df['SibSp'] == 0) & (df['Parch'] == 1),\n              (df['SibSp'] == 1) & (df['Parch'] == 0)]\nchoices = ['Has Family', 'No Family', 'New Family', 'Old Family']\n\ndf['Group'] = pd.cut(df['Survived'], [0, 1, 2, 3], labels=choices)\nresult = df.groupby('Group').mean()['Survived']\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "\ndf = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\ndf = df.reset_index(drop=True)\ndf = df.sort_values('cokey')\ndf = df.reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "\ndf = df.groupby('cokey').apply(lambda x: x.sort_values('A') if 'A' in x.columns else x)\ndf = df.reset_index(drop=True)\ndf = df.sort_values('cokey')\ndf = df.reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "\ndf.columns = pd.MultiIndex.from_tuples(df.columns.to_list(), names=['Caps', 'Lower'])", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "\ndf.columns = pd.MultiIndex.from_tuples(df.columns.to_series().apply(lambda x: (x[0], x[1], 'Middle') if x[2] == 'a' else (x[0], x[1], 'Lower')))\ndf", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "\ndf.columns = pd.MultiIndex.from_tuples(df.columns.to_series().apply(lambda x: (x[0], x[1], int(x[2]))))", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "\ndf['mean'] = df.groupby('a')['b'].transform('mean')\ndf['std'] = df.groupby('a')['b'].transform('std')\ndf = df[['mean', 'std']]\n\nresult = df\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "\ndf['mean'] = df.groupby('b')['a'].transform('mean')\ndf['std'] = df.groupby('b')['a'].transform('std')\ndf = df[['mean', 'std']]\n\nresult = df\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "\n# First, we need to calculate softmax and min-max normalization\ndf['softmax'] = df.groupby('a')['b'].transform(lambda x: x / x.sum())\ndf['min-max'] = df.groupby('a')['b'].transform(lambda x: x - x.min() / (x.max() - x.min()))\n\n# Then, we need to reset the index\ndf = df.reset_index(drop=True)\n\ndf\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "\n# First, we need to find the rows and columns that only contain zeros.\n# We can do this by checking if the sum of each row or column is zero.\n\n# First, we create a boolean mask for each row and column that contains zero.\nmask_row = df.sum(axis=1) == 0\nmask_col = df.sum(axis=0) == 0\n\n# Then, we create a new dataframe that only includes the rows and columns that contain zero.\ndf_result = df[mask_row | mask_col]\n\n# Finally, we transpose the dataframe to get the columns and rows in the correct order.\ndf_result = df_result.T\n\nresult = df_result\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "\n# First, we need to find the sum of each row and column.\nrow_sum = df.sum(axis=1)\ncol_sum = df.sum(axis=0)\n\n# Then, we create a mask that will be used to remove the rows and columns with sum of 0.\nmask = row_sum.values == col_sum.values == 0\n\n# Finally, we apply the mask to the dataframe.\nresult = df.loc[:, ~mask]\nresult.columns = df.loc[mask].columns\nresult.index = df.loc[mask].index\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "\n# First, find the maximum value in each row\nmax_values = df.max(axis=1)\n\n# Then, create a mask that is True where the maximum value is 2\nmask = max_values == 2\n\n# Finally, apply the mask to the original dataframe\nresult = df[mask]\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "\n# First, find the maximum value in each row\nmax_row = df.max(axis=1)\n\n# Then, replace the maximum value in each row with 0\ndf.replace(max_row, 0, inplace=True)\n\n# Finally, replace the maximum value in each column with 0\ndf.replace(df.max(), 0, inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=True)\nresult", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = pd.DataFrame(s).reset_index().rename(columns={'index': 'value', 0: '1'})\ndf = df.sort_values(['value', 'index'])\ndf = df.drop(columns=['value'])\ndf = df.reset_index(drop=True)\ndf['index'] = df['index'].astype('str')\ndf = df.rename(columns={'index': 'value', 0: '1'})\ndf = df.sort_values(['value', 'index'])\ndf = df.drop(columns=['value'])\ndf = df.reset_index(drop=True)\ndf\n", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "\ndf = df[(df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))) | df['A'].apply(lambda x: isinstance(x, str))]\nresult = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]\n", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "\ndf_filtered = df[df['A'].astype(str) == df['A']]\nresult = df_filtered[df_filtered['B'].astype(str) == df_filtered['B']]\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Then, merge this with the original DataFrame to get the rows with the maximum count\nresult = df.merge(max_counts, how='inner', on=['Sp', 'Mt'])\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp','Mt'])['count'].max().reset_index()\n\n# Then, merge this with the original DataFrame to get the rows with the max count\nresult = pd.merge(df, max_counts, on=['Sp','Mt'], how='inner')\n\nprint(result)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\n# First, find the minimum count for each group\nmin_counts = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\n\n# Then, merge this with the original DataFrame to get the rows with the minimum count\nresult = pd.merge(df, min_counts, on=['Sp', 'Mt'], how='inner')\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# First, find the maximum count for each group\nmax_counts = df.groupby(['Sp','Value'])['count'].max().reset_index()\n\n# Then, merge this with the original DataFrame to get the rows with the maximum count\nresult = pd.merge(df, max_counts, on=['Sp','Value'])\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "\ndf['Date'] = df['Member'].map(dict)\ndf = df.drop('Member', axis=1)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17/8/1926')\ndf = df.drop(columns=['Member'])\ndf = df.rename(columns={'Group': 'Date'})\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    df['Date'] = df['Member'].map(dict)\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.drop(columns=['Member'])\n    return df\n    ### END SOLUTION\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "\ndf['Date'] = df['Member'].map(dict)\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y').dt.strftime('%d-%b-%Y')\ndf = df.drop(columns=['Member'])\ndf = df.rename(columns={'Group': 'Group1'})\ndf = df.rename(columns={'Date': 'Date1'})\ndf = df.rename(columns={'Date1': 'Date'})\ndf = df.rename(columns={'Group1': 'Group'})\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).Date.transform('count')\ndf = df.sort_values(by=['Date'])\ndf\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year, 'Val']).Date.transform('count')\ndf\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Count_d'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year, 'Val']).Date.transform('count')\ndf['Count_w'] = df.groupby([df['Date'].dt.week, 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date', 'Val']).Date.transform('count')\ndf\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n\nresult1 = df.groupby(df.index).sum()\nresult1 = result1.loc[:, 'B':'C']\n\nresult2 = df.groupby(df.index).sum()\nresult2 = result2.loc[:, 'B':'C']\n\nresult1.name = 'B'\nresult2.name = 'C'\n\nprint(result1)\nprint(result2)\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n\nresult1 = df.resample('D').sum().loc[:, df.columns % 2 == 0]\nresult2 = df.resample('D').sum().loc[:, df.columns % 2 != 0]\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "\npivot_table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'B'], aggfunc={'D': np.sum, 'E': np.mean})\nresult = pivot_table\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D':np.sum, 'E':np.mean})\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "\npivot_table = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D':np.sum, 'E':np.mean})\nresult = pivot_table\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc={'D':np.max, 'E':np.min})\n", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\n\ndf['var2'] = df['var2'].str.split(',').apply(pd.Series).stack()\ndf = df.drop(columns=['var2'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'level_1': 'var2'})\ndf = df.set_index('id')\ndf = df.sort_index()\nresult = df\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\n\ndf['var2'] = df['var2'].str.split(',').apply(pd.Series, index=[0,1])\ndf = df.reindex(columns=['var1', 'var2'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'var1':'var2', 'var2':'var1'})\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::-1].reset_index(drop=True)\ndf = df.iloc[:,::", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\n\ndf['var2'] = df['var2'].str.split('-').apply(lambda x: [item[0] for item in x])\ndf['var2'] = df['var2'].apply(lambda x: pd.Series(x, index=['A', 'B', 'C', 'D', 'E', 'F']))\ndf = df.explode('var2')\ndf = df.drop(columns=['var2'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'var2'})\ndf = df.set_index('var1').rename(index={0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F'})\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1', 'var1': 'var2'})\ndf = df.rename(columns={'var2': 'var1',", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf = df.drop(columns=['row'])\ndf = df.rename(columns={'fips': 'row'})\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf = df.drop(columns=['row'])\ndf = df.rename(columns={'fips': 'row'})\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "\ndf[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf = df[['fips', 'medi', 'row']]\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndf = df.set_index('Name').replace(0, np.nan)\ndf = df.replace(-1, np.nan)\ndf = df.replace(2, np.nan)\ndf = df.replace(3, np.nan)\ndf = df.replace(4, np.nan)\ndf = df.replace(5, np.nan)\ndf = df.replace(6, np.nan)\ndf = df.replace(7, np.nan)\ndf = df.replace(8, np.nan)\ndf = df.replace(9, np.nan)\ndf = df.replace(10, np.nan)\ndf = df.replace(11, np.nan)\ndf = df.replace(12, np.nan)\ndf = df.replace(13, np.nan)\ndf = df.replace(14, np.nan)\ndf = df.replace(15, np.nan)\ndf = df.replace(16, np.nan)\ndf = df.replace(17, np.nan)\ndf = df.replace(18, np.nan)\ndf = df.replace(19, np.nan)\ndf = df.replace(20, np.nan)\ndf = df.replace(21, np.nan)\ndf = df.replace(22, np.nan)\ndf = df.replace(23, np.nan)\ndf = df.replace(24, np.nan)\ndf = df.replace(25, np.nan)\ndf = df.replace(26, np.nan)\ndf = df.replace(27, np.nan)\ndf = df.replace(28, np.nan)\ndf = df.replace(29, np.nan)\ndf = df.replace(30, np.nan)\ndf = df.replace(31, np.nan)\ndf = df.replace(32, np.nan)\ndf = df.replace(33, np.nan)\ndf = df.replace(34, np.nan)\ndf = df.replace(35, np.nan)\ndf = df.replace(36, np.nan)\ndf = df.replace(37, np.nan)\ndf = df.replace(38, np.nan)\ndf = df.replace(39, np.nan)\ndf = df.replace(40, np.nan)\ndf = df.replace(41, np.nan)\ndf = df.replace(42, np.nan)\ndf = df.replace(43, np.nan)\ndf = df.replace(44, np.nan)\ndf = df.replace(45, np.nan)\ndf = df.replace(46, np.nan)\ndf = df.replace(47, np.nan)\ndf = df.replace(48, np.nan)\ndf = df.replace(49, np.nan)\ndf = df.replace(50, np.nan)\ndf = df.replace(51, np.nan)\ndf = df.replace(52, np.nan)\ndf = df.replace(53, np.nan)\ndf = df.replace(54, np.nan)\ndf = df.replace(55, np.nan)\ndf = df.replace(56, np.nan)\ndf = df.replace(57, np.nan)\ndf = df.replace(58, np.nan)\ndf = df.replace(59, np.nan)\ndf = df.replace(60, np.nan)\ndf = df.replace(61, np.nan)\ndf = df.replace(62, np.nan)\ndf = df.replace(63, np.nan)\ndf = df.replace(64, np.nan)\ndf = df.replace(65, np.nan)\ndf = df.replace(66, np.nan)\ndf = df.replace(67, np.nan)\ndf = df.replace(68, np.nan)\ndf = df.replace(69, np.nan)\ndf = df.replace(70, np.nan)\ndf = df.replace(71, np.nan)\ndf = df.replace(72, np.nan)\ndf = df.replace", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndf = df.set_index('Name').replace(0, np.nan)\ndf = df.replace(-1, np.nan)\ndf = df.fillna(method='ffill').fillna(method='bfill')\ndf = df.replace(-1, np.nan)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df.replace(np.nan, 0)\ndf = df", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "\n    df = df.set_index('Name')\n    df = df.replace(0, np.nan)\n    df = df.replace(np.nan, 0)\n    df = df.fillna(df.mean(axis=1).values[:, None])\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n    df = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndf = df.set_index('Name').replace(0, np.nan)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 100\ndf = df.replace(np.nan, 0).cumsum()\ndf = df.div(df.iloc[:,0]).mul(100).fillna(0)\ndf = df.replace(np.nan, 0).cumsum().div(np.arange(1, 7)) * 1", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "\ndf['label'] = (df['Close'] - df['Close'].shift(1)).apply(lambda x: 1 if x > 0 else 0)\ndf['label'] = df['label'].replace(to_replace=0, method='ffill').fillna(0)\ndf['label'] = df['label'].astype(int)\ndf\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf.loc[0] = [df['DateTime'][0], df['Close'][0], 1]\ndf['DateTime'] = df['DateTime'].astype('datetime64[D]')\ndf\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: -1 if x < 0 else (1 if x > 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf = df.rename(columns={'DateTime': 'Date', 'Close': 'Price'})\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_timedelta(df['departure_time'] - df['arrival_time'])\ndf['Duration'] = df['Duration'].dt.days.astype(str) + ' days ' + df['Duration'].dt.seconds.astype(str).str.zfill(2)\ndf\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "\ndf['Duration'] = pd.to_timedelta(df['departure_time']) - pd.to_timedelta(df['arrival_time'])\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf = df.rename(index={0: 'Train A', 1: 'Train A', 2: 'Train A', 3: 'Train B', 4: 'Train B', 5: 'Train B'})\ndf", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "\ndf['Duration'] = pd.to_timedelta(df['departure_time'] - df['arrival_time'])\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf['arrival_time'] = pd.to_datetime(df['arrival_time']).dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['departure_time'] = pd.to_datetime(df['departure_time']).dt.strftime('%d-%b-%Y %H:%M:%S')\ndf = df.rename(index={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5'})\n", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "\nresult = df[df['key2'].str.endswith('e')].groupby('key1').size().reset_index(name='count')\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "\nmin_result = df.idxmin()\nmax_result = df.idxmax()\nprint(min_result)\nprint(max_result)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "\nmode_result = df.mode(axis=0)[0][0]\nmedian_result = df.median()\n\nprint(\"Mode: \", mode_result)\nprint(\"Median: \", median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "\n\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "\nmask = (df['closing_price'] < 100) | (df['closing_price'] > 101)\nresult = df[~mask]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "\ndf1 = df.groupby(\"item\")[\"diff\"].transform('min')\nresult = df[df.diff == df1].drop(columns=['diff'])\n", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0].where(df['SOURCE_NAME'].str.contains('_'), df['SOURCE_NAME'])\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0].where(df['SOURCE_NAME'].str.contains('_'), df['SOURCE_NAME'])\n    return df\n    ### END SOLUTION\n\nprint(f(example_df))\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# First, we need to find the number of NaN values in the column\nnan_count = df['Column_x'].isna().sum()\n\n# Then, we calculate the number of values to replace\nreplacement_count = int(0.5 * nan_count)\n\n# We create a mask for the rows with NaN values\nmask = df['Column_x'].isna()\n\n# We replace the first 50% of NaN values with 0 and the last 50% with 1\ndf['Column_x'][mask] = np.where(mask, np.where(mask[:replacement_count], 0, 1), df['Column_x'][mask])\n\ndf\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values\nnan_count = df['Column_x'].isna().sum()\n\n# Calculate the number of values to replace\nreplacement_count = int(nan_count * 0.3)\n\n# Create a mask for the values to replace\nmask = df['Column_x'].isna()\n\n# Replace the values\ndf['Column_x'][mask] = df['Column_x'][mask].apply(lambda x: x if x == 0 else 0.5 if x == 1 else 1)\n\n# Replace the first 'replacement_count' values with '0'\ndf['Column_x'][0:replacement_count] = 0\n\n# Replace the last 'replacement_count' values with '1'\ndf['Column_x'][-replacement_count:] = 1\n\nEND SOLUTION\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# First, we need to count the number of 0's and 1's in the column\ncount_0 = df['Column_x'].sum()\ncount_1 = len(df) - count_0\n\n# Then, we create a new column with the desired distribution\ndf['Column_x'] = np.where(df['Column_x'] == 0, 1, 0)\n\n# If the distribution is not 50/50, we need to fill the remaining values\nif count_0 > count_1:\n    df['Column_x'] = df['Column_x'].replace(0, 1)\nelif count_0 < count_1:\n    df['Column_x'] = df['Column_x'].replace(1, 0)\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame(list(zip(a['one'], b['one'])), columns=['one', 'two'])\nresult['one'] = result['one'].astype(str) + ', ' + result['two'].astype(str)\nresult = result.iloc[:, 0].apply(lambda x: tuple(x.split(', ')))\nresult", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "\nresult = pd.concat([a, b, c], axis=1).groupby(level=0, axis=1).apply(lambda x: x.iloc[0])\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\n# First, we need to find the length of the longest dataframe.\nmax_len = max(a.shape[0], b.shape[0])\n\n# Then, we create a new dataframe with the maximum length.\nresult = pd.DataFrame(np.empty((max_len, 2), dtype=object), columns=['one', 'two'])\n\n# We fill the new dataframe with the tuples.\nresult['one'] = a['one']\nresult['two'] = b['two']\n\n# If a and b have different lengths, we fill the remaining rows with np.nan.\nresult['one'] = result['one'].fillna(np.nan)\nresult['two'] = result['two'].fillna(np.nan)\n\n# Finally, we convert the tuples to a list of tuples.\nresult[['one', 'two']] = result[['one', 'two']].apply(tuple, axis=1)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult.columns = [' (1, 10]', ' (10, 25]', ' (25, 50]', ' (50, 100]')\nresult.index = ['jane', 'john']\n", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "\ndf['text'] = df['text'].apply(lambda x: ','.join(x.values.astype(str)))\nresult = df['text'].str.cat(sep=', ')\n", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "\ndf['text'] = df['text'].apply(lambda x: '-'.join(x))\nresult = df.iloc[0]\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "\ndf['text'] = df['text'].apply(lambda x: ','.join(x.split()))\nresult = df['text'].str.cat(sep=', ')\n", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "\nresult = pd.Series(df['text'].tolist(), name='text').to_frame('text')\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "\ndf['text'] = df['text'].astype(str).str.cat(sep='-')\nresult = df['text'].values[0]\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "\ndf2 = df2.merge(df1[['id', 'city', 'district']], how='left', on=['id', 'city', 'district'])\nresult = pd.concat([df1, df2], axis=0)\nresult = result.sort_values('id').reset_index(drop=True)\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\ndf1['date'] = pd.to_datetime(df1['date'])\ndf1 = df1.sort_values('date')\n\ndf2['date'] = pd.to_datetime(df2['date'])\ndf2 = df2.sort_values('date')\n\ndf1 = df1.set_index('id')\ndf2 = df2.set_index('id')\n\nresult = pd.concat([df1, df2], axis=1)\nresult['date'] = result['date'].dt.strftime('%d-%b-%Y')\nresult = result.reset_index()\nresult\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\ndf1 = df1.merge(df2, on=['id', 'date'], how='left')\ndf1 = df1.sort_values(['id', 'date'])\ndf1 = df1.groupby(['id', 'city', 'district']).apply(lambda x: x.iloc[0]).reset_index(drop=True)\nresult = df1\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "\nmerged = pd.merge(C, D, on='A', how='outer')\nresult = pd.DataFrame(merged[['A', 'B_x']].values, columns=['A', 'B'])\nresult['B'] = merged['B_y']\nresult.set_index('A', inplace=True)\n", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "\nmerged = pd.merge(C, D, on='A', how='outer')\nresult = pd.concat([C, D.loc[~D['A'].isin(C['A'])]]).sort_values('A')\n", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "\n# First, merge the dataframes on the 'A' column\nmerged = pd.merge(C, D, on='A', how='outer')\n\n# Then, create a new column 'duplicated' that is True if 'A' is in the index of the merged dataframe,\n# and False otherwise\nmerged['duplicated'] = merged.index.isin(C.index)\n\n# Finally, sort the dataframe by the 'A' column\nresult = merged.sort_values('A')\n\nprint(result)\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "\ndf['list'] = df.groupby('user')[['time', 'amount']].apply(list)\ndf['list'] = df['list'].apply(lambda x: [(x[0], x[1]) for x in x])\ndf['list'] = df['list'].apply(lambda x: [(min(x[0]), max(x[0])) for x in x])\ndf['list'] = df['list'].apply(lambda x: [(min(x[1]), max(x[1])) for x in x])\ndf['list'] = df['list'].apply(lambda x: [(min(x[0]), max(x[0])) for x in x])\ndf['list'] = df['list'].apply(lambda x: [(min(x[2]), max(x[2])) for x in x])\ndf['list'] = df['list'].apply(lambda x: [(min(x[3]), max(x[3])) for x in x])\ndf['list'] = df['list'].apply(lambda x: [(min(x[4]), max(x[4])) for x in x])\ndf = df.drop(columns=['time', 'amount'])\ndf = df.rename(columns={'list': 'user_transaction'})\nresult = df\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "\ndf['amount-time'] = list(zip(df['time'], df['amount']))\nresult = df.groupby('user')['amount-time'].apply(list)\nresult\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "\ndf['amount-time'] = list(zip(df['time'], df['amount']))\nresult = df.groupby('user')['amount-time'].apply(list)\nresult\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\ndf = pd.DataFrame(series.values.tolist(), index=series.index).T\ndf = df.stack().reset_index().rename(columns={'level_0':'file', 0:'value'})\ndf = df.pivot(index='file', columns='file', values='value').reset_index()\ndf = df.sort_index(ascending=False).reset_index(drop=True)\ndf_concatenated = df\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\ndf_concatenated = pd.concat([series], axis=1).T.reset_index(level=0, drop=True).T\ndf = df_concatenated.stack().reset_index(name='value').drop('level_1', 1)\ndf.columns = ['name', 'index']\ndf = df.pivot(index='index', columns='name', values='value').reset_index(drop=True)\ndf['index'] += 1\ndf = df.rename(index=lambda x: 'file'+str(x))\ndf", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "\n# Find the column name that contains the string 'spike'\nfor col in df.columns:\n    if s in col:\n        result = col\n        break\n\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "\n# Find the column name that contains the string 'spike'\nfor col in df.columns:\n    if s in col:\n        result = col\n        break\n\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "\n# Find the column name that contains 'spike'\nfor col in df.columns:\n    if s in col:\n        result = col\n        break\n\n# Rename the column\ndf.rename(columns={result: 'spike1'}, inplace=True)\n\nprint(df)\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# First, we need to flatten the list to a single series\nflat_codes = pd.Series(df['codes'].explode())\n\n# Then, we can use the groupby function to split the series into multiple columns\nresult = flat_codes.groupby(flat_codes.index // len(df['codes'])).apply(pd.Series)\n\n# Finally, we need to reset the index\nresult = result.reset_index(drop=True)\n\n# If there are any missing values, we can fill them with NaN\nresult.fillna(pd.Series([np.nan]*len(result), index=result.index), inplace=True)\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# First, we need to flatten the list to a single series\nflat_codes = pd.Series(df['codes'].explode())\n\n# Then, we can use the groupby function to split the series into multiple columns\nresult = flat_codes.groupby(flat_codes.index // len(df['codes'])).apply(pd.Series)\n\n# Finally, we need to reset the index\nresult = result.reset_index(drop=True)\n\n# If there are missing values, you can fill them with NaN\nresult.fillna(pd.Series([np.nan]*len(result), index=result.index), inplace=True)\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# First, we need to flatten the list to a single series\nflat_codes = pd.Series(df['codes'].explode())\n\n# Then, we can use the groupby function to split the series into multiple columns\nresult = flat_codes.groupby(flat_codes.index // len(df['codes'])).apply(pd.Series)\n\n# Finally, we need to reset the index\nresult = result.reset_index(drop=True)\n\n# If there are any missing values, we can fill them with NaN\nresult.fillna(pd.Series([float('NaN')], index=[0]), inplace=True)\n\nprint(result)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\n# First, you need to convert the list to string\ndf['col1'] = df['col1'].apply(lambda x: '[' + ','.join(map(str, x)) + ']')\n\n# Then, you can convert the string to list\nresult = df['col1'].str.split('[|]').apply(lambda x: [i[0] for i in x])\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\n# First, we need to flatten the list and convert it to a string\nflattened_list = [str(item) for sublist in df['col1'].tolist() for item in sublist]\n\n# Then, we join the elements of the list into a string\nresult = ','.join(flattened_list)\n\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\n# First, convert the list to a string\ndf['col1'] = df['col1'].apply(lambda x: ','.join(map(str, x)))\n\n# Then, concatenate the list into one string\nresult = ','.join(df['col1'].values)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "df = df.set_index('Time').resample('2min').mean()\ndf = df.reset_index()\ndf", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "df = df.set_index('Time').resample('3T').sum().reset_index()\ndf['Time'] = df['Time'].dt.floor_time('3T')\ndf = df.set_index('Time').resample('3T').mean().reset_index()\ndf['Time'] = df['Time'].dt.floor_time('3T')\ndf = df.sort_values('Time')\ndf", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='first')\ndf\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='dense', ascending=False)\ndf\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='dense')\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\ndf", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "\n\nresult = df[df.index.get_level_values('a').isin(filt.index)]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "\n\nresult = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "\nresult = df.iloc[0].ne(df.iloc[8]).index[df.iloc[0].ne(df.iloc[8])]\n", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "\nresult = df.iloc[0].eq(df.iloc[8]).where(lambda x: x.isnull(), '')\n", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "\nmask = df.apply(lambda x: x.isnull() == x.isnull(), axis=0)\nresult = mask.stack().squeeze()\nresult = result.reset_index(level=1, drop=True).to_list()\n", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "\nresult = [(x, y) for x in df.iloc[0].where(np.isnan, np.nan).tolist() for y in df.iloc[8].where(np.isnan, np.nan).tolist() if x != y]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "\ndf = df.stack().reset_index()\ndf.columns = ['_'.join(x) for x in df.columns]\ndf = df.iloc[0].reset_index(drop=True)\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df.columns.str.split('_', 1).str[0]))\ndf = df.transpose().reset_index()\ndf.columns.name = None\ndf = df.iloc[:,1:]\ndf = df.rename(index={0:'A',1:'B',2:'C',3:'D',4:'E'})\ndf = df.set_index('_'.join(df", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "\ndf = df.T.stack().reset_index().T\ndf.columns = ['_'.join(col) for col in df.columns]\ndf = df.iloc[0].reset_index(drop=True)\ndf = df.rename(index={0: 'A_0', 1: 'B_0', 2: 'C_0', 3: 'D_0', 4: 'E_0'})\ndf = df.rename(index={5: 'A_1', 6: 'B_1', 7: 'C_1', 8: 'D_1', 9: 'E_1'})\ndf = df.rename(index={10: 'A_2', 11: 'B_2', 12: 'C_2', 13: 'D_2', 14: 'E_2'})\ndf = df.reset_index(drop=True)\ndf = df.T\n", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "\ndf.sort_values(by=['treatment','dose','time'], inplace=True)\nresult = df\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "\ndf.sort_values(by=['treatment','time','VIM'], inplace=True)\nresult = df\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "\n# Convert the 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create a new column 'Day' that will hold the day of the date\ndf['Day'] = df['Date'].dt.day\n\n# Create a new column 'Month' that will hold the month of the date\ndf['Month'] = df['Date'].dt.month\n\n# Create a new column 'Year' that will hold the year of the date\ndf['Year'] = df['Date'].dt.year\n\n# Filter the dataframe to keep only the dates '2020-02-17' and '2020-02-18'\nresult = df[(df['Year'] == 2020) & (df['Month'] == 2) & (df['Day'] == 17)]\n\n# Reset the index\nresult.reset_index(inplace=True)\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create a new column 'Day'\ndf['Day'] = df['Date'].dt.day\n\n# Create a new column 'Weekday'\ndf['Weekday'] = df['Date'].dt.day_name()\n\n# Create a new column 'Month'\ndf['Month'] = df['Date'].dt.month\n\n# Create a new column 'Year'\ndf['Year'] = df['Date'].dt.year\n\n# Filter the rows where the day is not 'Friday'\nresult = df[df['Weekday'] != 'Friday']\n\n# Convert the 'Date' column back to string format\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y')\n\nresult\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "\n# Filter out the correlation values that are less than 0.3\nmask = np.where(corr < 0.3, True, False)\n\n# Apply the mask to the correlation matrix\nresult = corr.mask(mask)\n\n# Convert the masked values to NaN\nresult = result.apply(lambda x: np.nan if x < 0.3 else x)\n\n# Convert the NaN values to 0\nresult = result.fillna(0)\n\n# Convert the DataFrame to a dictionary\nresult = result.to_dict()\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "\nmask = np.where(np.abs(corr) > 0.3, corr, np.nan)\nresult = mask.stack().reset_index(drop=True).rename(columns={0:'correlation'})\nresult = result.loc[result['correlation'] != np.nan]\nresult = result.loc[result['correlation'] > 0.3]\n", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "\ndf.rename(columns={df.columns[-1]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.rename(columns={df.columns[0]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndf['frequent'] = df.apply(lambda row: 1 if row.max() == row else 0, axis=1)\ndf['freq_count'] = df.apply(lambda row: list(row).count(1), axis=1)\n\ndf", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndf['frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n\ndf", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\n# First, find the most common values in each row\ncommon_values = df.apply(lambda x: list(set(x.values[x == True]) & set(x.values[x == False])), axis=1)\n\n# Then, find the count of each value\ncounts = df.apply(lambda x: list(x.values).count(0) if 0 in x.values else list(x.values).count(1) if 1 in x.values else 'No 1s or 0s found', axis=1)\n\n# Combine the two results into a new dataframe\ndf_new = pd.concat([common_values, counts], axis=1)\ndf_new.columns = ['frequent', 'freq_count']\n\nprint(df_new)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "\ndf['bar'] = df.groupby(['id1','id2'])['bar'].transform('mean')\ndf = df.groupby(['id1','id2'])['foo','bar'].mean().reset_index()\nresult = df\n", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "\ndf['foo'] = df['foo'].astype('float')\ndf['bar'] = df['bar'].replace('NULL', 0)\n\nresult = df.groupby(['id1','id2'])['foo','bar'].mean().reset_index()\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='inner')\nresult = df_c[['EntityNum', 'foo', 'a_col']]\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum', how='inner')\nresult = df_c[['EntityNum','foo','b_col']]\n", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape\n", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[np.logical_not(np.isnan(x))]\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)\n", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = x.tolist()\nresult = [i for i in result if i != np.nan]\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((len(a), 4))\nb[:, a] = 1", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((len(a), 4))\nfor i, val in enumerate(a):\n    b[i, val] = 1\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((len(a), 5))\nb[:, np.arange(5)] = np.eye(5)[a]\n", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "\n# find the minimum and maximum values in a\nmin_val = a.min()\nmax_val = a.max()\n\n# create a range from min to max\nrange_val = max_val - min_val\n\n# create a one-hot encoding\nb = (a - min_val) / range_val\n\n# convert to integer\nb = b.astype(int)\n\n# create a binary mask\nmask = np.zeros_like(a, dtype=bool)\n\n# set the mask to True at the indices of the one-hot encoding\nnp.putmask(mask, b, True)\n\n# invert the mask to get the one-hot encoding\nb = np.logical_not(mask)\n\nprint(b)\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "\n# First, find the smallest and largest values in a\nmin_val = a.min()\nmax_val = a.max()\n\n# Then, create a one-hot encoding array\nb = np.zeros((a.shape[0], a.shape[1], max_val - min_val + 1))\n\n# Set the appropriate values in b to 1\nb[a - min_val] = 1\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.array(A).reshape(-1, ncol)", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.array(A).reshape(-1, ncol)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.pad(a, ((0, 0), (shift, 0)), mode='constant')\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.lib.pad(a, ((0, 0), shift), mode='constant')\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "\n# Get the maximum value\nmax_value = np.max(a)\n\n# Get the indices of the maximum value\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape)\n\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a), a.shape)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": "\n    max_value = np.amax(a)\n    max_indices = np.where(a == max_value)\n    return max_indices\n    ### END SOLUTION\nf(example_a)\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "\n# Get the maximum value in the array\nmax_value = np.amax(a)\n\n# Get the indices of the maximum value\nmax_indices = np.argwhere(a == max_value)\n\n# Get the second largest value\nsecond_largest = np.sort(a.ravel())[-2]\n\n# Get the unraveled index of the second largest value\nresult = np.unravel_index(np.argwhere(a == second_largest)[0], a.shape)\n\nprint(result)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "\na = a[:, ~np.isnan(a).any(axis=0)]\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "import numpy as np\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \nresult = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = a[:, permutation]", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravel_index(np.argmin(a), a.shape)\nresult = min_index\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "\nimport numpy as np\na = np.array([[1, 0], [0, 2]])\nresult = np.unravel_index(np.argmin(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.radians(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.radians(degree))", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "\ndef is_degree(number):\n    if np.sin(np.radians(number)) > 0.999:\n        return 0\n    else:\n        return 1\n\nresult = is_degree(number)\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.degrees(np.arcsin(value))\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "\ndef pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant', constant_values=0)\n\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "\ndef pad(A, length):\n    return np.pad(A, (0, length - A.size), 'constant', constant_values=0)[:length]\n\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.linalg.matrix_power(a, power)", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(example_a, 2))\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divmod(numerator, denominator)\nprint(result)", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.divide(numerator, denominator)\n    return (result.astype(int), )\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = (numerator // denominator, numerator % denominator)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3\n", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum.outer(a, b, c)", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a)\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a)[-1::-1]\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.diag(a)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.diag(np.fliplr(a).diagonal())\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "\nresult = [item for sublist in X for item in sublist]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "\nresult = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = [i for sublist in X for i in sublist]\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "\nresult = [X[i, j] for i in range(X.shape[0]) for j in range(X.shape[1])]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "\nresult = np.sum(a[..., col] * multiply_number, axis=col)\n", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "\nresult = np.sum(a[row, :] * multiply_number)\n", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "\ndef get_maximal_independent_vectors(a):\n    n = a.shape[1]\n    result = []\n    for i in range(2**n):\n        vec = []\n        for j in range(n):\n            if ((i >> j) & 1) == 1:\n                vec.append(j)\n        if len(set(vec)) == len(vec):\n            result.append(vec)\n    return result\n\nresult = get_maximal_independent_vectors(a)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]\n", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b)", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "\n# calculate the t-statistic\nt_stat = np.abs(np.mean(a) - np.mean(b)) / np.sqrt((np.var(a) / len(a)) + (np.var(b) / len(b)))\n\n# calculate the p-value\np_value = scipy.stats.ttest_ind(a, b, equal_var=True, nan_policy='omit')[1]\n\nprint(p_value)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the pooled variance\npooled_variance = ((anobs - 1) / (anobs + bnobs - 2) * avar + (bnobs - 1) / (anobs + bnobs - 2) * bvar) / ((anobs + bnobs - 2) / (anobs + bnobs - 2))\n# calculate the t-statistic\nt_statistic = (amean - bmean) / (np.sqrt(pooled_variance) * np.sqrt(1 / (anobs + bnobs - 2)))\np_value = scipy.stats.t.sf(np.abs(t_statistic), anobs + bnobs - 2) * 2\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "\n# Convert the 2D array to 1D\nA = A.reshape(-1)\nB = B.reshape(-1)\n\n# Use list comprehension to create a new list that contains elements from A that are not in B\noutput = [i for i in A if i not in B]\n\n# Reshape the output to 2D array\noutput = np.array(output).reshape(-1, 1)\n", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "\n# Convert 2D arrays to 1D\nA = A.reshape(-1)\nB = B.reshape(-1)\n\n# Use set operations to get the elements in A but not in B and vice versa\noutput = list(set(A) - set(B)) + list(set(B) - set(A))\n\n# Convert list to numpy array\noutput = np.array(output)\n\n# Reshape to 2D for the final output\noutput = output.reshape(-1, 3)\n\nprint(output)\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.zeros_like(b)\nfor i in range(3):\n    for j in range(3):\n        for k in range(3):\n            index = np.argsort(a[i, j, :])\n            result[i, j, :] = b[i, j, index]\nresult", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "\n# Create a mask to ignore the columns to be deleted\nmask = np.ones(len(a), dtype=bool)\nfor index in del_col:\n    mask[index-1] = False\n\n# Apply the mask to the array\nresult = a[mask]\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, dtype=object, order='K')\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.diff(a, axis=1) == 0)", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.apply_along_axis(lambda x: x[0] == x, axis=1, arr=a)\n", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.equal(a, a[0]))\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.trapz(np.trapz(z, x, axis=0), y, axis=0)", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": "\n    return (np.cos(x))**4 + (np.sin(y))**2\n    ### END SOLUTION\n\nresult = scipy.integrate.dblquad(f, 0, 1, lambda x: 0, lambda x: 1)\nprint(result)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "\ndef ecdf(data):\n    # sort the data\n    x = np.sort(data)\n    # calculate the number of data points\n    n = x.shape[0]\n    # calculate the cumulative sum of the data\n    y = np.arange(1, n+1) / n\n    return y\n\nresult = ecdf(grades)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n\nresult = ecdf(grades)\nresult = result(eval)\nprint(result)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "\ndef ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n\necdf_values = ecdf(grades)\n\n# find the longest interval [low, high) that satisfies ECDF(x) < threshold\nlow = 0\nhigh = 1\nwhile ecdf_values[high] < threshold:\n    high += 1\nlow = 0\nwhile ecdf_values[low] > threshold:\n    low -= 1\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.choice([0, 1], size=size, p=[one_ratio, 1-one_ratio])", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = a.argsort()[::-1]", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a) + np.arange(len(a))\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "\n# First, find the indices of the N biggest elements\nindices = np.argpartition(a, -N)[-N:]\n\n# Then, sort these indices in decreasing order\nindices = np.sort(indices)\n\n# Finally, reverse the indices to get them in increasing order\nindices = indices[::-1]\n\n# Convert the indices to a list\nresult = indices.tolist()\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.linalg.matrix_power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "\n# First, we need to reshape the array to 2D\na = a.reshape(-1, 2)\n\n# Then, we need to transpose the 2D array to get the 3D array\na = a.T\n\n# Finally, we reshape the 3D array to the desired size\nresult = a.reshape(-1, 2, 2)\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "\ndef sliding_window(a, window_size):\n    shape = a.shape\n    strides = a.strides\n    result = np.lib.stride_tricks.as_strided(a, (shape[0]-window_size[0]+1, shape[1]-window_size[1]+1), window_size)\n    return result.reshape(shape[0]-window_size[0]+1, shape[1]-window_size[1]+1, *window_size)\n\nwindow_size = (2, 2)\nresult = sliding_window(a, window_size)\nprint(result)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "\n# First, we need to reshape the array to 2D\na = a.reshape(-1, 2)\n\n# Then, we need to transpose the 2D array to get the 3D array\na = a.T\n\n# Finally, we reshape the 3D array to the desired size\nresult = a.reshape(-1, 2, 2)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "\n# calculate the number of patches\nnum_patches_x = a.shape[0] // patch_size\nnum_patches_y = a.shape[1] // patch_size\n\n# create the result array\nresult = np.empty((num_patches_x, num_patches_y, patch_size, patch_size))\n\n# iterate over the patches\nfor i in range(num_patches_x):\n    for j in range(num_patches_y):\n        # get the patch\n        patch = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch.shape[1]]\n        # add the patch to the result array\n        result[i, j] = patch\n\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i, j] = a[i, j].flatten()\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "\n# calculate the number of patches\nnum_patches_x = a.shape[0] // patch_size\nnum_patches_y = a.shape[1] // patch_size\n\n# create the result array\nresult = np.empty((num_patches_x, num_patches_y, patch_size, patch_size))\n\n# iterate over the patches\nfor i in range(num_patches_x):\n    for j in range(num_patches_y):\n        # get the patch\n        patch = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch.shape[1]]\n        # add the patch to the result array\n        result[i, j] = patch\n\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high+1]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high+1]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high+1]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "\na = np.array(eval(string))\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "\n# Generate samples from log-uniform distribution\nresult = np.random.uniform(np.log(min), np.log(max), n)\n\n# Convert back to original scale\nresult = np.exp(result)\n\n# Make sure the result is within the original range\nresult = np.maximum(min, result)\nresult = np.minimum(max, result)\n\nprint(result)\n", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "\n# Generate samples from log-uniform distribution\nresult = np.random.exponential(scale=np.power(np.e, min), size=n) * np.power(np.e, max)\n\n# Convert to linear scale\nresult = np.power(result, 1/np.e)\n\n# Scale to the desired range\nresult = (result - np.min(result)) / (np.max(result) - np.min(result))\n\n# Convert to the original scale\nresult = np.power(result, np.e)\n", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return (np.log(np.random.uniform(low=min, high=max, size=n)) / np.log(max - min)) + min\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[0] = a * A[0]\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.sub2ind(a.shape, *index)", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = np.sub2ind(dims, index)\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "df = pd.DataFrame(np.zeros((2,3)), index=index, columns=columns)", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "\nresult = np.zeros(a.size)\nfor i in range(a.size):\n    result[i] = a[accmap[i]]\n\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.maximum.reduce(a[index])\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "\nresult = np.zeros(len(accmap))\nfor i in range(len(accmap))\n    result[i] += a[accmap[i]]\nprint(result)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = a[index]\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "\nx = np.array(x)\ny = np.array(y)\nz = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        z[i][j] = elementwise_function(x[i][j], y[i][j])\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, p=probabilit)", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "\n# Create a new array with the desired size\nnew_a = np.zeros((len(a) + high_index + low_index, len(a[0]) + high_index + low_index))\n\n# Copy the original array into the new array\nnew_a[low_index:len(a) + low_index, low_index:len(a[0]) + low_index] = a\n\n# Now you can slice the new array\nresult = new_a[high_index:len(a) + high_index, high_index:len(a[0]) + high__index]\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.iscomplex(x)]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_max = [max(bin) for bin in bin_data]\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "\n# First, we need to bin the data\nbins = np.array_split(data, bin_size)\n\n# Then, we calculate the mean for each bin\nbin_data_mean = [bin.mean() for bin in bins]\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "\n# First, we need to reverse the array to start from the end\ndata_reversed = np.flipud(data)\n\n# Then, we create the bins\nbin_data = np.array_split(data_reversed, len(data) // bin_size)\n\n# Now, we calculate the mean of each bin\nbin_data_mean = [np.mean(bin) for bin in bin_data]\n\n# Finally, we reverse the mean back to the original order\nbin_data_mean = np.flipud(np.array(bin_data_mean))\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "\ndef bin_data(data, bin_size):\n    return [data[-bin_size:], data[:-bin_size]]\n\ndef calculate_means(bins):\n    return [np.mean(bin) for bin in bins]\n\nbins = [bin_data(data, bin_size) for bin_size in range(1, len(data)+1)]\nbin_data_mean = calculate_means(bins)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "\ndef bin_data(data, bin_size):\n    return [data[:len(data) - len(data) % bin_size)]\n\ndef calculate_means(bins):\n    return [np.mean(bin) for bin in bins]\n\nbins = [bin_data(row, bin_size) for row in data.tolist()]\nbin_data_mean = calculate_means(bins)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    t = np.clip((x - x_min) / (x_max - x_min), 0.0, 1.0)\n    return t * t * (3 - 2 * t)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0.0, 1.0)\n    return x_min + (x_max - x_min) * ((6 * t - 1) / (6 * N) + 1) if N == 5 else x\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\n# First, you need to create a circular version of b\nb_circ = np.concatenate((b, b[:2]))\n\n# Then, you can perform the circular correlation\nresult = np.correlate(b_circ, a, mode='full')\n\n# Finally, you need to remove the extra values that are not part of the circular correlation\nresult = result[len(b):]\n\nprint(result)\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.groupby('major').apply(lambda x: x.values.reshape(-1, 1, 5)).values", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = df.groupby('major').apply(lambda x: x.values.reshape(-1, 1, 5)).values", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "\nresult = np.array([list(np.binary_repr(num, width=m)) for num in a]).astype(int)\n", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "\ndef convert_to_binary(a, m):\n    result = np.zeros((len(a), m), dtype=np.uint8)\n    for i in range(len(a)):\n        result[i] = np.unpackbits(np.uint8(a[i]))\n    return result\n\nresult = convert_to_binary(a, m)\nprint(result)\n", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "\n# Convert each integer to binary and pad with zeros to make its length equal to m\nbinary_array = np.binary_repmat(a, m, axis=0).T\n\n# Convert binary array to numpy array\nbinary_array = binary_array.astype(int)\n\n# Compute exclusive OR of all rows\nresult = np.bitwise_xor.reduce(binary_array, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "mu = np.mean(a)\nsigma = np.std(a)\nresult = (mu-3*sigma, mu+3*sigma)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "mu = np.mean(a)\nsigma = np.std(a)\nresult = (mu-2*sigma, mu+2*sigma)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    start = mean - 3*std_dev\n    end = mean + 3*std_dev\n    return (start, end)\n    ### END SOLUTION ###\nf()\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "\nmean = np.mean(a)\nstd_dev = np.std(a)\n\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\n\nresult = (a >= lower_bound) & (a <= upper_bound)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.compressed(), percentile)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, :] = 0\na[:, 0] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "\nmax_values = np.amax(a, axis=1)\nmask = np.where(a == max_values, True, False)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "\nmin_value = np.min(a, axis=1)[:, np.newaxis]\nmask = a == np.repeat(min_value, a.shape[0])\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "\n# Convert the lists to numpy arrays\npostal_codes = np.array(post)\ndistances = np.array(distance)\n\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(postal_codes, distances)[0, 1]\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ijk->ijm', X)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.einsum('ijk->ijm', Y)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a\n", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "\n# Convert A to a set for faster lookup\nA_set = set(A)\n\n# Use set intersection to find common elements in A and B\nC = list(A_set & set(B))\n\nprint(C)\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "\nC = A[np.in1d(A, B)]\n", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "\nmask = np.isin(A, B[..., np.newaxis])\nC = A[mask.any(axis=1)]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = rankdata(a, method='max') - 1\n", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "rankdata(a, method='max')", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    return rankdata(a)[::-1]\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack([x_dists, y_dists])[0]\n", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack([x_dists, y_dists])[0]\n", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "\nresult = a[:, second, third]", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "\nl1 = np.linalg.norm(X, 1)\nresult = X / l1[:, np.newaxis]\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "\ndef l2_normalize(X):\n    L2 = LA.norm(X, axis=1, ord=2)\n    return X / L2[:, np.newaxis]\n\nresult = l2_normalize(X)\nprint(result)\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "\ndef linf_norm(X):\n    return np.array([LA.norm(v, np.inf) for v in X.T])\n\nresult = linf_norm(X)\nprint(result)\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].apply(lambda x: target in str(x))\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "\n# Calculate the Euclidean distance between each point in the dataset and the first point\ndistances = np.sqrt(np.sum((a - a[0])**2, axis=1))\n\n# Create a boolean mask to exclude the first point from the comparison\nmask = np.where(np.arange(len(a)) != 0)\n\n# Get the distances of the remaining points\nresult = distances[mask]\n\nprint(result)\n", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "\n# Calculate the Euclidean distance\ndef euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b) ** 2))\n\n# Calculate the distance matrix\nresult = np.array([[euclidean_distance(a[i], a[j]) for j in range(len(a)) if i != j] for i in range(len(a))])\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "\n# Calculate the Euclidean distance\ndef euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b) ** 2))\n\n# Calculate the distance matrix\nresult = np.array([[euclidean_distance(a[i], a[j]) for j in range(len(a)) if i != j] for i in range(len(a))])\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# First, remove all the zero values\na = a[a != 0]\n\n# Then, remove the adjacent duplicates\nresult = np.diff(a)\n\n# Finally, remove the first element of the result array (which is zero)\nresult = result[result != 0]\n\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# First, find the indices of non-zero elements\nindices = np.where(a != 0)[0]\n\n# Then, create a new array with only the non-zero elements\nresult = a[indices]\n\n# Finally, remove the adjacent duplicates\nresult = np.diff(result)\n\n# If the first element is zero, remove it\nif result[0] == 0:\n    result = result[1:]\n\nprint(result)\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame(np.column_stack((lat[0], lon[0], val[0])), columns=['lat', 'lon', 'val'])\nfor i in range(1, len(lat)):\n    df = df.append(pd.DataFrame(np.column_stack((lat[i], lon[i], val[i])), columns=['lat', 'lon', 'val']))\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(lat, lon,val)\n    ### BEGIN SOLUTION\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    return df\n    ### END SOLUTION\n    # return the solution in this function\n    # df = f(", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df.max(axis=1)\ndf = df.drop('maximum', 1)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'index': 'lat', 0: 'lon', 1: 'val', 2: 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\ndf = df.set_index(['lat', 'lon', 'val'])\ndf = df.unstack(level=-1)\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'maximum'})\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "\ndef rolling_window(a, size):\n    shape = a.shape\n    return (a.reshape(-1, size[0]*size[1])[:-size[0]*size[1]//shape[0]*shape[1]].reshape(-1, size[0], size[1])\n    result = [rolling_window(a, size) for _ in range(a.size)]\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "\ndef rolling_window(a, size):\n    shape = a.shape\n    return (a.reshape(-1, size[0]*size[1])[:-size[0]*size[1]//shape[0]*shape[1]].reshape(-1, size[0], size[1])\n    result = [rolling_window(a, size) for _ in range(a.size)]\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "\n# If you want to get the last dimension of the array\nresult = Z[-1]\n\n# If you want to get the second last dimension\nresult = Z[-2]\n\n# If you want to get the third last dimension\nresult = Z[-3]\n\n# If you want to get all dimensions\nresult = Z\n", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "\n# For 2D array\nif a.ndim == 2:\n    result = a[-1:, :]\n\n# For 3D array\nelif a.ndim == 3:\n    result = a[-1:, :, :]\n", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = any(np.array_equal(c, contour) for contour in CNTS)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = any(np.array_equal(c, contour) for contour in CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.diag(i)\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[np.triu_indices(len(a), k=1)] = 0\n", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "\nstart_ts = pd.to_datetime(start)\nend_ts = pd.to_datetime(end)\ndelta = end_ts - start_ts\nstep = delta / n\n\nresult = pd.date_range(start=start_ts, end=end_ts, periods=n)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "\nresult = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No such index\")\nelse:\n    print(\"Index: \", result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "\nindices = np.where((x == a) & (y == b))\nresult = indices[0]\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "\n# calculate the coefficients a, b and c using the formula:\n# (sum(x) * sum(y) - n * sum(x * y)) / (sum(x) * sum(x) - n * (sum(x) ** 2))\n# and\n# (sum(y) - n * sum(y) / sum(x)) / (sum(x) ** 2 - n * (sum(x) ** 2))\n\na = (np.sum(x) * np.sum(y) - np.sum(x * y)) / (np.sum(x) * np.sum(x) - np.sum(x) * np.sum(x))\nb = (np.sum(y) - np.sum(y) / np.sum(x)) / (np.sum(x) ** 2 - np.sum(x) * np.sum(x))\nc = np.sum(y) / np.sum(x) - a * np.sum(x) * b / np.sum(x)\n\nresult = [a, b, c]\nresult = [i for i in result if i != '...'] # remove any possible float to int conversion\nresult.reverse() # reverse the order to match the order of coefficients\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "\n# fit the model\npopt, pcov = np.polyfit(x, y, degree, full=True)\n\n# reverse the coefficients to get the parameters in the order of highest to lowest\nresult = popt[::-1]\n\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "\ndf = df.sub(a, axis=1)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,il->ijkl', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = np.mean(a_scaled, axis=0)", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "\nmask = arr < -10\nmask2 = arr >= 15\nmask3 = mask | mask2\n\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "\n# Create a mask for the first dimension\nmask = np.argwhere(arr < n1).flatten()\n\n# Create a mask for the second dimension\nmask2 = np.argwhere(arr >= n2).flatten()\n\n# Create a mask for the third dimension\nmask3 = np.argwhere(arr < n1) == np.argwhere(arr >= n2)\n\n# Apply the mask to the array\narr[mask] = 0\narr[mask2] = 30\narr[mask3] = arr[mask3] + 5\n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "# print the number of times s1 is not equal to s2 (should be 0)\nresult = np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "# convert NaN to 0\ns1[np.isnan(s1)] = 0\ns2[np.isnan(s2)] = 0\n# count the number of different elements\nresult = np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a[1]) and np.array_equal(a[0], a[2])\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all() for arr in a)\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "\n# calculate how many zeros to add to the right and bottom\npad_width = ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1]))\n\n# zero pad the array\nresult = np.pad(a, pad_width, mode='constant')\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "\n# Calculate how many rows and columns are needed to reach the desired shape\npad_rows = np.maximum(0, shape[0] - a.shape[0])\npad_cols = np.maximum(0, shape[1] - a.shape[1])\n\n# Zero pad the array\nresult = np.pad(a, ((0, pad_rows), (0, pad_cols)))\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "\n# calculate how many times the array needs to be padded\npad_width = [(0, max(0, shape[0] - a.shape[0])) for i in range(2)]\npad_width += [(0, max(0, shape[1] - a.shape[1])) for i in range(2)]\n\n# pad the array\nresult = np.pad(a, pad_width, mode='constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    diff_rows = shape[0] - arr.shape[0]\n    diff_cols = shape[1] - arr.shape[1]\n    \n    padded_arr = np.pad(arr, ((0, diff_rows), (0, diff_cols)))\n    \n    return padded_arr\n    ### END SOLUTION ###\n\nf(example_arr, (93,13))\n", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "\n# calculate the number of rows and columns to pad\npad_rows = np.array(shape[0] - a.shape[0])\npad_cols = np.array(shape[1] - a.shape[1])\n\n# create a mask for the padding\nmask = np.zeros((93, 13))\nmask[:pad_rows, :pad_cols] = 1\n\n# zero pad the array\nresult = np.where(mask == 1, 0, a)\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0,12)]).reshape(a.shape[0]/3,3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "\nresult = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[..., np.arange(b.shape[0]), np.repeat(np.arange(b.shape[1]), b.shape[1]), b]", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[..., b[:, 0]])\n", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "\nmask = df['a'].between(1, 4)\nresult = df.loc[mask, 'b'].values\nresult = np.where(mask, df.loc[mask, 'b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "\n# First, create a mask where the value is 1 (black)\nmask = im == 1\n\n# Then, apply the mask to the array\nresult = np.ma.array(im, mask=mask)\n\n# Finally, convert the masked array to a regular array\nresult = result.data\n\n# Remove the first and last rows and columns\nresult = result[1:-1, 1:-1]\n\nprint(result)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "\n# find the indices of non-zero elements\nindices = np.argwhere(A != 0)\n\n# slice the array\nresult = A[np.min(indices[:, 0]):np.max(indices[:, 0])+1, np.min(indices[:, 1]):np.max(indices[:, 1])+1]", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\n# First, we need to find the indices of non-zero elements in the image\nindices = np.argwhere(im != 0)\n\n# Then, we find the minimum and maximum indices for each row and column\nmin_row, max_row = np.min(indices[:, 0]), np.max(indices[:, 0])\nmin_col, max_col = np.min(indices[:, 1]), np.max(indices[:, 1])\n\n# Finally, we create the result array\nresult = np.zeros((max_row - min_row + 1, max_col - min_col + 1))\n\n# We copy the non-zero elements to the result array\nresult[min_row:max_row+1, min_col:max_col+1] = im[min_row:max_row+1, min_col:max_col+1]\n\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "\n# First, find the indices of non-zero elements\nindices = np.argwhere(im != 0)\n\n# Then, create a new array with only the non-zero elements\nresult = im[indices[:,0].min():indices[:,0].max()+1, indices[:,1].min():indices[:,1].max()+1]\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.figure(figsize=(10, 6))\n\n# line styles\nline_styles = ['-', '--', '-.', ':']\n\nfor line_style in line_styles:\n    plt.plot(x, np.random.randn(10) + np.random.randn(10) * 0.1, line_style, label=line_style)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.figure(figsize=(10, 6))\n\n# line styles\nline_styles = ['-', '--', '-.', ':']\n\nfor line_style in line_styles:\n    plt.plot(x, np.random.randn(10) + np.random.randn(10) * 0.1, line_style, label=line_style)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, marker='d')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.plot(x, y, marker='d', linestyle='-', linewidth=3)\nplt.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, color='red')\nplt.fill_between(range(2, 4), x[2:4], color='red', alpha=0.5)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = 2 * x + 1\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "plt.figure(figsize=(8, 6))\nsns.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    palette=\"deep\",\n    height=6,\n    aspect=0.6,\n)\nplt.show()\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "plt.figure(figsize=(10,6))\nsns.set(style=\"whitegrid\")\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(data=df, x='x', y='y')\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\nplt.show()\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.legend(title='x, y, z', title_fontsize=20)\n\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color='black')\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), \"r-\", lw=5, markersize=30)\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "plt.xticks([0, 2, 4, 6, 8, 10])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.imshow(H, cmap='hot', interpolation='nearest')\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "plt.xticks(rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "plt.title(myTitle, fontsize=16, fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "plt.ylim(0, 2.5)\nplt.plot(x, y)\nplt.gca().invert_yaxis()\nplt.show()\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5])\nplt.show()\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.figure(figsize=(10,6))\nplt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().get_yaxis().get_major_formatter().labelOnlyBase = False\nplt.ticklabel_format(style='plain')\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax.plot([0, 10], [2, 0], linestyle='--', color='red')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, ax = plt.subplots(2, sharex=True)\nax[0].plot(x, y1, label='sin')\nax[1].plot(x, y2, label='cos')\n\nfor ax_ in ax:\n    ax_.set_ylim(-1.2, 1.2)\n    ax_.set_xlim(-6, 6)\n    ax_.spines['top'].set_color('none')\n    ax_.spines['bottom'].set_color('none')\n    ax_.spines['left'].set_color('none')\n    ax_.spines['right'].set_color('none')\n    ax_.get_xaxis().set_ticks()\n    ax_.get_yaxis().set_ticks()\n    ax_.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, \n                    labeltop=False, bottom=False, top=False, labelleft=False, labelright=False)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "plt.figure(figsize=(10, 4))\nplt.plot(x, y1, label='sin')\nplt.plot(x, y2, label='cos')\nplt.legend(loc='upper right')\nplt.grid(True)\nplt.title('Sine and Cosine Waves')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.setp(plt.gca().get_xticklabels(), visible=False)\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks(rotation=90)\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks([3, 4])\nplt.grid(True, which='both')\nplt.show()\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4, 0.5))\nplt.grid(True, which='both')\nplt.grid(axis='y', which='both', linestyle='--')\nplt.show()\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks([3, 4])\nplt.xticks([1, 2])\nplt.show()\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6),\n                        gridspec_kw={'hspace': 0.05, 'wspace': 0.05})\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\nplt.clf()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xticks(np.arange(len(row_labels)) + .5, minor=True)\nax.set_xticklabels(row_labels, minor=True)\nax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.xticks(np.arange(10))\nplt.ylabel(\"Y\")\nplt.yticks(np.arange(10))\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks([])\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().tick_params(axis='y', which='both', right=True)\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.ylabel(\"Y\")\nplt.xticks(x)\nplt.yticks(y)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green', scatter_kws={'color':'blue'})\nplt.show()\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, color='green', marginal_ticks=True)\nplt.show()\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\nplt.show()\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10,5))\nplt.bar(df[\"celltype\"], df[\"s1\"], label='s1')\nplt.bar(df[\"celltype\"], df[\"s2\"], label='s2')\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10,6))\nplt.bar(df[\"celltype\"], df[\"s1\"], label='s1')\nplt.bar(df[\"celltype\"], df[\"s2\"], label='s2')\nplt.xlabel(\"celltype\", rotation=45)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.xticks(np.arange(10))\nplt.yticks(np.arange(10))\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='Y over X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Y over X')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "plt.figure(figsize=(10, 6))\nplt.plot([0.22058956, 0.33088437, 2.20589566], [0, 0, 0], '--', color='red')\nplt.text(0.22, 0.3, '0.22', color='red')\nplt.text(0.33, 0.3, '0.33', color='red')\nplt.text(2.2, 0.3, '2.2', color='red')\nplt.ylim(-0.1, 0.4)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.imshow(rand_mat, cmap='hot', interpolation='nearest')\nplt.xticks(numpy.arange(4), xlabels, rotation=90)\nplt.yticks(numpy.arange(4), ylabels)\nplt.yticks(numpy.flip(numpy.arange(4), axis=0))\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[1].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].set_title('Y')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "plt.figure(figsize=(8,6))\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", s=30)\nplt.show()\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.scatter(a, b, c)\nplt.xlabel('a')\nplt.ylabel('b')\nplt.title('Scatter plot of a over b')\nplt.show()\n\n# add text annotation to each data point\nfor i in range(len(a)):\n    plt.annotate(c[i], (a[i], b[i]))\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(label='Legend', title='Legend', bbox_to_anchor=(0.5, 1.05), loc='upper center')\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, bins=5, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, ax1 = plt.subplots(2, 1, figsize=(10, 10))\nax2 = fig.add_subplot(ax1)\n\n# Create the first subplot\nax1.plot(x, y, 'b-')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Subplot 1')\n\n# Create the second subplot\nax2.plot(x, y, 'r-')\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nax2.set_title('Subplot 2')\n\n# Create the third subplot\nax3 = fig.add_subplot(ax2)\nax3.plot(x, y, 'g-')\nax3.set_xlabel('x')\nax3.set_ylabel('y')\nax3.set_title('Subplot 3')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist([x, y], bins=5, label=['x', 'y'],\n         histtype='bar', color=['blue', 'green'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.plot([a, c], [b, d])\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2, 1)\n\n# create a colormap\ncmap = plt.get_cmap('viridis')\n\n# create a colormap with x and y\ncmap_x = cmap(0.5)\ncmap_y = cmap(0.5)\n\n# plot x and y on the first subplot\nax[0].imshow(x, cmap=cmap_x)\nax[0].set_title('x')\n\n# plot y on the second subplot\nax[1].imshow(y, cmap=cmap_y)\nax[1].set_title('y')\n\n# create a colorbar for the first subplot\ncbar = plt.colorbar(ax[0].imshow(x, cmap=cmap_x), ax=ax[0])\n\n# create a colorbar for the second subplot\ncbar = plt.colorbar(ax[1].imshow(y, cmap=cmap_y), ax=ax[1])\n\nplt.show()\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "plt.figure()\nplt.subplot(2, 1, 1)\nplt.plot(x, y, 'r')\nplt.title('Y and Z')\nplt.subplot(2, 1, 2)\nplt.plot(a, z, 'b')\nplt.title('Y and Z')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.plot([x[0] for x in points], [x[1] for x in points], 'o-')\nplt.yscale('log')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line plot of points')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.plot(x, y)\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\nax.set_xlabel('x')\nax.set_ylabel('y')\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "plt.figure()\nfor line, color in zip(lines, c):\n    plt.plot(*line, color=color)\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.plot(x, y)\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('x (log-scale)')\nplt.ylabel('y (log-scale)')\nplt.title('y over x on a log-log plot')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\n\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Value\")\n    plt.title(\"Line plot of data\")\n    plt.legend(loc='best')\n    plt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "plt.hist(data, bins=5, edgecolor='black')\nplt.gca().yaxis.set_major_formatter(mtick='%1.0f')\nplt.gca().yaxis.set_tick_params(labelsize=10)\nplt.title('Histogram of Data')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, 'r--o', markersize=5, markeredgewidth=0.5)\nplt.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1)\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[1].plot(x, a, label='a')\naxs[1].plot(x, y, label='y')\naxs[0].legend(loc='upper right')\naxs[1].legend(loc='upper right')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharey=False)\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticklabels(['second'])\nax.xaxis.set_label_position('top')\nplt.show()\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label='$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(np.arange(0, 10, 2))\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\nplt.tick_params(axis='x', which='both', rotation=-60)\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.yticks(rotation=-60)\nplt.xticks(rotation='top')\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(np.arange(2010, 2020), alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0.05)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(2, 1)\naxs[0].plot(x, y)\naxs[0].set_title('Figure')\naxs[1].plot(x, y)\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.figure(figsize=(10, 5))\nplt.plot(df.index, df[\"Type A\"], label='Type A')\nplt.plot(df.index, df[\"Type B\"], label='Type B')\nplt.xlabel(\"Index\")\nplt.ylabel(\"Value\")\nplt.title(\"Line Chart of Data\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='///')\nplt.show()\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none', marker='|')\nplt.show()\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*', hatch='*')\nplt.show()\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*v', s=100)\nplt.show()\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.xlim(0, 5)\nplt.ylim(0, 4)\nplt.imshow(data, extent=(0, 5, 0, 4))\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.figure(figsize=(9, 6))\nplt.bar(d.keys(), d.values(), color=[c[i] for i in d.keys()])\nplt.xlabel(\"Keys\")\nplt.ylabel(\"Values\")\nplt.title(\"Bar plot with colors\")\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.axvline(x=3, color='r', label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 10))\nplt.pie(height, labels=labels, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedgeprops={'width': 0.4})\nplt.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle='--', color='0.6')\nplt.show()\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.axis('equal')\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.axis('equal')\nplt.legend(labels, loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'r--o', markersize=10, markeredgewidth=2, markeredgecolor='k', markerfacecolor='none')\nplt.show()\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color='green')\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "plt.bar(np.arange(len(blue_bar)), blue_bar, color='b', align='center')\nplt.bar(np.arange(len(orange_bar)), orange_bar, color='orange', align='center')\n\n# Create a list of the x-coordinates for the bars\nx_coords = np.arange(len(blue_bar))\n\n# Create a list of the labels for the bars\nlabels = ['Blue', 'Orange']\n\n# Create a list of the widths for the bars\nwidth = [0.3]\n\n# Create a list of the colors for the bars\ncolors = ['b', 'orange']\n\n# Create a list of the bars\nbars = [blue_bar, orange_bar]\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar names\nbar_names = ['Blue', 'Orange']\n\n# Create a list of the bar", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, ax1 = plt.subplots()\n\ncolor = 'tab:red'\nax1.set_xlabel('x')\nax1.set_ylabel('y', color=color)\nax1.plot(x, y, color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\nax2 = ax1.twinx()\ncolor = 'tab:blue'\nax2.set_ylabel('z', color=color)\nax2.plot(a, z, color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\nplt.show()\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.xticks(np.arange(0, 11, 1))\nplt.yticks(np.arange(0, 11, 1))\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "\nsns.catplot(x=\"bill_length_mm\", y=\"species\", hue=\"sex\", data=df, height=5, aspect=0.9, kind=\"bar\", col=\"species\")\nplt.show()\n\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.gca().add_patch(plt.Circle((0.5, 0.5), 0.2, fill=False, color='red'))\nplt.gca().set_aspect('equal')\nplt.show()\n", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.plot(x, y, label='Line')\nplt.legend(loc='best', labelspacing=0.1)\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.plot(x, y, label='Line')\nplt.legend(loc='best', handlelength=0.3)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='best')\nplt.show()\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.show()\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.imshow(data, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(1, figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Figure 1', fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(y, x)\nplt.gca().invert_xaxis()\nplt.show()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 1", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', s=50)\nplt.show()\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "plt.figure(figsize=(15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(x, y)\nplt.subplot(2, 2, 2)\nplt.plot(x, y)\nplt.subplot(2, 2, 3)\nplt.plot(x, y)\nplt.subplot(2, 2, 4)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 6))\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.fill_between(x, y - error, y + error, alpha=0.5, facecolor='blue', interpolate=True)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot with Error Bars')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, -1], 'k-', linewidth=2)\nplt.plot([-1, 1], [0, 0], 'k-', linewidth=2)\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color='red')\n\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "plt.figure(figsize=(10, 5))\nplt.subplot(121)\nplt.plot(x, y, label='Y')\nplt.title('Y')\nplt.legend()\nplt.subplot(122)\nplt.plot(a, z, label='Z')\nplt.title('Z')\nplt.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('y = x')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fmticon=True)\nplt.colorbar()\nplt.title('Random 10x10 Matrix')\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 8))\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.heatmap(df, cmap='viridis')\nplt.title('Penguins Dataset')\nplt.show()\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.xticks(x)\nplt.yticks(y)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.plot(x, y)\nplt.xticks(np.arange(0, 11, 1))\nplt.yticks(np.arange(0, 11, 1))\nplt.margins(x=0, y=0)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks([])\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "plt.figure(figsize=(10, 6))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=6, aspect=0.9, kind=\"point\")\nplt.suptitle('Group: Fat', y=1.03)\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=6, aspect=0.9, kind=\"point\")\nplt.suptitle('Group: No Fat', y=1.03)\nplt.show()\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "plt.figure(figsize=(10,6))\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=.7, kind=\"point\")\nplt.xticks(rotation=90)\nplt.xlabel(\"Exercise Time\")\nplt.ylabel(\"Pulse\")\nplt.title(\"Exercise Time vs Pulse by Kind and Diet\")\nplt.show()\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "plt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\nplt.title(\"Time vs Pulse\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pulse\")\nplt.legend(title=\"Exercise\")\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nsns.scatterplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\nplt.title(\"Time vs Pulse\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pulse\")\nplt.legend(title=\"Exercise\")\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='best', bbox_to_anchor=(0, 1))\nplt.show()\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "fig, ax = plt.subplots()\nax.plot(t, a, label='sin(t)')\nax.plot(t, b, label='cos(t)')\nax.plot(t, c, label='sin(t) + cos(t)')\nax.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "plt.figure(figsize=(8,6))\nstrip = sns.stripplot(x=\"sex\", y=\"bill_length_mm\", data=df, jitter=0.3, hue=\"species\", palette=\"deep\")\nplt.title(\"Bill Length by Sex and Species\")\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Bill Length (mm)\")\nplt.legend(title=\"Species\", loc=\"upper right\")\nplt.show()\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\nsns.FacetGrid(df, row=\"b\", hue=\"b\", sharex=False, sharey=False).map(sns.pointplot, \"a\", \"c\").add_legend(ax=axes[0])\nsns.FacetGrid(df, row=\"b\", hue=\"b\", sharex=False, sharey=False).map(sns.pointplot, \"a\", \"c\").add_legend(ax=axes[1])\nplt.xticks(np.arange(0, 31, 2))\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(elev=30, azim=60)\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[0, 1])\n\naxes = []\nfor i in range(nrow * ncol):\n    ax = plt.subplot(gs[i])\n    axes.append(ax)\n\nfor ax in axes:\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(x[i], cmap='gray')\n    i += 1\n\nplt.show()\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "\n# create a session to run the operations\nwith tf.Session() as sess:\n    # initialize all variables\n    sess.run(tf.global_variables_initializer())\n    \n    # assign a new value to x\n    x.assign(1)\n    \n    # print the value of x\n    print(sess.run(x))\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\nnum_classes = 10\nnum_labels = len(labels)\n\nresult = tf.constant(0, dtype=tf.int32)\n\nfor i, label in enumerate(labels):\n    result = tf.tensor_scatter_nd_update(result, [[i]], 1)\n    result = tf.tensor_scatter_nd_update(result, [[i, label]], 1)\n\nresult = tf.tensor_scatter_nd_update(result, [[i, label]], 1)\n\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\nnum_classes = 10\nnum_samples = len(labels)\n\n# Create a tensor with zeros\nresult = tf.zeros((num_samples, num_classes), dtype=tf.int32)\n\n# Set the values of the tensor\nfor i, label in enumerate(labels):\n    result[i, label] = 1\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "\nnum_classes = 10\nnum_labels = len(labels)\n\nresult = tf.constant(0, dtype=tf.int32)\n\nfor i, label in enumerate(labels):\n    result = tf.bitwise.not_equal(result, tf.constant(label, dtype=tf.int32))\n\nresult = tf.bitwise.bitwise_and(result, tf.constant([[1]*num_classes]*num_labels, dtype=tf.int32))\n\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": "\n    result = tf.one_hot(labels, depth=10)\n    return tf.cast(result, tf.int32)\n    ### END SOLUTION\n\nprint(f(example_labels))\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\nnum_classes = 10\nnum_samples = len(labels)\n\n# Create a tensor with shape [num_samples, num_classes]\nlabels_tensor = tf.constant(labels, dtype=tf.int32)\n\n# Create a tensor with shape [num_samples, num_classes]\nresult = tf.one_hot(labels_tensor, depth=num_classes)\n\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "\ndef my_map_func(i):\n  return [i, i+1, i+2]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.py_function(\n  func=my_map_func, inp=[input], Tout=(tf.int32, tf.int32, tf.int32)\n))\nds = ds.unbatch()\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return [i for sublist in input for i in sublist]\n    ### END SOLUTION ###\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(map_func=lambda input: tf.py_function(\n  func=f, inp=[input], Tout=[tf.int64]\n))\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a tensor of zeros with the same shape as your lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.int32)\n\n# Reverse the mask to get the desired output\nmask = tf.math.logical_not(mask)\n\n# Expand the mask to match the desired output shape\nmask = tf.expand_dims(mask, axis=-1)\n\nresult = mask\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of zeros with the same shape as your lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.int32)\n\n# Reverse the mask to get the desired output\nmask = tf.math.logical_not(mask)\n\n# Expand the mask to match the desired output shape\nmask = tf.expand_dims(mask, axis=-1)\n\nresult = mask\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a tensor of zeros with the same shape as your lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n\n# Fill the mask with ones\nmask = tf.cast(mask, tf.float32)\n\nresult = mask\n\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": "\n    max_length = tf.constant(8)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\n    padded_mask = tf.pad(mask, [[0, max_length - tf.shape(mask)[0]]], constant_values=0)\n    return tf.cast(padded_mask, tf.int32)\n    ### END SOLUTION\n\nprint(f(example_lengths))\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\n# Create a tensor of zeros with the same shape as your lengths tensor\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32)\n\n# Reverse the mask to get the desired output\nmask = tf.math.logical_not(mask)\n\n# Add a leading 1 to the mask\nmask = tf.pad(mask, [[1, 0]])\n\nresult = mask\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.transpose(tf.stack([tf.expand_dims(a, 0), tf.expand_dims(b, 0)]))\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "\n    cartesian_product = tf.expand_dims(a, 0) * tf.expand_dims(b, 1)\n    result = tf.concat(cartesian_product, 2)\n    return result\n    ### END SOLUTION\n\nprint(f(example_a, example_b).numpy())\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "\n# Reshape the tensor\nresult = tf.reshape(a, [50, 100, 512])\n\n# Alternatively, you can use tf.squeeze() to remove the third dimension\nresult = tf.squeeze(a, [2])\n", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.expand_dims(a, axis=2)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "\n# Reshape the tensor\nresult = tf.reshape(a, [1, 50, 100, 1, 512])\n\n# Alternatively, you can use tf.expand_dims\nresult = tf.expand_dims(tf.expand_dims(tf.expand_dims(a, 1), 2), 0)\n\nprint(result)\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\n# Calculate the L2 distance\ndiff = tf.square(tf.subtract(a, b))\n\n# Reduce by row\nresult = tf.reduce_sum(diff, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "\n# Calculate the L2 distance\ndiff = tf.square(tf.subtract(a, b))\n\n# Reduce the differences along the column dimension\nresult = tf.reduce_sum(diff, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": "\n    return tf.reduce_sum(tf.square(tf.sub(A, B)))\n    ### END SOLUTION\n\nprint(f(example_a, example_b).numpy())\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "\nresult = tf.tensor_scatter_nd_update(x, tf.stack([y, z]), z)\n\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "\nm = tf.gather_nd(x, tf.stack([row, col]))\n\nresult = sess.run(m)\n\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "\n    return tf.gather_nd(x, [y,z])\n    ### END SOLUTION\nresult = f(example_x,example_y,example_z)\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\n# Reshape A and B to match the dimensions of the dot product\nA = tf.reshape(A, [10, 20, 30, 1])\nB = tf.reshape(B, [10, 20, 30, 1])\n\n# Perform the dot product\nresult = tf.einsum('ijk,ilk->ijl', A, B)\n\n# Reshape the result to match the original shape\nresult = tf.reshape(result, [10, 10, 20])\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "\n# Reshape A and B to match the dimensions of the dot product\nA = tf.reshape(A, [10, 20, -1])\nB = tf.reshape(B, [10, -1, 30])\n\n# Perform the dot product\nresult = tf.einsum('ijk,ilk->ijl', A, B)\n\n# Reshape the result to match the original shape\nresult = tf.reshape(result, [10, 20, 20])\n\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\nx = tf.constant(x)\nx = tf.strings.unicode_transcode(x, \"UTF-8\")\nresult = tf.strings.regex_full_match(x, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z0-9]*$\")\nresult = tf.strings.lower(result)\nresult = tf.strings.regex_full_match(result, \"^[a-zA-Z", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": "\n    return [tf.strings.unicode_transcode(byte_tensor) for byte_tensor in x]\n    ### END SOLUTION\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "\nmask = tf.cast(tf.math.reduce_sum(tf.cast(tf.math.greater(x, 0), dtype=tf.int32), axis=-1) > 0, dtype=tf.float32)\nresult = tf.reduce_sum(x * mask, axis=-1) / tf.reduce_sum(mask, axis=-1)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = tf.math.reduce_any(x, axis=-1)  # mask where non-zero values are present\nresult = tf.reduce_sum(x, axis=-1) / tf.cast(mask, tf.float32)  # average over non-zero values\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    mask = tf.math.reduce_any(x, axis=-1)\n    masked_x = tf.boolean_mask(x, mask)\n    non_zero_count = tf.math.reduce_sum(tf.cast(mask, dtype=tf.float32), axis=-1)\n    return tf.math.divide(tf.reduce_sum(masked_x, axis=-1), non_zero_count)\n    ### END SOLUTION\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\ntry:\n    Session = tf.compat.v1.Session\nexcept AttributeError:\n    Session = tf.Session\n\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\n\nwith Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\nresult = tf.argmax(a, axis=1)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\n\nresult = tf.argmax(a, axis=1)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": "\n    max_indices = tf.argmax(a, axis=1)\n    return max_indices\n    ### END SOLUTION\n\nresult = f(example_a)\nprint(result)\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "\nresult = tf.argmin(a, axis=0) + 1\nprint(result)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save('export/1')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=4, dtype=tf.int32)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[114], minval=2, maxval=5, dtype=tf.int32)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    random_integers = tf.random.uniform(shape=[10], minval=1, maxval=4, dtype=tf.int32)\n    return random_integers\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "\nimport tensorflow as tf\n\nresult = tf.__version__\nprint(result)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "coeff = np.polyfit(np.log(x), y, 1)\nresult = np.array([np.exp(coeff[1]), coeff[0]])\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "coeff = np.polyfit(x, y, 2)\nresult = np.poly1d(coeff)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "popt, pcov = scipy.optimize.curve_fit(lambda t, a, b, c: a * np.exp(b * t) + c, x, y, p0=p0)\nresult = popt\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\n", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat = stats.ks_2samp(x, y)\np_value = test_stat.pvalue\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4\n\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.norm.cdf(z_scores, loc=0, scale=1) * 2\np_values", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [scipy.stats.norm.sf(z)*2 for z in z_scores]", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = [(p - 0.5) / np.sqrt(p * (1 - p) / 100) for p in p_values]", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([stddev],loc=mu)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm([stddev], loc=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa.multiply(sb)", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": "\n    return sA.multiply(sB)\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "\n# Create a grid from the points\nx, y, z = np.mgrid[0:3:31j, 0:3:31j, 0:3:31j]\n\n# Interpolate the moisture V to the grid points\ninterp = scipy.interpolate.griddata(points, V, (x, y, z), method='cubic')\n\n# Interpolate the moisture V to the requested point\nresult = interp(request[0])\n\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\nprint(result)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, mode='reflect')\nxrot, yrot = data_rot.shape[1] - x0, data.orig.shape[0] - y0\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "\nresult = M.diagonal()", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": "\n    test_statistic, p_value = stats.kstest(times, 'uniform')\n    return test_statistic, p_value\n    ### END SOLUTION\nf(example_times, example_rate, example_T)\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, 'uniform')\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "dist_matrix = scipy.spatial.distance.cdist(points1, points2)\nresult = scipy.optimize.linear_sum_assignment(dist_matrix)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "dist_matrix = scipy.spatial.distance.cdist(points1, points2)\nresult = scipy.optimize.linear_sum_assignment(dist_matrix)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\nc = sparse.csr_matrix(b.toarray())\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "labels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "labels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": "\n    labels, n = ndimage.label(img > threshold)\n    return labels, n\n    ### END SOLUTION\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\nlabels, num_labels = ndimage.label(img > threshold)\ncenters = ndimage.center_of_mass(img, labels, range(1, num_labels + 1))\ndistances = np.sqrt((centers[0] - 0)**2 + (centers[1] - 0)**2)\nresult = distances.tolist()\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T - np.diag(M.diagonal())\nM = sparse.csr_matrix(M)\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": "\n    sA = (sA + sA.T)/2\n    sA = sA.tocsr()\n    ### END SOLUTION\n    return sA\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\n# Remove isolated single cells\nsquare = scipy.ndimage.binary_fill_holes(square)\n\n# Remove small blobs\nlabels, n_regions = scipy.ndimage.label(square)\nscipy.ndimage.measurements.center_of_mass(labels, square, index=np.arange(1, n_regions+1))\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Remove isolated single cells\nsquare_eroded = scipy.ndimage.grey_erosion(square, size=1)\n\n# Remove isolated single cells\nsquare_dilated = scipy.ndimage.grey_dilation(square, size=1)\n\n# Remove isolated single cells\nsquare = square_eroded + square_dilated - square\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=2)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=2)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=3)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=3)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=4)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=4)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=5)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=5)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=6)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=6)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=7)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=7)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=8)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=8)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=9)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=9)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=10)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=10)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=11)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=11)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=12)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=12)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=13)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=13)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=14)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=14)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=15)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=15)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=16)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square, size=16)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_dilation(square, size=17)\n\n# Remove isolated single cells\nsquare = scipy.ndimage.grey_erosion(square", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "\n# Convert the sparse column to a numpy array\ncol_array = col.toarray()\n\n# Get the max and min values\nMax = np.amax(col_array)\nMin = np.amin(col_array)\n\nprint(\"Max: \", Max)\nprint(\"Min: \", Min)\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "\n# Convert the sparse column to a dense vector\ncol_dense = col.toarray()\n\n# Get the median and mode of the dense vector\nMedian = np.median(col_dense)\nMode = stats.mode(col_dense)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "popt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\nflat_array = example_array.ravel()\n\n# Calculate the pairwise Euclidean distances\ndistances = scipy.spatial.distance.cdist(flat_array, flat_array, metric='euclidean')\n\n# Convert the 2D array back to a 2D array\nresult = np.array(distances).reshape(len(flat_array), len(flat_array))\n\n# Convert distances from cells to metres\nresult = result * example_array.mean()\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Convert the 2D array to 1D\nflat_array = example_array.ravel()\n\n# Calculate pairwise distances\ndistances = scipy.spatial.distance.cdist(flat_array, flat_array, metric='cityblock')\n\n# Convert the 1D array back to 2D\ndistances = distances.reshape(len(flat_array), len(flat_array))\n\n# Find the minimum distance for each pair\nmin_distances = distances.min(axis=1)\n\n# Find the minimum distance for each pair\nmin_distances = min_distances.min(axis=0)\n\n# Print the result\nprint(min_distances)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Convert the raster to a 1D array\n    flat_array = example_array.flatten()\n    # Calculate pairwise distances\n    distances = scipy.spatial.distance.pdist(flat_array)\n    # Convert distances back to 2D array\n    distances_2D = np.array(distances).reshape(len(flat_array), len(flat_array))\n    ### END SOLUTION ###\n    return distances_2D\n\nf(example_array)\n\nI've tried to implement the solution but I'm not sure how to proceed. I've tried to use the cdist function from scipy.spatial.distance, but it seems to be unable to handle the 2D array I have.\n\nA: You can use the scipy.spatial.distance.pdist function to calculate pairwise distances between all regions. However, this function returns a 1D array, not a 2D array. To convert this to a 2D array, you can use the reshape function.\n\nHere is the corrected code:\n\n```python\nimport numpy as np\nimport scipy.spatial.distance\n\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\nflat_array = example_array.flatten()\ndistances = scipy.spatial.distance.pdist(flat_array)\ndistances_2D = np.array(distances).reshape(len(flat_array), len(flat_array))\n\nprint(distances_2D)\n```\n\nThis code will output a 2D array where each element is the pairwise Euclidean distance between two regions.\n", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], s=4)\n    result[i] = interpolate.splev(x_val, tck)\nresult = result.reshape((5, 100))\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x = np.array([x1, x2, x3, x4])\nstatistic, critical_values, significance_level = ss.anderson_ksamp(x)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nresult = ss.anderson_ksamp([x1, x2])\nprint(result)\n", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "\ndef tau1(x):\n    y = np.array(df['A'])\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = df['B'].rolling(3).apply(tau1)\ndf['AC'] = df['C'].rolling(3).apply(tau1)\ndf['BC'] = df['B'].rolling(3).apply(lambda x: 1/tau1(x))\n\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "result = bool(sa.nnz == 0)", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "result = bool(sa.nnz == 0)", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "wilcoxon_result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = wilcoxon_result.pvalue\n", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n    ### END SOLUTION ###\n\nprint(f(example_pre_course_scores, example_during_course_scores))\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "\nfrom scipy.stats import kurtosis\n\nkurtosis_result = kurtosis.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interpolator = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interpolator(x, y)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    interpolated = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n    result = interpolated(s, t)\n    return result\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "result = np.zeros(len(extraPoints))\nfor i, point in enumerate(extraPoints):\n    for region in vor.regions:\n        if point in region:\n            result[i] = len(region)\n            break\nprint(result)\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "from scipy.spatial import ConvexHull\n\n# Create a ConvexHull object\nch = ConvexHull(extraPoints)\n\n# Get the regions\nregions = ch.regions\n\n# Get the indices of the regions\nresult = [len(region) for region in regions]\n\nprint(result)\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "\n# find the maximum size among all vectors\nmax_size = max([len(vec) for vec in vectors])\n\n# create a sparse matrix with padded zeros\nresult = sparse.coo_matrix((max_size - len(vec)) * np.ones(len(vec)))\n\n# add vectors to the sparse matrix\nfor vec in vectors:\n    result.data[result.data < vec] = vec\n\n# convert the sparse matrix to a dense matrix\nresult = result.todense()\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=(1, 1))", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M.toarray()[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "\nresult = [M.get_coo()[i, j] for i, j in zip(row, column)]\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "f = scipy.interpolate.interp1d(x, array[:, 5, :], kind='cubic')\nnew_array = f(x_new)", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u, o2, x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner[0]\n    P = P_inner[0] + P_outer/2\n    return P\n\nprob = NormalDistro(u, o2, x)", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner[0]\n    P = P_inner[0] + P_outer/2\n    return P\n    ### END SOLUTION ###\n\nThe error is because you are trying to call a function that is not a function, but a module. You should call the function directly.\n\nAlso, you should use the `scipy.integrate.quad` function instead of `scipy.integrate`.\n\nHere is the corrected code:\n\n```python\nimport scipy.integrate\nimport math\nimport numpy as np\n\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\n\ndef f(x = 2.5, u = 1, o2 = 3):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner[0]\n    P = P_inner[0] + P_outer/2\n    return P\n```\n\nThis code should work for you.\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "\n# Create a 2D array of size N x N with ones\nA = np.ones((N, N))\n\n# Compute the DCT of the 2D array\ndct_A = sf.dct(A)\n\n# Normalize the DCT matrix\nresult = dct_A / np.sqrt(np.pi)\n\n# The result is a NxN DCT matrix with ortho-mode normed\n", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "diags(matrix, [0, 0, 0, -1, 1], (5, 5)).toarray()\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "\ndef binomial_pmf(n, p, k):\n    return scipy.stats.binom.pmf(k, n, p)\n\nresult = np.array([[binomial_pmf(n, p, k) for k in range(n+1)] for n in range(n+1)])\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "z_scores = stats.zscore(df)\nresult = pd.DataFrame(z_scores, index=df.index, columns=df.columns)\n", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "\n# First, calculate the z-scores for each sample\nz_scores = stats.zscore(df, axis=1)\n\n# Then, create a new dataframe with the original data and the z-scores\nresult = pd.concat([df.reset_index().rename(columns={'probegenes': 'data'}), z_scores], axis=1)\n\n# Rename the columns\nresult.columns = ['probegenes', 'data', 'sample1', 'sample2', 'sample3']\n\n# Set the index back to 'probegenes'\nresult.set_index('probegenes', inplace=True)\n\nresult\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "\ndf['data'] = df.apply(lambda row: pd.Series({'sample1': row['sample1'], 'sample2': row['sample2'], 'sample3': row['sample3']}), axis=1)\ndf['zscore'] = df.apply(lambda row: pd.Series({'sample1': stats.zscore(row['sample1']), 'sample2': stats.zscore(row['sample2']), 'sample3': stats.zscore(row['sample3'])}), axis=1)\n\nresult = pd.concat([df['data'], df['zscore']], axis=1)\nresult.columns = ['type', 'sample1', 'sample2', 'sample3']\nresult.loc[result['type'] == 'data'] = result.loc[result['type'] == 'data', ['sample1', 'sample2', 'sample3']].round(3)\nresult.loc[result['type'] == 'zscore'] = result.loc[result['type'] == 'zscore', ['sample1', 'sample2', 'sample3']].round(3)\nresult.drop('type', axis=1, inplace=True)\nresult\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "\ny = np.linspace(-1, 1, shape[0])\nx = np.linspace(-1, 1, shape[1])\n\ny, x = np.meshgrid(y, x)\n\nmid = np.zeros(shape + (2,))\nmid[:, :, 0] = y\nmid[:, :, 1] = x\n\nresult = distance.cdist(mid, mid, 'euclidean')\n\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "\ndef get_distance_2(shape):\n    rows, cols = shape\n    y, x = np.ogrid[:rows, :cols]\n    center = np.array([rows // 2, cols // 2])\n    mid = np.abs(x - center[0]) + np.abs(y - center[1])\n    return mid\n\nresult = get_distance_2(shape)\nprint(result)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": "\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    center = np.array([[shape[0]//2, shape[1]//2]])\n    distances = distance.cdist(center, [y, x], 'euclidean')\n    return distances\n    ### END SOLUTION\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "\nresult = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return np.sum((a[:, None] * x[:, None]**2)**2, axis=0)\n\ndef residual(x, a, y):\n    return np.sum((y - func(x, a))**2)\n\nout = scipy.optimize.minimize(residual, x0, args=(a, y))['x']\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "\ndef func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return np.sum((func(x) - y) ** 2)\n\nout = scipy.optimize.minimize(residual, x0, method='L-BFGS-B', bounds=[(-1/x_lower_bounds) * x_true for x_true in x_true])\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\n\ndef dN1_dt_simple(t, N1):\n    if 0 < t < 2*np.pi:\n        return -100 * (N1 + np.sin(t))\n    else:\n        return 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.hstack([sa, sb])", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "result, error = scipy.integrate.quad(lambda x: 2*c*x, low, high)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": "\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n    ### END SOLUTION\n    # return the result\n\n# test the function\nprint(f(c=5, low=0, high=1))\n", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "\n# Construct a dense matrix with the scalar value x\nx_matrix = np.array([x]).reshape(-1, 1)\n\n# Add the scalar to the sparse matrix\nV_plus_x = V.add(x_matrix)\n\n# Convert the result back to a sparse matrix\nV = sparse.dok_matrix(V_plus_x)\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "\n# Create a dense matrix with the same size as V, filled with x\ndense_x = sparse.coo_matrix((x, V.shape), shape=V.shape)\n\n# Add V and dense_x\nV += dense_x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "\n# Construct a dense matrix with the scalar value x\nx_matrix = sparse.diags([0, x])\n\n# Construct a dense matrix with the scalar value y\ny_matrix = sparse.diags([y])\n\n# Add the two matrices\nV_plus_x = x_matrix + V\n\n# Add the result to y\nV_plus_x_plus_y = V_plus_x + y_matrix\n\n# Convert the result to a sparse matrix\nV = V_plus_x_plus_y.tocsr()\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "\n# iterate through columns\nfor Col in xrange(sa.shape[1]):\n    # get the column\n    Column = sa[:,Col].toarray()\n    # calculate the length\n    Len = np.linalg.norm(Column)\n    # normalize the column\n    NormalizedColumn = Column / Len\n    # update the original column\n    sa[:,Col] = NormalizedColumn\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "\n# extract each column of the matrix\nfor col in range(sa.shape[1]):\n    column = sa[:, col].toarray()\n    # get the column length\n    length = np.linalg.norm(column)\n    # normalize the column\n    normalized_column = column / length\n    # update the original column in the matrix\n    sa[:, col] = sparse.csr_matrix(normalized_column)\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "\na = (a > 0).astype(int)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "\na = (a != 0).astype(int)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "\n# Calculate the Euclidean distance between each data point and the centroid\ndistances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\n\n# Find the index of the minimum distance for each cluster\nresult = np.argmin(distances, axis=0)\n\n# Add 1 to the result because Python uses 0-based indexing\nresult = result + 1\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "\n# Calculate the Euclidean distance between each data point and each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\n\n# Find the index of the minimum distance for each data point\nclosest_indices = np.argmin(distances, axis=0)\n\n# Extract the closest data point for each cluster\nresult = data[closest_indices]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "\n# Calculate the Euclidean distance between each data point and each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\n\n# Find the k-th smallest distances\nindices = np.argpartition(distances, k, axis=0)[:k, :]\n\n# The indices of the k-th closest data point to the centroid of each cluster\nresult = indices\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result[x] = fsolve(lambda a: eqn(a, x, b), 0.5)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "result = []\nfor x, a in zip(xdata, adata):\n    sol = fsolve(eqn, x0=x, args=(a,))\n    result.append([x, sol[0]])\nresult = np.array(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "\n# Fit the data to the model\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the goodness of fit\nresult = sp.stats.kstest(sample_data, 'norm')\n\nprint('Statistic:', result.statistic)\nprint('P-value:', result.pvalue)\n", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "\n# Fit the data to the model\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start + np.arange(len(sample_data)), sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Create a new function with the fitted parameters\nnew_function = lambda x: bekkers(x, *popt)\n\n# Calculate the K-S test\nks_test = sp.stats.kstest(sample_data - new_function(range_start + np.arange(len(sample_data))))\n\n# Check if the p-value is less than 0.05 to reject the null hypothesis\nresult = ks_test.pvalue < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "\n# Convert the 'Time' column to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Set the 'Time' column as the DataFrame index\ndf.set_index('Time', inplace=True)\n\n# Calculate the rolling integral\nintegral_df = df['A'].rolling(window='25S').apply(integrate.trapz)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "f = scipy.interpolate.griddata(x, y, eval, method='linear')\nresult = f(2.7, 2.3)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "\n# calculate the number of observations\nn = a.shape[0]\n\n# calculate the number of categories\nk = a.shape[1]\n\n# calculate the total sum of the data\ntotal_sum = n / np.sum(a)\n\n# calculate the total sum of the data squared\ntotal_sum_sq = total_sum ** 2\n\n# calculate the sum of each category\nsum_cat = np.sum(a, axis=0)\n\n# calculate the sum of each category squared\nsum_cat_sq = sum_cat ** 2\n\n# calculate the sum of each category to the total sum\nsum_cat_to_total_sum = sum_cat / total_sum\n\n# calculate the sum of each category to the total sum squared\nsum_cat_to_total_sum_sq = sum_cat_to_total_sum ** 2\n\n# calculate the sum of each category to the total sum cubed\nsum_cat_to_total_sum_cubed = sum_cat_to/total_sum ** 3\n\n# calculate the sum of each category to the total sum fourth\nsum_cat_to_total_sum_fourth = sum_cat_to_total_sum ** 4\n\n# calculate the sum of each category to the total sum fifth\nsum_cat_to_total_sum_fifth = sum_cat_to_total_sum ** 5\n\n# calculate the sum of each category to the total sum sixth\nsum_cat_to_total_sum_sixth = sum_cat_to_total_sum ** 6\n\n# calculate the sum of each category to the total sum seventh\nsum_cat_to_total_sum_seventh = sum_cat_to_total_sum ** 7\n\n# calculate the sum of each category to the total sum eighth\nsum_cat_to_total_sum_eighth = sum_cat_to_total_sum ** 8\n\n# calculate the sum of each category to the total sum ninth\nsum_cat_to_total_sum_ninth = sum_cat_to_total_sum ** 9\n\n# calculate the sum of each category to the total sum tenth\nsum_cat_to_total_sum_tenth = sum_cat_to_total_sum ** 10\n\n# calculate the sum of each category to the total sum eleventh\nsum_cat_to_total_sum_eleventh = sum_cat_to_total_sum ** 11\n\n# calculate the sum of each category to the total sum twelfth\nsum_cat_to_total_sum_twelfth = sum_cat_to_total_sum ** 12\n\n# calculate the sum of each category to the total sum thirteenth\nsum_cat_to_total_sum_thirteenth = sum_cat_to_total_sum ** 13\n\n# calculate the sum of each category to the total sum fourteenth\nsum_cat_to_total_sum_fourteenth = sum_cat_to_total_sum ** 14\n\n# calculate the sum of each category to the total sum fifteenth\nsum_cat_to_total_sum_fifteenth = sum_cat_to_total_sum ** 15\n\n# calculate the sum of each category to the total sum sixteenth\nsum_cat_to_total_sum_sixteenth = sum_cat_to_total_sum ** 16\n\n# calculate the sum of each category to the total sum seventeenth\nsum_cat_to_total_sum_seventeenth = sum_cat_to_total_sum ** 17\n\n# calculate the sum of each category to the total sum eighteenth\nsum_cat_to_total_sum_eighteenth = sum_cat_to_total_sum ** 18\n\n# calculate the sum of each category to the total sum nineteenth\nsum_cat_to_total_sum_nineteenth = sum_cat_to_total_sum ** 19\n\n# calculate the sum of each category to the total sum twentieth\nsum_cat_to_total_sum_twentieth = sum_cat_to_total_sum ** 20\n\n# calculate the sum of each category to the total sum twenty-first\nsum_cat_to_total_sum_twenty_first = sum_cat_to_total_sum ** 21\n\n# calculate the sum of each category to the total sum twenty-second\nsum_cat", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = fp(popt, x)\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "\n# find the local maxima and minima\npeaks, _ = signal.find_peaks(arr, width=None, rel_height=0.5)\n\n# find the relative extrema\nrel_extrema = [i for i in peaks if all(arr[i-n:i+n+1] <= arr[i])]\n\nresult = rel_extrema\nprint(result)\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "\n# find the local maximums and minimums\nrows, cols = np.where(signal.local_max(arr, n))\n\n# combine the rows and columns\nresult = list(zip(rows.tolist(), cols.tolist()))\n\nprint(result)\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "\n# Select only numeric columns\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\n\n# Calculate z-scores\nz_scores = np.abs(stats.zscore(df[num_cols]))\n\n# Filter rows based on z-scores\ndf = df[(z_scores < 3).all(axis=1)]\n", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], ['target']))\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], ['target']))\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], ['target']))\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": "\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], ['target']))\n    return df\n    ### END SOLUTION\n\nprint(solve(data))\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# First, we need to extract the list of unique elements from the dataframe\nunique_elements = df['Col3'].apply(pd.Series).stack().unique().tolist()\n\n# Then, we create a dataframe with zeros for each unique element\ndf_out = pd.DataFrame(0, index=np.arange(len(df)), columns=unique_elements)\n\n# Now, we iterate over the dataframe and update the corresponding columns\nfor index, row in df.iterrows():\n    for element in row['Col3']:\n        df_out.loc[index, element] = 1\n\ndf_out", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# First, we need to convert the list to a DataFrame\nlist_of_lists = [['Apple'], ['Orange', 'Banana'], ['Grape']]\ndf_out = pd.DataFrame(list_of_lists, columns=['Apple', 'Orange', 'Banana', 'Grape'])\n\n# Then, we need to fill NaNs with 0s\ndf_out = df_out.fillna(0)\n\n# Finally, we need to convert the DataFrame to a list of lists\nlist_of_lists = df_out.values.tolist()\n\n# Now, we can convert the list of lists to a one-hot-encoded DataFrame\ndf_out = pd.get_dummies(pd.DataFrame(list_of_lists))\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# First, we need to convert the list to a binary matrix\n# We use a dictionary to map each fruit to a unique integer\nfruit_dict = {'Apple': 0, 'Banana': 1, 'Grape': 2, 'Orange': 3, 'Suica': 4}\n\n# Then, we apply the function to each row\nfor index, row in df.iterrows():\n    # We create a binary matrix of shape (1, num_fruits)\n    binary_matrix = np.zeros((1, len(fruit_dict)))\n    # We set the corresponding value to 1\n    for fruit in row['Col4']:\n        binary_matrix[0, fruit_dict[fruit]] = 1\n    # We add the binary matrix to the dataframe\n    df.loc[index, 'Apple':'Suica'] = binary_matrix[0]\n\ndf_out = df\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# First, we need to flatten the list of fruits\ndf['Col3'] = df['Col3'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a dictionary to map each fruit to a unique integer\nfruit_dict = {v: i for i, v in enumerate(set(df['Col3'].values.ravel()), 1)}\n\n# Now, we apply the dictionary to the list of fruits\ndf['Col3'] = df['Col3'].apply(lambda x: [fruit_dict.get(item, 0) for item in x])\n\n# Finally, we reshape the DataFrame to have the right dimensions\ndf_out = df.set_index('Col1').stack().unstack(level=0).reset_index()\ndf_out.columns.name = None\n\n# We also need to convert the 'Col2' to float\ndf_out['Col2'] = df_out['Col2'].astype(float)\n\nprint(df_out)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# First, we need to flatten the list of fruits\ndf['Col3'] = df['Col3'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Then, we create a dictionary to map each fruit to a unique integer\nfruit_dict = {v: i for i, v in enumerate(set(df['Col3'].values.tolist() + ['']))}\n\n# Now, we apply the dictionary to the list of fruits\ndf['Col3'] = df['Col3'].apply(lambda x: [fruit_dict[i] for i in x])\n\n# Finally, we reshape the dataframe to get the one-hot-encoded data\ndf_out = df.set_index('Col1').stack().unstack(level=0).reset_index()\ndf_out.columns.name = None\n\n# We drop the original column\ndf_out = df_out.drop('Col3', 1)\n\n# We rename the first level of the index\ndf_out.columns = ['Col1', 'Col2', 'Apple', 'Orange', 'Banana', 'Grape']\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\npredicted_test= svmmodel.predict_proba(x_test)\nproba = [p[1] for p in predicted_test]", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "\nmodel.fit(X, y)\nproba = model.predict_proba(x_predict)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "\n# Convert the sparse matrix to a DataFrame\ndf_transform = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Concatenate the original dataframe and the transformed dataframe\ndf = pd.concat([df_origin, df_transform], axis=1)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "\n# Convert the sparse matrix to a DataFrame\ndf_transform = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Merge the original DataFrame and the transformed DataFrame\ndf = pd.concat([df_origin, df_transform], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": "\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\nclf.steps.append(('new_step', SomeNewStep()))\nclf.fit(X_train, y_train)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps.append(('new_step', SVC()))\nclf.fit(X_train, y_train)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PCA()))\nclf", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\nmodel = xgb.XGBRegressor()\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'learning_rate': [0.01, 0.1, 0.2]\n}\n\ngrid = GridSearchCV(model, param_grid, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, verbose=1, \n                    fit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]})\ngrid.fit(trainX, trainY)\n\n# Get the best model\nbest_model = grid.best_estimator_\n\n# Predict\nc = best_model.predict(testX)\n\n# Score\nb = grid.score(testX, testY)", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nmodel = xgb.XGBRegressor()\nparamGrid = {\n    'n_estimators': [50, 100, 150],\n    'learning_rate': [0.01, 0.1, 0.2]\n}\n\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, \n                          fit_params={\"early_stopping_rounds\":42, \"eval_metric\" : \"mae\", \"eval_set\" : [[testX, testY]]})\n\ngridsearch.fit(trainX, trainY)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test.index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    t_pred = scaler.inverse_transform(data['t'])\n    return t_pred\n    ### END SOLUTION ###\n\nThis is a common way to handle this issue. The inverse_transform method is used to convert the scaled data back to its original form.\n\nRemember that the inverse_transform method expects a 1-D array as input. If your 't' column is a multi-dimensional array, you will need to reshape it before using inverse_transform.\n\nAlso, keep in mind that the inverse_transform method will return a numpy array, not a pandas series. If you want to keep the result as a pandas series, you can use the following code:\n\n```python\nt_pred = pd.Series(scaler.inverse_transform(data['t'].reshape(-1, 1)))\n```\n\nThis will reshape the array to a 1-D series.\n", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "\nmodel_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "\nmodel_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "\nmodel_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)\n", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nclf = grid_search.best_estimator_\nproba = clf.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nrgr = regressor.fit(X, y)\n\n# Predict\npredict = rgr.predict(X_test)", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\n\n# Reshape y to match the shape of X\ny = y.reshape(-1, 1)\n\n# Fit the model\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nregressor.fit(X, y)\n\n# Predict\npredict = regressor.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\n\n# Define your preprocessor\ndef preprocess(s):\n    return s.upper()\n\n# Define your TfidfVectorizer\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n\n# Create a DataFrame\ndf = pd.DataFrame({'text': ['This is the first document.', 'This is the second document.', 'And the third one.', 'Is this the first document?']})\n\n# Fit and transform the data\nX = tfidf.fit_transform(df['text'])\n\n# Print the shape of the transformed data\nprint(X.shape)\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\n\ndef prePro(text):\n    return text.lower()\n\nvectorizer = TfidfVectorizer(preprocessor=prePro)\n\n# Now you can use the vectorizer\ntfidf = vectorizer.fit_transform(corpus)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# First, you need to convert your DataFrame to a numpy array\ndata_array = data.values\n\n# Then, you can apply the preprocessing.scale function to the numpy array\nscaled_data = preprocessing.scale(data_array)\n\n# Finally, you need to convert the scaled numpy array back to a DataFrame\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\ndf_out = scaled_data_df\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# First, you need to convert your DataFrame to a numpy array\ndata_array = data.values\n\n# Then, apply the preprocessing.scale function to the numpy array\nscaled_data = preprocessing.scale(data_array)\n\n# Finally, convert the scaled numpy array back to a DataFrame\ndf_out = pd.DataFrame(scaled_data, columns=data.columns)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "\nfeature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).reset_index().rename(columns={'index':'feature'})\nselected_features = feature_importances.loc[feature_importances['importance'] > 0, 'feature'].tolist()\ncolumn_names = selected_features", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# Get the feature names\nfeature_names = X.columns.values[model.get_support()]\n\n# Assign the feature names to the variable 'column_names'\ncolumn_names = feature_names\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "\ncolumn_names = [X.columns[i] for i in np.where(model.get_support())[0]]\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "\ncolumn_names = [X.columns[i] for i in np.where(model.get_support())[0]]\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\ndistances = km.transform(X)\nindices = np.argsort(distances[:, p])\nclosest_50_samples = X[indices[:50]]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\ndistances = km.transform(X)\nclosest_50_samples = X[np.argpartition(distances, 50)[:, :50]]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\ndistances = km.transform(X)\nindices = np.argsort(distances[:, p])\nclosest_100_samples = X[indices[:100]]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": "\n    km.fit(X)\n    centers = km.cluster_centers_\n    distances = np.linalg.norm(X - centers[p], axis=1)\n    closest_samples = X[np.argmin(distances)]\n    return closest_samples\n    ### END SOLUTION\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# Convert categorical variable to dummy variables\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# convert categorical variable to dummy variables\nX_train = pd.get_dummies(X_train)\n\nEND SOLUTION\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\nsvr = SVR(kernel='rbf')\nsvr.fit(X, y)\npredict = svr.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\n\nclf = svm.SVC(kernel='rbf')\nclf.fit(X, y)\npredict = clf.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\nmodel = SVR(kernel='poly', degree=2)\nmodel.fit(X, y)\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\nsvr = SVR(kernel='poly', degree=2)\nsvr.fit(X, y)\npredict = svr.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "\ntfidf_matrix = tfidf.transform(documents)\nquery_tfidf_matrix = tfidf.transform(queries)\ncosine_similarities_of_queries = cosine_similarity(query_tfidf_matrix, tfidf_matrix)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "\ntfidf_matrix = tfidf.transform(documents)\nquery_tfidf_matrix = tfidf.transform(queries)\ncosine_similarities_of_queries = cosine_similarity(query_tfidf_matrix, tfidf_matrix)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    query_vectors = tfidf.transform(queries)\n    cosine_similarities = cosine_similarity(query_vectors, tfidf.transform(documents))\n    return cosine_similarities\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# Convert the features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D-array\nnew_features = np.array(new_features)\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(f)\n\n# Convert the DataFrame to a numpy array\nnew_f = df.values\n\n# Convert the numpy array to a 2D-array\nnew_f = np.array(new_f)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# Convert the features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D-array\nnew_features = np.array(new_features)\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a DataFrame\n    df = pd.DataFrame(features)\n    \n    # Convert the DataFrame to a numpy array\n    arr = df.values\n    \n    # Convert the numpy array to a 2D-array\n    new_features = np.array([list(x) for x in arr])\n    \n    return new_features\n    ### END SOLUTION ###\n\nfeatures = solve(features)\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Convert the features to a DataFrame\ndf = pd.DataFrame(features)\n\n# Convert the DataFrame to a numpy array\nnew_features = df.values\n\n# Convert the numpy array to a 2D-array\nnew_features = np.array(new_features)\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Convert the 2-D list to a numpy array\ndata_matrix = np.array(data_matrix)\n\n# Perform hierarchical clustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average').fit(data_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix from your data\ndist_matrix = pd.DataFrame(data_matrix).set_index(0).set_index(1).stack().unstack()\n\n# Create an instance of the AgglomerativeClustering model\nclustering = AgglomerativeClustering(n_clusters=2, linkage_tree=True)\n\n# Fit the model to the distance matrix\nclustering.fit(dist_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Convert the 2-D list to a numpy array\nsimM = np.array(simM)\n\n# Perform hierarchical clustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average').fit(simM)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "\ndistances = pd.DataFrame(data_matrix, index=['prof1', 'prof2', 'prof3'], columns=['prof1', 'prof2', 'prof3'])\n\ndistances_array = distances.values\n\ncluster_labels = scipy.cluster.hierarchy.fcluster(scipy.cluster.hierarchy.linkage(distances_array, method='average'), 2, criterion='maxclust')\n\ncluster_labels = ['prof' + str(label) for label in cluster_labels]\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "\ndistances = pd.DataFrame(data_matrix, index=['prof1', 'prof2', 'prof3'], columns=['prof1', 'prof2', 'prof3'])\n\ndistances_array = distances.values\n\ndistances_array\n\ncluster_labels = scipy.cluster.hierarchy.fcluster(scipy.cluster.hierarchy.linkage(distances_array, method='average'), 2, criterion='maxclust')\n\ncluster_labels", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "\ndistM = pd.DataFrame(simM, index=['fruit1', 'fruit2', 'fruit3'], columns=['fruit1', 'fruit2', 'fruit3'])\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set.index('fruit3').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit3').stack().reset_index().rename(columns={'level_1':'fruit1', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit1', 0:'value'})\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('fruit3').stack().reset_index().rename(columns={'level_1':'fruit1', 0:'value'})\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('fruit3').stack().reset_index().rename(columns={'level_1':'fruit1', 0:'value'})\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('fruit1').stack().reset_index().rename(columns={'level_1':'fruit2', 0:'value'})\ndistM = distM.set_index('fruit2').stack().reset_index().rename(columns={'level_1':'fruit3', 0:'value'})\ndistM = distM.set_index('", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = sklearn.preprocessing.StandardScaler().fit(data)\ncentered_scaled_data = scaler.transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import FunctionTransformer\n\nbox_cox = FunctionTransformer(lambda x: np.power(x + 1, -1), check_inverse=False)\nbox_cox_data = box_cox.transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from scipy.stats import boxcox\nbox_cox_data, fitted = boxcox(data)", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "\nvectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\S+', max_features=5000, stop_words='english')\ntransformed_text = vectorizer.fit_transform(text)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the data into features and target variable\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataframe into features and target\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the data into features and target variable\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    from sklearn.model_selection import train_test_split\n    X = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    return X_train, y_train, X_test, y_test\n    ### END SOLUTION\n\n\nA: Here is a solution using sklearn's train_test_split:\n\n```python\nfrom sklearn.model_selection import train_test_split\n\ndef split_data(data, test_size=0.2):\n    X = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    return X_train, y_train, X_test, y_test\n```\n\nThis function splits the data into training and testing sets. The test_size parameter is the proportion of the dataset to include in the test split. If you want to split the data into 80% training and 20% testing, you can set test_size=0.2.\n\nThe random_state parameter is used to ensure that the split is reproducible. If you don't want the splits to be reproducible, you can set it to a constant value (like 42).\n\nThe function returns the training and testing sets for the features and the target variable separately.\n\nYou can call this function like this:\n\n```python\nX_train, y_train, X_test, y_test = split_data(dataset, 0.2)\n```\n\nThis will split your data into 80% training and 20% testing.\n", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "\nmse = df['mse'].values\nmse = mse.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2, n_init=10).fit(mse)\nlabels = kmeans.predict(mse)\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\nkmeans = KMeans(n_clusters=2, n_init=10).fit(df['mse'].values.reshape(-1, 1))\nlabels = kmeans.predict(df['mse'].values.reshape(-1, 1))\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "\nclf = LinearSVC(C=1.0, penalty='l1', dual=False)\nclf.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(clf.coef_ != 0)]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "\nclf = LinearSVC(C=1.0, penalty='l1', dual=False)\nclf.fit(X, y)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(clf.coef_ != 0)]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": "\n    clf = LinearSVC(C=1.0, penalty='l1', dual=False).fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[clf.coef_ != 0]\n    return selected_feature_names\n    ### END SOLUTION\n\n# Test the function\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nvectorizer = vectorizer.fit(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = vectorizer.transform(corpus).toarray()\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n\n# Reverse the order of the features\nfeature_names = feature_names[::-1]\nX = X[:,::-1]\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = []\n\nfor col in df1.columns[1:]: # skip the first column\n    df2 = df1[['Time', col]].dropna()\n    X = df2['Time'].values.reshape(-1,1)\n    Y = df2[col].values.reshape(-1,1)\n    regression = LinearRegression().fit(X, Y)\n    slopes.append(regression.coef_[0][0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[['Time', col]].dropna()\n    X = df2['Time'].values.reshape(-1,1)\n    Y = df2[col].values.reshape(-1,1)\n    regression = LinearRegression().fit(X, Y)\n    slopes.append(regression.coef_[0][0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# Initialize the LabelEncoder\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n    ### END SOLUTION\n\ndf = Transform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "ElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "\nscaler = MinMaxScaler()\nnp_array_transformed = scaler.fit_transform(np_array)\ntransformed = pd.DataFrame(np_array_transformed, columns=np_array.columns)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "\nscaler = MinMaxScaler()\nnp_array_transformed = scaler.fit_transform(np_array)\ntransformed = pd.DataFrame(np_array_transformed, columns=np_array.columns)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": "\n    scaler = MinMaxScaler()\n    a_scaled = scaler.fit_transform(a)\n    return a_scaled\n    ### END SOLUTION\n\nnp_array = np_array.values.reshape(-1, 1)\ntransformed_array = Transform(np_array)\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "close_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\n\npredict = clf.predict(b)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = []\nfor i in range(len(X)):\n    new_X.append([int(X[i][0]), X[i][1]])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = pd.DataFrame(X, columns=['feature1', 'feature2'])\nnew_X = new_X.apply(pd.factorize)\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = []\nfor i in range(len(X)):\n    new_X.append([int(X[i][1]), X[i][0]])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# Split the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n\n# Fit the model\nlogReg.fit(X, y)\n\n# Predict\npredict = logReg.predict(X)\n\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Separate the features and the target variable\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n\n# Predict the target variable\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# First, you need to sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Then, calculate the number of days in the dataframe\nn_days = features_dataframe.shape[0]\n\n# Calculate the number of days for the train set\ntrain_days = int(n_days * 0.2)\n\n# Calculate the number of days for the test set\ntest_days = n_days - train_days\n\n# Create the train and test dataframes\ntrain_dataframe = features_dataframe.iloc[:train_days]\ntest_dataframe = features_dataframe.iloc[train_days:]\n\n# Reset the index\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\n# Sort the train and test dataframes by date\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\n\n# Now you can use train_dataframe and test_dataframe for your model\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# First, you need to sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Then, calculate the number of days in the dataframe\nn_days = features_dataframe.shape[0]\n\n# Calculate the number of days in the train set\ntrain_days = int(n_days * 0.8)\n\n# Create the train and test dataframes\ntrain_dataframe = features_dataframe.iloc[:train_days]\ntest_dataframe = features_dataframe.iloc[train_days:]\n\n# Reset the index\ntrain_dataframe = train_dataframe.reset_index(drop=True)\ntest_dataframe = test_dataframe.reset_index(drop=True)\n\n# Now, you can use train_dataframe and test_dataframe for your model\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    train_size = int(len(features_dataframe) * 0.8)\n    train_dataframe = features_dataframe[:train_size]\n    test_dataframe = features_dataframe[train_size:]\n    return train_dataframe, test_dataframe\n    ### END SOLUTION\n\ntrain_dataframe, test_dataframe = solve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2_scale', 'X3_scale']] = scaler.fit_transform(df[['X2', 'X3']])\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "\nmyData['new_A2'] = scaler.fit_transform(myData[['Month', 'A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['Month', 'A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "\ncount = CountVectorizer(lowercase=False, token_pattern='[^\\s]+')\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.loc[:, ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\nfull_results.columns = ['params', 'score', 'std', 'rank']\nfull_results.sort_values('rank', inplace=True)\nfull_results\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nfrom sklearn.externals.joblib import dump, load\n\n# Save the model to a file\ndump(fitted_model, 'sklearn_model.pkl')\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\n\n# Get the current learning rate\ncurrent_lr = optim.param_groups[0]['lr']\n\n# Create a new optimizer with a new learning rate\nnew_optim = torch.optim.SGD(..., lr=0.001)\n\n# Copy the parameters from the old optimizer to the new one\nnew_optim.param_groups = optim.param_groups\n\n# Replace the old optimizer with the new one\noptim = new_optim\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n\n# Get the current learning rate\ncurrent_lr = optim.param_groups[0]['lr']\n\n# Create a new optimizer with a new learning rate\nnew_optimizer = torch.optim.SGD(..., lr=0.001)\n\n# Copy the parameters from the old optimizer to the new one\nnew_optimizer.param_groups[0]['lr'] = current_lr\n\n# Replace the old optimizer with the new one\noptim = new_optimizer\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\n\n# Get the current learning rate\ncurrent_lr = optim.param_groups[0]['lr']\n\n# Create a new optimizer with a new learning rate\nnew_optim = torch.optim.SGD(..., lr=0.0005)\n\n# Copy the parameters from the old optimizer to the new one\nnew_optim.param_groups[0]['lr'] = current_lr\n\n# Replace the old optimizer with the new one\noptim = new_optim\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n\n# Define a custom learning rate scheduler\nclass CustomLearningRateScheduler(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, num_steps):\n        self.num_steps = num_steps\n        super(CustomLearningRateScheduler, self).__init__(optimizer)\n\n    def step(self):\n        if self.last_epoch % self.num_steps == 0:\n            self.base_optimizer.param_groups[0]['lr'] *= 0.9\n\n# Create the scheduler\nscheduler = CustomLearningRateScheduler(optim, num_steps=5)\n\n# Train your model\nfor epoch in range(num_epochs):\n    train_loss = train_model(model, optimizer, scheduler)\n    # ...\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "\n# Convert gensim's Word2Vec model to PyTorch's Embedding\nembedding = torch.nn.Embedding.from_pretrained(torch.tensor(word2vec[word2vec.wv.index2word]))\n\n# Use the embedding layer to embed your input data\nembedded_input = embedding(input_Tensor)\n\nprint(embedded_input)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": "\n    word2vec_weights = torch.tensor(word2vec[word2vec.wv.vocab.keys()]).detach()\n    return word2vec_weights\n    ### END SOLUTION\nget_embedded_input(input_Tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.numpy())", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.numpy())", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(x.numpy())", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "\nC = torch.masked_select(B, A_log[:, None])\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "\nA_logical = torch.ByteTensor(A_logical)\nC = torch.index_select(B, 0, A_logical.nonzero().squeeze())\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "\nA_log = torch.tensor(A_log, dtype=torch.bool)\nC = torch.masked_select(B, A_log[:, None])\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "\nC = torch.masked_select(B, A_log[:, None])\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION ###\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "\nA_log = torch.ByteTensor(A_log)\nC = torch.index_select(B, 0, A_log.nonzero().squeeze())\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\nidx = idx.cpu().numpy()\nB = B.cpu().numpy()\n\nC = torch.from_numpy(np.array([[B[i, idx[j]] for j in range(len(idx))] for i in range(len(B))]))\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "\nx_tensor = torch.tensor(np.array([np.array(i) for i in x_array]), dtype=torch.float32)", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "\n# Convert numpy array to pandas dataframe\ndf = pd.DataFrame(x_array)\n\n# Convert pandas dataframe to torch tensor\nx_tensor = torch.tensor(df)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    return torch.tensor(np.array(a))\n    ### END SOLUTION ###\n\n# Test the function\nx_tensor = Convert(x_array)\nprint(x_tensor)\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": "\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask\n    ### END SOLUTION\n\nmask = get_mask(lens)\nprint(mask)\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor\ntensor_2D = torch.tensor(Tensor_2D)\n\n# Create a 1D tensor from 2D tensor\ntensor_1D = tensor_2D.view(-1)\n\n# Create a diagonal matrix from 1D tensor\ndiagonal_matrix = torch.diag(tensor_1D)\n\n# Reshape the diagonal matrix to 3D tensor\ntensor_3D = diagonal_matrix.view(index_in_batch, -1)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    return torch.diag(t)\n    ### END SOLUTION ###\n\nTensor_3D = Convert(Tensor_2D)\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.cat([a.unsqueeze(0), b.unsqueeze(0)], 0)\nab = torch.stack([a, b], 0)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "\n# Resize the tensors to match the shape of the second tensor\na = a.view(-1, 114, 514)\nb = b.view(-1, 24, 514)\n\n# Stack the tensors along the second dimension\nab = torch.stack([a, b], 0)\n\n# Convert the tensor to numpy\nab = ab.numpy()\n\n# Reshape the numpy array to match the desired output shape\nab = ab.reshape(-1, 138, 514)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    c = torch.cat([a, b], 0)\n    return c\n    ### END SOLUTION ###\n\n\nA: You can use torch.cat to concatenate tensors along a new dimension. If you want to concatenate tensors along dimension 0 (i.e., concatenate along the first dimension), you can use the following code:\n\n```python\nimport torch\n\n# Assuming a and b are your tensors\na = torch.randn(2, 11)\nb = torch.randn(1, 11)\n\n# Concatenate along dimension 0\nc = torch.cat([a, b], 0)\n\nprint(c.shape)  # Output: torch.Size([3, 11])\n```\n\nIn this code, torch.cat concatenates the tensors along dimension 0 (i.e., concatenates the first dimension). The output tensor c has a shape of (3, 11).\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "\nmask = torch.arange(max(lengths)).expand_as(lengths).to(device) < lengths.unsqueeze(1)\na[mask] = 0\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "\nmask = torch.arange(max(lengths)).unsqueeze(0).expand_as(lengths).to(lengths.device)\na = a * (mask < lengths.unsqueeze(-1).expand_as(lengths))\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "\nmask = torch.arange(max(lengths)).expand_as(lengths).to(device) < lengths.unsqueeze(1)\na[mask] = 0\n", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "\nmask = torch.arange(max(lengths)).unsqueeze(0).expand_as(lengths).to(device) < lengths.unsqueeze(1)\na = a * (mask.float().unsqueeze(-1).expand_as(a))\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "\nnew_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.stack(lt)\n    return tensor_of_tensors\n    ### END SOLUTION ###\n\n\nA: The error is because you are trying to convert a list of tensors to a tensor. The torch.stack() function is used to stack a list of tensors along a new dimension.\n\nHere is the correct way to convert a list of tensors to a tensor:\n\n```python\nimport torch\n\nlist_of_tensors = [torch.randn(3), torch.randn(3), torch.randn(3)]\ntensor_of_tensors = torch.stack(list_of_tensors)\n```\n\nThis will stack the tensors in the list along the new dimension.\n\nIf you want to convert a list of tensors to a tensor of tensors, you can use the following code:\n\n```python\nimport torch\n\nlist_of_tensors = [[torch.randn(3), torch.randn(3), torch.randn(3)] for _ in range(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\n```\n\nThis will create a tensor of tensors where each tensor in the list is a 3-dimensional tensor.\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "\nresult = torch.index_select(t, 0, torch.from_numpy(idx).long())\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "\nresult = torch.index_select(t, 0, torch.tensor(idx).long())\n", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "\nresult = torch.index_select(t, 0, torch.tensor(idx))\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\n# Get the maximum ids\nmax_ids = torch.argmax(x, 1)\n\n# Gather the selected slices\nresult = torch.gather(x, 1, max_ids.unsqueeze(-1).expand(-1,-1,2))[:,0]\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\n# Get the maximum ids\nmax_ids = torch.argmax(scores, 1)\n\n# Gather the selected slices\nresult = torch.gather(x, 1, max_ids.unsqueeze(-1).expand(-1,-1,114))\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Convert ids to tensor\nids_tensor = torch.tensor(ids, dtype=torch.int64)\n\n# Create a mask for the selected indices\nmask = ids_tensor == 1\n\n# Gather the selected slices\nresult = torch.masked_select(x, mask)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "\n_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1).data.tolist()\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "\n_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1).data.tolist()\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "\n_, predicted = torch.max(softmax_output, 1)\ny = predicted.view(-1).data.tolist()\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    _, predicted = torch.max(softmax_output, 1)\n    return predicted\n    ### END SOLUTION ###\n\nprint(solve(softmax_output))\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "\n# Get the maximum value in each row\nmax_values = torch.max(softmax_output, dim=1)[0]\n\n# Get the index of the maximum value\ny = torch.argmax(softmax_output, dim=1)\n\n# Add 1 to the index because Python uses 0-based indexing\ny = y + 1\n\nprint(y)\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "\nimages = Variable(torch.from_numpy(images).float())\nlabels = Variable(torch.from_numpy(labels).long())\n\n_, C, H, W = images.size()\nimages = images.permute(0, 3, 2, 1).contiguous()\nlabels = labels.permute(0, 3, 2, 1).contiguous()\n\nimages = images.view(-1, C)\nlabels = labels.view(-1)\n\nloss = F.cross_entropy(images, labels, weight=None, size_average=True)\n\nloss = loss.sum()\n\nprint(loss.item())\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "\ncnt_equal = torch.sum((A == B).float())\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "\ncnt_equal = torch.sum((A == B).float())\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "\ncnt_not_equal = torch.sum((A != B).float())\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": "\n    cnt_equal = torch.sum(A == B)\n    return cnt_equal\n    ### END SOLUTION\n\nA, B = load_data()\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "\nx = min(A.shape[0], B.shape[0])\ncnt_equal = torch.sum((A[-x:] == B[-x:]).float())\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "\nx = min(A.shape[0], B.shape[0])\ncnt_not_equal = torch.sum((A[-x:] != B[-x:]).float())\n", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = [a[:, :, i:i+1, :, :] for i in range(chunk_dim)]\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = [a[:, :, i, :, :] for i in range(chunk_dim)]\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "\noutput = torch.where(mask.bool(), clean_input_spectrogram, output)\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "\noutput = torch.where(mask == 0, clean_input_spectrogram, output)\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x))\nmin_y = torch.min(torch.abs(y))\n\nsigned_min = torch.where(torch.abs(x) < torch.abs(y), min_x, min_y) * torch.sign(torch.where(torch.abs(x) < torch.abs(y), x, y))\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = torch.where(torch.abs(x) > torch.abs(y), sign_x, sign_y) * max\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": "\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x), torch.abs(y))\n    min_y = torch.min(torch.abs(x), torch.abs(y))\n    signed_min = torch.where(torch.abs(x) < torch.abs(y), min_x, min_y)\n    result = torch.where(torch.abs(x) < torch.abs(y), sign_x, sign_y) * signed_min\n    return result\n    ### END SOLUTION\n\n\nA: You can use the torch.where function to select the minimum value based on the condition. Here is the solution:\n\n```python\nimport torch\n\ndef solve(x, y):\n    # Compute the minimum value\n    min_x = torch.where(torch.abs(x) < torch.abs(y), x, y)\n    min_y = torch.where(torch.abs(x) < torch.abs(y), y, x)\n\n    # Compute the sign\n    sign_x = torch.sign(min_x)\n    sign_y = torch.sign(min_y)\n\n    # Multiply the signs with the minimum value\n    result = torch.where(torch.abs(x) < torch.abs(y), sign_x, sign_y) * min_x\n\n    return result\n```\n\nThis function will keep the elements with the minimum absolute values in each tensor, and the sign of the minimum value will be kept.\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# Calculate the average of the last column of 'a' and the first column of 'b'\naverage = (a[:, -1] + b[:, 0]) / 2\n\n# Create a new tensor with the dimensions of 'a' and 'b'\nresult = torch.cat([a[:, :-1], average.unsqueeze(1), b[:, 1:]], dim=1)\n\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a.squeeze()\n    b = b.squeeze()\n    c = torch.cat([a[:, 1:], b[:, :-1]], 1)\n    c = c.unsqueeze(1)\n    result = c.expand(-1, -1, a.size(0), -1) + a[:, :-1]\n    result = result.view(-1, result.size(-1))\n    return result\n    ### END SOLUTION ###\n\n\nA: You can use the torch.cat function to concatenate the tensors along a new dimension. Here is how you can do it:\n\n```python\nimport torch\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\n# Squeeze the tensors to remove the extra dimensions\na = a.squeeze()\nb = b.squeeze()\n\n# Concatenate the tensors along the second dimension\nc = torch.cat([a[:, 1:], b[:, :-1]], 1)\n\n# Expand the tensor to match the shape of 'a'\nc = c.unsqueeze(1).expand(-1, -1, a.size(0), -1)\n\n# Add the first column of 'a' to the concatenated tensor\nc = c + a[:, :-1]\n\n# Reshape the tensor to the original shape\nresult = c.view(-1, c.size(-1))\n\nprint(result)\n```\n\nThis will give you the desired output:\n\n```\ntensor([[1, 2, 4, 6, 7],\n        [1, 2, 4, 6, 7]])\n```\n\nThe code first squeezes the tensors to remove the extra dimensions, then concatenates the tensors along the second dimension. It then expands the tensor to match the shape of 'a' and adds the first column of 'a' to the concatenated tensor. Finally, it reshapes the tensor to the original shape.\n", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "\n# Reshape the tensor to have 2 rows and 4 columns\nt = t.reshape(2, 4)\n\n# Create a new tensor with zeros\nnew = torch.zeros(1, 4)\n\n# Stack the tensors\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "\n# Reshape the tensor to the desired shape\nt = t.reshape(2, 2, 1)\n\n# Stack the tensor with the new tensor\nresult = torch.cat([t, new], dim=0)\n\n# Convert the result back to a 4-dimensional tensor\nresult = result.unsqueeze(0)\n\nprint(result)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "\n# Reshape the tensor to the desired shape\nt = t.reshape(1, 4, 2)\n\n# Stack the tensor with the new tensor\nresult = torch.stack([t, new])\n\n# Convert the tensor to a numpy array\nresult = result.numpy()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data.perm(0, 1).contiguous(), W)\nresult = result.squeeze()\nresult = result.view(10, 2, 3)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
