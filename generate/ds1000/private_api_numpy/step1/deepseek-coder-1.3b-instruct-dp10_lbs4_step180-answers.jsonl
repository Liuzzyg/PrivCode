{"id": 0, "code": "result = df.iloc[List]", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.loc[List]\nresult['Type'] = df['Type'].values[List]\nresult", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\ndef change_values(df):\n    for col in df.columns:\n        counts = df[col].value_counts()\n        if counts.max() >= 2:\n            df[col] = df[col].replace(counts[counts >= 2].index, counts[counts >= 2].index[0])\n    return df\n\nresult = change_values(df)\nprint(result)\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\ndef change_values(df):\n    for col in df.columns:\n        counts = df[col].value_counts()\n        if counts.max() >= 3:\n            df[col] = df[col].replace(counts[counts >= 3].index, 'other')\n    return df\n\nresult = change_values(df)\nprint(result)\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    for col in df.columns:\n        if col in ['Qu1', 'Qu2', 'Qu3']:\n            counts = df[col].value_counts()\n            for index, value in counts.items():\n                if value >= 2:\n                    df.loc[df[col] == index, col] = 'other'\n    return df\n\nprint(f())\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a function to change values in columns Qu1 according to value_counts() when value count greater or equal 3\ndef change_values_in_column_Qu1(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts >= 3].index, counts[counts >= 3].index)\n\n# Create a function to change values in columns Qu2 and Qu3 according to value_counts() when value count greater or equal 2\ndef change_values_in_columns_Qu2_and_Qu3(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts >= 2].index, counts[counts >= 2].index)\n\n# Apply the functions to the columns Qu1 and Qu2\ndf['Qu1'] = df['Qu1'].apply(change_values_in_column_Qu1)\ndf['Qu2'] = df['Qu2'].apply(change_values_in_columns_Qu2_and_Qu3)\ndf['Qu3'] = df['Qu3'].apply(change_values_in_columns_Qu2_and_Qu3)\n\n# Create a function to create values others for column Qu2\ndef create_others_for_column_Qu2(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts < 2].index, 'other')\n\n# Apply the function to the column Qu2\ndf['Qu2'] = df['Qu2'].apply(create_others_for_column_Qu2)\n\n# Create a function to create values others for column Qu3\ndef create_others_for_column_Qu3(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts < 3].index, 'other')\n\n# Apply the function to the column Qu3\ndf['Qu3'] = df['Qu3'].apply(create_others_for_column_Qu3)\n\n# Create a function to create values others for column Qu1\ndef create_others_for_column_Qu1(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts < 3].index, 'other')\n\n# Apply the function to the column Qu1\ndf['Qu1'] = df['Qu1'].apply(create_others_for_column_Qu1)\n\n# Create a function to create values others for column Qu2\ndef create_others_for_column_Qu2(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts < 2].index, 'other')\n\n# Apply the function to the column Qu2\ndf['Qu2'] = df['Qu2'].apply(create_others_for_column_Qu2)\n\n# Create a function to create values others for column Qu3\ndef create_others_for_column_Qu3(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts < 3].index, 'other')\n\n# Apply the function to the column Qu3\ndf['Qu3'] = df['Qu3'].apply(create_others_for_column_Qu3)\n\n# Create a function to create values others for column Qu1\ndef create_others_for_column_Qu1(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts < 3].index, 'other')\n\n# Apply the function to the column Qu1\ndf['Qu1'] = df['Qu1'].apply(create_others_for_column_Qu1)\n\n# Create a function to create values others for column Qu2\ndef create_others_for_column_Qu2(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts < 2].index, 'other')\n\n# Apply the function to the column Qu2\ndf['Qu2'] = df['Qu2'].apply(create_others_for_column_Qu2)\n\n# Create a function to create values others for column Qu3\ndef create_others_for_column_Qu3(column):\n    counts = column.value_counts()\n    return column.replace(counts[counts < 3].index, 'other')\n\n# Apply the function to the column Qu3\ndf['Qu3'] = df['Qu3'].apply(create", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a function to replace values in column Qu1\ndef replace_values_in_column(column):\n    counts = df[column].value_counts()\n    for value, count in counts.items():\n        if count >= 3:\n            df[column] = df[column].replace(value, 'other')\n        elif count >= 2:\n            df[column] = df[column].replace(value, value)\n    return df\n\n# Apply the function to column Qu1\ndf = replace_values_in_column('Qu1')\n\n# Apply the function to column Qu2\ndf = replace_values_in_column('Qu2')\n\n# Apply the function to column Qu3\ndf = replace_values_in_column('Qu3')\n\nresult = df\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\nresult = df.groupby('url').max().reset_index()\nresult['keep_if_dup'] = result['keep_if_dup'].apply(lambda x: 'Yes' if x == 'Yes' else 'No')\nresult", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndef df_to_dict(df):\n    result = {}\n    for name, v1, v2, v3 in zip(df['name'], df['v1'], df['v2'], df['v3']):\n        if name not in result:\n            result[name] = {}\n        result[name][v1] = {v2: v3}\n    return result\n\nresult = df_to_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n\nresult = f(example_df)\nprint(result)\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf['datetime'] = df['datetime'].apply(lambda x: x.replace('-06:00', ''))\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\ndf['datetime'] = df['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\ndf = df.sort_values('datetime')\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# Convert the message column to a list of dictionaries\ndf['message'] = df['message'].apply(lambda x: eval(x))\n\n# Create a new dataframe with the keys from the first dictionary in the message column\nresult = pd.DataFrame(df['message'][0].keys())\n\n# Add the values from the first dictionary to the new dataframe\nresult = result.append(df['message'][0].values, ignore_index=True)\n\n# Reset the index of the result dataframe\nresult.reset_index(drop=True, inplace=True)\n\n# Print the result dataframe\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df['score'] * 10\ndf.loc[df['product'].isin(products), 'score'] = df.loc[df['product'].isin(products), 'score'] * 10\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10\ndf['score'] = df['score'].where(df['product'].isin(products), 10)\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "\nfor product in products:\n    df.loc[df['product'].between(product[0], product[1]), 'score'] *= 10\n\nprint(df)\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\ndf['score'] = df['score'].apply(lambda x: 1 if x in products else x)\ndf['score'] = (df['score'] - df['score'].min()) / (df['score'].max() - df['score'].min())\n\ndf", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.apply(lambda row: 'A' if row['A'] == 1 else 'B' if row['B'] == 1 else 'C' if row['C'] == 1 else 'D', axis=1)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "\n# Create a new column that concatenates the binary columns\ndf['category'] = df['A'].astype(str) + df['B'].astype(str) + df['C'].astype(str) + df['D'].astype(str)\n\n# Convert the new column to categorical\ndf['category'] = df['category'].astype('category')\n\n# Create a new column that maps the binary columns to the categorical column\ndf['category'] = df['category'].map({'0000': 'A', '0001': 'B', '0010': 'C', '0011': 'D', '0100': 'E', '0101': 'F', '0110': 'G', '0111': 'H', '1000': 'I', '1001': 'J', '1010': 'K', '1011': 'L', '1100': 'M', '1101': 'N', '1110': 'O', '1111': 'P'})\n\n# Drop the original binary columns\ndf = df.drop(['A', 'B', 'C', 'D'], axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\ndef convert_to_list(row):\n    return [col for col in df.columns if row[col] == 1]\n\ndf['category'] = df.apply(convert_to_list, axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%Y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf['Date'] = df['Date'].apply(lambda x: x[:10])\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace(' ', '-'))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-', ' '))\ndf['Date'] = df['Date'].apply(lambda x: x.replace('-", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\n\nprint(df)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "\ndf['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(-1)\n\nprint(df)\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "\ndf['#1'] = df['#1'].shift(1, axis=1)\ndf['#2'] = df['#2'].shift(1, axis=1)\n\ndf = df.dropna()\n\nprint(df)\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted = df.shift(1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first = df.shift(1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last_first_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the first row of the first column shifted down 1 row\ndf_shifted_first_last_last_last = df.shift(-1).reset_index(drop=True)\n\n# Create a new dataframe with the last row of the first column shifted to the first row\ndf_shifted_last", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'X' for col in df.columns]\nprint(df)\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col for col in df.columns]\nprint(df)\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "\n# Create a list of column names\ncolumn_names = ['HeaderA', 'HeaderB', 'HeaderC', 'HeaderX']\n\n# Create a list of new column names\nnew_column_names = ['XHeader' + column for column in column_names]\n\n# Rename the columns\ndf.rename(columns=dict(zip(column_names, new_column_names)), inplace=True)\n\nprint(df)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "result = df.groupby('group').agg({col: 'mean' for col in df.columns if 'val' in col})\nresult", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group').agg({col: 'sum' for col in df.columns if 'val' in col})\nresult['group_color'] = df['group_color']\nresult", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndef aggregate_columns(df):\n    # Create a dictionary to store the aggregated values\n    aggregated_values = {}\n\n    # Iterate over the columns in the data frame\n    for col in df.columns:\n        # If the column ends with '2' or '42', aggregate it\n        if col.endswith('2') or col.endswith('42'):\n            aggregated_values[col] = df[col].agg(['first', 'sum', 'mean'])\n\n    # Create a new data frame with the aggregated values\n    new_df = pd.DataFrame(aggregated_values)\n\n    return new_df\n\n# Call the function and print the result\nresult = aggregate_columns(df)\nprint(result)\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "result = df.loc[row_list, column_list].mean(axis=1)\nresult", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "result = df.loc[row_list, column_list].sum()\nresult", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "result = df.loc[row_list, column_list].sum()\nresult = result.sort_values(ascending=False).drop_duplicates().sort_index()\nresult = result.sort_index()\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(ascending=False)\nresult = result.sort_values(", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.groupby(['id', 'temp', 'name']).size().unstack().fillna(0)\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "result = df.groupby(df.columns[0]).value_counts()\nresult.index.name = 'id'\nresult.columns.name = 'temp'\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['id', 'temp']).count()\nresult.columns = ['id', 'temp', 'count']\nresult = result.reset_index()\nresult.columns = ['id', 'temp', 'count']\nresult = result.groupby(['", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0, 1]].combine_first(df.iloc[[2, 3]])\nresult.columns = ['Sample type', 'Concentration', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\nresult", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0, 1]].combine_first(df.iloc[[2, 3]])\nresult.columns = ['Nanonose', 'Concentration', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\nresult", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "\ndef fill_nan(x):\n    x[x.notnull()] = 1\n    x[x.isnull()] = 2\n    return x\n\nresult = df.apply(fill_nan, axis=1)\nresult", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "\ndef replace_nan(x):\n    return x.replace(np.nan, x.mean())\n\ndf = df.apply(replace_nan, axis=0)\nresult = df\nresult", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "result = df.where(df.notnull(), df.mean())\nresult", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the sum of the value for each group\nresult = df.groupby('lab')['value'].sum().where(df['less_than_thresh'], 0)\n\n# Reset the index and drop the less_than_thresh column\nresult = result.reset_index().drop('less_than_thresh', axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.groupby('lab')['value'].mean()\n\n# Create a new column that indicates whether the value is less than the threshold\ndf['less_than_thresh'] = df['value'] < thresh\n\n# Group by the lab column and calculate the average of the value column for each group\nresult = df.", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\ndf['value'] = df['value'].rolling(window=section_right - section_left + 1).mean()\n\n# Create a new column with the average of the values in the section\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\nresult = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\nresult", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndef exp_column(df, prefix):\n    return df.assign(**{f'{prefix}_{col}': np.power(df[col], 1) for col in df.columns})\n\nresult = exp_column(df, 'exp')\nprint(result)\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\nresult = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])\nresult", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = df[col].apply(sigmoid)\n\nresult = df\nresult", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Get the index of the maximum value in each column\nmax_idx = df.idxmax()\n\n# Create a mask that is True for the maximum value in each column and False for the minimum value in each column\nmask = (max_idx == min_idx)\n\n# Apply the mask to the column of the maximum value\nresult = max_idx[mask]\n\nprint(result)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\n# Get the index of the minimum value in each column\nmin_idx = df.idxmin()\n\n# Create a mask where the maximum value is greater than the minimum value\nmask = df.apply(lambda x: x > x.min(), axis=1)\n\n# Apply the mask to the index of the minimum value\nresult = min_idx[mask]\n\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "\n# Create a new data frame with all dates and a zero value for the val column\nnew_df = pd.DataFrame({'dt': pd.date_range(start=df['dt'].min(), end=df['dt'].max()), 'val': 0})\n\n# Merge the new data frame with the original data frame\nresult = pd.merge(new_df, df, on=['dt', 'user'], how='left')\n\n# Fill the zero value for the val column\nresult['val'] = result['val'].fillna(0)\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "\n# Create a new data frame with all dates from the minimum to the maximum date\nnew_df = pd.DataFrame({'user': df['user'].unique(), 'dt': pd.date_range(start=df['dt'].min(), end=df['dt'].max())})\n\n# Merge the new data frame with the original data frame\nresult = pd.merge(new_df, df, on=['user', 'dt'], how='left')\n\n# Fill the val column with 0\nresult['val'] = result['val'].fillna(0)\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "\n# Create a new dataframe with all the dates and a value of 233\nnew_df = pd.DataFrame({'dt': pd.date_range(start=df['dt'].min(), end=df['dt'].max()), 'val': 233})\n\n# Merge the new dataframe with the original dataframe\nresult = pd.merge(new_df, df, on=['dt'], how='left')\n\n# Fill the NaN values with 0\nresult['val'] = result['val'].fillna(0)\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "\n# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all the dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Join the original data frame with the new data frame on the 'user' column\nresult = pd.merge(new_df, df, on='user')\n\n# Fill in the maximum val of the user for the val column\nresult['val'] = result.groupby('user')['val'].transform(max)\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "\n# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a date range from the minimum to the maximum\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# Create a new data frame with the date range and the maximum value for each user\nresult = pd.DataFrame({'dt': date_range, 'user': df['user'], 'val': df['val'].max()})\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Convert the dt column to string\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "result = df.groupby('name').apply(lambda x: x.assign(name=x.index + 1)).reset_index(drop=True)\nresult", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "\n# Create a unique ID for each name\nunique_ids = df['name'].unique()\n\n# Create a dictionary to map the unique ID to the unique ID for each name\nid_map = dict(zip(unique_ids, range(1, len(unique_ids) + 1)))\n\n# Replace the a column with the unique ID for each name\ndf['a'] = df['name'].map(id_map)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    df['name'] = df['name'].astype('category').cat.codes\n    df['name'] += 1\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "\n# Create a new column 'ID' with a unique ID for each name\ndf['ID'] = df['name'].factorize()[0]\n\n# Replace the name with the ID\ndf['name'] = df['ID']\n\n# Create a new column 'ID' with a unique ID for each name\ndf['ID'] = df['name'].factorize()[0]\n\n# Replace the name with the ID\ndf['name'] = df['ID']\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.pivot_table(index='user', columns='date', values='value', fill_value=0)\ndf['date'] = df['date'].apply(lambda x: x.strftime('%m/%d/%Y'))\ndf = df.reset_index()\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'user': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf = df.rename(columns={'value': 'date'})\ndf = df.rename(columns={'date': 'value'})\ndf =", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'01/12/15': 'others', 'value': 'value'})\ndf = df.drop(columns='02/12/15')\ndf = df.rename(columns={'user': 'user', 'value': 'value'})\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.pivot_table(index='user', columns='date', values='value', fill_value=None)\ndf = df.reset_index()\ndf.columns = ['user', 'date', 'value']\ndf['date'] = df['date'].apply(lambda x: pd.to_datetime(x))\ndf['someBool'] = df['someBool'].apply(lambda x: x == True)\ndf", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df.c > 0.5][columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df.c > 0.45][columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": " ###\n    return df[df['c'] > 0.5][columns]\n\n    ### END SOLUTION ###\n\n# create a dataframe\ndf = pd.DataFrame(np.random.rand(4, 5), columns=list('abcde'))\n\n# print the original dataframe\nprint(df)\n\n# print the result\nprint(f(df, columns=['b', 'e']))\n\n# convert the result to a numpy array\nresult = f(df, columns=['b', 'e']).to_numpy()\nprint(result)\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": " ###\n    df_subset = df[df['c'] > 0.5][columns]\n    df_subset['sum'] = df_subset[columns[0]] + df_subset[columns[1]]\n    return df_subset\n\n# test the function\ndf = pd.DataFrame({'a': [0.945686, 0.919359, 0.976163, 0.459039],\n                   'b': [0.000710, 0.667057, 0.621712, 0.788318],\n                   'c': [0.909158, 0.462478, 0.208423, 0.309892],\n                   'd': [0.892892, 0.008204, 0.980471, 0.100539],\n                   'e': [0.473096, 0.473096, 0.048334, 0.753992]})\n\nprint(f(df, columns=['b', 'e']))\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": " ###\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION ###\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint(f(df))\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndef get_dates(row):\n    return [row['date'] + pd.DateOffset(days=i) for i in range(1, X)]\n\nresult = df[~df['date'].isin(df.apply(get_dates, axis=1))]\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef get_dates(row, X):\n    dates = []\n    for i in range(X):\n        dates.append(row['date'] + pd.DateOffset(weeks=i))\n    return dates\n\nresult = df[~df.apply(get_dates, X=X, axis=1)]\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef get_dates(row, X):\n    dates = []\n    for i in range(X):\n        dates.append(row['date'] + pd.DateOffset(weeks=i))\n    return dates\n\nresult = df[~df['date'].isin(get_dates(df.iloc[0], X))]\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\nresult = df.groupby(df.index // 3).mean()\nresult['col1'] = result['col1'].round(1)\nresult", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\nresult = df.groupby(df.index // 3).mean()\nresult['col1'] = result['col1'].astype(int)\nresult", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n# Create a new column with the bin number\ndf['bin'] = (df['col1'] // 4).astype(int)\n\n# Create a new column with the bin number\ndf['bin_num'] = (df['col1'] % 4).astype(int)\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the bin number\ndf['bin_num'] = df['bin_num'].replace([0, 1, 2, 3], [1, 2, 3, 4])\n\n# Create a new column with the", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\nresult = df.groupby(df.index // 3).mean()\nresult['col1'] = result['col1'].round(2)\nresult", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_first'] = df.loc[0:1, 'col1'].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df.loc[-3:, 'col1'].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df.loc[-2:, 'col1'].mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_first'] = df.loc[0:2, 'col1'].sum()\n\n# Create", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_next'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_next'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_next_last'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_next_last'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_next'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_next'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_next_last_last'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_next_last_last'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_next_last'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_next_last'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_next_last_last_last'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_next_last_last_last'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_next_last_last'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_next_last_last'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_next_last_last_last_last'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_next_last_last_last_last'] = df['col1'].head(2).mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1_sum_last_next_last_last_last'] = df['col1'].tail(3).sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1_avg_last_next_last_last_last'] = df['col1'].tail(2).mean()\n\n# Create a new column with the sum of the first 3 rows\ndf['col1_sum_next_last_last_last_last'] = df['col1'].head(3).sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1_avg_next_last_last_last_last'] = df['col1'].head(2).mean()\n\n# Create a new column with the", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df['A'] = df['A'].replace(0, np.nan)\ndf['A'] = df['A'].fillna(method='ffill')\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df['A'] = df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A'].replace(0, df['A", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\ndf['A'] = df['A'].replace(0, np.nan)\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).max())\ndf['A'] = df['A'].replace(np.nan, df['A'].replace(0, np.nan).rolling(window=2).", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_days'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\ndf", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\ndf['time_day'] = df['time_day'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\ndf", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    df['time'] = df['duration'].str.split(' ').str[0]\n    df['number'] = df['duration'].str.split(' ').str[1]\n    df['time_days'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number'].astype(int)\nprint(df)\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.all(df1[columns_check_list] == df2[columns_check_list], axis=1)", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = [all(df1[column] == df2[column]) for column in columns_check_list]\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    # Convert the date column to datetime and set it as the index\n    df['date'] = pd.to_datetime(df['date'])\n    # Set the date as the index\n    df.set_index('date', inplace=True)\n    # Create a numpy array of date, x and y\n    output = df.values\n    ### END SOLUTION ###\n    return output\n\n# Test the function\ndf = pd.DataFrame({'x': [100, 90, 80], 'y': [7, 8, 9], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index(['id', 'date'])\n    df = df.swaplevel(0, 1)\n    return df\n    ### END SOLUTION ###\n\n# test the function\ndf = pd.DataFrame({'x': ['abc'], 'y': [100], 'date': ['3/1/1994', '9/1/1994', '3/1/1995']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot(index='Country', columns='year', values='value')\ndf = df.reset_index()\ndf.columns = ['Country', 'year', 'var1', 'var2']\ndf = df[['Country', 'year', 'var1', 'var2']]\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='Variable')\ndf = df.reset_index()\ndf = df.rename(columns={'Variable': 'var1', 'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var1')\ndf = df.reset_index()\ndf = df.rename(columns={'var1': 'var2'})\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot(index='Country', columns='year', values='var2')\ndf = df.reset_index()\ndf = df.rename(columns={'var2': 'var1'})\ndf = df.sort_values(['Country', 'year'], ascending=", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\n# Create a list of column names that start with 'Value'\ncolumn_names = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new dataframe with only the rows where all values are less than 1\ndf_filtered = df", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\ndf = df.abs()\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > 1, 0)\ndf = df.where(df > ", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a list of column names that start with 'Value'\ncolumn_names = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new dataframe with the columns that have a value greater than 1\nnew_df = df[column_names].apply(lambda x: x[x > 1])\n\n# Remove the 'Value_' from the column names\nnew_df.columns = [col.replace('Value_', '') for col in new_df.columns]\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df['A'] = df['A'].str.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    df['A'] = df['A'].str.replace('&AMP;', '&')\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\ndf['A'] = df['A'].str.replace('&LT;', '<')\ndf['A'] = df['A'].str.replace('&GT;', '>')\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df['A'] = df['A'].replace('&AMP;', '&')\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "\ndef validate_single_space_name(name: str) -> str:\n    if ' ' in name:\n        return name.split(' ')[0]\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: None if ' ' not in x else x.split(' ')[1])\n\ndf = df[['first_name', 'last_name']]\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x if validate_single_space_name(x) else None)\n\ndf", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^(.*) (.*) (.*)$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return match_obj.groups()\n    else:\n        return None\n\ndf['first_name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(validate_single_space_name))\n\ndf = df[['first_name', 'middle_name', 'last_name']]\n\nprint(df)\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = df2.merge(df1, on='Timestamp', how='left')\nresult['data'] = result['data'].fillna(result['stuff'])\nresult = result.drop(['stuff'], axis=1)\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge_asof(df1['Timestamp'], df2['Timestamp'], direction='forward')\nresult['data'] = df2['stuff']\nresult = result.drop(['Timestamp_x', 'Timestamp_y'], axis=1)\nresult = result.rename(columns={'Timestamp_x': 'Timestamp'})\nresult = result.sort_values('Timestamp')\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 else row['col1'] if row['col3'] <= 50 else row['col1'], axis=1)\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\nerrors = []\nfor value in df['Field1']:\n    if not isinstance(value, int):\n        errors.append(value)\n\nprint(errors)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndef get_integers(row):\n    return [int(x) for x in row if isinstance(x, int)]\n\ndf[\"Field1\"] = df[\"Field1\"].apply(get_integers)\n\nprint(df[\"Field1\"].values.tolist())\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    result = []\n    for index, value in df['Field1'].iteritems():\n        if not isinstance(value, int):\n            result.append(value)\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "\ndf['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n\ndf", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)\nresult", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\n# Create a new column 'distance' that calculates the euclidean distance between each car and its nearest neighbour\ndf['distance'] = np.sqrt((df['x'] - df['x'].groupby(df['time']).transform('mean'))**2 + (df['y'] - df['y'].groupby(df['time']).transform('mean'))**2)\n\n# Create a new column 'nearest_neighbour' that finds the nearest neighbour for each car\ndf['nearest_neighbour'] = df.groupby('time')['distance'].transform('min')\n\n# Create a new column 'nearest_neighbour_distance' that finds the distance to the nearest neighbour\ndf['nearest_neighbour_distance'] = df['distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car' that finds the car of the nearest neighbour\ndf['nearest_neighbour_car'] = df.groupby('time')['nearest_neighbour'].transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neighbour_car_distance' that finds the distance to the nearest neighbour car\ndf['nearest_neighbour_car_distance'] = df['nearest_neighbour_car_distance'].groupby(df['time']).transform('min')\n\n# Create a new column 'nearest_neigh", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\n# Create a new column 'farmost_neighbour' that contains the farthest car for each time\ndf['farmost_neighbour'] = df.groupby('time')['car'].transform('max')\n\n# Create a new column 'euclidean_distance' that contains the euclidean distance between each car and its farthest neighbour\ndf['euclidean_distance'] = df.groupby('time')['car'].transform(lambda x: (x - df.loc[df['time'] == x.max(), 'car'].values[0])**2)\n\n# Calculate the average of the euclidean distances for each time\ndf['euclidean_distance'] = df.groupby('time')['euclidean_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group' that groups the data by time\ndf['time_group'] = df['time'].groupby(df['time']).transform(lambda x: x.index[0])\n\n# Create a new column 'time_group_neighbour' that contains the farthest neighbour for each time\ndf['time_group_neighbour'] = df.groupby('time_group')['car'].transform('max')\n\n# Create a new column 'time_group_distance' that contains the euclidean distance between each time and its farthest neighbour\ndf['time_group_distance'] = df.groupby('time_group')['euclidean_distance'].transform(lambda x: x.mean())\n\n# Calculate the average of the euclidean distances for each time group\ndf['time_group_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_group_neighbour_distance' that contains the euclidean distance between each time group and its farthest neighbour\ndf['time_group_neighbour_distance'] = df.groupby('time_group')['time_group_distance'].transform(lambda x: x.mean())\n\n# Create a new column 'time_", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: ', '.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.flatten()), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "\ndef concat_keywords(df):\n    df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.flatten()), axis=1)\n    return df\n\ndf = concat_keywords(df)\nprint(df)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "\ndef concat_keywords(df):\n    df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().values.tolist()), axis=1)\n    return df\n\ndf = concat_keywords(df)\nprint(df)\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Set Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index[~df.index.isin(sample_df.index)]\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\n# Select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Change value of ProductId to zero\nsample_df['ProductId'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = df.index[~df.index.isin(sample_df.index)]\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\n# Create a new column with the number of rows for each user\ndf['num_rows'] = df.groupby('UserId').transform('count')\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number of rows to select\ndf['num_to_select'] = df['num_rows'] * 0.2\n\n# Create a new column with the number", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\nresult", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "\n# Create a new column with the index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep='last').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1','col2'], keep='first').astype(int)\n\n# Create a new column with the index of the first occurrence of each row\ndf['index_first'] = df.duplicated(subset=['col1','col2'], keep=False).astype(int)\n\n# Create a new column with the index of the last occurrence of each row\ndf['index_last'] = df.duplicated(subset=['col1", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    # create a new column 'index_original'\n    df['index_original'] = df.index\n    # find the duplicates\n    duplicates = df.duplicated(subset=['col1','col2'], keep='first')\n    # return the duplicates\n    return df.loc[duplicates == True]\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\nresult", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "\n# Create a new column 'index_original' that contains the original index of the last occurrence of each row\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\n\n# Create a new column 'duplicate' that is True if the row is a duplicate and False otherwise\ndf['duplicate'] = df.duplicated(subset=['col1','col2'], keep='last')\n\n# Select the rows where duplicate is True\nresult = df.loc[df['duplicate'] == True]\n\n# Drop the 'duplicate' and 'index_original' columns\nresult = result.drop(['duplicate', 'index_original'], axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\n# Create a new column that contains the maximum count for each group\ndf['max_count'] = df.groupby(['Sp', 'Mt'])['count'].transform('max')\n\n# Create a new column that contains the rows where count equals the maximum count\nresult = df[df['count'] == df['max_count']]\n\n# Drop the max_count column\nresult = result.drop(columns=['max_count'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "\n# Create a new column 'max_count' which holds the maximum count for each group\ndf['max_count'] = df.groupby(['Sp','Mt'])['count'].transform('max')\n\n# Create a new column 'max_count_rows' which holds the rows with the maximum count for each group\nresult = df[df['count'] == df['max_count']]\n\n# Drop the 'max_count' column\nresult = result.drop(['max_count'], axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\n# Create a group by object\ngroup = df.groupby(['Sp', 'Mt'])\n\n# Find the minimum count for each group\nmin_count = group['count'].min()\n\n# Find the rows where count equals the minimum count\nresult = group[group['count'] == min_count].reset_index()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\n# Create a group by object\ngroup = df.groupby(['Sp','Value'])\n\n# Find the max count in each group\nmax_count = group['count'].max()\n\n# Find the rows where count equals max\nresult = group[group['count'] == max_count].reset_index()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category!=@filter_list\")", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "\ndef create_melt_columns(df):\n    columns = df.columns.tolist()\n    result = pd.DataFrame()\n    for i in range(len(columns)):\n        for j in range(i, len(columns)):\n            temp = df[columns[i]].rename(columns={columns[i]: columns[i]})\n            temp = temp.join(df[columns[j]].rename(columns={columns[j]: columns[j]}))\n            result = result.append(temp)\n    return result\n\nresult = create_melt_columns(df)\nprint(result)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "result = pd.melt(df, value_vars=[('col1', 'col2', 'col3'), ('col1', 'col2', 'col4'), ('col1', 'col2', 'col5'), ('col1', 'col2', 'col6')], var_name='variable_0', value_name='value')\nresult['variable_1'] = df.columns[0]\nresult['variable_2'] = df.columns[1]\nresult", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] < 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != np.nan else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: x.sum() if x.sum() != np.nan else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l')['v'].apply(lambda x: x.sum() if x.sum() != np.nan else np.nan)\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship(column1, column2):\n    if column1.nunique() == 1:\n        return 'one-to-one'\n    elif column2.nunique() == 1:\n        return 'one-to-many'\n    elif column1.nunique() == 2 and column2.nunique() == 2:\n        return 'many-to-many'\n    else:\n        return 'many-to-one'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {relationship(df[column1], df[column2])}')\n\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship(column1, column2):\n    if len(column1) == len(column2) and len(column1) == len(column1[column1 == column2]):\n        return 'one-2-one'\n    elif len(column1) == len(column2) and len(column1) == len(column1[column1 != column2]):\n        return 'one-2-many'\n    elif len(column1) == len(column2) and len(column1) == len(column1[column1 == column2]):\n        return 'many-2-one'\n    elif len(column1) == len(column2) and len(column1) == len(column1[column1 != column2]):\n        return 'many-2-many'\n    else:\n        return 'unknown'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {relationship(df[column1], df[column2])}')\n\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "\ndef relationship(x, y):\n    if x.max() == y.min() and x.min() == y.max():\n        return 'one-to-one'\n    elif x.max() == y.min() and x.min() != y.max():\n        return 'one-to-many'\n    elif x.min() == y.max() and x.max() != y.min():\n        return 'many-to-one'\n    elif x.min() == y.max() and x.max() == y.min():\n        return 'many-to-many'\n    else:\n        return 'none'\n\nresult = df.apply(lambda x: relationship(x, df[x.name]), axis=1)\nresult", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship(x, y):\n    if x.shape[0] == 1:\n        return 'one-2-one'\n    elif x.shape[0] == y.shape[0]:\n        return 'one-2-many'\n    elif y.shape[0] == 1:\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\nfor c1 in df.columns:\n    for c2 in df.columns:\n        result.loc[c1, c2] = relationship(df[c1], df[c2])\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "\n# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n\n# remove the dupes that don't have an bank account\nresult = dfiban_uniq[dfiban_uniq['bank'].notna()]\n\nprint(result)\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\ndef hello_world():\n    print(\"Hello, World!\")\n\nhello_world()\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "\n# Create a new column that indicates whether the passenger has a family or not\ndf['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\n\n# Group by the 'Family' column and calculate the mean of the 'Survived' column\nresult = df.groupby('Family')['Survived'].mean()\n\nprint(result)\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "\n# Create a new column that indicates whether the passenger has a family or not\ndf['HasFamily'] = (df['Survived'] > 0) | (df['Parch'] > 0)\n\n# Create a new column that indicates whether the passenger has a family or not\ndf['NoFamily'] = (df['Survived'] == 0) & (df['Parch'] == 0)\n\n# Calculate the mean of the two groups\nresult = df.groupby(['HasFamily', 'NoFamily'])['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndef group_condition(row):\n    if row['SibSp'] == 1 and row['Parch'] == 1:\n        return 'Has Family'\n    elif row['SibSp'] == 0 and row['Parch'] == 0:\n        return 'No Family'\n    elif row['SibSp'] == 0 and row['Parch'] == 1:\n        return 'New Family'\n    elif row['SibSp'] == 1 and row['Parch'] == 0:\n        return 'Old Family'\n    else:\n        return 'Unknown'\n\ndf['Group'] = df.apply(group_condition, axis=1)\nresult = df.groupby('Group')['Survived'].mean()\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\nresult", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "\ndef std_mean(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('a').apply(std_mean)\nresult.columns = ['mean', 'std']\nresult", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "\ndef std_mean(x):\n    return np.std(np.mean(x))\n\nresult = pd.DataFrame({'mean': df.groupby('b')['a'].apply(std_mean), 'std': np.repeat(np.std(df.groupby('b')['a']), df.groupby('b').size())})\nresult", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = df.groupby('a')['b'].apply(softmax)\ndf['min-max'] = df.groupby('a')['b'].apply(min_max)\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "result = df.loc[:, (df.sum() != 0)]\nresult = result.loc[(result.sum(axis=1) != 0), :]\nresult = result.loc[(result.sum(axis=0) != 0), :]\nresult", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "result = df[df.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\nresult", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "\n# Create a mask to select the rows and columns with maximum value 2\nmask = (df == 2)\n\n# Apply the mask to the dataframe\nresult = df.where(mask, 0)\n\n# Remove the rows and columns with maximum value 2\nresult = result.drop(result.index[result.max().index[0]], axis=0)\nresult = result.drop(result.columns[result.max().index[1]], axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "result = df.applymap(lambda x: x if x==2 else 0)\nresult", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "result = s.sort_values(ascending=[True, False])\nresult", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.reset_index().sort_values(['index', 0]).reset_index(drop=True)\ndf['index'] = df['index'].astype(str)\ndf['index'] = df['index'].str.replace(' ', '')\ndf['index'] = df['index'].str.replace('tf', '')\ndf['index'] = df['index'].str.replace('p', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('!', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].str.replace('.', '')\ndf['index'] = df['index'].", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str) == df['A']]\nresult", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\n# Create a new column that contains the maximum count for each group\ndf['max_count'] = df.groupby(['Sp', 'Mt'])['count'].transform('max')\n\n# Create a new column that contains the rows where count equals the maximum count\nresult = df[df['count'] == df['max_count']]\n\n# Drop the max_count column\nresult = result.drop(columns=['max_count'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "\n# Create a new column 'max_count' which holds the maximum count for each group\ndf['max_count'] = df.groupby(['Sp','Mt'])['count'].transform('max')\n\n# Create a new column 'max_value' which holds the maximum value for each group\ndf['max_value'] = df.groupby(['Sp','Mt'])['Value'].transform('max')\n\n# Create a new DataFrame with the rows where count is maximum for each group\nresult = df[df['count'] == df['max_count']]\n\n# Drop the max_count and max_value columns\nresult = result.drop(['max_count', 'max_value'], axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\n# Create a group by object\ngroup = df.groupby(['Sp', 'Mt'])\n\n# Find the minimum count for each group\nmin_count = group['count'].min()\n\n# Find the rows where count equals the minimum count\nresult = group[group['count'] == min_count].reset_index()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\n# Create a group by object\ngroup = df.groupby(['Sp','Value'])\n\n# Find the max count in each group\nmax_count = group['count'].max()\n\n# Find the rows where count equals max\nresult = group[group['count'] == max_count].reset_index()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(np.datetime64('17/8/1926'))\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": " ###\n    df['Date'] = df['Member'].map(dict)\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna(pd.to_datetime('17/8/1926'))\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\nprint(df)\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Year', 'Month'])['Date'].transform('count')\ndf['Count_m'] = df.groupby(['Date', 'Year', 'Month'])['Date'].transform('count')\ndf['Count_y'] = df.groupby(['Year', 'Month'])['Date'].transform('count')\ndf", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date','Val']).Val.transform('count')\ndf", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date']).size()\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size()\ndf['Count_y'] = df.groupby([df['Date'].dt.year]).size()\ndf['Count_w'] = df.groupby([df['Date'].dt.weekday]).size()\ndf['Count_Val'] = df.groupby(['Date', 'Val']).size()\nprint(df)\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date')['B'].apply(lambda x: x.sum() if x.sum() == 0 else x.sum()).reset_index()\nresult2 = df.groupby('Date')['B'].apply(lambda x: x.sum() if x.sum() == 0 else x.sum()).reset_index()\nresult1['B'] = result1['B'].apply(lambda x: x if x == 0 else 1)\nresult2['B'] = result2['B'].apply(lambda x: x if x == 0 else 1)\nresult1['C'] = result1['C'].apply(lambda x: x if x == 0 else 1)\nresult2['C'] = result2['C'].apply(lambda x: x if x == 0 else 1)\nresult1 = result1.rename(columns={'B': 'zero'})\nresult2 = result2.rename(columns={'B': 'non-zero'})\nresult1['Date'] = pd.to_datetime(result1['Date'])\nresult2['Date'] = pd.to_datetime(result2['Date'])\nresult1\nresult2", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "\n# Create a new column that indicates whether the value is even or odd\ndf['Even'] = df['B'].apply(lambda x: x % 2 == 0)\n\n# Count the number of even and odd values for each date\nresult1 = df.groupby('Date')['Even'].value_count()\n\n# Create a new column that indicates whether the value is odd or even\ndf['Odd'] = df['B'].apply(lambda x: x % 2 != 0)\n\n# Count the number of odd and even values for each date\nresult2 = df.groupby('Date')['Odd'].value_count()\n\nprint(result1)\nprint(result2)\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=np.mean)\nresult['E'] = pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=np.mean)\nresult", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['mean'] = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nresult", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=np.mean)\nresult['E'] = pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=np.mean)\nresult", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=1)\n\nresult = df.assign(var1=lambda df: df['var1'].str.split(',').str[0]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n\nresult = result.assign(var2=lambda df: df['var2'].str.split(',').str[1:]).compute()\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=1)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split(','))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.tolist()))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply(lambda x: x.reshape(-1, 1)))\n\nresult = result.assign(var2=lambda df: df['var2'].apply", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(df, npartitions=1)\n\nresult = df.assign(var2=lambda df: df['var2'].str.split('-')).explode('var2').compute()\n\nresult = result.reset_index(drop=True)\n\nprint(result)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if string[i] == ' ':\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf['fips'] = df['fips'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf['fips'] = df['fips'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf['row'] = df['row'].str.split(' ', 1, expand=True)[1]\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df['row'].str.strip()\ndf['row'] = df", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndf = df.set_index('Name').replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.cumsum()\ndf = df.divide(df.sum(axis=1), axis=0)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.replace(0, 1)\ndf = df.", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndef calc_cumulative_average(df):\n    df = df.set_index('Name')\n    df = df.replace(0, 1)\n    df = df.cumsum()\n    df = df / df.sum(axis=1)[:, None]\n    df = df.replace(0, 1)\n    return df\n\ndf = calc_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    df = df.set_index('Name')\n    df = df.replace(0, 1)\n    df = df.divide(df.sum(axis=1), axis=0)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n    df = df.replace(0, 1)\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndef calc_cumulative_average(df):\n    for col in df.columns[1:]:\n        df[col] = df[col].replace(0, 1)\n        df[col] = df[col].cumsum() / df[col].sum()\n    return df\n\ndf = calc_cumulative_average(df)\nprint(df)\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)) > 0\ndf['label'] = df['label'].replace({False: 0, True: 1})\ndf['label'] = df['label'].replace(True, 1)\ndf['label'] = df['label'].replace(False, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf, 0)\ndf['label'] = df['label'].replace(-np.inf, 0)\ndf['label'] = df['label'].replace(np.nan, 0)\ndf['label'] = df['label'].replace(np.inf,", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['label'] = df['label'].replace(0, 1)\n\nprint(df)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = df.departure_time.subtract(df.arrival_time)\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = pd.to_timedelta(df['Duration'])\ndf['Duration'] = df['Duration'].apply(lambda x: x.days*24*60*60 + x.seconds)\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(':')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split('.')[0])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\ndf['Duration'] = df['Duration'].apply", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = df.departure_time.sub(df.arrival_time)\ndf['Duration'] = df['Duration'].apply(lambda x: x.seconds)\nprint(df)\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df['departure_time'] - df['arrival_time']\ndf['Duration'] = df['Duration'].apply(lambda x: x.seconds)\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size()\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df.groupby('key1')['key2'].apply(lambda x: x.endswith('e')).sum()\nresult", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.index[df.value.argmax()]\nmin_result = df.index[df.value.argmin()]\nprint(max_result, min_result)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.mode(axis=0)[0][0]\nmedian_result = df.median()[0]\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df[~(df['closing_price'].between(99, 101))]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "result = df.groupby(\"item\")[\"diff\"].min()\nresult = result.reset_index()\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff\"].min()]\nresult = result[result[\"otherstuff\"] == result[\"otherstuff\"].min()]\nresult = result[result[\"diff\"] == result[\"diff", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\nprint(df)\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df['SOURCE_NAME']\n\nprint(f())\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "\n# Create a mask for the first 50% of NaN values\nmask = np.isnan(df['Column_x'])\n\n# Create a mask for the last 50% of NaN values\nmask |= np.isnan(df['Column_x'][::-1])\n\n# Apply the mask to the column\ndf['Column_x'][mask] = 1\ndf['Column_x'][~mask] = 0\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values in the column\nnan_count = df['Column_x'].isnull().sum()\n\n# Calculate the number of NaN values to be filled\nnan_to_fill = nan_count * 0.3\n\n# Calculate the number of NaN values to be filled in the first 30% of the column\nnan_to_fill_first = nan_to_fill * 0.3\n\n# Calculate the number of NaN values to be filled in the middle 30% of the column\nnan_to_fill_middle = nan_to_fill * 0.7\n\n# Calculate the number of NaN values to be filled in the last 30% of the column\nnan_to_fill_last = nan_to_fill * 0.7\n\n# Create a new column with the values to be filled\ndf['Column_x_filled'] = df['Column_x'].copy()\n\n# Fill the NaN values in the first 30% of the column with '0'\ndf['Column_x_filled'].loc[:nan_to_fill_first] = 0\n\n# Fill the NaN values in the middle 30% of the column with '0.5'\ndf['Column_x_filled'].loc[nan_to_fill_first:nan_to_fill_last] = 0.5\n\n# Fill the NaN values in the last 30% of the column with '1'\ndf['Column_x_filled'].loc[nan_to_fill_last:] = 1\n\n# Print the new column\nprint(df['Column_x_filled'])\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "\n# Create a mask for the NaN values\nmask = df['Column_x'].isna()\n\n# Create a binary array of 0 and 1\nbinary = np.array([0, 1])\n\n# Create a random array of 0 and 1\nrandom_array = np.random.choice(binary, size=(len(mask), 2))\n\n# Create a new column with the random array\ndf['Column_x'] = np.where(mask, random_array, df['Column_x'])\n\n# Print the new column\nprint(df['Column_x'])\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "result = pd.DataFrame([(a.iloc[0, 0], b.iloc[0, 0]), (a.iloc[1, 0], b.iloc[1, 0])], columns=['one', 'two'])\nresult", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "result = pd.DataFrame([(a.iloc[0, 0], b.iloc[0, 0], c.iloc[0, 0]), (a.iloc[1, 0], b.iloc[1, 0], c.iloc[1, 0])], columns=['one', 'two'])\nresult", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\n# Create a new dataframe with the same shape as a and b\nresult = pd.DataFrame(np.nan, index=a.index, columns=a.columns)\n\n# Iterate over the rows of a and b\nfor i in range(a.shape[0]):\n    # Iterate over the columns of a and b\n    for j in range(a.shape[1]):\n        # Set the value of the current row and column to a and b\n        result.loc[i, j] = (a.loc[i, j], b.loc[i, j])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "result = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack().fillna(0)\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "\nresult = df.assign(text=lambda x: ', '.join(x['text']))\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "\nresult = df['text'].apply(lambda x: '-'.join(x.split('-'))).values.reshape(-1, 1)\nresult = pd.DataFrame(result, columns=['text'])\nresult['text'] = result['text'].str.strip()\nresult", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', '))).reset_index(drop=True)\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].values.reshape(-1, 1).tolist()\nresult = [', '.join(result)]\nresult = pd.DataFrame(result, columns=['text'])\nresult", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].apply(lambda x: x.replace('-', ''))\nresult = result.reset_index(drop=True)\nresult = result.rename(index={0: 'jkl-ghi-def-abc'})\nresult", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.concat([df1, df2], axis=0)\nresult['city'] = result['city'].replace({'bj': 'bj', 'sh': 'sh'})\nresult['district'] = result['district'].replace({'ft': 'ft', 'hp': 'hp'})\nresult = result.sort_values(['id', 'date'])\nresult = result.groupby(['id', 'city', 'district', 'date']).first().reset_index()\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n# Create a new column 'date_formatted' with the date in the format '01-Jan-2019'\ndf1['date_formatted'] = df1['date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n\n# Merge df1 and df2 on 'id' and 'date_formatted'\nresult = pd.merge(df1, df2, on=['id', 'date_formatted'], how='left')\n\n# Fill the NaN values with 0\nresult = result.fillna(0)\n\n# Reset the index of the result\nresult = result.reset_index(drop=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "result = pd.concat([df1, df2], axis=0)\nresult = result.groupby(['id', 'city', 'district', 'date']).first().reset_index()\nresult", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['B_x'] = result['B_x'].fillna(result['B_y'])\nresult['B_y'] = result['B_y'].fillna(result['B_x'])\nresult = result[['A', 'B_x', 'B_y']]\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['B_x'] = result['B_x'].fillna(result['B_y'])\nresult['B_y'] = result['B_y'].fillna(result['B_x'])\nresult = result[['A', 'B_x', 'B_y']]\nresult", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult['dulplicated'] = result['A'].apply(lambda x: x in D['A'].values)\nresult", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).tolist()\nresult = [sorted([time, amount], key=lambda x: x[0]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[1]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[2]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[3]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[4]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[5]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[6]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[7]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[8]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[9]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[10]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[11]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[12]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[13]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[14]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[15]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[16]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[17]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[18]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[19]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[20]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[21]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[22]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[23]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[24]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[25]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[26]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[27]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[28]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[29]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[30]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[31]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[32]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[33]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[34]) for time, amount in result]\nresult = [sorted([time, amount], key=lambda x: x[3", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "\ndef sort_tuple(tuple_list):\n    return sorted(tuple_list, key=lambda x: x[1])\n\nresult = df.groupby('user').apply(lambda x: sort_tuple(x[['time', 'amount']].values.tolist()))\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'time': 'time-tuple', 'amount': 'amount-tuple'})\nresult = result[['user', 'time-tuple', 'amount-tuple']]\nresult\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist()).tolist()\nresult = [[(time, amount)] for time, amount in result]\nresult = pd.DataFrame(result, columns=['time-amount-tuple'])\nresult = result.sort_values(by='time-amount-tuple')\nresult = result.drop(columns='time-amount-tuple')\nresult = result.reset_index(drop=True)\nresult\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\n# Create a new dataframe with the index as the file names and the values as the numpy arrays\ndf = pd.DataFrame(series.values(), index=series.index, columns=series.index)\n\n# Reset the index to get the desired output\ndf = df.reset_index()\n\n# Rename the columns to get the desired output\ndf = df.rename(columns=lambda x: x)\n\n# Print the dataframe\nprint(df)\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\n# Create a dataframe from the series\ndf = pd.DataFrame(series.values, index=series.index, columns=series.index)\n\n# Reset the index\ndf = df.reset_index()\n\n# Rename the columns\ndf.columns = ['name', 'value']\n\n# Reset the index\ndf = df.reset_index()\n\n# Rename the columns\ndf.columns = ['name', '0', '1', '2', '3']\n\n# Print the dataframe\nprint(df)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.columns[df.columns.str.contains(s)].to_string(index=False)\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "result = df.columns[df.columns.str.contains(s)]\nresult = result.to_frame().transpose()\nresult.columns = ['spike' + str(i) for i in range(1, len(result.columns) + 1)]\nresult", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# Create a new dataframe with the first column as the index\ndf_new = df.set_index(0)\n\n# Create a new dataframe with the second column as the index\ndf_new = df_new.set_index(1)\n\n# Create a new dataframe with the third column as the index\ndf_new = df_new.set_index(2)\n\n# Create a new dataframe with the fourth column as the index\ndf_new = df_new.set_index(3)\n\n# Create a new dataframe with the fifth column as the index\ndf_new = df_new.set_index(4)\n\n# Create a new dataframe with the sixth column as the index\ndf_new = df_new.set_index(5)\n\n# Create a new dataframe with the seventh column as the index\ndf_new = df_new.set_index(6)\n\n# Create a new dataframe with the eighth column as the index\ndf_new = df_new.set_index(7)\n\n# Create a new dataframe with the ninth column as the index\ndf_new = df_new.set_index(8)\n\n# Create a new dataframe with the tenth column as the index\ndf_new = df_new.set_index(9)\n\n# Create a new dataframe with the eleventh column as the index\ndf_new = df_new.set_index(10)\n\n# Create a new dataframe with the twelfth column as the index\ndf_new = df_new.set_index(11)\n\n# Create a new dataframe with the thirteenth column as the index\ndf_new = df_new.set_index(12)\n\n# Create a new dataframe with the fourteenth column as the index\ndf_new = df_new.set_index(13)\n\n# Create a new dataframe with the fifteenth column as the index\ndf_new = df_new.set_index(14)\n\n# Create a new dataframe with the sixteenth column as the index\ndf_new = df_new.set_index(15)\n\n# Create a new dataframe with the seventeenth column as the index\ndf_new = df_new.set_index(16)\n\n# Create a new dataframe with the eightteenth column as the index\ndf_new = df_new.set_index(17)\n\n# Create a new dataframe with the nineteenth column as the index\ndf_new = df_new.set_index(18)\n\n# Create a new dataframe with the twentieth column as the index\ndf_new = df_new.set_index(19)\n\n# Create a new dataframe with the twenty-first column as the index\ndf_new = df_new.set_index(20)\n\n# Create a new dataframe with the twenty-second column as the index\ndf_new = df_new.set_index(21)\n\n# Create a new dataframe with the twenty-third column as the index\ndf_new = df_new.set_index(22)\n\n# Create a new dataframe with the twenty-fourth column as the index\ndf_new = df_new.set_index(23)\n\n# Create a new dataframe with the twenty-sixth column as the index\ndf_new = df_new.set_index(24)\n\n# Create a new dataframe with the twenty-eighth column as the index\ndf_new = df_new.set_index(25)\n\n# Create a new dataframe with the twenty-ninth column as the index\ndf_new = df_new.set_index(26)\n\n# Create a new dataframe with the thirty-first column as the index\ndf_new = df_new.set_index(27)\n\n# Create a new dataframe with the thirty-second column as the index\ndf_new = df_new.set_index(28)\n\n# Create a new dataframe with the thirty-third column as the index\ndf_new = df_new.set_index(29)\n\n# Create a new dataframe with the thirty-fourth column as the index\ndf_new = df_new.set_index(30)\n\n# Create a new dataframe with the thirty-sixth column as the index\ndf_new = df_new.set_index(", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# Create a new dataframe with the first column as the index\ndf_new = df.set_index(0)\n\n# Create a new dataframe with the second column as the index\ndf_new = df_new.set_index(1)\n\n# Create a new dataframe with the third column as the index\ndf_new = df_new.set_index(2)\n\n# Create a new dataframe with the fourth column as the index\ndf_new = df_new.set_index(3)\n\n# Create a new dataframe with the fifth column as the index\ndf_new = df_new.set_index(4)\n\n# Create a new dataframe with the sixth column as the index\ndf_new = df_new.set_index(5)\n\n# Create a new dataframe with the seventh column as the index\ndf_new = df_new.set_index(6)\n\n# Create a new dataframe with the eighth column as the index\ndf_new = df_new.set_index(7)\n\n# Create a new dataframe with the ninth column as the index\ndf_new = df_new.set_index(8)\n\n# Create a new dataframe with the tenth column as the index\ndf_new = df_new.set_index(9)\n\n# Create a new dataframe with the eleventh column as the index\ndf_new = df_new.set_index(10)\n\n# Create a new dataframe with the twelfth column as the index\ndf_new = df_new.set_index(11)\n\n# Create a new dataframe with the thirteenth column as the index\ndf_new = df_new.set_index(12)\n\n# Create a new dataframe with the fourteenth column as the index\ndf_new = df_new.set_index(13)\n\n# Create a new dataframe with the fifteenth column as the index\ndf_new = df_new.set_index(14)\n\n# Create a new dataframe with the sixteenth column as the index\ndf_new = df_new.set_index(15)\n\n# Create a new dataframe with the seventeenth column as the index\ndf_new = df_new.set_index(16)\n\n# Create a new dataframe with the eightteenth column as the index\ndf_new = df_new.set_index(17)\n\n# Create a new dataframe with the nineteenth column as the index\ndf_new = df_new.set_index(18)\n\n# Create a new dataframe with the twentieth column as the index\ndf_new = df_new.set_index(19)\n\n# Create a new dataframe with the twenty-first column as the index\ndf_new = df_new.set_index(20)\n\n# Create a new dataframe with the twenty-second column as the index\ndf_new = df_new.set_index(21)\n\n# Create a new dataframe with the twenty-third column as the index\ndf_new = df_new.set_index(22)\n\n# Create a new dataframe with the twenty-fourth column as the index\ndf_new = df_new.set_index(23)\n\n# Create a new dataframe with the twenty-fifth column as the index\ndf_new = df_new.set_index(24)\n\n# Create a new dataframe with the twenty-sixth column as the index\ndf_new = df_new.set_index(25)\n\n# Create a new dataframe with the twenty-seventh column as the index\ndf_new = df_new.set_index(26)\n\n# Create a new dataframe with the twenty-eighth column as the index\ndf_new = df_new.set_index(27)\n\n# Create a new dataframe with the twenty-ninth column as the index\ndf_new = df_new.set_index(28)\n\n# Create a new dataframe with the thirty-first column as the index\ndf_new = df_new.set_index(29)\n\n# Create a new dataframe with the thirty-second column as the index\ndf_new = df_new.set_index(30)\n\n# Create a new dataframe with the thirty-third column as the index\ndf_new = df_new.set_index", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# Create a new dataframe with the first column as the index\ndf_new = df.set_index(0)\n\n# Create a new dataframe with the second column as the index\ndf_new = df_new.set_index(1)\n\n# Create a new dataframe with the third column as the index\ndf_new = df_new.set_index(2)\n\n# Create a new dataframe with the fourth column as the index\ndf_new = df_new.set_index(3)\n\n# Create a new dataframe with the fifth column as the index\ndf_new = df_new.set_index(4)\n\n# Create a new dataframe with the sixth column as the index\ndf_new = df_new.set_index(5)\n\n# Create a new dataframe with the seventh column as the index\ndf_new = df_new.set_index(6)\n\n# Create a new dataframe with the eighth column as the index\ndf_new = df_new.set_index(7)\n\n# Create a new dataframe with the ninth column as the index\ndf_new = df_new.set_index(8)\n\n# Create a new dataframe with the tenth column as the index\ndf_new = df_new.set_index(9)\n\n# Create a new dataframe with the eleventh column as the index\ndf_new = df_new.set_index(10)\n\n# Create a new dataframe with the twelfth column as the index\ndf_new = df_new.set_index(11)\n\n# Create a new dataframe with the thirteenth column as the index\ndf_new = df_new.set_index(12)\n\n# Create a new dataframe with the fourteenth column as the index\ndf_new = df_new.set_index(13)\n\n# Create a new dataframe with the fifteenth column as the index\ndf_new = df_new.set_index(14)\n\n# Create a new dataframe with the sixteenth column as the index\ndf_new = df_new.set_index(15)\n\n# Create a new dataframe with the seventeenth column as the index\ndf_new = df_new.set_index(16)\n\n# Create a new dataframe with the eightteenth column as the index\ndf_new = df_new.set_index(17)\n\n# Create a new dataframe with the nineteenth column as the index\ndf_new = df_new.set_index(18)\n\n# Create a new dataframe with the twentieth column as the index\ndf_new = df_new.set_index(19)\n\n# Create a new dataframe with the twenty-first column as the index\ndf_new = df_new.set_index(20)\n\n# Create a new dataframe with the twenty-second column as the index\ndf_new = df_new.set_index(21)\n\n# Create a new dataframe with the twenty-third column as the index\ndf_new = df_new.set_index(22)\n\n# Create a new dataframe with the twenty-fourth column as the index\ndf_new = df_new.set_index(23)\n\n# Create a new dataframe with the twenty-sixth column as the index\ndf_new = df_new.set_index(24)\n\n# Create a new dataframe with the twenty-eighth column as the index\ndf_new = df_new.set_index(25)\n\n# Create a new dataframe with the twenty-ninth column as the index\ndf_new = df_new.set_index(26)\n\n# Create a new dataframe with the thirty-first column as the index\ndf_new = df_new.set_index(27)\n\n# Create a new dataframe with the thirty-second column as the index\ndf_new = df_new.set_index(28)\n\n# Create a new dataframe with the thirty-third column as the index\ndf_new = df_new.set_index(29)\n\n# Create a new dataframe with the thirty-fourth column as the index\ndf_new = df_new.set_index(30)\n\n# Create a new dataframe with the thirty-sixth column as the index\ndf_new = df_new.set_index(", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\nresult = df['col1'].apply(lambda x: [item for sublist in x for item in sublist])\nresult = result.tolist()\nprint(result)\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\nresult = df['col1'].apply(lambda x: ', '.join(map(str, x[::-1])))\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\nresult = df['col1'].apply(lambda x: ', '.join(map(str, x)))\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "\n# Convert the time to minutes\ndf['Time'] = df['Time'].apply(lambda x: x.minute + x.hour*60 + x.day*24*60 + x.month*24*60*60 + x.year*24*60*60*60)\n\n# Group by the time and calculate the mean of the values\ndf = df.groupby('Time').mean()\n\n# Convert the time back to the original format\ndf['Time'] = df.index.apply(lambda x: pd.to_datetime(x) + pd.to_timedelta(x.minute + x.hour*60 + x.day*24*60 + x.month*24*60*60 + x.year*24*60*60*60, unit='m'))\n\n# Reset the index\ndf = df.reset_index()\n\n# Rename the columns\ndf.columns = ['Time', 'Value']\n\n# Interpolate the values\ndf['Value'] = df['Value'].interpolate()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "\n# Convert the time to minutes\ndf['Time'] = df['Time'].apply(lambda x: x.minute + x.hour*60 + x.day*24*60 + x.month*24*60*60 + x.year*24*60*60*60)\n\n# Bin the time\ndf['Time'] = df['Time'].apply(lambda x: x // 3 * 3)\n\n# Sum the values\ndf = df.groupby('Time').mean()\n\n# Interpolate the values\ndf = df.interpolate(method='linear')\n\n# Reset the index\ndf = df.reset_index()\n\n# Rename the columns\ndf.columns = ['Time', 'Value']\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[df.isnull().all(axis=1)]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.columns[df.isnull().all(axis=1)]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[df.isnull().all(axis=1)]", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(x, y) for x, y in zip(df.iloc[0], df.iloc[8]) if x != y or (x == np.nan and y != np.nan) or (y == np.nan and x != np.nan)]\nresult", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0]\ndf = df.transpose().reset_index(drop=True)[0", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.transpose().reset_index(drop=True)\ndf.columns = df.columns.str.replace('_0', '')\ndf = df.", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\nprint(df)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\nprint(df)", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by=['time'])\nresult", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(['time', 'VIM'])\nresult", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "dates_to_delete = ['2020-02-17', '2020-02-18']\nresult = df[~df.index.isin(dates_to_delete)]\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Create a new column 'Day' that contains the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' that contains the weekday number\ndf['Weekday'] = df['Date'].dt.weekday()\n\n# Create a new column 'Weekday_Name' that contains the weekday name\ndf['Weekday_Name'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday_Number' that contains the weekday number\ndf['Weekday_Number'] = df['Date'].dt.weekday()\n\n# Create a new column 'Year' that contains the year\ndf['Year'] = df['Date'].dt.year\n\n# Create a new column 'Month' that contains the month\ndf['Month'] = df['Date'].dt.month\n\n# Create a new column 'Day_Number' that contains the day number\ndf['Day_Number'] = df['Date'].dt.day\n\n# Create a new column 'Hour' that contains the hour\ndf['Hour'] = df['Date'].dt.hour\n\n# Create a new column 'Minute' that contains the minute\ndf['Minute'] = df['Date'].dt.minute\n\n# Create a new column 'Second' that contains the second\ndf['Second'] = df['Date'].dt.second\n\n# Create a new column 'Millisecond' that contains the millisecond\ndf['Millisecond'] = df['Date'].dt.millisecond\n\n# Create a new column 'Date_Time' that contains the date and time\ndf['Date_Time'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S\")\n\n# Create a new column 'Date_Time_Millisecond' that contains the date and time with millisecond\ndf['Date_Time_Millisecond'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%f\")\n\n# Create a new column 'Date_Time_Second' that contains the date and time with second\ndf['Date_Time_Second'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%S\")\n\n# Create a new column 'Date_Time_Minute' that contains the date and time with minute\ndf['Date_Time_Minute'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%M\")\n\n# Create a new column 'Date_Time_Hour' that contains the date and time with hour\ndf['Date_Time_Hour'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%H\")\n\n# Create a new column 'Date_Time_Day' that contains the date and time with day\ndf['Date_Time_Day'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%D\")\n\n# Create a new column 'Date_Time_Month' that contains the date and time with month\ndf['Date_Time_Month'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%m\")\n\n# Create a new column 'Date_Time_Year' that contains the date and time with year\ndf['Date_Time_Year'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%Y\")\n\n# Create a new column 'Date_Time_Millisecond_Second' that contains the date and time with millisecond and second\ndf['Date_Time_Millisecond_Second'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%f\")\n\n# Create a new column 'Date_Time_Second_Millisecond' that contains the date and time with second and millisecond\ndf['Date_Time_Second_Millisecond'] = df['Date'].dt.strftime(\"%d-%b-%Y %H:%M:%S.%S.%f\")\n\n# Create a new column 'Date_Time_Minute_Second' that contains the date and time with minute and second\ndf['Date_Time_Minute_Second'] = df['Date'].dt.strftime(\"%d-%b", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr[corr > 0.3]\nresult = result[result > 0.3]\nresult = result.stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr.where(corr > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0.3)\nresult = result.where(result > 0.3, 0", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "result = df.rename(columns={df.columns[-1]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "result = df.rename(columns={df.columns[0]: 'Test'})\nprint(result)\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndef find_frequent(row):\n    return row.max()\n\ndef find_freq_count(row):\n    return row.sum()\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndef find_frequent(row):\n    return row.value_counts().max()\n\ndef find_freq_count(row):\n    return row.value_counts().values[0]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    values = row.values.tolist()\n    unique_values = list(set(values))\n    counts = [values.count(value) for value in unique_values]\n    return unique_values, counts\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df['frequent'].apply(lambda x: x[1])\n\nprint(df)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "result = df.groupby(['id1', 'id2'])['foo', 'bar'].mean().reset_index()\nresult", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "\n# Convert the bar column to numeric if it is not NULL\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce')\n\n# Fill NULL values with 0\ndf['bar'] = df['bar'].fillna(0)\n\n# Group by id1 and id2 and calculate the mean of foo and bar\nresult = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\n\n# Reset the index of the result\nresult = result.reset_index()\n\n# Convert the bar column to float if it is not NULL\nresult['bar'] = pd.to_float(result['bar'])\n\n# Fill NULL values with 0\nresult['bar'] = result['bar'].fillna(0)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result[['EntityNum', 'foo', 'a_col']]\nresult", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult = result.drop(['a_col'], axis=1)\nresult", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[~np.isnan(x)]", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x[np.isnan(x)] = np.inf\n", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = x.tolist()\nresult = [i for i in result if not np.isnan(i)]\nresult = np.array(result)\nresult = result.tolist()\nresult", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((a.shape[0], 4))\nb[:, 0] = np.where(a == 0, 1, 0)\nb[:, 1:] = np.where(a == 1, 1, 0)\nb[:, 3] = np.where(a == 3, 1, 0)\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nb[np.arange(a.shape[0]), a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i, j] = a[i, j]\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.reshape(A, (-1, ncol))\nB = B[:-1, :]\nEND SOLUTION!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    return A.reshape(-1, ncol)\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\nresult", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=1)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=0)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape)\nprint(result)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    return max_value, max_index\n\nresult = f(example_a)\nprint(result)\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2)[-2], a.shape)\nprint(result)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "z = np.any(np.isnan(a), axis=0)\na = np.delete(a, z, axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\nprint(result)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a[:, permutation])", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "result = np.unravel_index(np.argmin(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "result = np.unravel_index(np.argmax(a), a.shape)\nprint(result)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.argwhere(a == a.min())\nprint(result)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.degrees(degree))\nprint(result)", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.deg2rad(degree))", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * 180 / np.pi\nresult = result / 180 * 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult = result % 360\nresult", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "result = np.pad(A, (0, length - A.shape[0]), 'constant')\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant', constant_values=0)\n\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "print(np.power(a, power))\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(example_a, 2))\nprint(f(example_a, 3))\nprint(f(example_a, 4))\nprint(f(example_a, 5))\nprint(f(example_a, 6))\nprint(f(example_a, 7))\nprint(f(example_a, 8))\nprint(f(example_a, 9))\nprint(f(example_a, 10))\nprint(f(example_a, 11))\nprint(f(example_a, 12))\nprint(f(example_a, 13))\nprint(f(example_a, 14))\nprint(f(example_a, 15))\nprint(f(example_a, 16))\nprint(f(example_a, 17))\nprint(f(example_a, 18))\nprint(f(example_a, 19))\nprint(f(example_a, 20))\nprint(f(example_a, 21))\nprint(f(example_a, 22))\nprint(f(example_a, 23))\nprint(f(example_a, 24))\nprint(f(example_a, 25))\nprint(f(example_a, 26))\nprint(f(example_a, 27))\nprint(f(example_a, 28))\nprint(f(example_a, 29))\nprint(f(example_a, 30))\nprint(f(example_a, 31))\nprint(f(example_a, 32))\nprint(f(example_a, 33))\nprint(f(example_a, 34))\nprint(f(example_a, 35))\nprint(f(example_a, 36))\nprint(f(example_a, 37))\nprint(f(example_a, 38))\nprint(f(example_a, 39))\nprint(f(example_a, 40))\nprint(f(example_a, 41))\nprint(f(example_a, 42))\nprint(f(example_a, 43))\nprint(f(example_a, 44))\nprint(f(example_a, 45))\nprint(f(example_a, 46))\nprint(f(example_a, 47))\nprint(f(example_a, 48))\nprint(f(example_a, 49))\nprint(f(example_a, 50))\nprint(f(example_a, 51))\nprint(f(example_a, 52))\nprint(f(example_a, 53))\nprint(f(example_a, 54))\nprint(f(example_a, 55))\nprint(f(example_a, 56))\nprint(f(example_a, 57))\nprint(f(example_a, 58))\nprint(f(example_a, 59))\nprint(f(example_a, 60))\nprint(f(example_a, 61))\nprint(f(example_a, 62))\nprint(f(example_a, 63))\nprint(f(example_a, 64))\nprint(f(example_a, 65))\nprint(f(example_a, 66))\nprint(f(example_a, 67))\nprint(f(example_a, 68))\nprint(f(example_a, 69))\nprint(f(example_a, 70))\nprint(f(example_a, 71))\nprint(f(example_a, 72))\nprint(f(example_a, 73))\nprint(f(example_a, 74))\nprint(f(example_a, 75))\nprint(f(example_a, 76))\nprint(f(example_a, 77))\nprint(f(example_a, 78))\nprint(f(example", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = np.floor_(result)\nresult = (result[0], result[1])\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.array([numerator, denominator])\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = np.divide(numerator, denominator)\nresult = (result.numerator, result.denominator)\nresult", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3\nprint(result)\n", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(a, np.maximum(b, c))", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a[-1, :])\nprint(result)\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a[-1, :])\nresult", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.concatenate((np.diag(a.reshape(-1)), np.diag(a.reshape(-1)[::-1])))\nresult = result.reshape(a.shape[0], a.shape[1])\nresult", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(a[:, 0]), np.diag(a[:, -1])))\nresult = np.reshape(result, (a.shape[0], a.shape[1]))\nprint(result)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = [x for x in X.flatten()]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.empty(X.shape[0], dtype=object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = []\n    for i in range(len(X)):\n        for j in range(len(X[i])):\n            result.append(X[i][j])\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty(X.shape[0], dtype=np.object)\nfor i in range(X.shape[0]):\n    result[i] = X[i]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\nprint(result)", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)\n", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)\n", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "def get_maximal_independent_set(a):\n    # Get the maximum number of independent vectors\n    max_independent_set = np.argmax(np.sum(a, axis=0))\n    # Get the independent vectors\n    independent_set = a[:, max_independent_set]\n    return independent_set\n\nresult = get_maximal_independent_set(a)\nprint(result)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]\n", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "p_value = scipy.stats.ttest_ind(a, b, equal_var=True, nan_policy='omit')[1]", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "# Create a random array of size 100 with random numbers\nc = np.random.randn(100)\n\n# Create a random array of size 100 with random numbers\nd = np.random.randn(100)\n\n# Calculate the t-test\nt_statistic, p_value = scipy.stats.ttest_ind(a, b)\n\n# Calculate the t-test for c and d\nt_statistic_c, p_value_c = scipy.stats.ttest_ind(c, d)\n\n# Print the p-value\nprint(p_value)\nprint(p_value_c<0.05)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the weighted mean and variance\nweighted_mean = (anobs * amean + bnobs * bmean) / (anobs + bnobs)\nweighted_var = np.sqrt((anobs * avar + bnobs * bvar) / (anobs + bnobs))\n\n# calculate the t-value\nt_value = (weighted_mean - amean) / np.sqrt(weighted_var**2 + avar**2)\n\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_value, anobs + bnobs - 2)\n\n# print the p-value\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((B, A)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((A, B)), return_index=True)[1]\noutput = output[np.unique(output, return_index=True)[1]]\noutput = np.unique(np.concatenate((", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape)\nprint(c)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape) # (3, 3, 3, 3, 3)\nprint(c) # [[[ 0  1  2]\n#  [ 3  4  5]\n#  [ 6  7  8]]\n# [[18 19 20]\n#  [21 22 23]\n#  [24 25 26]]\n# [[ 9  1  2]\n#  [ 3  4  5]\n#  [ 6  7  8]]]", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape)\nprint(c)\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)\nresult = np.argsort(a, axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(3*3*3), axis=2)\nresult = np.take(result, np.arange(3*3*3), axis=1)\nresult = np.take(result, np.arange(3*3*3), axis=0)\nresult = np.take(result, np.arange(", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)\n", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\nprint(result)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\nprint(a)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(a == a[0], axis=1)", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(a == a[0], axis=0)", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(a == a[0], axis=1)\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "def f(x, y):\n    return (np.cos(x))**4 + (np.sin(y))**2\n\nx, y = np.meshgrid(x, y)\nresult = np.sum(f(x, y))\nprint(result)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    return (np.cos(x))**4 + (np.sin(y))**2\n    ### END SOLUTION ###\n\n# create a 2D array with the function values\nresult = f(example_x, example_y)\n\n# create a 2D array with the weights\nweights = np.ones((20, 30))\n\n# calculate the integral\nintegral = np.sum(weights * result)\nprint(integral)\n#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nresult(eval)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\ndef longest_interval(x, threshold):\n  # find the longest interval [low, high) that satisfies ECDF(x) < threshold\n  low, high = x[0], x[0]\n  for i in range(1, len(x)):\n    if x[i] < threshold:\n      high = x[i]\n    else:\n      if high - low > x[i] - x[i - 1]:\n        high = x[i]\n      low = x[i - 1]\n  return low, high\n\nlow, high = longest_interval(grades, threshold)\nprint(low, high)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[nums == 0] = 1 - one_ratio\nnums = np.random.randint(2, size=size)\nnums[nums == 1] = one_ratio\nnums[num", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.tensor(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\nresult = result.tolist()\nresult", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[::-1][:N]\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])\nresult = result.reshape(-1, 2, 2)\nresult", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])\nresult = result.reshape(-1, 2, 2)\nresult", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "result = np.array([a[i:i+2, j:j+2] for i in range(a.shape[0]-1) for j in range(a.shape[1]-1)])\nresult = result.reshape(-1, 2, 2)\nresult", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\n\n# calculate the number of patches\nnum_patches = a.shape[0] // patch_size\n\n# create the patches\npatches = np.array([a[i:i + patch_size, j:j + patch_size] for i in range(num_patches) for j in range(num_patches)])\n\n# remove the extra rows and columns\npatches = patches[:num_patches, :num_patches]\n\n# print the patches\nprint(patches", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.zeros((h, w))\nfor i in range(h):\n    for j in range(w):\n        result[i, j] = a[i, j, 0]\nresult", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\n\n# calculate the number of patches\nnum_patches = a.shape[0] // patch_size\n\n# create the patches\npatches = np.array([a[i:i + patch_size, j:j + patch_size] for i in range(num_patches) for j in range(num_patches)])\n\n# remove the extra rows and columns\npatches = patches[:num_patches, :num_patches]\n\n# print the patches\nprint(patches", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "import numpy as np\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(string)\nprint(a)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.random.uniform(min, max, n)\nresult = np.log(result)\nresult = np.exp(result)\nresult", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.random.uniform(np.log(min), np.log(max), n)\nresult = np.exp(result)\nresult", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    return np.log(np.random.uniform(min, max, n))\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3, 0))", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.flatnonzero(a[index])[0]", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = np.sub2ind(a.shape, index)\nprint(result)", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df)\nprint(df.values.dtype)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.array([np.sum(a[accmap == i]) for i in np.unique(accmap)])\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])\nprint(result)\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0], axis=0)\nprint(result)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.minimum(a[index])\nprint(result)\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "z = np.array([[elementwise_function(x[i][j], y[i][j]) for j in range(len(x[i]))] for i in range(len(x))])\nprint(z)\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, p=probabilit)\nprint(result)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "result = np.pad(a, ((0, 0), (low_index, high_index)), 'constant')[low_index:high_index, low_index:high_index]", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[x.real != 0]", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_max = np.max(bin_data, axis=1)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\nprint(bin_data_mean)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (1 - np.tanh((x - x_min) / (x_max - x_min)))\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0, 1)\n    t = t * t * (3 - 2 * t)\n    return x_min + t * (x_max - x_min)\n\nprint(smoothclamp(x, x_min, x_max, N))", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding_full = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding_full[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding_full_full = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding_full_full[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding_full_full_full = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding_full_full_full[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding_full_full_full_full = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding_full_full_full_full[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding_full_full_full_full_full = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding_full_full_full_full_full[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding_full_full_full_full_full_full = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding_full_full_full_full_full_full[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding_full_full_full_full_full_full_full = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding_full_full_full_full_full_full_full[i, j] = np.correlate(a, b, mode='full')[i]\n\n# Create a circular correlation matrix with zero padding\ncorr_matrix_zero_padding_full_full_full_full_full_full_full_full = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix_zero_padding_full_full_full_full_full_full_full_full[i, j] =", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.values.reshape(4, 15, 5)\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i in range(4)])\nresult = np.array([np.array(result[i]) for i", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array([[df.loc[(name, timestamp)] for timestamp in times] for name in names])\nresult = np.reshape(result, (15, 4, 5))\nprint(result)\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.unpackbits(np.uint8(a))\nresult = result.reshape(-1, m)\nresult = np.where(result == 0, 0, 1)\nresult = np.where(result == 1, 1, 0)\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:m]\nresult = np.pad(result, (0, m - len(result)), 'constant')\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "result = np.unpackbits(np.uint8(a)).reshape(-1, m)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)), result), axis=1)\nresult = np.concatenate((np.ones((result.shape[0], 1)),", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc -= 3*\u03c3\n\u03bc += 3*\u03c3\nresult = (\u03bc-\u03c3, \u03bc+\u03c3)\nresult", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc - 2\u03c3\n\u03bc + \u03bc!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std = np.std(a)\n    result = (mean - 3*std, mean + 3*std)\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc_plus_2\u03c3 = \u03bc + 2*\u03c3\n\u03bc_minus_2\u03c3 = \u03bc - 2*\u03c3\nresult = (a >= \u03bc_minus_2\u03c3) & (a <= \u03bc_plus_2\u03c3)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "DataArray[DataArray < 0] = np.nan\nprob = np.percentile(DataArray, percentile)\nprint(prob)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[0, 1] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.all(a == np.amax(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1)[:, None], axis=1)", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "import numpy as np\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n\n# Create a 2D array where the first column is the postal codes and the second column is the distance\ndata = np.array([post, distance]).T\n\n# Calculate the Pearson correlation coefficient\nresult = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\nprint(result)\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.dot(X, X.T)", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((Y.shape[0], Y.shape[1]))\nfor i in range(Y.shape[1]):\n    X[:, i] = np.sum(Y[:, i], axis=0)\n", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = A[A not in B]\nC = A", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[np.in1d(A, B)]\n", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "C = A[np.logical_or(A[:, None] == B[None, :], A == B)]", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.array(len(a) - rankdata(a)) - 1\nprint(result)\n", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.array(len(a) * [0], dtype=int)\nfor i in range(len(a)):\n    result[i] = len(a) - i - 1\nresult = np.array(result, dtype=int)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 1, 1, result)\nresult = np.where(result == 2, 2, result)\nresult = np.where(result == 3, 3, result)\nresult = np.where(result == 4, 4, result)\nresult = np.where(result == 5, 5, result)\nresult = np.where(result == 6, 6, result)\nresult = np.where(result == 7, 7, result)\nresult = np.where(result == 8, 8, result)\nresult = np.where(result == 9, 9, result)\nresult = np.where(result == 10, 10, result)\nresult = np.where(result == 11, 11, result)\nresult = np.where(result == 12, 12, result)\nresult = np.where(result == 13, 13, result)\nresult = np.where(result == 14, 14, result)\nresult = np.where(result == 15, 15, result)\nresult = np.where(result == 16, 16, result)\nresult = np.where(result == 17, 17, result)\nresult = np.where(result == 18, 18, result)\nresult = np.where(result == 19, 19, result)\nresult = np.where(result == 20, 20, result)\nresult = np.where(result == 21, 21, result)\nresult = np.where(result == 22, 22, result)\nresult = np.where(result == 23, 23, result)\nresult = np.where(result == 24, 24, result)\nresult = np.where(result == 25, 25, result)\nresult = np.where(result == 26, 26, result)\nresult = np.where(result == 27, 27, result)\nresult = np.where(result == 28, 28, result)\nresult = np.where(result == 29, 29, result)\nresult = np.where(result == 30, 30, result)\nresult = np.where(result == 31, 31, result)\nresult = np.where(result == 32, 32, result)\nresult = np.where(result == 33, 33, result)\nresult = np.where(result == 34, 34, result)\nresult = np.where(result == 35, 35, result)\nresult = np.where(result == 36, 36, result)\nresult = np.where(result == 37, 37, result)\nresult = np.where(result == 38, 38, result)\nresult = np.where(result == 39, 39, result)\nresult = np.where(result == 40, 40, result)\nresult = np.where(result == 41, 41, result)\nresult = np.where(result == 42, 42, result)\nresult = np.where(result == 43, 43, result)\nresult = np.where(result == 44, 44, result)\nresult = np.where(result == 45, 45, result)\nresult = np.where(result == 46, 46, result)\nresult = np.where(result == 47, 47, result)\nresult = np.where(result == 48, 48, result)\nresult = np.where(result == 49, 49, result)\nresult = np.where(result == 50, 50, result)\nresult = np.where(result == 51, 51, result)\nresult = np.where(result == 52, 52, result)\nresult = np.where(result == 53, 53, result)\nresult = np.where(result == 54,", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    result = np.array(a)[np.argsort(np.array(a)[::-1])]\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))\nprint(dists)\n", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))\nprint(dists)\n", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()\nprint(result)\n", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))\nprint(arr.shape)", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.sum(X, axis=1)\nresult = X / l1.reshape(-1, 1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "result = np.array([LA.norm(v,ord=2) for v in X])\nprint(result)\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = np.array([v / x for v in X])\nprint(result)\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].apply(lambda x: x.find(target) != -1)\nresult = np.select(conditions, choices, default=np!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "import scipy.spatial\n\nresult = scipy.spatial.distance.pdist(a, 'euclidean')\nresult = result.reshape(a.shape[0], a.shape[0])\nresult = np.sqrt(result)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == np.inf, 0, result)\nresult = np.where(result == np.nan, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 1, 0, result)\nresult", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "import scipy.spatial\n\nresult = scipy.spatial.distance.pdist(a, 'euclidean')\nresult = result.reshape(a.shape[0], a.shape[0])\nresult = np.sqrt(result)\nresult = np.transpose(result)\nresult = np.round(result, 2)\nresult = np.where(result == 0, 0, result)\nresult = np.where(result == 1, 0, result)\nresult = np.where(result == 2, 0, result)\nresult = np.where(result == 3, 0, result)\nresult = np.where(result == 4, 0, result)\nresult = np.where(result == 5, 0, result)\nresult = np.where(result == 6, 0, result)\nresult = np.where(result == 7, 0, result)\nresult = np.where(result == 8, 0, result)\nresult = np.where(result == 9, 0, result)\nresult = np.where(result == 10, 0, result)\nresult = np.where(result == 11, 0, result)\nresult = np.where(result == 12, 0, result)\nresult = np.where(result == 13, 0, result)\nresult = np.where(result == 14, 0, result)\nresult = np.where(result == 15, 0, result)\nresult = np.where(result == 16, 0, result)\nresult = np.where(result == 17, 0, result)\nresult = np.where(result == 18, 0, result)\nresult = np.where(result == 19, 0, result)\nresult = np.where(result == 20, 0, result)\nresult = np.where(result == 21, 0, result)\nresult = np.where(result == 22, 0, result)\nresult = np.where(result == 23, 0, result)\nresult = np.where(result == 24, 0, result)\nresult = np.where(result == 25, 0, result)\nresult = np.where(result == 26, 0, result)\nresult = np.where(result == 27, 0, result)\nresult = np.where(result == 28, 0, result)\nresult = np.where(result == 29, 0, result)\nresult = np.where(result == 30, 0, result)\nresult = np.where(result == 31, 0, result)\nresult = np.where(result == 32, 0, result)\nresult = np.where(result == 33, 0, result)\nresult = np.where(result == 34, 0, result)\nresult = np.where(result == 35, 0, result)\nresult = np.where(result == 36, 0, result)\nresult = np.where(result == 37, 0, result)\nresult = np.where(result == 38, 0, result)\nresult = np.where(result == 39, 0, result)\nresult = np.where(result == 40, 0, result)\nresult = np.where(result == 41, 0, result)\nresult = np.where(result == 42, 0, result)\nresult = np.where(result == 43, 0, result)\nresult = np.where(result == 44, 0, result)\nresult = np.where(result == 45, 0, result)\nresult = np.where(result == 46, 0, result)\nresult = np.where(result == 47, 0, result)\nresult = np.where(result == 48, 0, result)\nresult = np.where(result == 49, 0, result)\nresult = np.where(result == 50, 0, result)\nresult = np.where(result == 51, 0, result)\nresult = np.where(result == 52, 0, result)\nresult = np.where(result == 53, 0, result)\nresult = np.where(result == 54, 0, result)\nresult = np.where(result == 55, 0, result)\nresult", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "import scipy.spatial\n\nresult = scipy.spatial.distance.pdist(a, 'euclidean')\nresult = result.reshape(a.shape[0], a.shape[0])\nresult = np.transpose(result)\nresult = np.triu(result)\nresult = np.sort(result)\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\nresult = np.array([result[i] for i in range(len(result))])\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# Remove the adjacent (before removing) duplicate non-zero value and all the zero value\nresult = np.unique(a[a != 0])\n\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\n# Remove adjacent duplicates\na = np.where(a[:-1] != a[1:], a, a[1:])\n\n# Remove zero values\na = a[a!=0]\n\nresult = a", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame({'lat': lat[0], 'lon': lon[0], 'val': val[0]})\nfor i in range(1, lat.shape[0]):\n    df = df.append(pd.DataFrame({'lat': lat[i], 'lon': lon[i], 'val': val[i]}, ignore_index=True))\ndf", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    df = df.sort_values(['lat', 'lon'])\n    df = df.reset_index(drop=True)\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf['maximum'] = df.max(axis=1)\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'lat': 'latitude', 'lon': 'longitude', 'val': 'value', 'maximum': 'maximum_value'})\ndf = df.sort_values(by=['latitude', 'longitude'])\ndf = df.reset_index(drop=True)\ndf", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, size[0])\n    strides = a.strides + a.strides\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    shape = a.shape\n    shape = shape[:-1] + (shape[-1] - size[0] + 1, size[0])\n    strides = a.strides + a.strides\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    return np.mean(a)\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:-1]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "result = c in CNTS\nprint(result)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "result = any(np.array_equal(c, cnt) for cnt in CNTS)\nprint(result)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\ndf", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.matrix([[12.22151125, 0, 0, 0],[0,4.92815942, 0, 0],[0,0,2.06380839,0 ],[0,0,0,0.29766152]])\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[a.shape[0]//2:-1:2] = 0", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\n\n# calculate the number of days between the start and end dates\nnum_days = (end_date - start_date).days\n\n# calculate the number of intervening elements\nnum_intervening_elements = num_days * n\n\n# create a linear space of dates\ndates = pd.date_range(start_date, end_date, num_intervening_elements)\n\n# convert the dates to pd.DatetimeIndex type\ndates = pd.DatetimeIndex(dates)\n\n# convert the dates to numpy array\ndates_array = dates.to_numpy()\n\n# convert the numpy array to pandas Series\ndates_series = pd.Series(dates_array)\n\n# assign the dates series to the result variable\nresult = dates_series\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    print(\"No index found\")\nelse:\n    print(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))\nresult = indices[0]\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "def f(x, a, b, c):\n    return a * x ** 2 + b * x + c\n\ndef squared_error(x, y, a, b, c):\n    return np.sum((f(x, a, b, c) - y) ** 2)\n\ndef gradient(x, y, a, b, c):\n    return np.array([2 * np.sum((f(x, a, b, c) - y) * x), \n                     np.sum((f(x, a, b, c) - y) * x), \n                     np.sum((f(x, a, b, c) - y))])\n\ndef solve(x, y, a, b, c):\n    return np.linalg.solve(gradient(x, y, a, b, c), y)\n\nresult = solve(x, y, 1, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "# Create a matrix with the x values and a column of ones\nx_matrix = np.column_stack((np.ones(len(x)), np.array(x)))\n# Calculate the coefficients\ncoeff = np.linalg.solve(x_matrix, y)\n# Create a function that returns the value of the function at a given x\ndef f(x, coeff):\n    return coeff[0] + coeff[1] * x + coeff[2] * x ** 2\n# Calculate the squared error\nsquared_error = np.sum((f(x, coeff) - y) ** 2)\n# Find the parameters that minimize the squared error\nresult = np.array(coeff)[np.argsort(squared_error)[::-1]]\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = [0, 1, 2, 3]\ndf = df - temp_arr\nprint(df)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)\nprint(result)\n", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\nprint(result)\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\nprint(result)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(a)\nprint(result)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "mask = arr < -10\narr[mask] = 0\n\nmask = arr >= 15\narr[mask] = 30\n\nmask = arr >= -10\narr[mask] = arr[mask] + 5\n\narr[~mask] = 30\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr_temp = arr.copy()\nmask = arr_temp < n1\nmask2 = arr_temp < n2\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arr_temp[mask3] + 5\narr[~mask2] = 30\n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(s1 != s2)[0].shape[0]\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "result = np.nonzero(s1 != s2)[0].shape[0]\nresult += np.nonzero(np.isnan(s1) != np.isnan(s2))[0].shape[0]\nprint(result)\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a[1])\nprint(result)\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all(axis=1) for arr in a)\nprint(result)\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, 93 - a.shape[0]), (0, 13 - a.shape[1])), 'constant')\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), 'constant')\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "result = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), 'constant', constant_values=element)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # calculate the maximum shape\n    max_shape = np.array(shape)\n    # calculate the maximum size\n    max_size = np.array(arr.shape)\n    # calculate the padding size\n    padding_size = max_shape - max_size\n    # zero pad the array\n    result = np.pad(arr, padding_size, mode='constant', constant_values=0)\n    ### END SOLUTION ###\n    return result\n\n# test the function\nprint(f())\n# test the function with a different shape\nprint(f(shape=(60, 15)))\n# test the function with a different array\nprint(f(arr=np.ones((60, 15))))\n# test the function with a large array\nprint(f(arr=np.ones((1000, 1000))))\n# test the function with a large shape\nprint(f(shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000, 1000)))\n# test the function with a large array and shape\nprint(f(arr=np.ones((1000, 1000)), shape=(1000", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "import numpy as np\n\ndef zeros_padding(a, shape):\n    a_shape = a.shape\n    shape_diff = shape - a_shape\n    if shape_diff[0] > 0:\n        a = np.pad(a, ((0, shape_diff[0] - a_shape[0]), (0, 0)), mode='constant')\n    if shape_diff[1] > 0:\n        a = np.pad(a, ((0, 0), (0, shape_diff[1] - a_shape[1])), mode='constant')\n    return a\n\nresult = zeros_padding(a, shape)\nprint(result)\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), b[:, 0], b[:, 1]]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0]), b[:, 0], b[:, 1]]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b[:, 0]], axis=2)\nresult = np.sum(result, axis=1)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum(result, axis=0)\nresult = np.sum", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b[:, 2]])\nprint(result)\n", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = np.where(df['a'] > 1, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "result = np.array([[0,1,1,1], [1,1,0,1], [0,0,1,1]])\nprint(result)\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "# Find the indices of the non-zero elements\nindices = np.where(A != 0)\n\n# Get the minimum and maximum indices for each dimension\nmin_indices = np.min(indices, axis=1)\nmax_indices = np.max(indices, axis=1)\n\n# Create a slice of the array with the non-zero elements\nresult = A[min_indices[0]:max_indices[0]+1, min_indices[1]:max_indices[1]+1]\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\n# Create a mask where the image is not all zero\nmask = np.all(im != 0, axis=1)\n\n# Create a mask where the image is not all zero in the column\nmask_col = np.all(im != 0, axis=0)\n\n# Create a mask where the image is not all zero in the row\nmask_row = np.all(im != 0, axis=1)\n\n# Create a mask where the image is not all zero in the column and the row\nmask_col_row = np.all(im != 0, axis=0) & np.all(im != 0, axis=1)\n\n# Create a mask where the image is not all zero in the column and the row and the column\nmask_col_row_row = np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0)\n\n# Create a mask where the image is not all zero in the column and the row and the column and the row\nmask_col_row_row_row = np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1)\n\n# Create a mask where the image is not all zero in the column and the row and the column and the row and the column\nmask_col_row_row_row_row = np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0)\n\n# Create a mask where the image is not all zero in the column and the row and the column and the row and the column and the row\nmask_col_row_row_row_row_row = np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1)\n\n# Create a mask where the image is not all zero in the column and the row and the column and the row and the column and the row and the column\nmask_col_row_row_row_row_row_row = np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1)\n\n# Create a mask where the image is not all zero in the column and the row and the column and the row and the column and the row and the column and the row\nmask_col_row_row_row_row_row_row_row = np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0) & np.all(im != 0, axis=1) & np.all(im != 0, axis=0)\n\n# Create a mask where the image is not all zero in the column and the row and the column and the row and the column and the row and the column and the row and the column\nmask_col_row_row_row_row_row_row_row_row = np.all(im != 0, axis=0)", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "result = np.where(im.any(axis=1), im, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result, 0)\nresult = np.where(result.any(axis=0), result, 0)\nresult = np.where(result.any(), result, 0)\nresult = np.where(result.any(axis=1), result,", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\nsns.scatterplot(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.figure(figsize=(10, 5))\nfor i in range(10):\n    y = np.random.rand(10)\n    plt.plot(x, y, label=f'Line {i}')\n\n# draw a line for each different line style\nplt.plot(x, x, label='Line 10')\n\n# draw a line for each different line style\nplt.plot(x, 2*x, label='Line 11')\n\n# draw a line for each different line style\nplt.plot(x, 3*x, label='Line 12')\n\n# draw a line for each different line style\nplt.plot(x, 4*x, label='Line 13')\n\n# draw a line for each different line style\nplt.plot(x, 5*x, label='Line 14')\n\n# draw a line for each different line style\nplt.plot(x, 6*x, label='Line 15')\n\n# draw a line for each different line style\nplt.plot(x, 7*x, label='Line 16')\n\n# draw a line for each different line style\nplt.plot(x, 8*x, label='Line 17')\n\n# draw a line for each different line style\nplt.plot(x, 9*x, label='Line 18')\n\n# draw a line for each different line style\nplt.plot(x, 10*x, label='Line 19')\n\n# draw a line for each different line style\nplt.plot(x, 11*x, label='Line 20')\n\n# draw a line for each different line style\nplt.plot(x, 12*x, label='Line 21')\n\n# draw a line for each different line style\nplt.plot(x, 13*x, label='Line 22')\n\n# draw a line for each different line style\nplt.plot(x, 14*x, label='Line 23')\n\n# draw a line for each different line style\nplt.plot(x, 15*x, label='Line 24')\n\n# draw a line for each different line style\nplt.plot(x, 16*x, label='Line 25')\n\n# draw a line for each different line style\nplt.plot(x, 17*x, label='Line 26')\n\n# draw a line for each different line style\nplt.plot(x, 18*x, label='Line 27')\n\n# draw a line for each different line style\nplt.plot(x, 19*x, label='Line 28')\n\n# draw a line for each different line style\nplt.plot(x, 20*x, label='Line 29')\n\n# draw a line for each different line style\nplt.plot(x, 21*x, label='Line 30')\n\n# draw a line for each different line style\nplt.plot(x, 22*x, label='Line 31')\n\n# draw a line for each different line style\nplt.plot(x, 23*x, label='Line 32')\n\n# draw a line for each different line style\nplt.plot(x, 24*x, label='Line 33')\n\n# draw a line for each different line style\nplt.plot(x, 25*x, label='Line 34')\n\n# draw a line for each different line style\nplt.plot(x, 26*x, label='Line 35')\n\n# draw a line for each different line style\nplt.plot(x, 27*x, label='Line 36')\n\n# draw a line for each different line style\nplt.plot(x, 28*x, label='Line 37')\n\n# draw a line for each different line style\nplt.plot(x, 29*x, label='Line 38')\n\n# draw a line for each different line style\nplt.plot(x, 30", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.figure(figsize=(10, 5))\nfor i in range(10):\n    y = np.random.rand(10)\n    plt.plot(x, y, label=f'Line {i}')\n\n# draw a line for each different line style\nplt.plot(x, x, label='Line 10')\n\n# draw a line for each different line style\nplt.plot(x, 2*x, label='Line 11')\n\n# draw a line for each different line style\nplt.plot(x, 3*x, label='Line 12')\n\n# draw a line for each different line style\nplt.plot(x, 4*x, label='Line 13')\n\n# draw a line for each different line style\nplt.plot(x, 5*x, label='Line 14')\n\n# draw a line for each different line style\nplt.plot(x, 6*x, label='Line 15')\n\n# draw a line for each different line style\nplt.plot(x, 7*x, label='Line 16')\n\n# draw a line for each different line style\nplt.plot(x, 8*x, label='Line 17')\n\n# draw a line for each different line style\nplt.plot(x, 9*x, label='Line 18')\n\n# draw a line for each different line style\nplt.plot(x, 10*x, label='Line 19')\n\n# draw a line for each different line style\nplt.plot(x, 11*x, label='Line 20')\n\n# draw a line for each different line style\nplt.plot(x, 12*x, label='Line 21')\n\n# draw a line for each different line style\nplt.plot(x, 13*x, label='Line 22')\n\n# draw a line for each different line style\nplt.plot(x, 14*x, label='Line 23')\n\n# draw a line for each different line style\nplt.plot(x, 15*x, label='Line 24')\n\n# draw a line for each different line style\nplt.plot(x, 16*x, label='Line 25')\n\n# draw a line for each different line style\nplt.plot(x, 17*x, label='Line 26')\n\n# draw a line for each different line style\nplt.plot(x, 18*x, label='Line 27')\n\n# draw a line for each different line style\nplt.plot(x, 19*x, label='Line 28')\n\n# draw a line for each different line style\nplt.plot(x, 20*x, label='Line 29')\n\n# draw a line for each different line style\nplt.plot(x, 21*x, label='Line 30')\n\n# draw a line for each different line style\nplt.plot(x, 22*x, label='Line 31')\n\n# draw a line for each different line style\nplt.plot(x, 23*x, label='Line 32')\n\n# draw a line for each different line style\nplt.plot(x, 24*x, label='Line 33')\n\n# draw a line for each different line style\nplt.plot(x, 25*x, label='Line 34')\n\n# draw a line for each different line style\nplt.plot(x, 26*x, label='Line 35')\n\n# draw a line for each different line style\nplt.plot(x, 27*x, label='Line 36')\n\n# draw a line for each different line style\nplt.plot(x, 28*x, label='Line 37')\n\n# draw a line for each different line style\nplt.plot(x, 29*x, label='Line 38')\n\n# draw a line for each different line style\nplt.plot(x, 30", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, marker='d')\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.plot(x, y, 'd-', linewidth=2, markersize=10, marker='d')\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "highlight_range = np.array([2, 4])\nhighlight_color = 'r'\nhighlight_linewidth = 2\nhighlight_label = 'Highlight'\n\nplt.plot(highlight_range, np.zeros_like(highlight_range), '--', color=highlight_color, linewidth=highlight_linewidth, label=highlight_label)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = np.linspace(0, 2, 100)\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    style=\"Gender\",\n    kind=\"scatter\",\n    height=5,\n    aspect=1.5,\n)\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 6))\nsns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.legend(['xyz'], title='xyz', fontsize=20)\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"black\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), x, 'r-', lw=5, markersize=30, label='x')\nplt.plot(range(10), y, 'r-', lw=5, markersize=30, label='y')\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.array([0, 2, 4, 6, 8, 10])\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticks(g.get_xticks(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = np.sort(y)[::-1]\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.array([0, 1.5])\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "y_ticks = np.array([-1, 1])\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 10))\nplt.plot(x, label='x')\nplt.plot(y, label='y')\nplt.plot(z, label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "x = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = np.random.rand(10)\n\n# make all axes ticks integers\nx", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().yaxis.set_major_formatter(plt.matplotlib.ticker.ScalarFormatter())\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax.plot(x, y, '--')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[0].legend()\n\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_xlabel('x')\naxs[1].set_ylabel('y2')\naxs[1].legend()\n\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\nax[0].plot(x, y1, label='sin(x)')\nax[0].set_title('sin(x)')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y')\nax[0].legend()\nax[1].plot(x, y2, label='cos(x)')\nax[1].set_title('cos(x)')\nax[1].set_xlabel('x')\nax[1].set_ylabel('y')\nax[1].legend()\nax[0].spines['top'].set_visible(False)\nax[0].spines['bottom'].set_visible(False)\nax[0].spines['left'].set_visible(False)\nax[0].spines['right'].set_visible(False)\nax[1].spines['top'].set_visible(False)\nax[1].spines['bottom'].set_visible(False)\nax[1].spines['left'].set_visible(False)\nax[1].spines['right'].set_visible(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_on(False)\nax[1].set_frame_on(False)\nax[0].set_frame_", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks([])\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.array([3, 4]))\nplt.grid(axis='x')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4))\nplt.grid(axis='y', which='both', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 4))\nplt.xticks(np.arange(1, 2))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), constrained_layout=True)\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlim(0, 4)\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.gca().set_xticks(np.arange(0, 10, 2))\nplt.gca().set_xticks(np.arange(0, 10, 2), minorticks=1)\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks([])\nplt.yticks([])\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().tick_params(axis='y')\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\nplt.xlabel('X')\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green', line_kws={'color': 'green'}, scatter_kws={'color': 'blue'}, line_color='blue')\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "\n# Create a joint regression plot\njoint_regression = sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', color='green')\n\n# Change the line color in the regression to green but keep the histograms in blue\njoint_regression.ax_joint.lines[0].set_color('green')\n\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "joint_plot = sns.jointplot(x=tips['total_bill'], y=tips['tip'])\njoint_plot.plot()\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 6))\nfor celltype, s1, s2 in zip(df[\"celltype\"], df[\"s1\"], df[\"s2\"]):\n    plt.bar(celltype, s1, label=celltype, color=\"blue\")\n    plt.bar(celltype, s2, label=celltype, color=\"red\")\n\nplt.xlabel(\"celltype\")\nplt.ylabel(\"s1 and s2\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 6))\nfor celltype, s1, s2 in zip(df[\"celltype\"], df[\"s1\"], df[\"s2\"]):\n    plt.bar(celltype, s1, color=\"blue\", label=\"s1\")\n    plt.bar(celltype, s2, color=\"red\", label=\"s2\")\n\nplt.xlabel(\"celltype\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel('X', color='red')\nplt.ylabel('Y', color='red')\nplt.xticks(color='red')\nplt.yticks(color='red')\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='Y')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend(loc='upper left')\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, cmap='hot', aspect='auto')\nplt.xticks(range(len(xlabels)), xlabels, rotation=90)\nplt.yticks(range(len(ylabels)), ylabels)\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax.plot(time, temp, \"-\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[0].legend()\n\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\naxs[1].legend()\n\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "plt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\nplt.title(\"Scatter plot of bill_length_mm and bill_depth_mm\")\nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"bill_depth_mm\")\nplt.show()\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.figure(figsize=(10, 6))\nplt.scatter(a, b, c)\nfor i, j, k in zip(a, b, c):\n    plt.annotate(k, (i, j))\nplt.xlabel('a')\nplt.ylabel('b')\nplt.title('Scatter plot of a over b')\nplt.show()\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(labelspacing=0.1)\nplt.legend(title=\"Legend\", title_fontsize=14, title_fontweight=\"bold\")\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, alpha=0.5, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(2, 3, figsize=(10, 10))\n\n# Create a data frame with two columns\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create a subplot for the first column\naxs[0, 0].plot(df['x'], df['y'])\naxs[0, 0].set_title('First subplot')\n\n# Create a subplot for the second column\naxs[0, 1].plot(df['x'], df['y'])\naxs[0, 1].set_title('Second subplot')\n\n# Create a subplot for the third column\naxs[0, 2].plot(df['x'], df['y'])\naxs[0, 2].set_title('Third subplot')\n\n# Create a subplot for the fourth column\naxs[1, 0].plot(df['x'], df['y'])\naxs[1, 0].set_title('Fourth subplot')\n\n# Create a subplot for the fifth column\naxs[1, 1].plot(df['x'], df['y'])\naxs[1, 1].set_title('Fifth subplot')\n\n# Create a subplot for the sixth column\naxs[1, 2].plot(df['x'], df['y'])\naxs[1, 2].set_title('Sixth subplot')\n\n# Create a subplot for the seventh column\naxs[2, 0].plot(df['x'], df['y'])\naxs[2, 0].set_title('Seventh subplot')\n\n# Create a subplot for the eighth column\naxs[2, 1].plot(df['x'], df['y'])\naxs[2, 1].set_title('Eighth subplot')\n\n# Create a subplot for the ninth column\naxs[2, 2].plot(df['x'], df['y'])\naxs[2, 2].set_title('Ninth subplot')\n\n# Create a subplot for the tenth column\naxs[3, 0].plot(df['x'], df['y'])\naxs[3, 0].set_title('Tenth subplot')\n\n# Create a subplot for the eleventh column\naxs[3, 1].plot(df['x'], df['y'])\naxs[3, 1].set_title('Eleventh subplot')\n\n# Create a subplot for the twelfth column\naxs[3, 2].plot(df['x'], df['y'])\naxs[3, 2].set_title('Twelfth subplot')\n\n# Create a subplot for the thirteenth column\naxs[4, 0].plot(df['x'], df['y'])\naxs[4, 0].set_title('Thirteenth subplot')\n\n# Create a subplot for the fourteenth column\naxs[4, 1].plot(df['x'], df['y'])\naxs[4, 1].set_title('Fourteenth subplot')\n\n# Create a subplot for the fifteenth column\naxs[4, 2].plot(df['x'], df['y'])\naxs[4, 2].set_title('Fifteenth subplot')\n\n# Create a subplot for the sixteenth column\naxs[5, 0].plot(df['x'], df['y'])\naxs[5, 0].set_title('Sixteenth subplot')\n\n# Create a subplot for the seventeenth column\naxs[5, 1].plot(df['x'], df['y'])\naxs[5, 1].set_title('Seventeenth subplot')\n\n# Create a subplot for the eightteenth column\naxs[5, 2].plot(df['x'], df['y'])\naxs[5, 2].set_title('Eighteenth subplot')\n\n# Create a subplot for the nineteenth column\naxs[6, 0].plot(df['x'], df['y'])\naxs[6, 0].set_title('Nineteenth subplot')\n\n# Create a subplot for the twentieth column\naxs[6, 1].plot(df['x'], df", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Set the legend of the chart\nplt.legend()\n\n# Set the title of the chart\nplt.title('Histogram of x and y')\n\n# Set the x-axis label\nplt.xlabel('x')\n\n# Set the y-axis label\nplt.ylabel('Count')\n\n# Show the chart\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist([x, y], bins=10, alpha=0.5, label=['x', 'y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.plot([a, c], [b, d])\nplt.xlim([0, 5])\nplt.ylim([0, 5])\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, ax = plt.subplots(2, 1)\n\n# create a colormap with x and y\ncmap = plt.get_cmap('viridis')\n\n# create a colorbar for the first subplot\nax[0].imshow(x, cmap=cmap)\nax[0].colorbar()\n\n# create a colormap with x and y\ncmap = plt.get_cmap('viridis')\n\n# create a colorbar for the second subplot\nax[1].imshow(y, cmap=cmap)\nax[1].colorbar()\n\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[0].plot(x, z, label='Z')\naxs[0].set_title('Y and Z')\naxs[1].plot(a, y, label='Y')\naxs[1].plot(a, z, label='Z')\naxs[1].set_title('A and Z')\naxs[0].legend()\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.plot(points)\nplt.yscale('log')\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.plot(x, y)\nplt.title('Plot y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "plt.figure(figsize=(10, 10))\nfor line in lines:\n    plt.plot(*line, c=c[lines.index(line)])\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.plot(x, y)\nplt.loglog(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-log plot of y over x')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\n\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "\n# Calculate the sum of the data\ntotal_sum = np.sum(data)\n\n# Normalize the data\nnormalized_data = [d / total_sum for d in data]\n\n# Create a histogram of the normalized data\nplt.hist(normalized_data, bins=np.arange(0, 1.1, 0.1), edgecolor='black')\n\n# Set y tick labels as 10%, 20%, etc.\ny_tick_labels = [f'{int(d*100)}%' for d in np.arange(0.1, 1.1, 0.1)]\nplt.yticks(np.arange(0, 1.1, 0.1), y_tick_labels)\n\n# Set the x tick labels as the original data\nx_tick_labels = [f'{int(d)}' for d in data]\nplt.xticks(np.arange(0, 1.1, 0.1), x_tick_labels)\n\n# Set the x label and y label\nplt.xlabel('Data')\nplt.ylabel('Frequency')\n\n# Set the title\nplt.title('Histogram of Normalized Data')\n\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, alpha=0.5, linestyle='-')\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[0].legend()\n\naxs[1].plot(x, a, label='a')\naxs[1].plot(y, z, label='y')\naxs[1].legend()\n\nfig.legend(axs[0], ['y', 'a'], loc='upper right')\nfig.legend(axs[1], ['a', 'y'], loc='upper left')\n\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=False, sharey=False, figsize=(10, 10))\n\n# In the first subplot, plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\n\n# In the second subplot, plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticks(range(1, 10))\nax.set_xticklabels(['second'])\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(range(0, 10, 2), [2.1, 3, 7.6])\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60, horizontalalignment='left')\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.xticks(rotation=-60)\nplt.tick_params(axis='y')\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.xticks(np.arange(2010, 2020), np.arange(2010, 2020), alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0.05)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0.0, y=0.0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n\naxs.plot(x, y, label='y')\naxs.plot(x, y, label='y')\n\naxs.set_title('Figure')\naxs.legend()\n\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df['Type A'], df['Type B'])\nplt.xlabel('Type A')\nplt.ylabel('Type B')\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch=np.arange(10), marker='o')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none')\nplt.plot(x, y, '|', color='black', hatch='|')\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*', hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*', s=100, hatch='v')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, extent=[1, 5, 1, 4], aspect='auto')\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1], 'r-')\nplt.plot([3, 3], [1, 1], 'r-')\nplt.plot([3, 3], [1, 0], 'r-')\nplt.plot([3, 3], [0, 0], 'r-')\nplt.plot([3, 3], [0, 1],", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 10))\nplt.pie(height, labels=labels, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedge_width=0.4)\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b-')\nplt.grid(True, linestyle='-', color='0.6')\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle=':')\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'r-', alpha=0.5, linewidth=2, edgecolor='b')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Create a figure and a set of axes\nfig, ax = plt.subplots()\n\n# Create a bar plot with the blue bars\nax.bar(np.arange(3), blue_bar, color='b', alpha=0.5, label='Blue bars')\n\n# Create a bar plot with the orange bars\nax.bar(np.arange(3) + 0.2, orange_bar, color='orange', alpha=0.5, label='Orange bars')\n\n# Set the x-axis labels\nax.set_xlabel('Index')\n\n# Set the y-axis labels\nax.set_ylabel('Height')\n\n# Set the title of the plot\nax.set_title('Blue bars and Orange bars')\n\n# Set the legend of the plot\nax.legend()\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.plot(x, y)\nplt.axis([0, 10, 0, 10])\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"species\", data=df, height=5, aspect=0.8, share_y=False)\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(10, 10))\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.5, 0.5], [0, 1], 'k-')\nplt.plot([0, 1], [0.5, 0.5], 'k-')\nplt.plot([0.", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.3)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc=\"upper right\")\nplt.plot(x, y, marker=\"*\", label=\"Line\")\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, cmap='gray')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.plot(x, y)\nplt.title(\"Figure 1\", fontweight='bold')\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.plot(y, x)\nplt.plot(y, -x)\nplt.plot(x, -y)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(x, x)\nplt.plot(x, -x)\nplt.plot(", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('off')\n\n# Create a scatter plot of x over y\nplt.scatter(x, y, c='blue')\n\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolors='black')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor ax in axs.flat:\n    ax.plot(x, y)\n\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 10, 2), edgecolor='black')\nplt.xlabel('x')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o-')\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 0], 'w')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[1].plot(z, a, label='Z')\naxs[0].set_title('Y')\naxs[1].set_title('Z')\naxs[1].set_position([0.1, 0.1, 0.8, 0.8])\naxs[1].raise_axis()\naxs[1].set_axis_on()\naxs[1].set_axis_off()\naxs[1].set_title('Z')\naxs[1].axis('tight')\naxs[1].axis('off')\naxs[1].set_ylim([0, 1])\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlim([0, 1])\naxs[1].set_xticks([0, 1])\naxs[1].set_xticklabels(['X', 'Y'])\naxs[1].legend()\naxs[1].grid(True)\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_yticklabels(['Y', 'Z'])\naxs[1].set_xlabel('X')\naxs[1].set_ylabel('Z')\naxs[1].set_yticks([0, 1])\naxs[1].set_y", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs.flat:\n    ax.plot(x, y)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 6))\nplt.table(\n    df,\n    col_labels=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    row_labels=[\"penguin1\", \"penguin2\", \"penguin3\", \"penguin4\", \"penguin5\", \"penguin6\", \"penguin7\", \"penguin8\", \"penguin9\", \"penguin10\"],\n    cell_contents=df.values,\n    bbox=[0, 0, 1, 1],\n    col_width=[0.1, 0.1, 0.1, 0.1],\n    row_height=0.1,\n    cell_linewidth=0.5,\n    cell_color=\"white\",\n    linewidth=0.5,\n    edgecolor=\"white\",\n    align=\"center\",\n    cell_align=\"center\",\n    col_align=\"center\",\n    row_align=\"center\",\n)\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(x, x)\nplt.yticks(y, y)\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "plt.figure(figsize=(10, 6))\n\n# Create a scatter plot for \"time\" and \"pulse\"\nplt.subplot(2, 1, 1)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.title(\"Group: Fat\")\n\n# Create a scatter plot for \"time\" and \"pulse\"\nplt.subplot(2, 1, 2)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.title(\"Group: No Fat\")\n\n# Set the labels for the x and y axis\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pulse\")\n\n# Set the legend for the hue\nplt.legend(title=\"Kind\")\n\n# Set the legend for the column\nplt.legend(title=\"Diet\")\n\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "# Create a new dataframe with the columns \"time\" and \"pulse\"\ndf_time_pulse = df[[\"time\", \"pulse\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\ndf_time_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\ndf_time_kind = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"pulse\" and \"diet\"\ndf_pulse_diet = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with the columns \"pulse\" and \"kind\"\ndf_pulse_kind = df[[\"pulse\", \"kind\"]]\n\n# Create a new dataframe with the columns \"diet\" and \"kind\"\ndf_diet_kind = df[[\"diet\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\ndf_time_kind_new = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"pulse\" and \"kind\"\ndf_pulse_kind_new = df[[\"pulse\", \"kind\"]]\n\n# Create a new dataframe with the columns \"diet\" and \"kind\"\ndf_diet_kind_new = df[[\"diet\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\ndf_time_diet_new = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"pulse\" and \"diet\"\ndf_pulse_diet_new = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with the columns \"kind\" and \"diet\"\ndf_kind_diet_new = df[[\"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"kind\"\ndf_time_kind_new_new = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with the columns \"pulse\" and \"kind\"\ndf_pulse_kind_new_new = df[[\"pulse\", \"kind\"]]\n\n# Create a new dataframe with the columns \"diet\" and \"kind\"\ndf_diet_kind_new_new = df[[\"diet\", \"kind\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\ndf_time_diet_new_new = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"pulse\" and \"diet\"\ndf_pulse_diet_new_new = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with the columns \"kind\" and \"diet\"\ndf_kind_diet_new_new = df[[\"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\ndf_time_diet_new_new_new = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"pulse\" and \"diet\"\ndf_pulse_diet_new_new_new = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with the columns \"kind\" and \"diet\"\ndf_kind_diet_new_new_new = df[[\"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\ndf_time_diet_new_new_new_new = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with the columns \"pulse\" and \"diet\"\ndf_pulse_diet_new_new_new_new = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with the columns \"kind\" and \"diet\"\ndf_kind_diet_new_new_new_new = df[[\"kind\", \"diet\"]]\n\n# Create a new dataframe with the columns \"time\" and \"diet\"\ndf_time_diet_new_new_new_new_new = df[[\"time\", \"diet\"]]\n\n# Create a new data", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "plt.figure(figsize=(10, 6))\n\n# Create a scatter plot for the \"time\" column\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\nplt.ylabel(\"Pulse\")\nplt.xlabel(\"Time\")\nplt.title(\"Time vs Pulse\")\n\n# Create a scatter plot for the \"pulse\" column\nplt.subplot(1, 2, 2)\nsns.scatterplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df)\nplt.ylabel(\"Pulse\")\nplt.xlabel(\"Time\")\nplt.title(\"Time vs Pulse\")\n\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Plot y over x')\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label=\"y\")\nplt.legend(loc='upper right', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 6))\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, jitter=0.1, edgecolor=\"black\")\nplt.remove(stripplot.legend_.texts)\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "\n# Create a FaceGrid\nfacegrid = sns.FaceGrid(df, row_names=df[\"b\"], col_names=df[\"a\"], margin_titles=True)\n\n# Create a list of xticks\nxticks = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, 31, 2)\n\n# Create a list of xtick labels\nxtick_labels = np.arange(1, ", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.view_init(azimuth=100, elevation=50)\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.tick_params(axis='both', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[1, 1], height_ratios=[1, 1])\n\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.imshow(x[i])\n    ax.axis('off')\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nprint(x.numpy()) # prints: 1\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nprint(x.numpy())", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\nlabels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, depth=10)\nresult = tf.cast(result, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\nlabels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, 10)\nresult = tf.cast(result, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "labels = tf.constant([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n                      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n                      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n                      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                      [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]])\n\nresult = tf.cast(labels, tf.int32)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    # Create a tensor of zeros with the same shape as the labels\n    result = tf.zeros_like(labels)\n\n    # Set the values of the result tensor to the corresponding values in the labels tensor\n    result = tf.tensor_scatter_nd(result, tf.where(labels[:, None] >= 0), 1)\n\n    # Return the result tensor\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\nlabels = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n                      [1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n                      [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n                      [1, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n                      [1, 1, 1, 1, 1, 1, 1, 0, 1, 1]])\n\nresult = tf.math.greater(labels, 0)\nresult = tf.cast(result, tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return tf.constant([i, i+1, i+2])\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: my_map_func(input))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return tf.concat([input, input + 1, input + 2], axis=0)\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(f)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nresult = tf.zeros_like(lengths)\n\n#", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the same shape as the lengths tensor\nmask = tf.ones(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.zeros(tf.stack([lengths.shape[0], 8]))\n\n# Create a tensor of ones with the", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    max_length = tf.reduce_max(lengths)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\n    result = tf.cast(mask, tf.int32)\n    ### END SOLUTION ###\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n\n# Create a tensor of zeros with the same shape as the lengths tensor\nmask = tf.ones(tf.shape(lengths))\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([tf.reshape(a, [-1, 1]), tf.reshape(b, [-1, 1])], axis=1)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    return tf.stack([tf.repeat(a, len(b))]*len(b), axis=0)\n\nresult = f(example_a, example_b)\nprint(result)\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "result = tf.reshape(a, (50, 100, 512))\nprint(result)\n", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "result = tf.reshape(a, (50, 100, 1, 512))\nprint(result)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "result = tf.reshape(a, (1, 50, 100, 1, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1, 50, 100, 512))\nresult = tf.reshape(result, (1,", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)\nresult = result.numpy()\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)\nprint(result)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\n# calculate the L2 distance d(A,B) element-wise\nresult = tf.square(a - b)\n\n# reduce by row\nresult = tf.reduce_sum(result, axis=0)\n\nprint(result)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "result = tf.square(a - b)\nresult = tf.reduce_sum(result, axis=0)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    return tf.square(tf.sub(A, B))\n\nresult = f(A, B)\nprint(result)\nprint(tf.reduce_sum(result, axis=0))\nprint(tf.reduce_sum(result, axis=1))\nprint(tf.reduce_sum(result, axis=2))\nprint(tf.reduce_sum(result, axis=0, keepdims=True))\nprint(tf.reduce_sum(result, axis=1, keepdims=True))\nprint(tf.reduce_sum(result, axis=2, keepdims=True))\nprint(tf.reduce_sum(result, axis=0, keepdims=True, axis=1))\nprint(tf.reduce_sum(result, axis=1, keepdims=True, axis=1))\nprint(tf.reduce_sum(result, axis=2, keepdims=True, axis=1))\nprint(tf.reduce_sum(result, axis=0, keepdims=True, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=1, keepdims=True, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=2, keepdims=True, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=0, keepdims=True, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=1, keepdims=True, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=2, keepdims=True, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=0, keepdims=True, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=1, keepdims=True, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=2, keepdims=True, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=0, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=1, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=2, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=0, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=1, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=2, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=0, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=1, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=2, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=0, keepdims=True, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1, axis=1))\nprint(tf.reduce_sum(result, axis=1, keepdims=True, axis=1", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "result = tf.gather_nd(x, tf.stack([y, z], axis=1))\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = tf.gather_nd(x, tf.stack([row, col], axis=1))\nresult = m.numpy()\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    return x[y,z]\n\nresult = f(example_x, example_y, example_z)\nprint(result)\n### END SOLUTION #!!!", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\nresult = tf.einsum('ijk,ijk->ijk', A, B)\nresult = tf.math.multiply(A, B)\nresult = tf.reduce_sum(result, axis=2)\n\nprint(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "result = tf.einsum('ijk,ik->ijk', A, B)\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\nx = tf.constant(x)\nresult = tf.strings.unicode_transpose(x)\n\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    # Convert the bytes to strings\n    result = tf.strings.unicode_transpose(x)\n    # # Convert the strings to lower case\n    # result = tf.strings.lower(result)\n    # # Return the result\n    # return result\n    # # END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "\n# Create a mask where the last dimension is not zero\nmask = tf.math.not_equal(x, 0)\n\n# Sum the non-zero values in the last dimension\nsum_last_dimension = tf.reduce_sum(x, axis=-1, keepdims=True)\n\n# Divide the sum by the number of non-zero values in the last dimension\nresult = tf.where(mask, sum_last_dimension / tf.math.cast(tf.math.count_nonzero(mask, axis=-1), tf.float32), x)\n\nprint(result)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "x_nonzero = tf.where(x != 0, x, tf.zeros_like(x))\nresult = tf.reduce_mean(x_nonzero, axis=-1)\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\nresult = tf.where(x != 0, result, tf.zeros_like(result))\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": " ###\n    # get the non-zero entries\n    non_zero_entries = tf.where(x != 0, x, tf.ones_like(x))\n    # calculate the average\n    result = tf.reduce_sum(x, axis=-1) / tf.reduce_sum(non_zero_entries, axis=-1)\n    # return the result\n    return result\n\nprint(f(example_x))\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\n\nwith tf.compat.v1.Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "result = tf.argmax(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "result = tf.argmax(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    # get the maximum value in each row\n    max_values = tf.argmax(a, axis=1)\n    # return the result\n    return max_values\n\nresult = f(example_a)\nprint(result)\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1)\nprint(result)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save(\"my_model\")\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\n# Define the seed\ntf.random.set_seed(seed_x)\n\n# Define the distribution\ndistribution = tf.distributions.Categorical(probs=[0.1, 0.2, 0.3, 0.4])\n\n# Generate the tensor\nresult = distribution.sample(10)\n\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "\n# Define the seed\ntf.random.set_seed(seed_x)\n\n# Define the range of values\nrange_values = [2, 3, 4, 5]\n\n# Generate the random integers\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=5, dtype=tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": " ###\n    tf.random.set_seed(seed_x)\n    random_tensor = tf.random.uniform(shape=(10,), minval=1, maxval=4)\n    return random_tensor\n\nresult = f(seed_x=10)\nprint(result)\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "import tensorflow as tf\n\n# get the version of tensorflow\nresult = tf.__version__\nprint(result)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "result = scipy.polyfit(x, y, 1)\nresult = np.array([result[1], result[0]])\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the polynomial of order 1\nA, B = np.polyfit(x, y, 1)\nresult = np.array([A, B])\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, A, B, C):\n    return A * np.exp(B * x) + C\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array([result[0], result[1], result[2]])\nprint(result)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c - 2) ** 2 + (3 * a - b - c) ** 2 + sin(b) + cos(b) + 4)\n\ndef df(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, sin(b) + cos(b) + 4]\n\ndef ddf(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, 2 * sin(b) + cos(b) + 4]\n\ndef df_df(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, (sin(b) + cos(b) + 4) * 2]\n\ndef df_ddf(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, (sin(b) + cos(b) + 4) * 2, 2 * sin(b) + cos(b) + 4]\n\ndef df_df_df(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, 2 * sin(b) + cos(b) + 4]\n\ndef df_df_ddf(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, 2 * sin(b) + cos(b) + 4]\n\ndef df_df_df_df(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, 2 * sin(b) + cos(b) + 4]\n\ndef df_df_df_ddf(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, 2 * sin(b) + cos(b) + 4]\n\ndef df_df_df_df_df(x):\n    a, b, c = x\n    return [(a + b - c - 2) * 2, (3 * a - b - c) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, (sin(b) + cos(b) + 4) * 2, 2 * sin(b) + cos(b) + 4]\n\ndef df_df_df_df_ddf(x):\n   ", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.binomial(n=1, p=0.5).pmf(z_scores)\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = scipy.stats.norm.cdf(z_scores, mu, sigma)\np_values = 1 - p_values\np_values", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = np.array([scipy.stats.zscore(p_values[i]) for i in range(len(p_values))])\n", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(stddev, scale=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb\nprint(result)\n", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    result = sA * sB\n    return result\n\nprint(f())\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid from the points\nx, y, z = np.mgrid[np.min(points[:, 0]):np.max(points[:, 0]):100j,\n                   np.min(points[:, 1]):np.max(points[:, 1]):100j,\n                   np.min(points[:, 2]):np.max(points[:, 2]):100j]\n\n# Create a LinearNDInterpolator from the points\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\n\n# Get the interpolated value for the request point\nresult = interpolator(request)\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid of points\nx, y, z = np.mgrid[0:3:100j, 0:3:100j, 0:3:100j]\n\n# Create a LinearNDInterpolator object\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\n\n# Get the interpolated values\nresult = interpolator(request)\nprint(result)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, reshape=False)\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = M.diagonal()", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, 'uniform')\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # compute the K-S test\n    result = stats.kstest(times, 'uniform')\n    # return the result\n    return result\n\n# call the function\nresult = f(example_times, example_rate, example_T)\nprint(result)\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, 'uniform')\nprint(result.pvalue)", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = c1.reshape(1, -1)\nFeature = Feature.append(c2.reshape(1, -1))\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.reshape(-1, 1)\nFeature = Feature.", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "# Create a distance matrix between points\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are not the same and 0 otherwise\nmatching = np.where(np.abs(points1 - points2) > 1e-6, 1, 0)\n# Use scipy.optimize.linear_sum_assignment to find the best assignment of points\nresult = scipy.optimize.linear_sum_assignment(distances, matching)\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "# Create a distance matrix between points\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary matrix where each row is a pair of points and the value is 1 if the points are the same, 0 otherwise\nsame_points = np.equal(points1, points2)\n# Create a binary", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\nb = sparse.csr_matrix(b.toarray())\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "result = np.where(img > threshold, 1, 0)\nresult = ndimage.label(result)[0]\nresult = np.unique(result, return_counts=True)[1]\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result > 0, 1, 0)\nresult = np.where(result", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "result = np.where(img < threshold, 1, 0)\nresult = ndimage.label(result)[0]\nresult = np.unique(result, return_counts=True)[1]\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result > 1, 1, 0)\nresult = np.where(result", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use the ndimage.label function to count the number of regions of cells which value exceeds a given threshold\n    labels = ndimage.label(img > threshold)\n    # Return the number of regions\n    return labels[1].max()\n    ### END SOLUTION ###\n\nprint(f(img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\n# Find the regions of cells which value exceeds a given threshold\nmask = img > threshold\n\n# Find the center of mass of such regions\ncenter_of_mass = ndimage.center_of_mass(img, mask)\n\n# Find the distance between the center of mass of such regions and the top left corner\ndistance = np.sqrt((center_of_mass[0] - 0)**2 + (center_of_mass[1] - 0)**2)\n\n# Store the distance in the result list\nresult = distance.tolist()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "\n# Create a symmetric matrix\nsA = M.copy()\nsA[sA.transpose()] = sA\n\n# Make the matrix symmetric\nsA = sA + sA.transpose()\nsA = sA.tocsr()\n\n# Print the symmetric matrix\nprint(sA)\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    # Create a new lil_matrix with the same shape as sA\n    sA_symmetric = lil_matrix(sA.shape)\n\n    # Iterate over the elements of sA\n    for i in range(sA.shape[0]):\n        for j in range(sA.shape[1]):\n            # If the elements are not the same, set the value in the symmetric matrix\n            if i != j and sA[i, j] != 0:\n                sA_symmetric[j, i] = sA[i, j]\n\n    # Return the symmetric matrix\n    return sA_symmetric\n\n# Call the function\nsA_symmetric = f(example_sA)\nprint(sA_symmetric)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\n# Create a binary array with the same shape as the original array\nbinary = np.zeros_like(square)\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Erode the binary array to remove small features\nbinary = scipy.ndimage.binary_erosion(binary, 1)\n\n# Dilate the binary array to remove large features\nbinary = scipy.ndimage.binary_dilation(binary, 1)\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set the values of the binary array to 1 where the original array is 1\nbinary[square == 1] = 1\n\n# Set the values of the binary array to 0 where the original array is 0\nbinary[square == 0] = 0\n\n# Set", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\n# Create a mask of the square\nmask = np.zeros(square.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the square\neroded = scipy.ndimage.erosion(square, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded = scipy.ndimage.erosion(dilated, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded = scipy.ndimage.erosion(dilated, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded = scipy.ndimage.erosion(dilated, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded = scipy.ndimage.erosion(dilated, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded = scipy.ndimage.erosion(dilated, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded = scipy.ndimage.erosion(dilated, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded = scipy.ndimage.erosion(dilated, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded = scipy.ndimage.erosion(dilated, mask)\n\n# Dilate the eroded square\ndilated = scipy.ndimage.dilation(eroded, mask)\n\n# Create a mask of the dilated square\nmask = np.zeros(dilated.shape, dtype=bool)\nmask[10:-10, 10:-10] = True\n\n# Erode the dilated square\neroded", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_value = np.max(col.toarray())\nmin_value = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Convert the column to a numpy array\ncol_array = col.toarray()\n\n# Get the median and mode\nmedian_value = np.median(col_array)\nmode_value = np.mode(col_array)\n\n# Print the median and mode\nprint(\"Median: \", median_value)\nprint(\"Mode: \", mode_value)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *args):\n    return sum(a * np.cos(np.pi * x * tau * n) for n, a in enumerate(args))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\narray_1d = example_array.flatten()\n# Calculate the pairwise Euclidean distances\ndistances = scipy.spatial.distance.pdist(array_1d, 'euclidean')\n# Convert the distances to a 2D array\ndistances_2d = np.reshape(distances, (len(array_1d), len(array_1d)))\n# Create a 2D array with the from ID, to ID, distance\nresult = np.array([(i, j, distances_2d[i, j]) for i in range(len(array_1d)) for j in range(len(array_1d)) if i != j])\n# Convert the distances from meters to pixels\nresult[:, 2] *= example_array.shape[0]\nresult[:, 2] *= example_array.shape[1]\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Create a 2D array with the ID of the regions and the distance between them\nresult = np.array([[i, j, np.abs(i - j)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n\n# Calculate the Manhattan distance between all regions\nresult = scipy.spatial.distance.cdist(result[:, 0:2], result[:, 0:2], metric='cityblock')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_array[i, j] != 0])\n    # Create a 2D array with the ID of the region and the ID of the nearest region\n    result = np.array([[i, j, np.sqrt((i - j) ** 2)] for i in range(example_array.shape[0]) for j in range(example_array.shape[1]) if example_", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "result = np.empty((5, 100))\nfor i in range(5):\n    tck, u = interpolate.splprep([x[i], x[-1, i]], y[i], s=4)\n    result[i] = interpolate.splev(x_val, tck)\nresult", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nx3 = np.array(x3)\nx4 = np.array(x4)\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\nprint(statistic)\nprint(critical_values)\nprint(significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nresult = ss.anderson_ksamp(x1, x2)\nprint(result)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(df['A']) # keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['AC'] = df['A'].rolling(3).apply(lambda x: tau1(x))\ndf['BC'] = df['B'].rolling(3).apply(lambda x: tau1(x))\n\nprint(df)\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nresult = is_csr_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n\nresult = is_lil_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores)[1]", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n\nprint(f())\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "import scipy.stats\n\nkurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interpolated = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interpolated(x, y)\nprint(result)\n", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a grid of points\n    x_grid, y_grid = np.meshgrid(s, t, indexing='ij')\n    # Create a function that returns the solution\n    def result(s, t):\n        return z * np.exp(-6.0 * (s * s + t * t))\n    # Create a function that returns the solution\n    def solution(s, t):\n        return result(s, t)\n    # Create a function that returns the solution\n    def solution_interpolated(s, t):\n        return scipy.interpolate.interp2d(s, t, solution)(s, t)\n    # Create a function that returns the solution\n    def solution_interpolated_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s, t)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_grid)\n    # Create a function that returns the solution\n    def solution_interpolated_grid_grid_grid(s, t):\n        return scipy.interpolate.interp2d(s_grid, t_grid, solution)(s_grid, t_", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "import numpy as np\n\n# Create a numpy array with the extra points\nextraPoints = np.array(extraPoints)\n\n# Create a numpy array with the voronoi regions\nregions = vor.regions\n\n# Create a numpy array with the voronoi vertices\nvertices = vor.vertices\n\n# Create a numpy array with the voronoi points\npoints = vor.points\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi points\npoints = np.array(points)\n\n# Create a numpy array with the voronoi", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "import numpy as np\n\n# Create a numpy array of the extra points\nextraPoints = np.array(extraPoints)\n\n# Create a numpy array of the points in the voronoi tessellation\npoints = np.array(points)\n\n# Create a numpy array of the voronoi regions\nregions = vor.regions\n\n# Create a numpy array of the voronoi vertices\nvertices = vor.vertices\n\n# Create a numpy array of the voronoi cells\ncells = vor.points\n\n# Create a numpy array of the voronoi cell indices\ncellIndices = vor.point_to_cell\n\n# Create a numpy array of the voronoi cell regions\ncellRegions = vor.point_to_cell_region\n\n# Create a numpy array of the voronoi cell vertices\ncellVertices = vor.point_to_cell_vertices\n\n# Create a numpy array of the voronoi cell cells\ncellCells = vor.point_to_cell_points\n\n# Create a numpy array of the voronoi cell cell indices\ncellCellIndices = vor.point_to_cell_point\n\n# Create a numpy array of the voronoi cell cell regions\ncellCellRegions = vor.point_to_cell_point_region\n\n# Create a numpy array of the voronoi cell cell vertices\ncellCellVertices = vor.point_to_cell_point_vertices\n\n# Create a numpy array of the voronoi cell cell cells\ncellCellCells = vor.point_to_cell_point_points\n\n# Create a numpy array of the voronoi cell cell cell indices\ncellCellCellIndices = vor.point_to_cell_point_point\n\n# Create a numpy array of the voronoi cell cell cell regions\ncellCellCellRegions = vor.point_to_cell_point_point_region\n\n# Create a numpy array of the voronoi cell cell cell vertices\ncellCellCellVertices = vor.point_to_cell_point_point_vertices\n\n# Create a numpy array of the voronoi cell cell cell cells\ncellCellCellCells = vor.point_to_cell_point_point_points\n\n# Create a numpy array of the voronoi cell cell cell cell indices\ncellCellCellCellIndices = vor.point_to_cell_point_point_point\n\n# Create a numpy array of the voronoi cell cell cell cell regions\ncellCellCellCellRegions = vor.point_to_cell_point_point_point_region\n\n# Create a numpy array of the voronoi cell cell cell cell vertices\ncellCellCellCellVertices = vor.point_to_cell_point_point_point_vertices\n\n# Create a numpy array of the voronoi cell cell cell cell cells\ncellCellCellCellCells = vor.point_to_cell_point_point_point_points\n\n# Create a numpy array of the voronoi cell cell cell cell cell indices\ncellCellCellCellCellIndices = vor.point_to_cell_point_point_point_point\n\n# Create a numpy array of the voronoi cell cell cell cell cell regions\ncellCellCellCellCellRegions = vor.point_to_cell_point_point_point_point_region\n\n# Create a numpy array of the voronoi cell cell cell cell cell vertices\ncellCellCellCellCellVertices = vor.point_to_cell_point_point_point_point_vertices\n\n# Create a numpy array of the voronoi cell cell cell cell cell cells\ncellCellCellCellCellCells = vor.point_to_cell_point_point_point_point_points\n\n# Create a numpy array of the voronoi cell cell cell cell cell cell indices\ncellCellCellCellCellCellIndices = vor.point_to_cell_point_point_point_point_point\n\n# Create a numpy array of the voronoi cell cell cell cell cell cell regions\ncellCellCellCellCellCellRegions = vor.point_to_cell_point_point_point_point_point_region\n\n# Create a numpy array of the voronoi cell cell cell cell cell cell", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "max_size = max([len(v) for v in vectors])\nresult = sparse.csc_matrix(np.array(vectors).reshape(len(vectors), max_size))\nresult = sparse.csc_matrix(np.pad(result.toarray(), ((0, 0), (0, max_vector_size - max_size)), 'constant'))\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=[1, 1])\nprint(b)\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[row[i], column[i]] for i in range(len(row))]\nresult = np.array(result)\nprint(result)\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, :, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u,o2,x)\nprint(prob)", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n\nprint(f(x = 2.5, u = 1, o2 = 3))\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a 2D array with random values\nA = np.random.rand(N, N)\n\n# Compute the DCT matrix\ndct_matrix = sf.dctn(A, norm='ortho')\n\n# Compute the inverse DCT matrix\ninv_dct_matrix = sf.idctn(dct_matrix, norm='ortho')\n\n# Compare the original array with the inverse DCT matrix\nresult = np.allclose(A, inv_dct_matrix)", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "result = sparse.diags(matrix, [-1, 0, 1], shape=(5, 5)).toarray()\nprint(result)\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "result = np.array([[scipy.stats.binom.pmf(i, N, p) for i in range(N+1)] for j in range(N+1)])\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x), axis=1)\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x))\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "result = df.reset_index().assign(data=df.values, zscore=stats.zscore(df.values))\nresult", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "df['data'] = df.apply(lambda x: np.round(x, 3), axis=1)\ndf['zscore'] = stats.zscore(df['data'])\nresult = df[['data', 'zscore']]\nresult", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y = np.arange(shape[0])\nx = np.arange(shape[1])\nmid = np.array([[0, 0], [shape[0]-1, shape[1]-1]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\nmid = np.array([[5., 4., 3., 3., 4., 5.],\n               [4., 3., 2., 2., 3., 4.],\n               [3., 2., 1., 1., 2., 3.],\n               [3., 2., 1., 1., 2., 3.],\n               [4., 3., 2., 2., 3., 4.],\n               [5., 4., 3., 3., 4., 5.]])\nresult = distance.cdist(np.dstack((y, x)), mid)\nprint(result)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.mgrid[0:shape[0], 0:shape[1]]\n    mid = np.stack((x, y), axis=2)\n    return distance.cdist(mid, mid, 'euclidean')\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\n\ndef main():\n    x0 = np.array([2, 3, 1, 4, 20])\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = scipy.optimize.minimize(residual, fit_params.values, args=(a, y))\n    print(out.x)\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return (y - func(x)) ** 2\n\nout = scipy.optimize.minimize(residual, x0, method='L-BFGS-B', bounds=[x_lower_bounds])\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) * (t < 2*np.pi)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])\nprint(result)", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "result = sparse.vstack([sa, sb])\nresult", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\n# load data\nc = np.loadtxt('cooltemp.dat')\n\n# create an empty array to store the solutions\nI = np.zeros(len(c))\n\n# loop over the data\nfor n in range(len(c)):\n    # equation\n    eqn = 2*c[n]*x\n    # integrate \n    result, error = scipy.integrate.quad(eqn, 0, 1)\n    I[n] = result\n\nprint(I)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n\n# create an array of c values\nc = array([5, 10, 15])\n\n# create an array to store the solutions\nI = array([])\n\n# loop over c values\nfor n in range(len(c)):\n    # calculate the solution\n    result = f(c=c[n])\n    # append the solution to the array\n    I = append(I, result)\n\n# print the solutions\nprint(I)\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "V += x\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V += x", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V += x\nV += y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor Col in range(sa.shape[1]):\n    # extract column\n    Column = sa[:, Col].data\n    # calculate length\n    Len = math.sqrt(np.sum(Column**2))\n    # normalize column\n    Column = Column / Len\n    # update original column\n    sa[:, Col] = Column\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].toarray()\n    # calculate the length of the column\n    length = np.sqrt(np.sum(column**2))\n    # normalize the column\n    column /= length\n    # update the original column in the matrix\n    sa[:, col] = column\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = np.where(a > 0, 1, 0)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = np.where(a == 0, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# Create a distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# Find the index of the closest element to each centroid\nresult = np.argmin(distances, axis=1)\nprint(result)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# Create a distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# Find the closest point to each cluster\nclosest_points = np.argmin(distances, axis=1)\n# Extract the closest point to each cluster\nresult = data[closest_points]\n", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# Create a distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# Create a linkage matrix\nlinkage = scipy.spatial.hierarchy.linkage(distances, method='ward')\n# Use the cut_tree function to get the cluster indices\ncluster_indices = scipy.spatial.hierarchy.cut_tree(linkage, n_clusters=k)\n# Create a dictionary to store the indices of the k-closest elements to each cluster\nresult = {}\nfor i in range(k):\n    # Get the indices of the k-th closest element to the centroid of cluster i\n    cluster_indices_i = np.where(cluster_indices == i)[0]\n    # Get the k-th closest element to the centroid of cluster i\n    closest_elements = data[cluster_indices_i[:k]]\n    # Store the indices of the k-closest elements to cluster i\n    result[i] = cluster_indices_i[:k]\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result.append(fsolve(eqn, x0=x, args=(a, b)))\nresult = np.array(result)\nprint(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    result.append(fsolve(eqn, x0=x, args=(a,)))\nresult = np.array(result)\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:, 0] > result[:, 1]]\nresult = result[result[:", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Define the distribution function\ndef bekkers_dist(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Fit the function to the data\nparams, params_cov = sp.optimize.curve_fit(bekkers_func, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nresult = sp.stats.kstest(sample_data, bekkers_dist, params)\nprint(result)", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Fit the function to the sample data\nparams, params_cov = sp.optimize.curve_fit(bekkers_func, range_start, range_end, p0=[estimated_a, estimated_m, estimated_d])\n\n# Create a distribution function from the fitted parameters\ndistribution_func = lambda x: bekkers_func(x, *params)\n\n# Perform the K-S test\nresult = stats.kstest(sample_data, distribution_func)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def rolling_integral(x, y):\n    return integrate.trapz(y, x)\n\ndf['Time'] = pd.to_datetime(df['Time'])\ndf['Time'] = df['Time'].apply(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\n\nintegral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral)\nintegral_df['Time'] = df['Time']\nintegral_df.set_index('Time', inplace=True)\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.index = integral_df.index.map(lambda x: x.replace(hour=x.hour, minute=x.minute, second=x.second, microsecond=x.microsecond))\nintegral_df.index.name = 'Time'\nintegral_df.", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "# Create a 2D array with the x and y coordinates\nx_y = np.array([x, y]).reshape(2, -1)\n\n# Create a 2D array with the evaluation points\neval_x_y = np.array([eval]).reshape(1, -1)\n\n# Use griddata to interpolate the value at the evaluation points\nresult = scipy.interpolate.griddata(x_y, y, eval_x_y, method='linear')\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "def multinomial_distribution(x, weights):\n    return np.sum(weights * x)\n\ndef multinomial_log_likelihood(weights, x):\n    return np.sum(multinomial_distribution(x, weights) * x)\n\ndef multinomial_log_likelihood_gradient(weights, x):\n    return np.sum(multinomial_distribution(x, weights) * x)\n\nweights = sciopt.minimize(multinomial_log_likelihood, weights, method='NELDER-MADAN', jac=multinomial_log_likelihood_gradient, options={'maxiter': 1000})['x']\nweights", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "result = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nprint(result)\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "result = np.where((arr[:-n] <= arr[n:]) | (arr[:-n] >= arr[n:]))[0]\nresult = np.concatenate((result, np.where((arr[-n:] <= arr[:-n]) | (arr[-n:] >= arr[:-n]))[0])\nresult = np.concatenate((result, np.where((arr[-n:] <= arr[-n-1]) | (arr[-n:] >= arr[-n-1]))[0])\nresult = np.concatenate((result, np.where((arr[-n-1:] <= arr[-n-2]) | (arr[-n-1:] >= arr[-n-2]))[0])\nresult = np.concatenate((result, np.where((arr[-n-2:] <= arr[-n-3]) | (arr[-n-2:] >= arr[-n-3]))[0])\nresult = np.concatenate((result, np.where((arr[-n-3:] <= arr[-n-4]) | (arr[-n-3:] >= arr[-n-4]))[0])\nresult = np.concatenate((result, np.where((arr[-n-4:] <= arr[-n-5]) | (arr[-n-4:] >= arr[-n-5]))[0])\nresult = np.concatenate((result, np.where((arr[-n-5:] <= arr[-n-6]) | (arr[-n-5:] >= arr[-n-6]))[0])\nresult = np.concatenate((result, np.where((arr[-n-6:] <= arr[-n-7]) | (arr[-n-6:] >= arr[-n-7]))[0])\nresult = np.concatenate((result, np.where((arr[-n-7:] <= arr[-n-8]) | (arr[-n-7:] >= arr[-n-8]))[0])\nresult = np.concatenate((result, np.where((arr[-n-8:] <= arr[-n-9]) | (arr[-n-8:] >= arr[-n-9]))[0])\nresult = np.concatenate((result, np.where((arr[-n-9:] <= arr[-n-10]) | (arr[-n-9:] >= arr[-n-10]))[0])\nresult = np.concatenate((result, np.where((arr[-n-10:] <= arr[-n-11]) | (arr[-n-10:] >= arr[-n-11]))[0])\nresult = np.concatenate((result, np.where((arr[-n-11:] <= arr[-n-12]) | (arr[-n-11:] >= arr[-n-12]))[0])\nresult = np.concatenate((result, np.where((arr[-n-12:] <= arr[-n-13]) | (arr[-n-12:] >= arr[-n-13]))[0])\nresult = np.concatenate((result, np.where((arr[-n-13:] <= arr[-n-14]) | (arr[-n-13:] >= arr[-n-14]))[0])\nresult = np.concatenate((result, np.where((arr[-n-14:] <= arr[-n-15]) | (arr[-n-14:] >= arr[-n-15]))[0])\nresult = np.concatenate((result, np.where((arr[-n-15:] <= arr[-n-16]) | (arr[-n-15:] >= arr[-n-16]))[0])\nresult = np.concatenate((result, np.where((arr[-n-16:] <= arr[-n-17]) | (arr[-n-16:] >= arr[-n-17]))[0])\nresult = np.concatenate((result, np.where((arr[-n-17:] <= arr[-n-18]) | (arr[-n-17:] >= arr[-n-18]))[0])\nresult = np.concatenate((result, np.where((arr[-n-18:] <= arr[-n-19]) | (arr[-n-18:] >= arr[-n-19]))[0])\nresult = np.concatenate((result, np.where((arr[-n-19:] <= arr[-n-20]) | (arr[-n-19:] >= arr[-n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "result = []\nfor i in range(arr.shape[0]):\n    for j in range(arr.shape[1]):\n        if j < n:\n            if arr[i, j] <= arr[i, j-n]:\n                result.append([i, j])\n        elif j > arr.shape[1] - n:\n            if arr[i, j] <= arr[i, j-n]:\n                result.append([i, j])\n        elif arr[i, j] <= arr[i, j-n] and arr[i, j] <= arr[i, j+n]:\n            result.append([i, j])\nresult = np.array(result)\nresult = result.tolist()\nresult", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "# Create a mask for the numerical columns\nnum_cols = ['NUM1', 'NUM2', 'NUM3']\nnum_mask = df[num_cols].apply(lambda x: np.abs(stats.zscore(x)) < 3, axis=1)\n\n# Create a mask for the categorical columns\ncat_cols = ['CAT1', 'CAT2', 'CAT3']\ncat_mask = df[cat_cols].apply(lambda x: x.isin(['pandas', 'r', 'julia', 'sas', 'stata', 'spss']), axis=1)\n\n# Create a mask for the combined columns\ncombined_mask = num_mask & cat_mask\n\n# Remove the outlying values\ndf = df[combined_mask]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], data['target']))\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + data['target_names'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], 'target'))\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                      columns=data['feature_names'] + ['target'])\n    return df\n\n# Call the function\ndata1 = solve(data)\nprint(data1)\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# Create a dataframe with unique elements from Col3\nunique_elements = pd.DataFrame(np.unique(df['Col3']), columns=['Apple', 'Orange', 'Banana', 'Grape'])\n\n# Create a dataframe with zeros with the same shape as df\nzero_df = unique_elements.copy()\nzero_df.loc[:, 'Apple'] = zero_df.loc[:, 'Apple'] == 0\nzero_df.loc[:, 'Orange'] = zero_df.loc[:, 'Orange'] == 0\nzero_df.loc[:, 'Banana'] = zero_df.loc[:, 'Banana'] == 0\nzero_df.loc[:, 'Grape'] = zero_df.loc[:, 'Grape'] == 0\n\n# Create a dataframe with ones with the same shape as df\none_df = unique_elements.copy()\none_df.loc[:, 'Apple'] = one_df.loc[:, 'Apple'] == 1\none_df.loc[:, 'Orange'] = one_df.loc[:, 'Orange'] == 1\none_df.loc[:, 'Banana'] = one_df.loc[:, 'Banana'] == 1\none_df.loc[:, 'Grape'] = one_df.loc[:, 'Grape'] == 1\n\n# Create a dataframe with zeros and ones\ndf_out = zero_df.copy()\ndf_out.loc[:, 'Apple'] = df['Col3'].apply(lambda x: 1 if 'Apple' in x else 0)\ndf_out.loc[:, 'Orange'] = df['Col3'].apply(lambda x: 1 if 'Orange' in x else 0)\ndf_out.loc[:, 'Banana'] = df['Col3'].apply(lambda x: 1 if 'Banana' in x else 0)\ndf_out.loc[:, 'Grape'] = df['Col3'].apply(lambda x: 1 if 'Grape' in x else 0)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# Create a list of unique names\nnames = np.unique(np.concatenate(df['Col3'].values))\n\n# Create a dataframe with the unique names as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(names))), columns=names)\n\n# Set the values to 1 for the names that exist in the row\nfor i in range(df.shape[0]):\n    for j in range(len(names)):\n        if names[j] in df.iloc[i, 2]:\n            df_out.iloc[i, j] = 1\n\n# Print the dataframe\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col4'].unique()\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col4'], values=unique_elements, drop_first=True)\n\n# Add the one-hot-encoded values to the original dataframe\ndf_out = pd.concat([df, df_out], axis=1)\n\n# Drop the original column\ndf_out = df_out.drop(['Col4'], axis=1)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df.iloc[:, -1].unique()\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df.iloc[:, -1], values=unique_elements)\n\n# Add the one-hot-encoded values to the original dataframe\ndf_out = df.join(df_out)\n\n# Drop the last column\ndf_out = df_out.drop(columns=df.columns[-1])\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# Create a list of unique elements in the last column\nunique_elements = df['Col3'].unique()\n\n# Create a dataframe with one-hot-encoded values\ndf_out = pd.get_dummies(df['Col3'], values=unique_elements, drop_first=True)\n\n# Add the one-hot-encoded values to the original dataframe\ndf_out = df.join(df_out)\n\n# Drop the original column\ndf_out = df_out.drop(['Col3'], axis=1)\n\n# Print the result\nprint(df_out)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\nproba = np.array([1 / (1 + np.exp(-x)) for x in svmmodel.decision_function(x_test)])\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\nproba = np.array([1 / (1 + np.exp(-x)) for x in proba[:, 1]])\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin\ndf_origin = csr_matrix(df_origin)\n\n# Create a sparse matrix from the transform_output\ntransform_output = csr_matrix(transform_output)\n\n# Create a sparse matrix from the df_origin", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "transform_output = transform_output.toarray()\ntransform_output = csr_matrix(transform_output)\n\ndf_origin = pd.DataFrame(df_origin)\n\ndf = pd.concat([df_origin, transform_output], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # Create a new dataframe with the original data and the transformed output\n    new_df = pd.concat([df, transform_output], axis=1)\n    # Return the new dataframe\n    return new_df\n\n# Call the function with your data\nresult = solve(df_origin, transform_output)\nprint(result)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete a step\ndel clf.steps[1]\n# Insert a step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.append(('delete_step', SVC()))\nclf.steps.append(('insert_step', SVC()))\nprint(clf.steps)\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\nclf.steps.append(('new_step', SVC()))\nclf.steps.append(('another_step', PolynomialFeatures()))\nprint(clf.steps)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps.append(('new_step', SVC()))\nclf.steps.append(('new_step_2', PolynomialFeatures()))\nprint(clf.steps)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PCA()))\nclf.steps.insert(2, ('t1919810', PolynomialFeatures()))\nclf.steps.insert(3, ('t1919810', SVC()))\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# Define the parameter grid\nparamGrid = {\n    \"early_stopping_rounds\": [10, 20, 30, 40],\n    \"learning_rate\": [0.01, 0.1, 0.2, 0.3],\n    \"max_depth\": [2, 3, 4, 5],\n    \"n_estimators\": [100, 200, 300, 400]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the GridSearchCV\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid)\n\n# Fit the GridSearchCV\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the best iteration\nbest_iteration = gridsearch.best_iteration_\n\n# Get the prediction\nc = gridsearch.predict(testX)\n\n# Get the score\nb = gridsearch.score(testX, testY)\n\n# Print the best parameters, best score, best iteration and prediction\nprint(\"Best parameters: \", best_params)\nprint(\"Best score: \", best_score)\nprint(\"Best iteration: \", best_iteration)\nprint(\"Prediction: \", c)\nprint(\"Score: \", b)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\n# Define the parameter grid\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n    'max_depth': [3, 4, 5, 6],\n    'n_estimators': [100, 200, 300, 400]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the GridSearchCV\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n\n# Fit the GridSearchCV\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the prediction\nc = gridsearch.predict(testX)\n\n# Get the score on the test set\nb = gridsearch.score(testX, testY)\n\n# Print the best parameters, best score, and prediction\nprint(\"Best parameters: \", best_params)\nprint(\"Best score: \", best_score)\nprint(\"Prediction: \", c)\nprint(\"Score on the test set: \", b)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_predicted = scaler.inverse_transform(scaled[:, 0])\n    # return t_predicted\n    return t_predicted\n\n# solve(data, scaler, scaled)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\nclf = grid_search.best_estimator_\nproba = clf.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X, y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef solve():\n    # create a example variable\n    tfidf = TfidfVectorizer(processor=preprocess)\n\n    # create a data frame\n    df = pd.DataFrame({'text': ['This is a test', 'Another test', 'Yet another test']})\n\n    # fit the vectorizer\n    tfidf.fit(df['text'])\n\n    # print the transformed text\n    print(tfidf.transform(df['text']))\n\nsolve()\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# example variable\ntext = [\"Hello World\", \"Python is a great language\"]\n\n# create a dataframe\ndf = pd.DataFrame(text, columns=[\"text\"])\n\n# fit the vectorizer\ntfidf.fit(df[\"text\"])\n\n# print the transformed text\nprint(tfidf.transform(df[\"text\"]))\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Convert the DataFrame to numpy array\ndata_array = data.values\n\n# Apply preprocessing.scale to the array\nscaled_array = preprocessing.scale(data_array)\n\n# Convert the scaled array back to a DataFrame\nscaled_data = pd.DataFrame(scaled_array, columns=data.columns)\n\n# Save the scaled DataFrame\ndf_out = scaled_data\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Create a numpy array from the DataFrame\narray = data.values\n\n# Apply preprocessing.scale to the array\narray = preprocessing.scale(array)\n\n# Create a new DataFrame from the scaled array\ndf_out = pd.DataFrame(array, columns=data.columns)\n\n# Print the new DataFrame\nprint(df_out)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "column_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "column_names = np.array([X.columns[i] for i in np.where(clf.feature_importances_ > 0.05)[0]])\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = X.columns[model.get_support()]\nprint(column_names)", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]\nclosest_50_samples = closest_50_samples[:50]\n", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[km.labels_ == p]\nclosest_50_samples = closest_50_samples[:50]\n", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]\nclosest_100_samples = closest_100_samples.reshape(-1, 1)\nclosest_100_samples = np.concatenate((closest_100_samples, km.labels_), axis=1)\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100, :]\nclosest_100_samples = np.sort(closest_100_samples, axis=0)\nclosest_100_samples = closest_100_samples[:100,", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    return centers[p]\n    ### END SOLUTION ###\n\nsamples = get_samples(p, X, km)\nprint(samples)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = pd.concat([X_train, X_train.drop(columns=[0])], axis=1)\n\n# create GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n\n# print the result\nprint(clf.predict(X_train))\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = pd.concat([X_train, X_train[0]], axis=1)\n\n# remove the categorical column\nX_train.drop(0, axis=1, inplace=True)\n\n# create GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n\n# print the result\nprint(clf)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVR model with Gaussian kernel\nmodel = SVR(kernel='rbf')\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the output\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn import svm\n\n# Create a SVM classifier\nclf = svm.SVC(kernel='gaussian')\n\n# Fit the model\nclf.fit(X, y)\n\n# Predict the response for the test dataset\npredict = clf.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict the values\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# predict X\npredict = model.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "\ndef get_cosine_similarities_of_queries(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    document_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.dot(query_matrix, document_matrix.T) / (np.sqrt(np.dot(query_matrix, document_matrix.T)) + 1e-9)\n    return cosine_similarities_of_queries\n\ncosine_similarities_of_queries = get_cosine_similarities_of_queries(queries, documents)\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "\ndef get_cosine_similarities_of_queries(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_matrix = tfidf.transform(queries)\n    document_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.dot(query_matrix, document_matrix.T) / (np.sqrt(np.dot(query_matrix, document_matrix.T)) + np.finfo(np.float32).eps)\n    return cosine_similarities_of_queries\n\ncosine_similarities_of_queries = get_cosine_similarities_of_queries(queries, documents)\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_matrix = tfidf.transform(documents)\n    cosine_similarities_of_queries = np.zeros((len(queries), len(documents)))\n    for i, query in enumerate(queries):\n        query_tfidf = tfidf.transform([query])\n        cosine_similarities_of_queries[i] = np.dot(query_tfidf, tfidf_matrix.T) / (np.linalg.norm(query_tfidf) * np.linalg.norm(tfidf_matrix.T))\n    ### END SOLUTION ###\n    return cosine_similarities_of_queries\n\n# Test the function\nqueries = [\"This is a test query\", \"This is another test query\", \"This is a third test query\"]\ndocuments = [\"This is a test document\", \"This is another test document\", \"This is a third test document\", \"This is a fourth test document\", \"This is a fifth test document\"]\nprint(solve(queries, documents))\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# Create a data frame from the features\ndf = pd.DataFrame(features)\n\n# Create a 2D array from the data frame\nnew_features = df.values\n\n# Convert the 2D array to a 2D array with one hot encoding\nnew_features = np.concatenate([np.ones(new_features.shape[0]), new_features], axis=1)\n\n# Print the new features\nprint(new_features)\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# Create a 2D array with zeros\nnew_f = np.zeros((len(f), len(f[0])))\n\n# Convert the features to a 2D array\nfor i in range(len(f)):\n    for j in range(len(f[i])):\n        new_f[i][j] = f[i][j]\n\n# Convert the 2D array to a 2D array with one hot encoding\nnew_f = np!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# Create a data frame from the features\ndf = pd.DataFrame(features)\n\n# Create a 2D array from the data frame\nnew_features = df.values\n\n# Convert the 2D array to a 2D array with one hot encoding\nnew_features = np.concatenate([np.ones((new_features.shape[0], 1)), new_features], axis=1)\n\n# Create a 2D array with one hot encoding\nnew_features = np.concatenate([np.ones((new_features.shape[0], 1)), new_features], axis=1)\n\n# Print the new features\nprint(new_features)\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Create a 2D array with zeros\n    new_features_array = np.zeros((features_array.shape[0], features_array.shape[1]))\n    # Set the values to 1\n    new_features_array[features_array == 1] = 1\n    # # Convert the 2D array to a DataFrame\n    # new_features_df = pd.DataFrame(new_features_array, columns=features[0])\n    # # Use sklearn's feature selection utilities\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_df = new_features_df.drop(['f1', 'f2'], axis=1)\n    # # new_features_df = new_features_df.drop(['f3', 'f4'], axis=1)\n    # # new_features_df = new_features_df.drop(['f5', 'f6'], axis=1)\n    # # new_features_", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Create a 2D array with zeros\nnew_features = np.zeros((len(features), len(features[0])))\n\n# Convert the features to a 2D array\nfor i, feature in enumerate(features):\n    for j, value in enumerate(feature):\n        new_features[i][j] = value\n\n# Convert the 2D array to a 2D array with one hot encoding\nnew_features = np.concatenate([np.ones((len(features), 1)), new_features], axis=1)\n\n# Create a DataFrame from the 2D array\nnew_features = pd.DataFrame(new_features, columns=features[0] + ['t1'])\n\n# Print the new features\nprint(new_features)\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the data matrix\ndata_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model to the data matrix\ncluster_labels = cluster.fit_predict(data_matrix)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model and predict the cluster labels\ncluster_labels = cluster.fit_predict(dist_matrix)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the similarity matrix\nsimM = np.array([[0, 0.6, 0.8], [0.6, 0, 0.111], [0.8, 0.111, 0]])\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the clustering model and get the cluster labels\ncluster_labels = cluster.fit_predict(simM)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# Create a cluster labels array\ncluster_labels = fcluster(linkage_matrix, 2, criterion='max')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a distance matrix\ndistances = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n# Create a linkage matrix\nlinkage_matrix = linkage(distances, 'ward')\n\n# Create a cluster labels\ncluster_labels = fcluster(linkage_matrix, 2, criterion='maximum')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage\n\n# Create a linkage matrix\nlinkage_matrix = linkage(simM, 'ward')\n\n# Get the cluster labels\ncluster_labels = np.argmax(linkage_matrix, axis=1)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\nbox_cox_transformer = BoxCoxTransformer()\nbox_cox_data = box_cox_transformer.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\nbox_cox_data = transformer.fit_transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnsonTransformer\n\ntransformer = YeoJohnsonTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n\n# Convert the transformed data to a pandas DataFrame\ntransformed_data = pd.DataFrame(transformed_data)\n\n# Print the transformed data\nprint(transformed_data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(analyzer='word')\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = np.concatenate((transformed_text, text), axis=1)\ntransformed_text = transformed_text.reshape(-1, 2)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text = transformed_text.reshape(-1, 1)\ntransformed_text =", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=[-1]), dataset[-1], test_size=0.2, random_state=42)\n\n# Define x and y\nx_train_x = x_train.drop(columns=[-1])\nx_train_y = x_train[-1]\nx_test_x = x_test.drop(columns=[-1])\nx_test_y = x_test[-1]\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features (x) and the target (y)\nx = data.drop(columns=[-1])\ny = data[-1]\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Split the dataset into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=[-1]), dataset[-1], test_size=0.3, random_state=42)\n\n# Define x and y\nx_train_x = x_train.drop(columns=[-1])\nx_train_y = x_train[-1]\nx_test_x = x_test.drop(columns=[-1])\nx_test_y = x_test[-1]\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": " ###\n    # split the data into training and testing sets\n    train_data = data[:int(0.8*(len(data)))]\n    test_data = data[int(0.8*(len(data))):]\n\n    # define x and y\n    x_train = train_data.drop([-1], axis=1)\n    y_train = train_data[-1]\n    x_test = test_data.drop([-1], axis=1)\n    y_test = test_data[-1]\n\n    return x_train, y_train, x_test, y_test\n\n# call the function\nx_train, y_train, x_test, y_test = solve(dataset)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "\n# reshape the mse values to 2D array\nmse_values = df['mse'].values.reshape(-1, 1)\n\n# create a KMeans object with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# fit the KMeans object to the mse values\nkmeans.fit(mse_values)\n\n# get the labels of the clusters\nlabels = kmeans.predict(mse_values)\n\n# print the labels\nprint(labels)\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# reshape the data to 2D array\nX = df['mse'].values.reshape(-1, 1)\n\n# create a KMeans object with 2 clusters\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# fit the model to the data\nkmeans.fit(X)\n\n# get the labels of the clusters\nlabels = kmeans.predict(X)\n\n# get the centroid of the clusters\ncentroids = kmeans.cluster_centers_\n\n# print the labels and centroids\nprint(labels)\nprint(centroids)\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "\n# Create a LinearSVC object with l1 penalty\nsvc = LinearSVC(penalty='l1', random_state=0)\n\n# Fit the model\nsvc.fit(X, y)\n\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(np.array(svc.coef_).flatten() != 0)]\n\nprint(selected_feature_names)\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "\n# Create a linear svm model\nmodel = LinearSVC(penalty='l1', loss='l2', max_iter=10000)\n\n# Fit the model\nmodel.fit(X, y)\n\n# Get the selected features\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[model.coef_[0] > 0]\n\nprint(selected_feature_names)\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    svc = LinearSVC(penalty='l1', solver='liblinear')\n    svc.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(svc.coef_ != 0)]\n    return selected_feature_names\n    ### END SOLUTION ###\n\n# Test the function\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nfeature_names = np.array(feature_names)\nfeature_names = np.sort(feature_names)\nprint(feature_names)\nprint(X.toarray())\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nfeature_names = np.array(feature_names)\nfeature_names = np.sort(feature_names)\nprint(feature_names)\nprint(X.toarray())\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vocabulary = ['Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX']\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nfeature_names = np.array(feature_names)\nfeature_names = np.sort(feature_names)\nX = np.array(X.toarray())\nX = np.sort(X, axis=1)\nX = np.array([np.where(feature_names == feature, 1, 0) for feature in feature_names])\nprint(feature_names)\nprint(X)\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vocabulary = ['Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX']\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\n\n# Create a data frame with the feature names and the transformed values\ndf = pd.DataFrame(X, columns=feature_names)\n\n# Create a new data frame with the transformed values, where the values are 1 if the feature is present and 0 otherwise\ndf_new = pd.DataFrame(np.where(df == 1, 1, 0), columns=feature_names)\n\n# Print the new data frame\nprint(df_new)\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = np.array([]) # initialize an empty array to store the slopes\n\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])] # remove NaN values for each column\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y) # fit the linear regression model\n    m = slope.coef_[0] # get the slope\n    slopes = np.concatenate((slopes, m), axis = 0) # append the slope to the array\n\nslopes = slopes.reshape(-1, 1) # reshape the array to a 1d array\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "\nslopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\n\nslopes = slopes.reshape(-1, 1)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed data\nprint(df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\n# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\n# Print the transformed dataframe\nprint(transformed_df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ### END SOLUTION ###\n    return df\n\ndef load_data():\n    df = pd.DataFrame({\n        'Sex': ['male', 'female', 'male', 'female'],\n        'Age': [20, 25, 30, 35],\n        'PassengerId': [1, 2, 3, 4],\n        'Survived': [0, 1, 0, 1]\n    })\n    return df\n\ndf = load_data()\nTransform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION ###\n\n# Test the function\na = np.array([[1, 2], [3, 4]])\nprint(Transform(a))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "predict = clf.predict([close, m5, m10, ma20])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array([['dsa', '2'], ['sato', '3']])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the dataframe to numpy array\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model with the data\nlogReg.fit(X, y)\n# Predict the class of the animal\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by=\"date\")\n\n# Calculate the number of days in the test set\ntest_days = len(features_dataframe) * 0.8\n\n# Calculate the number of days in the train set\ntrain_days = len(features_dataframe) - test_days\n\n# Create a new dataframe for the train set\ntrain_dataframe = features_dataframe.iloc[:train_days]\n\n# Create a new dataframe for the test set\ntest_dataframe = features_dataframe.iloc[train_days:]\n\n# Sort the train and test dataframes by date\ntrain_dataframe = train_dataframe.sort_values(by=\"date\")\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n\n# Print the train and test dataframes\nprint(train_dataframe)\nprint(test_dataframe)\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by=\"date\")\n\n# Calculate the number of days in the train set\ntrain_days = int(len(features_dataframe) * 0.8)\n\n# Create a new dataframe for the train set\ntrain_dataframe = features_dataframe.iloc[:train_days]\n\n# Create a new dataframe for the test set\ntest_dataframe = features_dataframe.iloc[train_days:]\n\n# Sort the test set by date\ntest_dataframe = test_dataframe.sort_values(by=\"date\")\n\n# Create a new dataframe for the test set with the dates after the last date in the train set\ntest_dataframe = test_dataframe[test_dataframe[\"date\"] > train_dataframe[\"date\"].max()]\n\n# Create a new dataframe for the train set with the dates before the first date in the test set\ntrain_dataframe = train_dataframe[train_dataframe[\"date\"] < test_dataframe[\"date\"].min()]\n\n# Print the train set and test set\nprint(train_dataframe)\nprint(test_dataframe)\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": " ###\n    # sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(by=\"date\")\n\n    # calculate the number of rows to be in the test set\n    test_size = int(len(features_dataframe) * 0.8)\n\n    # split the dataframe into train and test sets\n    train_dataframe = features_dataframe.iloc[:test_size]\n    test_dataframe = features_dataframe.iloc[test_size:]\n\n    return train_dataframe, test_dataframe\n\n# call the function\ntrain_dataframe, test_dataframe = solve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2', 'X3']] = scaler.fit_transform(df[['X2', 'X3']])\ndf['X2_scale'] = df[['X2']]\ndf['X3_scale'] = df[['X3']]\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "cols = myData.columns[2:4]\nfor month in myData['Month'].unique():\n    month_data = myData[myData['Month'] == month][cols]\n    month_data['new_' + cols] = scaler.fit_transform(month_data[cols])\n    myData[myData['Month'] == month][cols] = month_data['new_' + cols]\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = GridSearch_fitted.fit(X, y)\nfull_results.best_score_\nfull_results.best_params_\nfull_results.cv_results_\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results.sort_values(by='mean_fit_time', inplace=True)\nfull_results", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\n# Load your data\ndata = pd.read_csv('your_data.csv')\n\n# Fit the model\nmodel = IsolationForest()\nmodel.fit(data)\n\n# Save the model\nnp.save('sklearn_model', model)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "tfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = np.zeros((len(df), len(df)))\nfor i in range(len(df)):\n    for j in range(len(df)):\n        if i == j:\n            cosine_similarity_matrix[i, j] = 1.0\n        else:\n            cosine_similarity_matrix[i, j] = sklearn.metrics.cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])\ncosine_similarity_matrix = pd.DataFrame(cosine_similarity_matrix, index=df.index, columns=df.index)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\noptim.zero_grad()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n# Define a function to update the learning rate\ndef update_learning_rate(optimizer, epoch):\n    if epoch > 10:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.001\n\n# Create an optimizer\noptimizer = torch.optim.SGD(..., lr=0.01)\n\n# Update the learning rate after each epoch\nfor epoch in range(10):\n    update_learning_rate(optimizer, epoch)\n    # Perform your training steps\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\noptim.zero_grad()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define the learning rate\nlr = 0.05\n\n# Define the epoch number\nepoch_num = 10\n\n# Define the learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n\n# Set the learning rate\nfor epoch in range(epoch_num):\n    scheduler.step()\n    print(f\"Epoch {epoch}, Learning rate: {scheduler.get_last_lr()[0]}\")\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the word2vec model to a PyTorch embedding layer\nembedding_layer = torch.tensor(word2vec.wv.vectors)\n\n# Create a tensor with your input data\ninput_Tensor = torch.tensor(input_Tensor)\n\n# Embedding the input data using the word2vec weights\nembedded_input = torch.nn.functional.embedding(input_Tensor, embedding_layer)\nembedded_input = embedded_input.reshape(-1, 100)\nprint(embedded_input)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # Create a PyTorch embedding layer with the same shape as the input tensor\n    embedding_layer = torch.nn.Embedding(input_Tensor.shape[1], input_Tensor.shape[0])\n\n    # Load the weights from the word2vec model into the embedding layer\n    embedding_layer.weight.data.copy_(torch.tensor(word2vec.wv.vectors))\n\n    # Embed the input tensor\n    embedded_input = embedding_layer(input_Tensor)\n\n    return embedded_input\n    ### END SOLUTION ###\n\n# Test the function\nembedded_input = get_embedded_input(input_Tensor)\nprint(embedded_input)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(np.array(x))\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "A_logical = torch.ByteTensor(A_logical)\nB = B.to(torch.long)\nC = B[:, A_logical]\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "A_log = torch.ByteTensor([1, 1, 0]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "A_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    C = B[:, A_log]\n    ### END SOLUTION ###\n    return C\n\ndef load_data():\n    A_log = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_log, B\n\nA_log, B = load_data()\nC = solve(A_log, B)\nprint(C)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "A_log = torch.ByteTensor([0, 0, 1]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\n# Create a tensor from the index\nidx_tensor = torch.tensor(idx)\n\n# Create a tensor from the original data\nB_tensor = torch.tensor(B)\n\n# Use index_select to get the expected output\nC = torch.index_select(B_tensor, 1, idx_tensor)\n\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    return torch.tensor(a, dtype=torch.float16)\n    ### END SOLUTION ###\n\n# Test the function\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\nlens = torch.tensor([3, 5, 4])\nmax_len = lens.max().item()\nmask = torch.zeros(max_len, max_len)\nfor i in range(max_len):\n    mask[i, :lens[i]] = 1\n\nprint(mask)\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\nlens = torch.tensor([1, 9, 3, 5])\nmax_len = lens.max().item()\nmask = torch.zeros(max_len, max_len)\nfor i in range(max_len):\n    mask[i, :lens[i]] = 1\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\nlens = torch.tensor([3, 5, 4])\nmask = torch.arange(lens.max()).expand(lens.size(0), -1) < lens.unsqueeze(1)\nmask = mask.long()\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    max_len = np.max(lens)\n    mask = np.zeros((len(lens), max_len))\n    for i in range(len(lens)):\n        mask[i, :lens[i]] = 1\n    return mask\n\nlens = np.array([3, 5, 4])\nmask = get_mask(lens)\nprint(mask)\n### END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor with diagonal elements\ndiagonal_tensor = torch.diag(Tensor_2D)\n\n# Create a 2D tensor with drag elements\ndrag_tensor = torch.rand(Tensor_2D.shape[0], Tensor_2D.shape[1])\n\n# Create a 3D tensor with index_in_batch * drag_tensor\nindex_in_batch = torch.rand(Tensor_2D.shape[0])\nindex_in_batch = index_in_batch.reshape(-1, 1)\n\nTensor_3D = index_in_batch * drag_tensor\n\nprint(Tensor_3D)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    # Create a diagonal matrix from the tensor\n    diag_matrix = torch.diag(t)\n    # Create a matrix with the same shape as the tensor\n    matrix = torch.ones_like(t)\n    # Create a tensor with the same shape as the tensor\n    index_in_batch = torch.arange(t.shape[0])\n    # Create a tensor with the same shape as the tensor\n    result = torch.zeros_like(t)\n    # Set the diagonal of the result tensor to the diagonal matrix\n    result[index_in_batch, index_in_batch] = diag_matrix\n    ### END SOLUTION ###\n    return result\n\n# Test the function\nt = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(Convert(t))\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\nab = torch.stack((a, b), 0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = a.reshape(-1, 1)\nb = b.reshape(-1, 1)\nab = torch.stack((a, b), 0)\nab = ab.reshape(-1, 2)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    # stack the tensors along the first dimension\n    ab = torch.stack((a, b), 0)\n    # # convert the tensor to numpy\n    # ab_np = ab.numpy()\n    # # stack the numpy arrays along the first dimension\n    # ab_np = np.stack((a_np, b_np), 0)\n    # # convert the numpy array back to tensor\n    # ab_tensor = torch.from_numpy(ab_np)\n    # # ### END SOLUTION ###\n    return ab\n\n# a and b are numpy arrays\na_np = np.array([[1, 2, 3], [4, 5, 6]])\nb_np = np.array([[7, 8, 9], [10, 11, 12]])\n\n# a and b are tensors\na_tensor = torch.tensor(a_np)\nb_tensor = torch.tensor(b_np)\n\n# solve(a_tensor, b_tensor)\n# print(ab)\n# print(ab_tensor)\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor.numpy())\n# print(ab_np)\n# print(ab_tensor", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[ : , lengths : , : ] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[ : , lengths : , : ] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    tensor_of_tensors = torch.tensor(lt, dtype=torch.float32)\n    return tensor_of_tensors\n    ### END SOLUTION ###\n\n# Call the function\ntensor_of_tensors = Convert(list_of_tensors)\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = t[np.arange(t.shape[0]), idx]", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = t[np.arange(t.shape[0]), idx]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = t[idx[:, 0], idx[:, 1]]", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\nscores = x.reshape(70, 3)\nmax_scores = scores.max(axis=1)\nresult = x[np.arange(70), max_scores.reshape(-1, 1)]\nresult = result.reshape(70, 2)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\nscores = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nids = torch.tensor([[2], [1], [0]])\n\nresult = x[ids].gather(1, ids)\nprint(result)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a mask where the ids are 1\nmask = ids == 1\n\n# Gather the scores of the selected slices\nresult = x[mask].gather(1, ids[mask].unsqueeze(-1)).squeeze(-1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "y = torch.argmax(softmax_output, dim=1)\ny = y.reshape(-1, 1)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "y = torch.argmax(softmax_output, dim=1)\nprint(y)", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "y = torch.argmax(softmax_output, dim=1)\ny = y.reshape(-1, 1)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    # get the maximum value in each row\n    max_values = torch.max(softmax_output, 1)\n    # create a tensor with the maximum value\n    max_tensor = torch.tensor(max_values[1]).unsqueeze(1)\n    # create a tensor with the class index of the maximum value\n    class_tensor = torch.tensor(max_values[0]).unsqueeze(1)\n    # return the tensor\n    return class_tensor\n    ### END SOLUTION ###\n\n# test the function\nprint(solve(softmax_output))\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "y = torch.argmax(softmax_output, dim=1)\ny = y.reshape(-1, 1)\nreturn y", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "\n# Convert labels to one-hot encoding\nlabels = F.one_hot(labels, num_classes=3)\n\n# Calculate cross entropy loss\nloss = F.cross_entropy(images, labels)\n\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = np.sum(A == B)\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = np.sum(A == B)\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = np.sum(A != B)\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    cnt_equal = np.sum(A == B)\n    ### END SOLUTION ###\n    return cnt_equal\n\nA = np.random.rand(1000, 1)\nB = np.random.rand(1000, 1)\n\nprint(Count(A, B))\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "cnt_equal = np.sum(A[-x:] == B[-x:])", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = np.sum(A[-x:] != B[-x:])", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensors_31.append(a[:, :, :, i:i + chunk_dim, :])\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 10, 31, 1)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensors_31.append(a[:, :, i:i+1, :, :])\ntensors_31 = torch.stack(tensors_31)\nassert tensors_31.shape == (1, 3, 31, 40, 1)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "\noutput[mask == 1] = clean_input_spectrogram[mask == 1]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "\noutput[mask == 0] = clean_input_spectrogram[mask == 0]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x))\nmin_y = torch.min(torch.abs(y))\nsigned_min = torch.where(x < y, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\nsigned_max = torch.where(x > y, max_x, max_y) * sign_x * sign_y", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x))\n    min_y = torch.min(torch.abs(y))\n    min_x_sign = torch.where(x < 0, -1, 1)\n    min_y_sign = torch.where(y < 0, -1, 1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x_sign = min_x_sign.repeat(1, x.shape[1])\n    min_y_sign = min_y_sign.repeat(1, y.shape[1])\n    min_x = torch.where(x < 0, min_x, min_x_sign)\n    min_y = torch.where(y < 0, min_y, min_y_sign)\n    min_x = min_x.reshape(-1, 1)\n    min_y = min_y.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x = min_x.repeat(1, y.shape[1])\n    min_y = min_y.repeat(1, x.shape[1])\n    min_x_sign = min_x_sign.repeat(1, y.shape[1])\n    min_y_sign = min_y_sign.repeat(1, x.shape[1])\n    min_x = min_x.reshape(-1, 1)\n    min_y = min_y.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x = min_x.repeat(1, x.shape[1])\n    min_y = min_y.repeat(1, y.shape[1])\n    min_x_sign = min_x_sign.repeat(1, y.shape[1])\n    min_y_sign = min_y_sign.repeat(1, x.shape[1])\n    min_x = min_x.reshape(-1, 1)\n    min_y = min_y.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x = min_x.repeat(1, y.shape[1])\n    min_y = min_y.repeat(1, x.shape[1])\n    min_x_sign = min_x_sign.repeat(1, x.shape[1])\n    min_y_sign = min_y_sign.repeat(1, y.shape[1])\n    min_x = min_x.reshape(-1, 1)\n    min_y = min_y.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x = min_x.repeat(1, y.shape[1])\n    min_y = min_y.repeat(1, x.shape[1])\n    min_x_sign = min_x_sign.repeat(1, x.shape[1])\n    min_y_sign = min_y_sign.repeat(1, y.shape[1])\n    min_x = min_x.reshape(-1, 1)\n    min_y = min_y.reshape(-1, 1)\n    min_x_sign = min_x_sign.reshape(-1, 1)\n    min_y_sign = min_y_sign.reshape(-1, 1)\n    min_x = min_x.repeat(1, y.shape[1])", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    confidence_score = conf.item()\n    return confidence_score, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\nprint(confidence_score, class_name)\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# Create a tensor with the same shape as a and b\nresult = torch.zeros_like(a)\n\n# Calculate the average of the last column of a and the first column of b\nresult[:, 1:-1] = (a[:, 1:-1] + b[:, 0]) / 2\n\n# Calculate the average of the first column of a and the last column of b\nresult[:, 0] = (a[:, 0] + b[:, -1]) / 2\n\n# Calculate the average of the first column of a and the first column of b\nresult[:, 0] = (a[:, 0] + b[:, 0]) / 2\n\n# Calculate the average of the last column of a and the last column of b\nresult[:, -1] = (a[:, -1] + b[:, -1]) / 2\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a[:, :-1]\n    b = b[:, 1:]\n    result = a + b\n    return result\n    ### END SOLUTION ###\n\n# Test the function\na = torch.tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.tensor([[5, 6, 7], [5, 6, 7]])\nprint(solve(a, b))\n", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "t = torch.tensor([[1., 2., 3., 4.], [5., 6., 7., 8.]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nresult = torch.cat([new.repeat(t.shape[0], 1), t], dim=1)\nprint(result)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "t = torch.tensor([[1., 2.], [3., 4.]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nresult = torch.cat([new.repeat(t.shape[0], 1), t], dim=1)\nprint(result)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1]])\nresult = torch.stack([t, new])\nprint(result)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W)\nresult = result.squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
