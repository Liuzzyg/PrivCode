{"id": 0, "code": "result = df.iloc[List]", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.iloc[List]\nprint(result)\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana']}\n\n# Create a dictionary with the values of Qu3 that have at least two occurrences\ndict_values_other = {'Qu3': ['other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other']}\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update(dict_values)\n\n# Create a dictionary with the values of Qu1 and Qu2 that have at least two occurrences\ndict_values_other.update", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict = dict(zip(df.columns, df.value_counts()))\n\n# Create a dictionary with column names as keys and values as the value counts\nvalue_counts_dict =", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": " ###\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'other'],\n                  'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n    # create a dictionary with the values of Qu1 and Qu2\n    dict_values = {'Qu1': ['other', 'potato', 'cheese', 'banana', 'che", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n                    'Qu2': pd.value_counts(df.Qu2).values,\n                    'Qu3': pd.value_counts(df.Qu3).values}\n\n# Create a dictionary with column names and their value counts\nvalue_counts_dict = {'Qu1': pd.value_counts(df.Qu1).values,\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'Qu1': ['apple', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n               'Qu3': ['apple', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'Qu1': ['apple', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n               'Qu3': ['apple', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'Qu1': ['apple', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n               'Qu3': ['apple', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'Qu1': ['apple', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n               'Qu3': ['apple', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'Qu1': ['apple', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n               'Qu3': ['apple', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'Qu1': ['apple', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n               'Qu3': ['apple', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'Qu1': ['apple', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n               'Qu3': ['apple', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']}\n\n# Create a dictionary with the values of Qu1 and Qu2\ndict_values = {'Qu1': ['apple', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],\n               'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 's", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "result = df.drop_duplicates(subset='url', keep='first')\nresult", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\n# Create a new column 'keep_if_dup' with a boolean value\ndf['keep_if_dup'] = df['keep_if_dup'].apply(lambda x: x == 'Yes')\n\n# Drop duplicates\nresult = df.drop_duplicates(subset='url', keep='last')\n\n# Reset the 'keep_if_dup' column\nresult['keep_if_dup'] = result['keep_if_dup'].apply(lambda x: x if x else 'No')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndef convert_to_nested_dict(df):\n    result = {}\n    for index, row in df.iterrows():\n        if row['name'] not in result:\n            result[row['name']] = {}\n        result[row['name']][row['v1']] = {}\n        result[row['name']][row['v1']][row['v2']] = row['v3']\n    return result\n\nresult = convert_to_nested_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": " ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the tz_convert function to convert the timezone to UTC\n    # Then use the str() function to get the timezone offset\n    # Finally, use the str() function to get the timezone offset\n    # result = f(df)\n    ### END SOLUTION ###\n    return result\n\ndef f(df=example_df):\n    # return the solution in this function\n    # result = f(df)\n    ### BEGIN SOLUTION ###\n    # Use the", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\n# Convert the message to a dictionary\nmessage_dict = {}\nfor i in range(len(df)):\n    message_dict[i] = eval(df['message'][i])\n\n# Create a new dataframe from the dictionary\nnew_df = pd.DataFrame(list(message_dict.items()), columns=['index', 'message'])\n\n# Extract the key value pairs from the dictionary\nresult = new_df.drop(['index'], axis=1)\n\n# Convert the dictionary to a dataframe\nresult = pd.DataFrame(list(message_dict.items()), columns=['index', 'message'])\n\n# Drop the index column\nresult = result.drop(['index'], axis=1)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "for product in products:\n    df.loc[df['product'] == product, 'score'] *= 10\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10\ndf['score'] = df['score'].where(df['product'].isin(products), 0)\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "for product in products:\n    df.loc[df['product'].between(product[0], product[1]), 'score'] *= 10\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "for product in products:\n    df.loc[df['product'] == product, 'score'] = (df.loc[df['product'] == product, 'score'] - df.loc[df['product'] == product, 'score'].min()) / (df.loc[df['product'] == product, 'score'].max() - df.loc[df['product'] == product, 'score'].min())", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = ''\nfor i in range(len(df)):\n    if df.iloc[i, 0] == 1:\n        df.iloc[i, 0] = 0\n    if df.iloc[i, 1] == 1:\n        df.iloc[i, 1] = 0\n    if df.iloc[i, 2] == 1:\n        df.iloc[i, 2] = 0\n    if df.iloc[i, 3] == 1:\n        df.iloc[i, 3] = 0\n    if df.iloc[i, 4] == 1:\n        df.iloc[i, 4] = 0\n    if df.iloc[i, 5] == 1:\n        df.iloc[i, 5] = 0\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'A'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 1:\n        df.iloc[i, 6] = 'B'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 1 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'C'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 1 and df.iloc[i, 4] == 1 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'D'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 1 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 1 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'E'\n    if df.iloc[i, 0] == 0 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 1 and df.iloc[i, 3] == 1 and df.iloc[i, 4] == 1 and df.iloc[i, 5] == 1:\n        df.iloc[i, 6] = 'F'\n    if df.iloc[i, 0] == 1 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 0:\n        df.iloc[i, 6] = 'G'\n    if df.iloc[i, 0] == 1 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3] == 0 and df.iloc[i, 4] == 0 and df.iloc[i, 5] == 1:\n        df.iloc[i, 6] = 'H'\n    if df.iloc[i, 0] == 1 and df.iloc[i, 1] == 0 and df.iloc[i, 2] == 0 and df.iloc[i, 3", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = 'A'\ndf.loc[df['A'] == 1, 'category'] = 'B'\ndf.loc[df['B'] == 1, 'category'] = 'C'\ndf.loc[df['C'] == 1, 'category'] = 'D'\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "\n# Create a new column with the binary values as a list\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col]], axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.to_period('M')\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%b-%Y'))\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].apply(lambda x: x.strftime('%d-%b-%Y %A'))\ndf = df[df['Date'].between(List[0], List[1])]\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.columns = ['#1', '#2']\nshifted_df.iloc[0] = df.iloc[0]\nshifted_df.iloc[-1] = df.iloc[-1]\nprint(shifted_df)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.columns = ['#1', '#2']\nshifted_df.loc[0] = df.loc[0]\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#1'] = df.loc[0, '#1']\nshifted_df.loc[0, '#2'] = df.loc[0, '#2']\nshifted_df.loc[0, '#", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "shifted_df = df.shift(1, axis=1)\nshifted_df.iloc[0] = df.iloc[0]\nshifted_df.iloc[-1] = df.iloc[-1]\nprint(shifted_df)\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "\n# shift the first row of the first column down 1 row\ndf['#1'] = df['#1'].shift(1)\n\n# shift the last row of the first column up 1 row\ndf['#2'] = df['#2'].shift(-1)\n\n# reset the index\ndf = df.reset_index(drop=True)\n\n# print the dataframe\nprint(df)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "for col in df.columns:\n    df.rename(columns={col: col + 'X'}, inplace=True)\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "for col in df.columns:\n    df[col] = df[col] + 'X'\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "# Create a dictionary with the new column names\nnew_cols = {'HeaderA': 'HeaderAX',\n            'HeaderB': 'HeaderBX',\n            'HeaderC': 'HeaderCX',\n            'HeaderX': 'HeaderX'}\n\n# Use the dictionary to rename the columns\ndf = df.rename(columns=new_cols)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "# Get the column names that contain 'val'\nval_cols = [col for col in df.columns if 'val' in col]\n\n# Get the grouped mean for each of the value columns\nresult = df.groupby('group').agg({col: 'mean' for col in val_cols})\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "def sum_other_columns(df):\n    other_columns = [col for col in df.columns if col not in ['group', 'group_color']]\n    return df[other_columns].sum()\n\nresult = sum_other_columns(df)\nprint(result)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "def aggregate_values(df):\n    # Create a dictionary of aggregation functions\n    aggregation_functions = {'val1': 'sum', 'val2': 'mean', 'val42': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': 'sum', 'val6': 'sum'}\n\n    # Create a dictionary of aggregation functions for the other value columns\n    other_aggregation_functions = {'val3': 'sum', 'val5': '", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "def meanAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].mean(axis=axis)\n\nresult = meanAdvance(row_list, column_list, axis=0)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "def sumAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list, axis=0)\nprint(result)\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "def sumAdvance(row_list, column_list, axis):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\nresult = sumAdvance(row_list, column_list, axis=0)\nresult.drop_duplicates(keep='first', inplace=True)\nresult", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "result = df.apply(lambda x: x.value_counts())\nresult", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "result = df.isnull().sum()\nresult", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "result = df.value_counts()\nresult", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.iloc[[0,1]]", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.iloc[[0,1]]", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "result = df.apply(lambda x: x.replace(np.nan, x.mean()), axis=1)\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "result = df.apply(lambda x: x.where(x.isnull(), x.mean()), axis=1)\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "result = df.where(df.isnull(), np.nan)\nresult", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "# Find rows with value less than threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# Sum values of these rows\nresult = rows_less_than_thresh.sum()\n# Replace original rows with summation result\ndf.loc[rows_less_than_thresh.index] = result\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "# Find rows with value less than threshold\nrows_less_than_thresh = df[df['value'] < thresh]\n# Find rows with value greater than threshold\nrows_greater_than_thresh = df[df['value'] >= thresh]\n# Calculate average of values in rows greater than threshold\navg_value = rows_greater_than_thresh['value'].mean()\n# Create a new row with average value\nnew_row = pd.DataFrame({'value':[avg_value]}, index=[thresh])\n# Combine rows with value less than threshold and new row\nresult = pd.concat([rows_less_than_thresh, new_row])\n# Set index to lab\nresult.set_index('lab', inplace=True)\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "# find the rows in the section\nrows_in_section = df[(df['value'] >= section_left) & (df['value'] <= section_right)]\n# calculate the average of the rows in the section\naverage_value = rows_in_section['value'].mean()\n# replace the rows in the section with the average value\nresult = df.replace(rows_in_section, average_value)\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "result = df.assign(inv_A=1/df['A'], inv_B=1/df['B'])", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "result = df.assign(**{f'exp_{col}': lambda x: x.apply(lambda x: x**(1/x)) for col in df.columns})", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "inv_A = df['A'].map(lambda x: 1/x if x != 0 else 0)\ninv_B = df['B'].map(lambda x: 1/x if x != 0 else 0)\n\nresult = pd.DataFrame({\"A\": df['A'], \"B\": df['B'], \"inv_A\": inv_A, \"inv_B\": inv_B})\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = sigmoid(df[col])\n\nresult = df", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "# Find the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Find the index of the last occurrence of the column-wise maximum\nmax_index = df.apply(lambda x: x.argmax(), axis=1)\n\n# Find the index of the last occurrence of the column-wise maximum up to the minimum index\nresult = max_index[min_index]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "# Find the index of the minimum value in each column\nmin_index = df.idxmin()\n\n# Create a mask that is True at the index of the minimum value in each column\nmask = np.array([min_index[i] == idx for i in range(len(min_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmax_index = df.apply(lambda x: x.argmax())\n\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array([max_index[i] == idx for i in range(len(max_index))])\n\n# Create a mask that is True at the index of the maximum value in each column\nmask_max = np.array", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Expand the date column to have all the dates there while filling in 0 for the val column\nresult = new_df.merge(df, on='dt', how='left').fillna(0)\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Expand the date column to have all the dates there while filling in 0 for the val column\nresult = pd.merge(new_df, df, on='dt', how='left')\nresult['val'] = result['val'].fillna(0)\nresult", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Expand the date column to have all the dates there while filling in 233 for the val column\nnew_df = new_df.merge(df, on='dt', how='left')\nnew_df['val'] = new_df['val'].fillna(233)\n\n# Find the minimum and maximum date within the date column\nmin_date = new_df['dt'].min()\nmax_date = new_df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum to the maximum\nnew_df = pd.DataFrame({'dt': pd.date_range(start=min_date, end=max_date)})\n\n# Find the maximum val for each user\nmax_val = df.groupby('user')['val'].max()\n\n# Join the new data frame with the maximum val for each user\nresult = new_df.join(max_val.reset_index())\n\n# Fill the maximum val for each user with the maximum val for the user\nresult['val'] = result['val'].fillna(result['val'].max())\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "# Find the minimum and maximum date within the date column\nmin_date = df['dt'].min()\nmax_date = df['dt'].max()\n\n# Create a new data frame with all dates from the minimum date to the maximum date\nnew_df = pd.DataFrame(pd.date_range(start=min_date, end=max_date), columns=['dt'])\n\n# Join the new data frame with the original data frame on the 'user' column\nresult = pd.merge(new_df, df, on='user')\n\n# Fill the maximum val of the user for the val column\nresult['val'] = result.groupby('user')['val'].transform('max')\n\n# Convert the date column to the desired format\nresult['dt'] = result['dt'].dt.strftime('%d-%b-%Y')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "# Create a unique ID for each name\nunique_id = df['name'].apply(lambda x: x + 1)\n\n# Create a new column 'id' with unique ID\ndf['id'] = unique_id\n\n# Replace the 'name' column with the 'id' column\ndf['name'] = df['id']\n\n# Drop the 'id' column\ndf = df.drop(['id'], axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "# Create a unique ID for each value in column a\nunique_id = df['a'].apply(lambda x: x if x == 3 else 1)\n\n# Create a new column with unique ID\ndf['a'] = unique_id\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": " ###\n    # replace each name with a unique ID\n    df['name'] = df['name'].apply(lambda x: 1 if x == 'Aaron' else 2 if x == 'Brave' else 3)\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "# Create a unique ID for each name\nid_map = df['name'].drop_duplicates().to_dict()\n\n# Create a new column 'ID' with unique ID for each name\ndf['ID'] = df['name'].map(id_map)\n\n# Replace 'a' with 'ID'\ndf['a'] = df['ID']\n\n# Replace 'b' with 'ID'\ndf['b'] = df['ID']\n\n# Replace 'c' with 'ID'\ndf['c'] = df['ID']\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = pd.pivot_table(df, values='value', index='user', columns='date', aggfunc='first')\n", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df = df.pivot_table(index='user', columns='01/12/15', values='value')\ndf = df.rename(columns=lambda x: x.replace('/', ''))\ndf = df.reset_index()\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.pivot_table(index='user', columns='date', values='value', \n                    aggfunc=lambda x: x.values, \n                    margins=True)\n\n# replace None with NaN\ndf = df.replace(to_replace=None, value=np.nan)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to_replace=np.nan, value=None)\n\n# replace NaN with 0\ndf = df.replace(to_replace=np.nan, value=0)\n\n# replace NaN with None\ndf = df.replace(to", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df.c > 0.5][columns]\nresult = np.array(result)\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df['c'] > 0.45][columns].values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": " ###\n    # Use the loc function to select the rows where the value in column 'c' is greater than 0.5\n    # Then use the columns function to select the columns you want\n    # Finally, use the values function to convert the result to a numpy array\n    ### END SOLUTION ###\n    return df.loc[df['c'] > 0.5, columns].values\n\n# Create a DataFrame\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n# Call the function\nprint(f(df, ['b', 'e']))\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": " ###\n    # use the loc function to select the rows where the value for column 'c' is greater than 0.5\n    # then select only the columns 'b' and 'e' for those rows\n    # then compute the sum of the two columns for each element to the right of original columns\n    # return the result\n    ### END SOLUTION ###\n\n    # use the loc function to select the rows where the value for column 'c' is greater than 0.5\n    # then select only the columns 'b' and 'e' for those rows\n    # then compute the sum of the two columns for each element to the right of original columns\n    # return the result\n    result = df.loc[df.c > 0.5, ['b', 'e']].sum(axis=1)\n    return result\n\n# create a dataframe\ndf = pd.DataFrame({'a': [0.945686, 0.919359, 0.976163, 0.459039],\n                   'b': [0.000710, 0.667057, 0.621712, 0.788318],\n                   'c': [0.909158, 0.462478, 0.208423, 0.309892],\n                   'd': [0.892892, 0.008204, 0.980471, 0.100539],\n                   'e': [0.326670, 0.473096, 0.048334, 0.753992]})\n\n# call the function\nprint(f(df, ['b', 'e']))\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df[df[columns[0]] > 0.5][columns]\n    ### END SOLUTION\n\n# test the function\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint f(df, ['b', 'e'])\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndef find_overlap(row, X):\n    date_range = pd.date_range(start=row['date'], periods=X)\n    return date_range.isin(row['date'])\n\nresult = df[~df.apply(find_overlap, X=X, axis=1)]\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef find_overlap(row, X):\n    date = row['date']\n    date_format = pd.to_datetime(date)\n    dates = [date_format + pd.DateOffset(months=i) for i in range(X)]\n    return dates\n\nresult = df[~df.apply(find_overlap, X=X, axis=1)]\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef find_overlap(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v2(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v3(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v4(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v5(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v6(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v7(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v8(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v9(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v10(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v11(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v12(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2]\n    return date_format\n\ndef find_overlap_dates_v13(row, X):\n    date = row['date']\n    date_format = date.split('/')\n    date_format = date_format[1] + '-' + date_format[0] + '-' + date_format[2", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\n# calculate the number of rows in the dataframe\nn = len(df)\n\n# calculate the number of bins\nnum_bins = n // 3\n\n# create a new dataframe with the bins\nresult = pd.DataFrame({'col1':[num_bins]*num_bins})\n\n# calculate the bin values\nbin_values = np.linspace(0, num_bins, num_bins)\n\n# create a new dataframe with the bin values\nresult['col1'] = bin_values\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\n# Create a new column 'col2' which will be used to bin the data\ndf['col2'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col3' which will be used to bin the data\ndf['col3'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col4' which will be used to bin the data\ndf['col4'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col5' which will be used to bin the data\ndf['col5'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col6' which will be used to bin the data\ndf['col6'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col7' which will be used to bin the data\ndf['col7'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col8' which will be used to bin the data\ndf['col8'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col9' which will be used to bin the data\ndf['col9'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col10' which will be used to bin the data\ndf['col10'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col11' which will be used to bin the data\ndf['col11'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col12' which will be used to bin the data\ndf['col12'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col13' which will be used to bin the data\ndf['col13'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col14' which will be used to bin the data\ndf['col14'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col15' which will be used to bin the data\ndf['col15'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col16' which will be used to bin the data\ndf['col16'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col17' which will be used to bin the data\ndf['col17'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col18' which will be used to bin the data\ndf['col18'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col19' which will be used to bin the data\ndf['col19'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col20' which will be used to bin the data\ndf['col20'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col21' which will be used to bin the data\ndf['col21'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col22' which will be used to bin the data\ndf['col22'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col23' which will be used to bin the data\ndf['col23'] = (df['col1'].astype(int) // 3) * 3\n\n# Create a new column 'col24' which will be used to bin the data\ndf['col24'] = (", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\n# Create a new column with the bin values\ndf['col1'] = df['col1'].apply(lambda x: x // 4 * 4 + x % 4)\n\n# Group by the bin values and sum the values\nresult = df.groupby(df['col1'] // 4).sum()\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\n# Create a new column with the mean of the last 3 rows\ndf['col1'] = df['col1'].rolling(window=3).mean()\n\n# Drop the last 3 rows\ndf = df[:-3]\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# Create a new column with the sum of the first 3 rows\ndf['col1'] = df['col1'].iloc[:3].sum()\n\n# Create a new column with the average of the first 2 rows\ndf['col1'] = df['col1'].iloc[:2].mean()\n\n# Create a new column with the sum of the last 3 rows\ndf['col1'] = df['col1'].iloc[-3:].sum()\n\n# Create a new column with the average of the last 2 rows\ndf['col1'] = df['col1'].iloc[-2:].mean()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\n# Create a new column with the last 3 rows of the dataframe\nlast_3_rows = df[-3:]\n\n# Calculate the sum of the last 3 rows\nlast_3_sum = last_3_rows['col1'].sum()\n\n# Calculate the average of the last 3 rows\nlast_3_avg = last_3_rows['col1'].mean()\n\n# Create a new dataframe with the last 3 rows and the sum and average\nnew_df = pd.DataFrame({'col1':[last_3_sum, last_3_avg]})\n\n# Create a new dataframe with the last 2 rows and the sum and average\nnew_df_2 = pd.DataFrame({'col1':[last_3_sum, last_3_avg]})\n\n# Create a new dataframe with the last 3 rows and the sum and average\nnew_df_3 = pd.DataFrame({'col1':[last_3_sum, last_3_avg]})\n\n# Concatenate the new dataframes\nresult = pd.concat([new_df, new_df_2, new_df_3])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(method='ffill')\ndf = df.replace(np.nan, 0)\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df = df.replace(0, df.loc[0, 'A'])\ndf = df.replace(0, df.loc[0, 'A'])\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "df = df.replace(0, np.nan)\ndf = df.fillna(df.transform('max'))\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['duration'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['time_days'] = df['time'].apply(lambda x: x.replace('year', 365).replace('day', 1).replace('week', 7).replace('month', 30))\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['time_day'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": " ###\n    # replace the duration with the number and time\n    df['duration'] = df['duration'].str.split(' ', 1).str[0]\n    df['time'] = df['duration'].str.split(' ', 1).str[1]\n    # replace the time with the number of days\n    df['time'] = df['time'].replace(['year', 'month', 'week', 'day'], [365, 30, 7, 1])\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['duration'] = df['duration'].str.split(' ', 1).str[0]\ndf['number'] = df['duration'].str.split(' ', 1).str[1]\ndf['number'] = df['number'].apply(lambda x: x.replace('.', ''))\ndf['number'] = df['number'].apply(lambda x: x.replace(' ', ''))\ndf['number'] = df['number'].apply(lambda x: x.replace(' ', ''))\ndf['time'] = df['duration'].apply(lambda x: x.replace(' ', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace(' ', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda x: x.replace('.', ''))\ndf['time'] = df['time'].apply(lambda", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "result = np.where(np.all(np.array([df1[column] == df2[column] for column in columns_check_list]), axis=1))", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "result = np.all(np.where((df1[columns_check_list] == df2[columns_check_list]), 1, 0), axis=1)", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": " ###\n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Create a new column with the date and x, y as the first column\n    df['date'] = df['date'].apply(lambda x: x.date())\n    # Create a new column with the date and x, y as the second column\n    df['date'] = df['date'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n    # Create a new column with the date and x, y as the third column\n    df['date'] = df['date'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n    # Return the date, x, y as a numpy array\n    return df[['date', 'x', 'y']].values\n    ### END SOLUTION ###\n\n# Test the function\ndf = pd.DataFrame({'id': ['abc', 'abc', 'abc'],\n                   'date': ['3/1/1994', '9/1/1994', '3/1/1995'],\n                   'x': [100, 90, 80],\n                   'y': [7, 8, 9]})\nprint(f(df))\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": " ###\n    # parse date index using pd.to_datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # swap the two levels\n    df = df.set_index(['id', 'date'])\n    ### END SOLUTION ###\n    return df\n\n# test the function\ndf = pd.DataFrame({'x': ['abc'], 'y': [100], 'date': ['3/1/1994']})\nprint(f(df))\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf = df.pivot_table(index=['Country', 'year'], columns='Variable', values='value')\ndf = df.reset_index()\ndf = df.rename(columns={'Country': 'country', 'year': 'year', 'value': 'var1'})\ndf = df.drop(columns=['var2'])\ndf = df.sort_values(['country', 'year'])\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='var')\ndf = df.sort_values(['Country', 'year', 'Variable'], ascending=[True, True, True])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf = df.reset_index()\ndf = df.sort_values(['Country', 'year'], ascending=[True, False])\ndf = df.pivot_table(index='Country', columns='Variable', values='var', fill_value=0)\ndf", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "\n# Create a list of column names\ncolumn_names = [f'Value_{i}' for i in range(1, 11)]\n\n# Create a new data frame with only the columns with absolute value less than 1\nnew_df = df[column_names].apply(lambda x: x.abs() < 1, axis=1)\n\n# Create a new data frame with only the rows where all columns are less than 1\nnew_df = new_df.apply(lambda x: x.all(), axis=1)\n\n# Print the new data frame\nprint(new_df)\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "\n# Create a list of column names\ncolumn_names = ['Value_' + str(i) for i in range(1, 11)]\n\n# Create a new data frame with only the columns with absolute value more than 1\nnew_df = df[column_names].apply(lambda x: x.abs() > 1, axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df = new_df.apply(lambda x: x.all(axis=1), axis=1)\n\n# Create a new data frame with only the rows where all columns are more than 1\nnew_df =", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "\n# Create a list of column names that start with 'Value'\nvalue_columns = [col for col in df.columns if col.startswith('Value')]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df[value_columns]\n\n# Create a new data frame with only the columns that have a value more than 1\nnew_df = df", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df = df.replace('&AMP;', '&')", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df = df.replace({'&LT;': '<'}, regex=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": " ###\n    # replace &AMP; with '&'\n    df = df.replace({'&AMP;': '&'})\n    return df\n\nf(example_df)\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df = df.replace({'&AMP;': '&'}, regex=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'] = df['name'].apply(validate_single_space_name)\ndf['last_name'] = df['name'].apply(lambda x: None if x is None else x.split(' ')[1])\n\nprint(df)\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['1_name'] = df['name'].apply(validate_single_space_name)\ndf['2_name'] = df['name'].apply(lambda x: x.split(' ')[1] if ' ' in x else None)\n\nprint(df)\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "def validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['name'] = df['name'].apply(validate_single_space_name)\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = pd.merge(df2, df1, on='Timestamp', how='left')\nresult['data'] = result['data'].fillna(result['stuff'])\nresult = result.drop(['stuff'], axis=1)\nresult", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge(df1, df2, on='Timestamp', how='left')\nresult['data'] = df2['stuff']\nresult['stuff'] = df1['data']\nresult['Timestamp'] = df1['Timestamp']\nresult['stuff'] = df2['stuff']\nresult['data'] = df1['data']\nresult['Timestamp'] = df2['Timestamp']\nresult", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= 50, df['col1'], np.max(df[['col1', 'col2', 'col3']], axis=1))\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = np.where(df['col2'] > 50, df['col1'], np.sum(df[['col1', 'col2', 'col3']]))\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\nerrors = []\nfor index, row in df.iterrows():\n    if not pd.isnumeric(row[\"Field1\"]):\n        errors.append(row[\"Field1\"])\n\nprint(errors)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndef get_integer_values(row):\n    values = row.values\n    integer_values = []\n    for value in values:\n        if isinstance(value, int):\n            integer_values.append(value)\n        elif isinstance(value, float):\n            integer_values.append(int(value))\n    return integer_values\n\ninteger_values = df.apply(get_integer_values, axis=1)\nprint(integer_values)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": " ###\n    # create a new column with the error values\n    df['Error'] = df['Field1'].apply(lambda x: x if pd.isnumeric(x) else \"and\")\n    # return the new column\n    return df['Error']\n\nprint(f())\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1'] = df['val1'] / df['val1'].sum()\ndf['val2'] = df['val2'] / df['val2'].sum()\ndf['val3'] = df['val3'] / df['val3'].sum()\ndf['val4'] = df['val4'] / df['val4'].sum()\n", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)\nprint(result)\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": " ###\n    return df.loc[test]\n    ### END SOLUTION ###\n\ntest = ['TP3','TP12','TP18', 'TP3']\nresult = f(df, test)\nprint(result)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "# Create a new column 'distance' to store the euclidean distance between each car and its nearest neighbour\ndf['distance'] = np.sqrt((df['x'] - df['x'].shift(1))**2 + (df['y'] - df['y'].shift(1))**2)\n\n# Group by 'time' and 'car', then find the nearest neighbour for each car\nnearest_neighbour = df.groupby('time')['car'].transform(lambda x: x.loc[x.argmin()])\n\n# Calculate the average of the distances for each frame\naverage_distance = df.groupby('time')['distance'].mean()\n\n# Create a new dataframe with the nearest neighbour and the average distance\ndf2 = pd.DataFrame({'car': df['car'], 'nearest_neighbour': nearest_neighbour, 'average_distance': average_distance})\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "# Create a new column 'car_neighbour' that contains the nearest neighbour for each car\ndf['car_neighbour'] = df.groupby('car')['car'].transform(lambda x: x.iloc[x.argmin()])\n\n# Create a new column 'farmost_neighbour' that contains the farthest neighbour for each car\ndf['farmost_neighbour'] = df.groupby('car')['car'].transform(lambda x: x.iloc[x.argmax()])\n\n# Create a new column 'euclidean_distance' that contains the euclidean distance between each car and its nearest neighbour\ndf['euclidean_distance'] = df.groupby('car')['car'].transform(lambda x: np.sqrt((x - df['farmost_neighbour'].values[x.values])**2).mean())\n\n# Calculate the average of the distances for each frame using groupby\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].mean()\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf['keywords_all'] = df[cols].apply(lambda x: ', '.join(x.dropna()), axis=1)\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n\n# Create a new column 'keywords_all' by concatenating the values of the columns\ndf['keywords_all'] = df[cols].apply(lambda x: '-'.join(x.dropna()), axis=1)\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n\nfor col in cols:\n    df[col] = df[col].apply(lambda x: \"-\".join(x.dropna().tolist()))\n\nprint(df)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "cols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n\n# Create a new column 'keywords_all' by concatenating the last 4 columns\ndf['keywords_all'] = df[cols].apply(lambda x: '-'.join(x.dropna()), axis=1)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "# Select 20% of rows\nsample_df = df.sample(n=int(0.20 * len(df)), random_state=0)\n\n# Set Quantity to zero\nsample_df['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nsample_df.index = range(len(sample_df))\n\n# Print the resulting DataFrame\nprint(sample_df)\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "# Select 20% of rows\nsample_df = df.sample(n=int(0.20 * len(df)), random_state=0)\n\n# Set value of ProductId to zero\nsample_df['ProductId'] = 0\n\n# Keep indexes of the altered rows\nsample_df.index = range(len(sample_df))\n\n# Print the result\nprint(sample_df)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "# Create a mask for the rows to be randomly selected\nmask = df['UserId'].apply(lambda x: x % 1 == 0)\n\n# Randomly select 20% of rows for each user\nrandom_rows = df[mask].sample(n=int(0.20 * len(df)), random_state=0)\n\n# Set the Quantity column to zero\nrandom_rows['Quantity'] = 0\n\n# Keep the indexes of the altered rows\nrandom_rows.index = random_rows.index\n\n# Print the resulting DataFrame\nprint(random_rows)", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": " ###\n    # find the duplicates\n    duplicates = df.duplicated(subset=['col1','col2'], keep='first')\n    # find the index of the first duplicate\n    index_original = duplicates.where(duplicates == True).index\n    # add a new column with the index of the first duplicate\n    df['index_original'] = index_original\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate.index\nresult = duplicate", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "min_count = df['count'].min()\nresult = df[df['count'] == min_count]", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "result = pd.melt(df, value_vars=[('A', 'B', 'E'),\n                                  ('A', 'B', 'F'),\n                                  ('A', 'C', 'G'),\n                                  ('A', 'C', 'H'),\n                                  ('A', 'D', 'I'),\n                                  ('A', 'D', 'J')])\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "# Define the tuples for value_vars\nvalue_vars = [(x, y, z) for x in df.columns for y in df.columns for z in df.columns]\n\n# Use pd.melt to melt the data frame\nresult = pd.melt(df, value_vars=value_vars)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cummax()\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val'] < 0, 0)\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), 0)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "result = df.groupby('r')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), 0)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "result = df.groupby('l')['v'].apply(lambda x: np.sum(x.where(np.isnan(x), np.nan)) if not np.isnan(x).any() else np.nan)\nresult", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-to-many'\n    elif column1.min() == column2.max():\n        return 'many-to-one'\n    elif column1.min() == column2.min():\n        return 'one-to-many'\n    elif column1.max() == column2.max():\n        return 'many-to-many'\n    else:\n        return 'none'\n\nresult = []\nfor i in range(df.shape[1]):\n    for j in range(i + 1, df.shape[1]):\n        result.append(f'{df.columns[i]} {df.columns[j]} {relationship(df[df.columns[i]], df[df.columns[j]])}')\n\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship(column1, column2):\n    if column1.max() == column2.min():\n        return 'one-2-many'\n    elif column1.min() == column2.max():\n        return 'one-2-one'\n    else:\n        return 'many-2-many'\n\nresult = []\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.append(f'{column1} {column2} {relationship(column1, column2)}')\n\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def get_relationship(column1, column2, column3, column4, column5):\n    if column1.equals(column2) and column1.equals(column3) and column1.equals(column4) and column1.equals(column5):\n        return 'one-to-one'\n    elif column1.equals(column2) and column1.equals(column3) and column1.equals(column4) and column2.equals(column5):\n        return 'one-to-many'\n    elif column1.equals(column2) and column1.equals(column3) and column2.equals(column4) and column1.equals(column5):\n        return 'many-to-one'\n    elif column1.equals(column2) and column1.equals(column3) and column2.equals(column4) and column3.equals(column5):\n        return 'many-to-many'\n    else:\n        return 'unknown'\n\nresult = df.apply(lambda row: get_relationship(row['Column1'], row['Column2'], row['Column3'], row['Column4'], row['Column5']), axis=1)\nprint(result)\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship(x):\n    if x.max() == x.min():\n        return 'one-2-one'\n    elif x.max() == x.max() + 1:\n        return 'one-2-many'\n    elif x.min() == x.min() - 1:\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\nresult = df.apply(lambda x: relationship(x))\nresult", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "# get the index of unique values, based on firstname, lastname, email\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\nresult = dfiban_uniq\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "grouped = df.groupby(['Survived', 'SibSp', 'Parch']).mean()\ngrouped['Has Family'] = grouped['Survived'] * (grouped['SibSp'] > 0) | (grouped['Parch'] > 0)\ngrouped['No Family'] = grouped['Survived'] * (grouped['SibSp'] == 0) & (grouped['Parch'] == 0)\nresult = grouped.mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "grouped = df.groupby(['Survived', 'SibSp', 'Parch']).mean()\nresult = grouped.groupby(['Survived', 'SibSp']).mean()\nresult", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "grouped = df.groupby(['Survived', 'SibSp', 'Parch'])\nresult = grouped.mean()\nresult", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\n", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').sort_values('A').reset_index(drop=True)\nresult", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\nprint(df)", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.to_list())\nprint(df)", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "result = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\nresult", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "def mean_std(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('a').apply(mean_std)\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "def mean_std(x):\n    return np.mean(x), np.std(x)\n\nresult = df.groupby('b').apply(mean_std)\nresult", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x, axis=0)\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = softmax(df['b'])\ndf['min-max'] = min_max(df['b'])\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "result = df.loc[:, (df.sum(axis=0) != 0)]", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "result = df[df.sum(axis=1) != 0]\nresult = result[result.sum(axis=0) != 0]\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "max_value = df.max().max()\nresult = df[df[max_value] == 2]", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "max_value = df.max().max()\nresult = df.where(df <= max_value, 0)\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "s = s.sort_values(ascending=True)\ns = s.sort_index()\nresult = s", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "sorted_s = s.sort_values(ascending=True)\nsorted_s.index = sorted_s.index.sort_values()\ndf = pd.DataFrame({'index': sorted_s.index, '1': sorted_s.values})\ndf", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "result = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, float))]", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].apply(lambda x: isinstance(x, str))]", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "min_count = df['count'].min()\nresult = df[df['count'] == min_count]", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "max_count = df['count'].max()\nresult = df[df['count'] == max_count]", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": " ###\n    # map the dict to the date column\n    df['Date'] = df['Member'].map(dict)\n    # replace NaN with the date from the dict\n    df['Date'] = df['Date'].fillna(df['Member'].map(dict))\n    ### END SOLUTION ###\n    return df\n\nprint(f())\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17-Aug-1926')\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date', 'Year', 'Month']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Year', 'Month']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Year']).Date.transform('count')\nprint(df)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date','Val']).Date.transform('count')\nprint(df)", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_y'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_w'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby(['Date','Val']).Date.transform('count')\nprint(df)\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date')['B'].apply(lambda x: x.sum())\nresult2 = df.groupby('Date')['C'].apply(lambda x: x.sum())\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "result1 = df[df['B'] % 2 == 0].groupby('Date').count()\nresult2 = df[df['B'] % 2 != 0].groupby('Date').count()\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = np.sum(result['D'])\nresult['E'] = np.mean(result['E'])\nresult", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['mean'] = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult['D'] = np.sum(result['D'])\nresult['E'] = np.mean(result['E'])\nresult", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D', 'E'], rows=['B'], aggfunc=np.max)\nresult", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\ndef split_string_to_rows(df):\n    return df.assign(var1=lambda x: x['var1'].str.split(',').apply(lambda x: x.tolist()))\n\nresult = split_string_to_rows(df)\nprint(result)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\n# Split the column into multiple rows\nresult = df.assign(var2=lambda x: x['var2'].str.split(','))\n\n# Flatten the column\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(i) for i in x]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: x.flatten()))\n\n# Convert the column to integer\nresult = result.assign(", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\n# Split the column into multiple rows\nresult = df.assign(var2=lambda x: x['var2'].str.split('-'))\n\n# Flatten the column\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [item for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer\nresult = result.assign(var2=lambda x: x['var2'].apply(lambda x: [int(item) for sublist in x for item in sublist]))\n\n# Convert the column to integer", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if not string[i].isalpha():\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if string[i].isalpha():\n            continue\n        else:\n            special_char += 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df['fips'] = df['row'].str[:5]\ndf['row'] = df['row'].str[5:]\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df['fips'] = df['row'].str[:-1]\ndf['row'] = df['row'].str[-1]\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df['fips'] = df['row'].str[:4]\ndf['medi'] = df['row'].str[4:]\ndf", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\n# calculate the average for each row\nfor col in df.columns:\n    if col != 'Name':\n        df[col] = df[col].apply(lambda x: x / (x.sum() if x.sum() != 0 else 1))\n\n# calculate the cumulative average for each row\nfor col in df.columns:\n    if col != 'Name':\n        df[col] = df[col].cumsum() / (df[col].sum() if df[col].sum() != 0 else 1)\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\n# Calculate the cumulative average for each row from end to head\nfor i in range(1, 7):\n    df[f'2006-{i}'] = df[f'2006'].values[-i:]\n    df[f'2006-{i}'] = df[f'2006-{i}'].values / (i * 1.0)\n\n# Ignore zero values\ndf = df.where(df['2006'] != 0, 0)\n\n# Calculate the cumulative average for each row from end to head\nfor i in range(1, 7):\n    df[f'2006-{i}'] = df[f'2006'].values[-i:]\n    df[f'2006-{i}'] = df[f'2006-{i}'].values / (i * 1.0)\n\n# Ignore zero values\ndf = df.where(df['2006'] != 0, 0)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": " ###\n    # calculate the cumulative average for each row using pandas\n    # ignore if the value is zero\n    ### END SOLUTION ###\n    return result\n\n# call the function\nf()\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\n# Calculate the cumulative average for each row from end to head\nfor col in df.columns:\n    if col != 'Name':\n        df[col] = df[col].cumsum() / (np.arange(len(df[col])) + 1)\n\n# Ignore zero values\ndf = df.where(df[col] != 0, 0)\n\n# Calculate the cumulative average for each row from end to head\nfor col in df.columns:\n    if col != 'Name':\n        df[col] = df[col].cumsum() / (np.arange(len(df[col])) + 1)\n\n# Ignore zero values\ndf = df.where(df[col] != 0, 0)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] - df['Close'].shift(1)) > 0\ndf['label'] = df['label'].apply(lambda x: 1 if x else 0)\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\n# Create a new column 'label'\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n\n# Set the label for the first row to 1\ndf.loc[0, 'label'] = 1\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = pd.to_datetime(df['departure_time']) - pd.to_datetime(df['arrival_time'])\ndf['Duration'] = df['Duration'].apply(lambda x: str(x).split(' ')[0])\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = df['departure_time'].diff().apply(lambda x: x.seconds)\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['Duration'] = df.departure_time.apply(lambda x: (x - pd.to_datetime(x)) / 60)\ndf['Duration'] = df.Duration.apply(lambda x: x.seconds)\ndf['arrival_time'] = df.arrival_time.apply(lambda x: pd.to_datetime(x))\ndf['departure_time'] = df.departure_time.apply(lambda x: pd.to_datetime(x))\ndf['Duration'] = df.Duration.apply(lambda x: x.seconds)\ndf['arrival_time'] = df.arrival_time.apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\ndf['departure_time'] = df.departure_time.apply(lambda x: x.strftime('%d-%b-%Y %H:%M:%S'))\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "result = df[df['key2'] == 'one'].groupby('key1').size()\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "result = df[df['key2'] == 'two'].groupby('key1').size()\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "result = df[df['key2'].endswith('e')].groupby('key1').size()\nprint(result)\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.max(axis=0)\nmin_result = df.min(axis=0)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df['value'].mode()\nmedian_result = df['value'].median()\nprint(mode_result, median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "result = df[(99 <= df['closing_price']) & (df['closing_price'] <= 101)]", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "result = df[~(99 <= df['closing_price'] <= 101)]", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "min_diff = df['diff'].min()\nresult = df[df['diff'] == min_diff]", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "\ndef get_last_underscore(s):\n    return s.split('_')[-1] if '_' in s else s\n\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(get_last_underscore)\nprint(df)\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": " ###\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[-1]\n    return df\n\nf(example_df)\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "# Calculate the number of NaN values in the column\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# Calculate the number of NaN values to fill\nnan_to_fill = nan_count // 2\n\n# Create a mask for the NaN values\nmask = np.isnan(df['Column_x'])\n\n# Fill the NaN values with the mode of the column\ndf['Column_x'][mask] = df['Column_x'].mode()[0]\n\n# Fill the first nan_to_fill values with 0 and the last nan_to_fill values with 1\ndf['Column_x'][mask[:nan_to_fill]] = 0\ndf['Column_x'][mask[-nan_to_fill:]] = 1\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "# Calculate the number of NaN values\nnan_count = np.sum(np.isnan(df['Column_x']))\n\n# Calculate the number of NaN values to fill\nnan_to_fill = nan_count * 0.3\n\n# Calculate the number of NaN values to fill in the first 30%\nnan_to_fill_first_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the middle 30%\nnan_to_fill_middle_30 = np.floor(nan_to_fill * 0.6)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = nan_to_fill - nan_to_fill_first_30 - nan_to_fill_middle_30\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in the last 30%\nnan_to_fill_last_30 = np.floor(nan_to_fill * 0.3)\n\n# Calculate the number of NaN values to fill in", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "# Calculate the number of zeros and ones in the column\nzero_count = np.sum(df['Column_x'] == 0)\none_count = np.sum(df['Column_x'] == 1)\n\n# Calculate the number of zeros and ones to be filled\nzero_to_fill = zero_count // 2\none_to_fill = one_count // 2\n\n# Create a new column with zeros and ones\nnew_column = np.zeros(zero_to_fill + one_to_fill)\nnew_column[zero_to_fill:] = np.ones(one_to_fill)\n\n# Replace the original column with the new column\ndf['Column_x'] = new_column\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "a_b = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0])], columns=['one', 'two'])\nresult = a_b", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "a_b = pd.DataFrame([(a.iloc[0,0], b.iloc[0,0], c.iloc[0,0]), (a.iloc[1,0], b.iloc[1,0], c.iloc[1,0])], columns=['one', 'two'])\nresult = a_b", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "# Create a function to create a tuple from two dataframes\ndef create_tuple(a, b):\n    return np.array([(a.iloc[i, 0], b.iloc[i, 0]) for i in range(a.shape[0])]), np.array([(a.iloc[i, 1], b.iloc[i, 1]) for i in range(a.shape[0])])\n\n# Create a tuple from a and b\na_tuple, b_tuple = create_tuple(a, b)\n\n# Create a dataframe a_b\na_b = pd.DataFrame({'one': a_tuple.flatten(), 'two': b_tuple.flatten()})\n\n# Print the result\nprint(a_b)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins, labels=[1, 1, 1, 1]).value_counts())\nresult = groups.reset_index()\nresult.columns = ['username', 'views']\nresult", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins, labels=False).sum())\nresult = groups.values", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "groups = df.groupby('username').views.apply(lambda x: pd.cut(x, bins, labels=[1, 1, 1, 1, 1]).value_counts())\nresult = groups.reset_index()\nresult.columns = ['username', 'views']\nresult", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', ')))\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).values.tolist()\nresult = pd.DataFrame({'text': result})\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "result = df['text'].apply(lambda x: ', '.join(x.split(', ')))\nresult = result.reset_index(drop=True)\nresult", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "result = df['text'].to_series().str.split(', ', expand=True).apply(lambda x: ', '.join(x))\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "result = df['text'].apply(lambda x: '-'.join(x.split('-'))).to_frame('text')\nresult", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "result = pd.concat([df1, df2], axis=0)\nresult['city'] = result['city'].fillna(result['district'])\nresult['district'] = None\nresult = result.sort_values('id')\nresult", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n# merge df1 and df2 on id\nmerged_df = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with NaN\nmerged_df = merged_df.fillna(value='NaN')\n\n# sort by id and date\nmerged_df = merged_df.sort_values(['id', 'date'])\n\n# fill NaN with NaN\nmerged_df = merged_df.fillna(value='NaN')\n\n# reset index\nmerged_df = merged_df.reset_index(drop=True)\n\n# print result\nprint(merged_df)\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\n# merge df1 and df2 on id\nmerged_df = pd.merge(df1, df2, on='id', how='left')\n\n# fill NaN with NaN\nmerged_df = merged_df.fillna(value=np.nan)\n\n# sort by id and date\nmerged_df = merged_df.sort_values(['id', 'date'])\n\n# group by id and date, then fill NaN with NaN\nmerged_df = merged_df.groupby(['id', 'date']).apply(lambda x: x.fillna(np.nan))\n\n# reset index\nmerged_df = merged_df.reset_index(drop=True)\n\n# print result\nprint(merged_df)\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.sort_values('A', inplace=True)\nresult.reset_index(drop=True, inplace=True)\nresult", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "result = pd.merge(C, D, on='A', how='outer')\nresult.columns = ['A', 'B_x']\nresult['B_y'] = D['B']\nresult = result.sort_values('A')\nresult['B'] = result['B_x']\nresult['B_x'] = None\nresult = result.drop(['B_x'], axis=1)\nresult", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "C[\"dulplicated\"] = C[\"A\"].apply(lambda x: x in D[\"A\"].values)\nresult = C", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist())\nresult", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount']).values.tolist())\nresult", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "result = df.groupby('user')[['time', 'amount']].apply(lambda x: x.sort_values(['time', 'amount'], ascending=[True, False]).to_list()).to_list()\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result]\nresult = [list(x) for x in result", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "df_concatenated = pd.DataFrame(series.values.tolist(), index=series.index, columns=series.index)\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "df_concatenated = pd.DataFrame(series.values, index=series.index, columns=series.index)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df.columns[df.columns.str.contains(s)].tolist()\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df.columns[df.columns.str.contains(s)].to_string(index=False)\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "# Find the column names that contain the string 'spike'\nmatch_columns = df.columns[df.columns.str.contains(s)]\n\n# Create a new dataframe with the matched column names\nmatch_df = df[match_columns]\n\n# Rename the columns\nmatch_df.columns = [f'{s}-{i}' for i in range(1, match_df.shape[1] + 1)]\n\n# Print the result\nprint(match_df)\n\n# Access the column\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_df['spike-1'])\n\n# Access the column with the name\nprint(match_", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n# Create a new dataframe with the same index as the original one\nnew_df = pd.DataFrame(df.index)\n\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# Convert the list to a single column\ndf['codes'] = df['codes'].apply(lambda x: x[0] if x else None)\n\n# Split the list into columns\ndf = df.join(df['codes'].apply(lambda x: pd.Series(x)).add_prefix('code_'))\n\n# Fill NaN with None\ndf = df.fillna(None)\n\n# Sort the columns\ndf = df.sort_values(by='code_1')\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "result = df['col1'].apply(lambda x: x[0])\nresult = result.tolist()\nresult = [item for sublist in result for item in sublist]\nprint(result)\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x[::-1])))\nprint(result)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "result = df['col1'].apply(lambda x: ', '.join(map(str, x)))\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "# Convert Time to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Create a new column for binning\ndf['Time_bin'] = df['Time'].apply(lambda x: x.floor('2min'))\n\n# Group by bin and mean\nmean_values = df.groupby('Time_bin')['Value'].mean()\n\n# Create a new dataframe with bin and mean values\nmean_values_df = pd.DataFrame({'Time': mean_values.index, 'Value': mean_values.values})\n\n# Interpolate values\nmean_values_df['Value'] = mean_values_df['Value'].interpolate(method='linear')\n\n# Print the result\nprint(mean_values_df)\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "# Convert Time to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Create a new column for the bin\ndf['bin'] = df['Time'].apply(lambda x: x.hour // 3)\n\n# Group by bin and sum the values\nresult = df.groupby('bin').sum()\n\n# Reset the index\nresult = result.reset_index()\n\n# Rename the columns\nresult.columns = ['Time', 'Value']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %A %H:%M:%S')\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "result = df.columns[np.isnan(df.iloc[0]).values]", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "result = df.columns[np.all(df.isnull(), axis=1)]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "result = df.columns[np.isnan(df.iloc[0]).values].tolist() + df.columns[np.isnan(df.iloc[8]).values].tolist()", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "result = [(np.nan, np.nan) if np.isnan(x) and np.isnan(y) else (x, y) for x, y in zip(df.iloc[0], df.iloc[8])]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\nprint(ts)\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df = df.stack().reset_index(level=1, drop=True).unstack().reset_index(level=1, drop=True)\ndf = df.rename(columns=lambda x: f'{x}_1')\ndf = df.rename(columns=lambda x: f'{x}_2')\ndf = df.rename(columns=lambda x: f'{x}_3')\ndf = df.rename(columns=lambda x: f'{x}_4')\ndf = df.rename(columns=lambda x: f'{x}_5')\ndf = df.rename(columns=lambda x: f'{x}_6')\ndf = df.rename(columns=lambda x: f'{x}_7')\ndf = df.rename(columns=lambda x: f'{x}_8')\ndf = df.rename(columns=lambda x: f'{x}_9')\ndf = df.rename(columns=lambda x: f'{x}_10')\ndf = df.rename(columns=lambda x: f'{x}_11')\ndf = df.rename(columns=lambda x: f'{x}_12')\ndf = df.rename(columns=lambda x: f'{x}_13')\ndf = df.rename(columns=lambda x: f'{x}_14')\ndf = df.rename(columns=lambda x: f'{x}_15')\ndf = df.rename(columns=lambda x: f'{x}_16')\ndf = df.rename(columns=lambda x: f'{x}_17')\ndf = df.rename(columns=lambda x: f'{x}_18')\ndf = df.rename(columns=lambda x: f'{x}_19')\ndf = df.rename(columns=lambda x: f'{x}_20')\ndf = df.rename(columns=lambda x: f'{x}_21')\ndf = df.rename(columns=lambda x: f'{x}_22')\ndf = df.rename(columns=lambda x: f'{x}_23')\ndf = df.rename(columns=lambda x: f'{x}_24')\ndf = df.rename(columns=lambda x: f'{x}_25')\ndf = df.rename(columns=lambda x: f'{x}_26')\ndf = df.rename(columns=lambda x: f'{x}_27')\ndf = df.rename(columns=lambda x: f'{x}_28')\ndf = df.rename(columns=lambda x: f'{x}_29')\ndf = df.rename(columns=lambda x: f'{x}_30')\ndf = df.rename(columns=lambda x: f'{x}_31')\ndf = df.rename(columns=lambda x: f'{x}_32')\ndf = df.rename(columns=lambda x: f'{x}_33')\ndf = df.rename(columns=lambda x: f'{x}_34')\ndf = df.rename(columns=lambda x: f'{x}_35')\ndf = df.rename(columns=lambda x: f'{x}_36')\ndf = df.rename(columns=lambda x: f'{x}_37')\ndf = df.rename(columns=lambda x: f'{x}_38')\ndf = df.rename(columns=lambda x: f'{x}_39')\ndf = df.rename(columns=lambda x: f'{x}_40')\ndf = df.rename(columns=lambda x: f'{x}_41')\ndf = df.rename(columns=lambda x: f'{x}_42')\ndf = df.rename(columns=lambda x: f'{x}_43')\ndf = df.rename(columns=lambda x: f'{x}_44')\ndf = df.rename(columns=lambda x: f'{x}_45')\ndf = df.rename(columns=lambda x: f'{x}_46')\ndf = df.rename(columns=lambda x: f'{x}_47')\ndf = df.rename(columns=lambda x:", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.rename(columns=lambda x: x + '_0')\ndf = df.rename(columns=lambda x: x + '_1')\ndf = df.rename(columns=lambda x: x + '_2')\ndf = df.rename(columns=lambda x: x + '_3')\ndf = df.rename(columns=lambda x: x + '_4')\ndf = df.rename(columns=lambda x: x + '_5')\ndf = df.rename(columns=lambda x: x + '_6')\ndf = df.rename(columns=lambda x: x + '_7')\ndf = df.rename(columns=lambda x: x + '_8')\ndf = df.rename(columns=lambda x: x + '_9')\ndf = df.rename(columns=lambda x: x + '_10')\ndf = df.rename(columns=lambda x: x + '_11')\ndf = df.rename(columns=lambda x: x + '_12')\ndf = df.rename(columns=lambda x: x + '_13')\ndf = df.rename(columns=lambda x: x + '_14')\ndf = df.rename(columns=lambda x: x + '_15')\ndf = df.rename(columns=lambda x: x + '_16')\ndf = df.rename(columns=lambda x: x + '_17')\ndf = df.rename(columns=lambda x: x + '_18')\ndf = df.rename(columns=lambda x: x + '_19')\ndf = df.rename(columns=lambda x: x + '_20')\ndf = df.rename(columns=lambda x: x + '_21')\ndf = df.rename(columns=lambda x: x + '_22')\ndf = df.rename(columns=lambda x: x + '_23')\ndf = df.rename(columns=lambda x: x + '_24')\ndf = df.rename(columns=lambda x: x + '_25')\ndf = df.rename(columns=lambda x: x + '_26')\ndf = df.rename(columns=lambda x: x + '_27')\ndf = df.rename(columns=lambda x: x + '_28')\ndf = df.rename(columns=lambda x: x + '_29')\ndf = df.rename(columns=lambda x: x + '_30')\ndf = df.rename(columns=lambda x: x + '_31')\ndf = df.rename(columns=lambda x: x + '_32')\ndf = df.rename(columns=lambda x: x + '_33')\ndf = df.rename(columns=lambda x: x + '_34')\ndf = df.rename(columns=lambda x: x + '_35')\ndf = df.rename(columns=lambda x: x + '_36')\ndf = df.rename(columns=lambda x: x + '_37')\ndf = df.rename(columns=lambda x: x + '_38')\ndf = df.rename(columns=lambda x: x + '_39')\ndf = df.rename(columns=lambda x: x + '_40')\ndf = df.rename(columns=lambda x: x + '_41')\ndf = df.rename(columns=lambda x: x + '_42')\ndf = df.rename(columns=lambda x: x + '_43')\ndf = df.rename(columns=lambda x: x + '_44')\ndf = df.rename(columns=lambda x: x + '_45')\ndf = df.rename(columns=lambda x: x + '_46')\ndf = df.rename(columns=lambda x: x + '_47')\ndf = df.rename(columns=lambda x: x + '_48')\ndf = df.rename(columns=lambda x: x + '_49')\ndf = df.rename(columns=lambda x: x + '_50')\ndf = df.rename(columns=lambda x: x + '_51')\ndf = df.rename(columns=lambda x: x + '_52')\ndf = df.rename(columns=lambda x: x + '_53')\ndf = df.rename(columns", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "def avg(columns):\n    return np.mean(df[columns].values, axis=1)\n\ndf['Avg'] = avg(list_of_my_columns)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "for column in list_of_my_columns:\n    df[column] = df[column].mean()\n\n# Get Min, Max and Median\nfor column in list_of_my_columns:\n    df['Min '+column] = df[column].min()\n    df['Max '+column] = df[column].max()\n    df['Median '+column] = df[column].median()\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_values(by=['time'])", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(['time', 'VIM'], ascending=[True, False])", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "# Create a list of dates to delete\ndates_to_delete = ['2020-02-17', '2020-02-18']\n\n# Create a new DataFrame with the dates to delete\ndf_delete = pd.DataFrame(dates_to_delete, columns=['Date'])\n\n# Convert the Date column to datetime\ndf_delete['Date'] = pd.to_datetime(df_delete['Date'])\n\n# Delete the dates from the original DataFrame\ndf = df[~df['Date'].isin(dates_to_delete)]\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Create a new column 'Day' to store the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['Day_num'] = df['Day'].map({'Friday': 5, 'Saturday': 6, 'Sunday': 7, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5})\n\n# Create a new column 'Day_num' to store the day of the week as a number\ndf['", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "result = corr[np.abs(corr) > 0.3].stack().reset_index(level=1, drop=True).rename(columns={'0': 'Col1', '1': 'Col2'})\nresult['Pearson Correlation Coefficient'] = result['Col1'].apply(lambda x: corr.loc[x, x])\nresult", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "result = corr.where(corr > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0)\nresult = result.where(result > 0.3, 0", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "last_column = df.columns[-1]\ndf.rename(columns={last_column: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "result = df.rename(columns={df.columns[0]: 'Test'})", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "frequent = df.apply(lambda x: x.value_counts().index[0], axis=1)\nfreq_count = df.apply(lambda x: x.value_counts().values[0], axis=1)\n\ndf['frequent'] = frequent\ndf['freq_count'] = freq_count\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndef find_frequent(row):\n    return row.value_counts().index[0]\n\ndef find_freq_count(row):\n    return row.value_counts().values[0]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndef find_frequent(row):\n    values = row.values\n    unique_values = np.unique(values)\n    freq_count = np.zeros(unique_values.shape[0])\n    for i in range(unique_values.shape[0]):\n        freq_count[i] = np.sum(values == unique_values[i])\n    return unique_values, freq_count\n\nfrequent, freq_count = find_frequent(df.iloc[0])\nfrequent_list = frequent.tolist()\nfreq_count_list = freq_count.tolist()\n\ndf['frequent'] = frequent_list\ndf['freq_count'] = freq_count_list\n\nprint(df)\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "res = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nres", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "res = df.groupby(['id1', 'id2'])['foo', 'bar'].mean()\nres['bar'] = res['bar'].apply(lambda x: 0 if x == 'NULL' else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: 0 if x is None else x)\nres['bar'] = res['bar'].apply(lambda x: ", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, on='EntityNum', left_on_in='a_col', right_on_in='b_col')\nresult = result[['EntityNum', 'foo', 'a_col']]\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', how='left')\nresult.drop(['a_col'], axis=1, inplace=True)\nresult", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = np.nan_to_num(x, nan=np.nan)\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "x = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\nx = x[~np.isnan(x)]", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((a.shape[0], 4))\nb[:, 0] = a == 0\nb[:, 1] = a == 1\nb[:, 2] = a == 2\nb[:, 3] = a == 3\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((a.shape[0], a.max() + 1))\nb[np.arange(a.shape[0]), a] = 1", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((a.shape[0], a.max() + 1))\nb[:, a] = 1", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if a[i] == a[j]:\n            b[i, j] = 1\n        else:\n            b[i, j] = 0\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.shape[0], a.shape[1]), dtype=np.int8)\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        b[i, j] = a[i, j] + 1\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.reshape(A, (-1, ncol))\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.reshape(A, (nrow, -1))\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    if len(A) % ncol != 0:\n        A = A[:-len(A) % ncol]\n    return np.reshape(A, (-1, ncol))\n\nB = vec2matrix(A, ncol)\nprint(B)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "def shift(a, shift):\n    return np.roll(a, shift, axis=1)\n\nresult = shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "def shift(a, shift):\n    return np.roll(a, shift, axis=1)\n\nresult = shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "max_value = np.max(a)\nmax_index = np.unravel_index(np.argmax(a, axis=None), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "min_value = np.min(a)\nmin_position = np.unravel_index(np.argmin(a), a.shape)\nresult = np.ravel_index(min_position, a.shape)\nprint(result)", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\nprint(result)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": " ###\n    max_value = np.max(a)\n    max_index = np.unravel_index(np.argmax(a), a.shape)\n    return max_value, max_index\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "result = np.unravel_index(np.argpartition(a, -2, axis=-1)[1], a.shape)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "z = np.any(np.isnan(a), axis=0)\na = np.delete(a, z, axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = np.delete(a, np.where(np.isnan(a)))", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.take(a, permutation, axis=1)", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.take(a, permutation, axis=0)", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_index = np.unravel_index(np.argmin(a), a.shape)\nresult = (min_index[0], min_index[1])\nprint(result)", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(np.argmax(a), a.shape)\nresult = (max_index[0], max_index[1])\nprint(result)", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "min_index = np.unravel_index(np.argmin(a, axis=None), a.shape, order='F')\nresult = np.array([[0, 1], [1, 0]])\nprint(min_index)\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "result = np.sin(np.degrees(degree))", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "result = np.cos(np.deg2rad(degree))", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "result = np.arcsin(value) * 180 / np.pi\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.shape[0]), 'constant')\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\nprint(f(a, 2))\nprint(f(a, 3))\nprint(f(a, 4))\nprint(f(a, 5))\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nresult = (int(result[0]), int(result[1]))\nprint(result)", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    # Use numpy's floor_divide function to divide the numerator by the denominator\n    # and return the result as a tuple\n    return np.floor_divide(numerator, denominator)\n\n# Test the function\nprint(f(98, 42)) # Output: (7, 3)\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "if denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator // denominator, denominator // denominator)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.maximum(a, np.maximum(b, c))", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a.flatten())\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.diag(a.flatten())\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "result = np.concatenate((np.diag(a), np.diag(a.T)[::-1]))", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "result = np.concatenate((np.diag(a), np.diag(a.T)[::-1]))", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = np.array([x for x in X.flatten()])", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.sort(np.ravel(X))", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": " ###\n    result = np.empty((0, 1), dtype=np.object)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result = np.append(result, X[i, j])\n    return result\n    ### END SOLUTION ###\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty((X.shape[0], X.shape[1]), dtype=np.object)\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result[i, j] = X[i, j]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.fromstring(mystr, dtype=int, sep='')\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = np.cumsum(a[:, col] * multiply_number)", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = np.cumsum(a[row, :] * multiply_number)", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "result = np.multiply(a[row, :], divide_number)", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "# Find the maximum number of linearly independent vectors in a\nmax_independent_vectors = np.max(np.sum(a, axis=1))\n\n# Find the maximum set of linearly independent vectors\nresult = np.where(np.sum(a, axis=1) == max_independent_vectors)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "result = a.shape[0]", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "# calculate the t-statistic\nt_statistic = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_statistic, a.shape[0] + b.shape[0])\nprint(p_value)", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "# calculate the t-statistic\nt_stat = np.mean(a) - np.mean(b)\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_stat, np.size(a))\nprint(p_value)", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the weighted mean and variance\nweighted_mean = (anobs * amean + bnobs * bmean) / (anobs + bnobs)\nweighted_var = (anobs * avar + bnobs * bvar) / (anobs + bnobs)\n\n# calculate the t-value\nt_value = (weighted_mean - bmean) / np.sqrt((anobs * avar + bnobs * bvar) / (anobs + bnobs))\n\n# calculate the p-value\np_value = scipy.stats.t.pdf(t_value, anobs + bnobs)\n\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "output = A[~np.in1d(A, B)]", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "output = np.asarray([[1,1,2], [1,1,3], [0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0]])", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape)\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\nprint(c.shape)\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.sort(b, axis=1)\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, 0)", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "del_col = np.sort(del_col)[::-1]\ndel_col = del_col[del_col >= 0]\ndel_col = np.sort(del_--col)\nresult = np.delete(a, del_col, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a_l = a.tolist()\n    a_l.insert(pos, element)\n    a = np.asarray(a_l)\n    return a\n    ### END SOLUTION ###\n\nprint(f(a=example_a, pos=2, element=66))\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = np.array(array_of_arrays, copy=True)", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(np.all(a == a[0], axis=1))", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(np.all(a == a[0], axis=1))", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": " ###\n    return np.all(np.all(a == a[0], axis=1))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": " ###\n    # calculate the 2D integral over (cosx)^4 + (siny)^2\n    # use Simpson's rule\n    # result = np.sum(np.sum(np.sin(x)**2 + np.cos(y)**4, axis=1))\n    ### END SOLUTION ###\n    return result\n\n# call the function\nf(x, y)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  # normalize X to sum to 1\n  x = x / np.sum(x)\n  return np.cumsum(x)\n\nresult = ecdf(grades)\nprint(result)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "# compute the longest interval [low, high) that satisfies ECDF(x) < threshold for any x in [low, high)\nlow = np.min(grades)\nhigh = np.max(grades)\nwhile high - low > threshold:\n  high = np.max(grades[grades < high])\n  low = np.min(grades[grades < low])\n  if high - low < threshold:\n    break\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "randomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 1] = one_ratio\nrandomLabel[randomLabel == 0] = 1 - one_ratio\nrandomLabel = np.random.randint(2, size=size)\nrandomLabel[randomLabel == 1] = one_ratio\nrandomLabel[randomLabel == 0] = 1 - one_ratio\nrandomLabel", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = np.array(a)", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\nresult = result.tolist()\nresult = [result[i] for i in range(len(result))]\nprint(result)\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[::-1][:N]\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "patch_size = 2\npatch_count = a.shape[0] // patch_size\nresult = np.zeros((patch_count, patch_count, patch_size, patch_size))\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j, :, :] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "patch_size = 2\npatch_count = a.shape[0] // patch_size\nresult = np.zeros((patch_count, patch_count, patch_size, patch_size))\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "patch_size = 2\npatch_count = a.shape[0] // patch_size\nresult = np.zeros((patch_count, patch_count, patch_size, patch_size))\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j, :, :] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\n\npatch_count = a.shape[0] // patch_size\n\nresult = np.empty((patch_count, patch_count, patch_size, patch_size))\n\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w))\nprint(result)\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "patch_size = 2\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\n\npatch_size = 2\n\npatch_count = a.shape[0] // patch_size\n\nresult = np.empty((patch_count, patch_count, patch_size, patch_size))\n\nfor i in range(patch_count):\n    for j in range(patch_count):\n        result[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high]", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = a[:, low:high]", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.fromstring(string, sep=' ', dtype=np.float64)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "result = np.log(np.random.uniform(min, max, n))", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "result = np.log(np.random.uniform(np.exp(min), np.exp(max), n))", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": " ###\n    loguni = np.random.loguniform(min, max, n)\n    return loguni\n    ### END SOLUTION ###\n\nprint(f(min=1, max=np.e, n=10000))\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = A.copy()\nB[0] *= a\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = A.copy()\nB[1] = a * A[1] + b * B[0]\nfor t in range(2, len(A)):\n    B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,))", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.empty((3,0))", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "def sub2ind(dims, indices):\n    return np.ravel_multi_index(indices, dims)\n\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\nresult = sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "def sub2ind(dims, index):\n    return np.asscalar(np.prod(dims)) + np.asscalar(np.prod(index))\n\nresult = sub2ind(dims, index)\nprint(result)", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df.values.dtype)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "result = np.sum(a[accmap])\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.sum(a[accmap >= 0], axis=0)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.min(a[index])", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "def elementwise_function(element_1,element_2):\n    return (element_1 + element_2)\n\nz = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    z[i] = elementwise_function(x[i], y[i])\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, probabilit)\nprint(result)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "a = np.pad(a, ((0, 0), (low_index, high_index)), 'constant')\nresult = a[1:3, 1:3]", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "x = x[x >= 0]", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "x = np.delete(x, np.where(np.iscomplex(x)))\nresult = x", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_max = np.max(bin_data, axis=1)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "bin_data = np.array_split(data, bin_size)\nbin_data_mean = np.mean(bin_data, axis=1)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "bin_data = np.array_split(data, bin_size, axis=1)\nbin_data_mean = np.mean(bin_data, axis=1)\nbin_data_mean = bin_data_mean.tolist()\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    return x_min + (x_max - x_min) * (x - x_min) * (x - x_max)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    x_min = np.clip(x_min, 0, 1)\n    x_max = np.clip(x_max, 0, 1)\n    x = np.clip(x, x_min, x_max)\n    x = x * (x_max - x_min) + x_min\n    x = x * (x_max - x_min) + x_min\n    x = x * (x_max - x_min) + x_min\n    return x<x_min ? x_min : x>x_max ? x_max : x\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular correlation matrix\ncorr_matrix = np.zeros((len(a), len(b)))\nfor i in range(len(a)):\n    for j in range(len(b)):\n        corr_matrix[i, j] = np.corrcoef(a[i:], b[j:])[0, 1]\n\n# Create a circular", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(4)] for _ in range(4)])\nresult = np.reshape(result, (4, 15, 5))\nprint(result)\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array([[np.random.randint(10, size=(5, 4)) for _ in range(15)] for _ in range(15)])\nprint(result)\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:-1]\nresult = np.pad(result, (0, m - len(result)), 'constant')\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "result = np.unpackbits(np.uint8(a))\nresult = result[:m]\nresult = result.reshape(-1, m)\nresult", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "def convert_to_binary(a, m):\n    binary_array = np.unpackbits(np.uint8(a))\n    binary_array = binary_array[:m]\n    return binary_array\n\nresult = convert_to_binary(a, m)\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc_plus_three_sigma = \u03bc + 3*\u03c3\n\u03bc_minus_three_sigma = \u03bc - 3*\u03c3\nresult = (\u03bc_minus_three_sigma, \u03bc_plus_three_sigma)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "\u03bc = np.mean(a)\n\u03c3 = np.std(a)\n\u03bc_minus_2\u03c3 = \u03bc - 2*\u03c3\n\u03bc_plus_2\u03c3 = \u03bc + 2*\u03c3\nresult = (\u03bc_minus_2\u03c3, \u03bc_plus_2\u03c3)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": " ###\n    mean = np.mean(a)\n    std = np.std(a)\n    start = mean - 3*std\n    end = mean + 3*std\n    return (start, end)\n    ### END SOLUTION ###\n\nprint(f(a))\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "sigma = np.std(a)\nmean = np.mean(a)\nlower_bound = mean - 2*sigma\nupper_bound = mean + 2*sigma\noutlier = np.logical_or(a < lower_bound, a > upper_bound)\nresult = outlier", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data, percentile)\nprint(prob)", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, 0] = 0\na[0, 1] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.all(a == np.max(a, axis=1, keepdims=True), axis=1)", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.all(a == np.min(a, axis=1), axis=1)\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "result = np.corrcoef(post, distance)[0, 1]", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.matmul(X, np.transpose(X, (1, 0)))", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((Y.shape[0], Y.shape[1]))\nfor i in range(Y.shape[1]):\n    X[:, i] = np.dot(Y[:, i], Y[:, i].T)", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = np.any(a == number)", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\n# Create a set from B to avoid duplicates\nB_set = set(B)\n\n# Create a set from A to avoid duplicates\nA_set = set(A)\n\n# Create a set of all unique values in A that are not in B\nC = A_set - B_set\n\n# Convert the set to a list\nC = list(C)\n\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "A = A[~np.isin(A, B)]\n", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "# Create a boolean array that indicates whether the elements in A are in (1, 4) or (4, 8)\nmask = np.logical_or(np.logical_and(A == 1, B == 4), np.logical_and(A == 4, B == 8))\n# Use the mask to index A and B\nC = A[mask]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.max(a) - rankdata(a) + 1", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "result = np.max(rankdata(a)) - rankdata(a) + 1", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": " ###\n    return np.max(a) - np.min(a) + 1 - np.sum(np.diff(a))\n    ### END SOLUTION ###\n\nprint(f(example_a))\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dists = np.dstack((x_dists, y_dists))", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third].flatten()", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.sum(np.abs(X), axis=1)\nresult = X / l1.reshape(-1, 1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.array([LA.norm(v,ord=2) for v in X])\nresult = np.array([v/x[i] for i, v in enumerate(X)])\nprint(result)", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nresult = X / x[:, np.newaxis]\nprint(result)", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = [a['a'].str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "result = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if i == j:\n            result[i, j] = 0\n        else:\n            result[i, j] = np.linalg.norm(a[i] - a[j])\n", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "result = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if i != j:\n            result[i, j] = np.linalg.norm(a[i] - a[j])\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "result = np.zeros((dim, dim))\nfor i in range(dim):\n    for j in range(i + 1, dim):\n        result[i, j] = np.linalg.norm(a[:, i] - a[:, j])\n        result[j, i] = result[i, j]\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA, axis=0)\nprint(AVG)", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "a = np.unique(a)\nresult = a[a!=0]\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "# Find the indices of the non-zero values\nindices = np.where(a != 0)\n\n# Find the indices of the adjacent non-zero values\nadjacent_indices = np.where(a[indices] == a[indices[0]])\n\n# Find the indices of the zero values\nzero_indices = np.where(a == 0)\n\n# Find the indices of the adjacent zero values\nadjacent_zero_indices = np.where(a[zero_indices] == a[zero_indices[0]])\n\n# Find the indices of the zero values after the adjacent zero values\nafter_zero_indices = np.where(a[zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the adjacent non-zero values\nafter_non_zero_indices = np.where(a[adjacent_indices[0]] == 0)\n\n# Find the indices of the zero values after the adjacent zero values\nafter_adjacent_zero_indices = np.where(a[adjacent_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the adjacent non-zero values\nafter_after_non_zero_indices = np.where(a[after_non_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the adjacent zero values\nafter_after_adjacent_zero_indices = np.where(a[after_adjacent_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the adjacent non-zero values\nafter_after_after_non_zero_indices = np.where(a[after_after_non_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the adjacent zero values\nafter_after_after_adjacent_zero_indices = np.where(a[after_after_adjacent_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the zero values after the adjacent non-zero values\nafter_after_after_after_non_zero_indices = np.where(a[after_after_after_non_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the zero values after the adjacent zero values\nafter_after_after_after_adjacent_zero_indices = np.where(a[after_after_after_after_adjacent_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the zero values after the zero values after the adjacent non-zero values\nafter_after_after_after_after_non_zero_indices = np.where(a[after_after_after_after_after_non_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the zero values after the zero values after the zero values after the adjacent zero values\nafter_after_after_after_after_after_adjacent_zero_indices = np.where(a[after_after_after_after_after_after_adjacent_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the zero values after the zero values after the zero values after the zero values after the adjacent zero values\nafter_after_after_after_after_after_after_adjacent_zero_indices = np.where(a[after_after_after_after_after_after_after_adjacent_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the zero values after the zero values after the zero values after the zero values after the zero values after the adjacent zero values\nafter_after_after_after_after_after_after_after_adjacent_zero_indices = np.where(a[after_after_after_after_after_after_after_after_adjacent_zero_indices[0]] == 0)\n\n# Find the indices of the zero values after the zero values after the zero values after the zero values after the zero values after the zero values after the zero values after the zero", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "lat_val = np.concatenate((lat, val), axis=1)\nlon_val = np.concatenate((lon, val), axis=1)\n\ndf = pd.DataFrame({'lat': lat_val[:, 0], 'lon': lon_val[:, 0], 'val': lat_val[:, 1]})\n\nprint(df)\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": " ###\n    # Create a dataframe from the arrays\n    df = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n    # Sort the dataframe by the order of the arrays\n    df = df.sort_values(['lat', 'lon', 'val'])\n    # Return the sorted dataframe\n    return df\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "lat_val = np.max(lat, axis=1)\nlon_val = np.max(lon, axis=1)\nval_val = np.max(val, axis=1)\n\ndf = pd.DataFrame({'lat': lat_val, 'lon': lon_val, 'val': val_val})\n\nprint(df)\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, window_size):\n    shape = a.shape\n    shape = shape[-2:]\n    shape = shape + (shape[-1] - window_size[-1] + 1, window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n   ", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, window_size):\n    shape = a.shape\n    shape = shape[-2:]\n    shape = shape + (shape[-1] - window_size[-1] + 1, window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n    shape = shape + (window_size[-1], window_size[-1])\n   ", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\nprint(result)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": " ###\n    # compute the mean of the array of complex numbers\n    mean_value = np.mean(a)\n    # compute the mean of the imaginary part of the array\n    mean_imaginary = np.mean(a.imag)\n    # compute the mean of the real part of the array\n    mean_real = np.mean(a.real)\n    # compute the mean of the absolute value of the array\n    mean_abs = np.mean(np.abs(a))\n    # compute the mean of the absolute value of the imaginary part of the array\n    mean_abs_imag = np.mean(np.abs(a.imag))\n    # compute the mean of the absolute value of the real part of the array\n    mean_abs_real = np.mean(np.abs(a.real))\n    # compute the mean of the absolute value of the absolute value of the array\n    mean_abs_abs = np.mean(np.abs(np.abs(a)))\n    # compute the mean of the absolute value of the absolute value of the imaginary part of the array\n    mean_abs_abs_imag = np.mean(np.abs(np.abs(a.imag)))\n    # compute the mean of the absolute value of the absolute value of the real part of the array\n    mean_abs_abs_real = np.mean(np.abs(np.abs(a.real)))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the array\n    mean_abs_abs_abs = np.mean(np.abs(np.abs(np.abs(a))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the imaginary part of the array\n    mean_abs_abs_abs_imag = np.mean(np.abs(np.abs(np.abs(a.imag))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the real part of the array\n    mean_abs_abs_abs_real = np.mean(np.abs(np.abs(np.abs(a.real))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the array\n    mean_abs_abs_abs_abs = np.mean(np.abs(np.abs(np.abs(np.abs(a)))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the imaginary part of the array\n    mean_abs_abs_abs_abs_imag = np.mean(np.abs(np.abs(np.abs(np.abs(a.imag)))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the real part of the array\n    mean_abs_abs_abs_abs_real = np.mean(np.abs(np.abs(np.abs(np.abs(a.real)))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the array\n    mean_abs_abs_abs_abs_abs = np.mean(np.abs(np.abs(np.abs(np.abs(np.abs(a))))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the imaginary part of the array\n    mean_abs_abs_abs_abs_abs_imag = np.mean(np.abs(np.abs(np.abs(np.abs(np.abs(a.imag))))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the real part of the array\n    mean_abs_abs_abs_abs_abs_real = np.mean(np.abs(np.abs(np.abs(np.abs(np.abs(a.real))))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the array\n    mean_abs_abs_abs_abs_abs_abs = np.mean(np.abs(np.abs(np.abs(np.abs(np.abs(np.abs(a)))))))\n    # compute the mean of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the absolute value of the imaginary part of", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "result = Z[:,:,-1:]", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "result = a[-1:, :]", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "print(c in CNTS) # prints True\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "print(c in CNTS) # True\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(x_new, y_new)\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.diag(i)\nprint(i)", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[1:, 1:] = 0", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "start_date = pd.to_datetime(start)\nend_date = pd.to_datetime(end)\n\n# Calculate the number of days between the start and end dates\ndays = (end_date - start_date).days\n\n# Create a linear space of dates\ndates = pd.date_range(start=start_date, end=end_date, periods=n)\n\n# Convert the dates to a pandas DatetimeIndex\ndates = pd.DatetimeIndex(dates)\n\n# Print the dates\nprint(dates)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "result = np.where((x == a) & (y == b))[0][0]\nif result == -1:\n    result = -1\nelse:\n    result = result + 1\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "indices = np.where((x == a) & (y == b))\nresult = indices[0]", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "# calculate the coefficients a, b and c\na = np.sum((y - np.mean(y)) ** 2) / np.sum((x - np.mean(x)) ** 2)\nb = np.mean(y - a * np.mean(x) ** 2)\nc = np.mean(y - a * np.mean(x) * np.mean(x) - b * np.mean(x))\nresult = np.array([a, b, c])\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "# calculate the coefficients\ncoeffs = np.polyfit(x, y, degree)\n# sort the coefficients in descending order\nresult = np.sort(coeffs)[::-1]\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = np.array([0, 1, 2, 3])\ntemp_df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]])\n\nresult = np.subtract(temp_df, temp_arr)\nprint(result)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\nprint(result)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "mask = arr < -10\narr[mask] = 0\nmask = arr >= 15\narr[mask] = 30\narr[~mask] += 5\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr = np.where(arr < n1, 0, np.where(arr >= n2, 30, arr + 5))", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "print np.nonzero(s1 != s2)[0].shape[0]", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "# replace NaN with 0\ns1 = np.where(np.isnan(s1), 0, s1)\ns2 = np.where(np.isnan(s2), 0, s2)\n# compare s1 and s2\nresult = np.nonzero(s1 != s2)[0].shape[0]\nprint(result)\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.all(np.array(a) == np.array(a[0]))", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = np.all(np.isnan(a))", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "result = np.pad(a, ((0, shape[0]-a.shape[0]), (0, shape[1]-a.shape[1])), 'constant')", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "# find the maximum shape of the array\nmax_shape = np.max(shape)\n# create a zeros array with the maximum shape\nresult = np.zeros(max_shape)\n# fill the zeros array with the element\nresult[:a.shape[0], :a.shape[1]] = element\n# pad the zeros array to the right and bottom of the original array\nresult = np.pad(result, ((0, max_shape[0]-a.shape[0]), (0, max_shape[1]-a.shape[1])), 'constant')\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": " ###\n    # find the maximum shape of the array\n    max_shape = np.max(np.shape(arr))\n    # find the maximum shape of the shape\n    max_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape = np.max(np.shape(shape))\n    # find the maximum shape of the shape\n    max_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_shape_", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "# find the maximum shape\nmax_shape = np.max(shape)\n# calculate the padding\npadding = np.array([(max_shape - shape[0]) // 2, (max_shape - shape[1]) // 2])\n# zero pad the array\nresult = np.pad(a, padding, mode='constant')\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3, 3)", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b])", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1])[None, :], b[:, None]], axis=2)", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = np.where(df['a'] > 1, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "# Create a mask that is True for all rows except the first and last row\nmask = np.any(im[1:-1, :] != 0, axis=1)\n# Create a mask that is True for all columns except the first and last column\nmask_col = np.any(im[:, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and columns except the first and last row and column\nmask_all = np.any(im[1:-1, 1:-1] != 0, axis=1)\n# Create a mask that is True for all rows and", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "# Find the indices of the non-zero values in the array\nindices = np.where(A != 0)\n\n# Find the bounding box of the non-zero values\nmin_row, max_row = np.min(indices[0]), np.max(indices[0])\nmin_col, max_col = np.min(indices[1]), np.max(indices[1])\n\n# Slice the array to the bounding box\nresult = A[min_row:max_row+1, min_col:max_col+1]", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "def remove_peripheral_zeros(im):\n    row_count = np.sum(im, axis=1)\n    col_count = np.sum(im, axis=0)\n    row_mask = row_count > 0\n    col_mask = col_count > 0\n    return im[row_mask, :][:, col_mask]\n\nresult = remove_peripheral_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "# Find the row with the maximum number of zeros\nmax_zeros = np.max(np.sum(im, axis=1))\n# Find the row with the maximum number of zeros\nmax_row = np.argmax(np.sum(im, axis=1))\n# Create a new array with the row with the maximum number of zeros\nresult = im[max_row]", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 5))\nsns.lineplot(x, y, label='x-y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "line_styles = ['-', '--', '-.', ':']\nfor line_style in line_styles:\n    plt.plot(x, np.random.randn(10), line_style)\n\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "line_styles = ['-', '--', '-.', ':']\nfor line_style in line_styles:\n    plt.plot(x, np.random.randn(10), line_style)\n\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.figure(figsize=(10, 5))\nsns.lineplot(x, y, marker='d')\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 5))\nsns.lineplot(x, y, marker='d', linewidth=2)\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "highlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight_range = np.sort(highlight_range)\nhighlight_range = np.unique(highlight_range)\n\nhighlight_range = np.array([2, 4])\nhighlight_color = 'r'\n\nhighlight", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = x * 2\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.linspace(0, 1, 100)\ny = x * 2\n\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "seaborn.relplot(\n    data=df,\n    x=\"Height (cm)\",\n    y=\"Weight (kg)\",\n    hue=\"Gender\",\n    kind=\"scatter\",\n    height=5,\n    aspect=1,\n)\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\nsns.set_style('white')\nsns.set_style('whitegrid')\nsns.set_style('dark')\ns", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "sns.lineplot(x, y)\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label='cos(x)')\nplt.legend(title='xyz', title_fontsize=20, fontsize=16)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('cos(x)')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, color=\"k\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.setp(l, color='r')\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "x_rotated = np.rot90(x, 1)\ny_rotated = np.rot90(y, 1)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "x_rotated = np.rot90(x, 1, 1)\ny_rotated = np.rot90(y, 1, 1)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.linspace(0, 2 * np.pi, 10)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='hot')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "xlabel = \"X\"\nplt.xlabel(xlabel)\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticks(g.get_xticks() + 90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "# fit a very long title myTitle into multiple lines\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = np.sort(y)\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.linspace(0, 1.5, 11)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "y_ticks = np.linspace(-1, 1, 11)\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "x_y_z = np.stack([x, y, z], axis=1)\n\n# plot x, then y then z\nplt.figure(figsize=(10, 5))\nplt.plot(x_y_z[:, 0], x_y_z[:, 1], label='x')\nplt.plot(x_y_z[:, 0], x_y_z[:, 2], label='y')\nplt.plot(x_y_z[:, 1], x_y_z[:, 2], label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, facecolor='blue', edgecolor='black')\n\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "x = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# make all axes ticks integers\nx = np.arange(10", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.ticklabel_format(style='plain', axis='y')\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "# Create a lineplot with dashed line\nsns.lineplot(x=x, y=y, linestyle='-')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y1, label='sin(x)')\naxs[1].plot(x, y2, label='cos(x)')\naxs[0].set_xlabel('x')\naxs[1].set_xlabel('x')\naxs[0].set_ylabel('y1')\naxs[1].set_ylabel('y2')\naxs[0].legend()\naxs[1].legend()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "plt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(x, y1, 'r')\nplt.title('sin(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y2, 'b')\nplt.title('cos(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.ticklabel_format(axis='x', style='plain')\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(3, 4, 1))\nplt.grid(True, which='both')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(3, 4, 0.1))\nplt.grid(True, which='both')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(3, 4, 0.5))\nplt.xticks(np.arange(1, 2, 0.5))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "# axes = axes.flatten()\n# for ax in axes:\n#     ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n#     ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n# plt.tight_layout()\n# plt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlabel(\"Column Labels\")\nax.set_ylabel(\"Row Labels\")\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='y')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Plot of y over x')\nplt.legend()\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.xticks(())\nplt.yticks(())\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of y over x')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y, label='Y')\nplt.ylabel('Y')\nplt.xlabel('X')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "# Create a joint regression plot\nsns.jointplot(data=tips, x='total_bill', y='tip', color='green')\n\n# Create a distribution plot\nsns.distplot(tips['total_bill'], color='blue')\n\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "# Create a new dataframe with the regression line\nregression_line = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                               'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the histogram\nhistogram = pd.DataFrame({'total_bill': tips['total_bill'],\n                           'tip': tips['tip']})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill'].max(), 100),\n                                         'tip': np.linspace(tips['tip'].min(), tips['tip'].max(), 100)})\n\n# Create a new dataframe with the regression line and histogram\nregression_line_histogram = pd.DataFrame({'total_bill': np.linspace(tips['total_bill'].min(), tips['total_bill", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "# Create a joint regression plot\nsns.jointplot(data=tips, x='total_bill', y='tip')\n\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 5))\nfor i, celltype in enumerate(df[\"celltype\"]):\n    plt.bar(i, df[\"s1\"][i], label=celltype)\n\nplt.xlabel(\"celltype\")\nplt.ylabel(\"s1\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 5))\nfor i, celltype in enumerate(df[\"celltype\"]):\n    plt.bar(i, df[\"s1\"][i], label=celltype)\n    plt.bar(i, df[\"s2\"][i], label=celltype, bottom=df[\"s1\"][i])\n\nplt.xlabel(\"Celltype\")\nplt.xticks(range(len(df[\"celltype\"])), df[\"celltype\"], rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Plot of Y over X')\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend(loc='upper left')\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\nx = [0.22058956, 0.33088437, 2.20589566]\ny = [0, 0, 0]\nplt.plot(x, y, '--')\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(rand_mat, cmap='hot', aspect='auto')\nplt.xlabel(xlabels, rotation=90)\nplt.ylabel(ylabels, rotation=0)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "# copy the code of the above plot and edit it to have legend for all three cruves in the two subplots\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\n\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "# make a scatter plot\nplt.scatter(a, b, c)\n\n# annotate each data point with correspond numbers in c\nfor i, j in zip(a, b):\n    plt.annotate(str(c[a.index(i)]), (i, j))\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.title('Line Chart')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, bins=10, edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\n# Create a data frame with two columns\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create a line plot of the data\nax1.plot(x, y, 'r')\n\n# Create a line plot of the data\nax2.plot(x, y, 'g')\n\n# Set the title of the first subplot\nax1.set_title('Line 1')\n\n# Set the title of the second subplot\nax2.set_title('Line 2')\n\n# Set the x-axis label of the first subplot\nax1.set_xlabel('x')\n\n# Set the x-axis label of the second subplot\nax2.set_xlabel('x')\n\n# Set the y-axis label of the first subplot\nax1.set_ylabel('y')\n\n# Set the y-axis label of the second subplot\nax2.set_ylabel('y')\n\n# Set the legend of the first subplot\nax1.legend(['Line 1'])\n\n# Set the legend of the second subplot\nax2.legend(['Line 2'])\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, label='x')\nplt.hist(y, bins, alpha=0.5, label='y')\n\n# Set the labels of the x and y axis\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Set the title of the chart\nplt.title('Histogram of x and y')\n\n# Set the legend of the chart\nplt.legend()\n\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 5))\nplt.hist([x, y], bins=10, color=['r', 'b'], label=['x', 'y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "x = np.linspace(0, 5, 100)\ny = a * x + b\n\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "# create a colorbar\ncbar = plt.colorbar(plt.imshow(x, cmap='hot'), ax=plt.gca())\n# set the colorbar's label\ncbar.set_label('x')\n# create a colorbar for y\ncbar = plt.colorbar(plt.imshow(y, cmap='hot'), ax=plt.gca())\n# set the colorbar's label\ncbar.set_label('y')\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\nax1.plot(x, y, label='Y')\nax1.plot(a, z, label='Z')\nax1.set_title('Y and Z')\nax1.legend()\nax2.plot(x, z, label='Y')\nax2.plot(a, y, label='Z')\nax2.set_title('Z and Y')\nax2.legend()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "x, y = zip(*points)\n\nplt.plot(x, y, 'o-')\nplt.yscale('log')\n\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "x_log_y_log = np.log10(x) * np.log10(y)\n\nplt.loglog(x, y, 'o')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Log-log plot')\nplt.grid(True)\n\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "plt.figure(figsize=(10, 5))\nfor column in df.columns:\n    plt.plot(df.index, df[column], label=column)\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "# Calculate the total sum of data\ntotal_sum = np.sum(data)\n\n# Normalize the data to sum up to 1\nnormalized_data = [x / total_sum for x in data]\n\n# Format the y tick labels into percentage\ny_tick_labels = [\"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\"]\n\n# Create a histogram of normalized data\nplt.hist(normalized_data, bins=7, edgecolor='black')\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 1, 0.1))\nplt.xlabel('Percentage')\n\n# Set the x tick labels as percentage\nplt.xlim(0, 1)\n\n# Set the y tick labels as percentage\nplt.ylim(0, 1)\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 1, 0.1))\nplt.xlabel('Percentage')\n\n# Set the x tick labels as percentage\nplt.xlim(0, 1)\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 1, 0.1))\nplt.xlabel('Percentage')\n\n# Set the x tick labels as percentage\nplt.xlim(0, 1)\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 1, 0.1))\nplt.xlabel('Percentage')\n\n# Set the x tick labels as percentage\nplt.xlim(0, 1)\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 1, 0.1))\nplt.xlabel('Percentage')\n\n# Set the x tick labels as percentage\nplt.xlim(0, 1)\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 1, 0.1))\nplt.xlabel('Percentage')\n\n# Set the x tick labels as percentage\nplt.xlim(0, 1)\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 1, 0.1))\nplt.xlabel('Percentage')\n\n# Set the x tick labels as percentage\nplt.xlim(0, 1)\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.1))\nplt.ylabel('Percentage')\n\n# Set the x tick labels\nplt.xticks(np.arange(0, 1, 0.1))\nplt.xlabel('Percentage')\n\n# Set the x tick labels as percentage\nplt.xlim(0, 1)\n\n# Set the y tick labels\nplt.yticks(np.arange(0, 1, 0.", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, alpha=0.5, linestyle='-')\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='a')\naxs[0].legend()\naxs[1].plot(x, y, label='y')\naxs[1].plot(a, z, label='a')\naxs[1].legend()\n\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 10))\n\n# Create a regression plot for \"bill_depth_mm\" over \"bill_length_mm\"\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", ax=axs[0])\n\n# Create a regression plot for \"flipper_length_mm\" over \"bill_length_mm\"\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"flipper_length_mm\", ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xlabel(\"Second\")\nax.set_xlim(1, 10)\nax.set_xticks(range(1, 10))\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "x_extra = np.arange(2.1, 7.6, 0.1)\nplt.xticks(np.concatenate([x_extra, x]))\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(np.arange(2010, 2020), rotation=-60)\nplt.tick_params(axis='x', which='both', labelleft=False, labelright=False, left=False, right=False, labelright=False)\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.tick_params(axis='y', which='both', left=False, right=False, top=False, bottom=False, labelleft=False, labelright=False, labeltop=False, labelbottom=False, labelsize=10)\nplt.plot(x, y)\nplt.tick_params(axis='y', which='both', left=False, right=False, top=False, bottom=False, labelleft=False, labelright=False, labeltop=False, labelbottom=False, labelsize=10)\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.tick_params(axis='x', which='both', labelsize=10, alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(x=0, y=0)\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\naxs.plot(x, y, label='y')\naxs.plot(x, y, label='y')\naxs.set_title('Figure')\naxs.legend()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.plot(df[\"Type A\"], label=\"Type A\")\nplt.plot(df[\"Type B\"], label=\"Type B\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='|')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecolor='none')\nplt.scatter(x, y, edgecolor='none', hatch='|')\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, hatch='*')\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='*', hatch='v')\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "xlim = [1, 5]\nylim = [1, 4]\n\nplt.figure(figsize=(10, 10))\nplt.imshow(data, extent=[xlim[0], xlim[1], ylim[0], ylim[1]], aspect='auto')\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "x = np.linspace(0, 10, 100)\ny = np.linspace(0, 10, 100)\n\nplt.plot(x, y, label='cutoff')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 5))\nplt.bar(labels, height, width=0.5, color='blue')\nplt.title('Bar plot')\nplt.xlabel('Labels')\nplt.ylabel('Height')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedge_width=0.4)\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.minorticks_on()\nplt.grid(axis='x', which='minor', color='gray', linestyle='-', linewidth=0.5)\nplt.grid(axis='y', which='minor', color='gray', linestyle='-', linewidth=0.5)\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title(\"Pie Chart\")\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.figure(figsize=(10, 10))\nplt.pie(sizes, labels=labels, colors=colors, shadow=True, startangle=140, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title(\"Pie Chart\")\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, '--', alpha=0.5, edgecolor='none')\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color='g')\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Create a figure with 2 rows and 1 column\nfig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n# Create the bars for blue and orange bars\nax[0].bar(np.arange(3), blue_bar, color='b')\nax[1].bar(np.arange(3), orange_bar, color='orange')\n\n# Set the labels for the x-axis and y-axis\nax[0].set_xlabel('Bar')\nax[0].set_ylabel('Height')\nax[1].set_xlabel('Bar')\nax[1].set_ylabel('Height')\n\n# Set the title of the plot\nax[0].set_title('Blue bars')\nax[1].set_title('Orange bars')\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label='y')\naxs[0].plot(a, z, label='z')\naxs[0].legend()\naxs[1].plot(x, z, label='z')\naxs[1].legend()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "x_ticks = np.arange(0, 10, 1)\nplt.plot(x, y)\nplt.xticks(x_ticks)\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "sns.catplot(data=df, x=\"bill_length_mm\", y=\"species\", hue=\"sex\", col=\"species\", sharey=False, kind=\"bar\")\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(10, 10))\nplt.plot([0.5], [0.5], 'ro')\nplt.plot([0.5, 0.5], [0, 1], 'r')\nplt.plot([0, 1], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.5, 0.5], 'r')\nplt.plot([0.5, 0.5], [0.", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontsize=16, bold=True)\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Line')\nplt.legend(labelspacing=0.1)\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Line')\nplt.legend(handlelength=0.3)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\nplt.plot(x, y, marker=\"*\", label=\"Line\")\nplt.legend(loc='best')\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(data, cmap='hot')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(1, figsize=(10, 5))\nplt.title(\"Figure 1\", bold=True)\nplt.plot(x, y)\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.invert_xaxis()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolor='black')\n\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax, x_val, y_val in zip(axs.flat, x, y):\n    ax.plot(x_val, y_val)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('y over x')\n\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('x')\nplt.ylabel('frequency')\nplt.title('Histogram of x')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 5))\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.fill_between(x, y-error, y+error, alpha=0.2)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Error bars')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.plot([0, 0], [0, 0], 'w')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "for i in range(len(box_errors)):\n    ax.errorbar(box_position[i], box_height[i], yerr=box_errors[i], fmt='o', color=c[i])\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True)\nax1.plot(x, y, label='Y')\nax1.set_title('Y')\nax2.plot(a, z, label='Z')\nax2.set_title('Z')\nax2.set_title('Z')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xticks(np.arange(0, 10, 1))\n        axs[i, j].set_yticks(np.arange(0, 10, 1))\n        axs[i, j].set_xlabel('x')\n        axs[i, j].set_ylabel('y')\n\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.matshow(d, fignum=1)\nplt.matshow(d, fignum=2)\nplt.matshow(d, fignum=3)\nplt.matshow(d, fignum=4)\nplt.matshow(d, fignum=5)\nplt.matshow(d, fignum=6)\nplt.matshow(d, fignum=7)\nplt.matshow(d, fignum=8)\nplt.matshow(d, fignum=9)\nplt.matshow(d, fignum=10)\nplt.matshow(d, fignum=11)\nplt.matshow(d, fignum=12)\nplt.matshow(d, fignum=13)\nplt.matshow(d, fignum=14)\nplt.matshow(d, fignum=15)\nplt.matshow(d, fignum=16)\nplt.matshow(d, fignum=17)\nplt.matshow(d, fignum=18)\nplt.matshow(d, fignum=19)\nplt.matshow(d, fignum=20)\nplt.matshow(d, fignum=21)\nplt.matshow(d, fignum=22)\nplt.matshow(d, fignum=23)\nplt.matshow(d, fignum=24)\nplt.matshow(d, fignum=25)\nplt.matshow(d, fignum=26)\nplt.matshow(d, fignum=27)\nplt.matshow(d, fignum=28)\nplt.matshow(d, fignum=29)\nplt.matshow(d, fignum=30)\nplt.matshow(d, fignum=31)\nplt.matshow(d, fignum=32)\nplt.matshow(d, fignum=33)\nplt.matshow(d, fignum=34)\nplt.matshow(d, fignum=35)\nplt.matshow(d, fignum=36)\nplt.matshow(d, fignum=37)\nplt.matshow(d, fignum=38)\nplt.matshow(d, fignum=39)\nplt.matshow(d, fignum=40)\nplt.matshow(d, fignum=41)\nplt.matshow(d, fignum=42)\nplt.matshow(d, fignum=43)\nplt.matshow(d, fignum=44)\nplt.matshow(d, fignum=45)\nplt.matshow(d, fignum=46)\nplt.matshow(d, fignum=47)\nplt.matshow(d, fignum=48)\nplt.matshow(d, fignum=49)\nplt.matshow(d, fignum=50)\nplt.matshow(d, fignum=51)\nplt.matshow(d, fignum=52)\nplt.matshow(d, fignum=53)\nplt.matshow(d, fignum=54)\nplt.matshow(d, fignum=55)\nplt.matshow(d, fignum=56)\nplt.matshow(d, fignum=57)\nplt.matshow(d, fignum=58)\nplt.matshow(d, fignum=59)\nplt.matshow(d, fignum=60)\nplt.matshow(d, fignum=61)\nplt.matshow(d, fignum=62)\nplt.matshow(d, fignum=63)\nplt.matshow(d, fignum=64)\nplt.matshow(d, f", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nplt.table(\n    df,\n    col_width=[0.1, 0.1, 0.1, 0.1],\n    col_names=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    row_names=range(1, 11),\n    cell_contents=df.values,\n    bbox=[0, 0, 1, 1],\n)\nplt.show()\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, 'r')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.grid(True)\nplt.xlim([0, 10])\nplt.ylim([0, 10])\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xticks(np.arange(0, 10, 1))\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "# Create a new dataframe with only the rows where \"kind\" is \"Fat\"\ndf_fat = df[df[\"kind\"] == \"Fat\"]\n\n# Create a new dataframe with only the rows where \"kind\" is \"No Fat\"\ndf_no_fat = df[df[\"kind\"] == \"No Fat\"]\n\n# Create a scatter plot for the dataframe df_fat\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_fat, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.title(\"Group: Fat\")\n\n# Create a scatter plot for the dataframe df_no_fat\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_no_fat, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.title(\"Group: No Fat\")\n\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "# Create a new dataframe with only the columns \"time\", \"pulse\", and \"kind\"\ndf_scatter = df[[\"time\", \"pulse\", \"kind\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"kind\"\ndf_cat = df[[\"time\", \"kind\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"pulse\" and \"diet\"\ndf_pulse_diet_only = df[[\"pulse\", \"diet\"]]\n\n# Create a new dataframe with only the columns \"time\" and \"diet\"\ndf_time_diet_only = df[[\"time\", \"diet\"]]\n\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "# Create a scatter plot of the data\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.plot(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Plot of y over x')\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y, label='y')\nplt.legend(borderless=True)\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 5))\nplt.plot(t, a, label='a')\nplt.plot(t, b, label='b')\nplt.plot(t, c, label='c')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "stripplot = sns.stripplot(data=df, x=\"sex\", y=\"bill_length_mm\", hue=\"species\")\nstripplot.set_axis_labels(\"Sex\", \"Bill length (mm)\")\nstripplot.set_axis_ticks(np.arange(0, 10, 1))\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)\nstripplot.set_axis_limits(0, 10, 0, 10)", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5, share_all=True)\n\n# Create a FaceGrid object\nfacegrid = sns.FaceGrid(df, row_shape=(10, 1), aspect=0.5,", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_zlim([0, 1])\nax.set_view_angle(30, 10)\nax.set_view_angle(30, 10)\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.tick_params(axis='x', labelbottom=False, labeltop=False, labelleft=False, labelright=False, bottom=False, top=False, labelbottom=False)\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "# Create a gridspec with nrow and ncol\ngs = gridspec.GridSpec(nrow, ncol)\n\n# Create a subplot for each image\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.imshow(x[i])\n    ax.axis('off')\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x = tf.Variable(1)\nx = tf.cast(x, tf.int32)\nprint(x.numpy())\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nx = tf.cast(x, tf.int32)\nprint(x.numpy())", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, 10)\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nlabels = tf.reshape(labels, [1, 10])\nresult = tf.where(labels == 0, 0, 1)\nresult = tf.where(labels == 6, 1, result)\nresult = tf.where(labels == 5, 1, result)\nresult = tf.where(labels == 4, 1, result)\nresult = tf.where(labels == 2, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1, result)\nresult = tf.where(labels == 0, 1, result)\nresult = tf.where(labels == 1, 1,", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, 10)\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": " ###\n    # Create a tensor with the same shape as the labels\n    result = tf.zeros_like(labels)\n\n    # Iterate over the labels and set the corresponding value to 1\n    for i in range(len(labels)):\n        result[i, labels[i]] = 1\n\n    return result\n\n# Call the function\nprint(f())\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "labels = tf.constant([0, 6, 5, 4, 2])\nresult = tf.one_hot(labels, 10)\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return [[i, i+1, i+2]]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": " ###\n    return tf.concat([input, input + 1, input + 2], axis=0)\n\ndef map_func(input):\n    return tf.reshape(f(input), [-1])\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(map_func)\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n    for _ in range(9):\n        result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]])\nmask = tf.cast(lengths_padded > 0, tf.int32)\nmask = tf.reshape(mask, [4, 8])\nresult = tf.where(mask == 0, tf.zeros_like(mask), tf.ones_like(mask))\nprint(result)\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]])\nmask = tf.cast(lengths_padded > 0, tf.int32)\nmask = tf.where(mask, tf.ones_like(mask), tf.zeros_like(mask))\nresult = tf.where(mask, tf.ones_like(mask), tf.zeros_like(mask))\nprint(result)\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [0, 8-lengths.shape[0]]])\n\nmask = tf.cast(lengths_padded > 0, tf.float32)\n\nprint(mask)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": " ###\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths), 8), dtype=tf.int32)\n    # Create a mask of 1s and 0s with the length of the lengths tensor\n    mask = tf.ones((len(lengths),", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "lengths = tf.constant([4, 3, 5, 2])\nlengths_padded = tf.pad(lengths, [[0, 0], [1, 0]], 'CONSTANT')\nmask = tf.cast(lengths_padded > 0, tf.int32)\nresult = tf.where(mask == 0, tf.ones_like(lengths_padded), tf.zeros_like(lengths_padded))\nresult = tf.reshape(result, (4, 8))\nprint(result)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([a, b], axis=1)\nresult = tf.transpose(result)\nresult = tf.reshape(result, [-1, 2])\nresult = tf.reshape(result, [2, -1])\nresult = tf.transpose(result)\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": " ###\n    # Use tf.stack to combine the two tensors\n    # Then use tf.reshape to convert the result to a 2D array\n    # Finally use tf.transpose to swap the rows and columns\n    # result = tf.stack([a, b])\n    # result = tf.reshape(result, [-1, 2])\n    # result = tf.transpose(result)\n    # return result\n    ### END SOLUTION ###\n\nprint(f(a=example_a,b=example_b))\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "a = tf.reshape(a, (50, 100, 512))\nresult = a", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "a = tf.reshape(a, (50, 100, 1, 512))", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "a = tf.reshape(a, (1, 50, 100, 1, 512))\nresult = a", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = tf.reduce_sum(A, axis=1)", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = tf.reduce_prod(A, axis=1)", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "# calculate the square difference\ndiff = tf.square(a - b)\n# reduce the square difference by row\nresult = tf.reduce_sum(diff, axis=1)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "# calculate the square difference\ndiff = tf.square(a - b)\n# reduce the square difference by column\nresult = tf.reduce_sum(diff, axis=0)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": " ###\n    # calculate the square difference between A and B\n    square_diff = tf.square(tf.subtract(A, B))\n    # reduce the square difference by row\n    result = tf.reduce_sum(square_diff, axis=1)\n    ### END SOLUTION ###\n    return result\n\nprint(f(A=example_a,B=example_b))\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = x[y,z]\nresult = tf.gather(x, y, axis=1)\nresult = tf.gather(result, z, axis=1)\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = x[row, col]\nresult = tf.gather(x, col, row)\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": " ###\n    return tf.gather(x, y, axis=1)\n\nresult = f(x,y,z)\nprint(result)\n### END SOLUTION ###\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "C = tf.math.multiply(A, B)\nresult = tf.reduce_sum(C, axis=2)\nprint(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "C = tf.math.multiply(A, B)\nresult = tf.reduce_sum(C, axis=2)\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "x = tf.constant([\n    b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a'])\n\nresult = tf.strings.unicode_transpose(x)\nprint(result)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": " ###\n    # Convert the byte list to string list\n    result = tf.strings.unicode_transpose(x)\n    ### END SOLUTION ###\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "x = tf.where(x != 0, x, tf.zeros_like(x))\nx = tf.reduce_sum(x, axis=-1)\nx = tf.where(x != 0, x / x, tf.zeros_like(x))\nresult = x", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "x = tf.where(x != 0, x, tf.zeros_like(x))\nx = tf.reduce_mean(x, axis=-1)\nresult = tf.where(x != 0, x, tf.zeros_like(x))\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    # calculate the sum of the non-zero elements in the second to last dimension\n    sum_non_zero = tf.reduce_sum(x, axis=-1)\n    # calculate the average of the non-zero elements in the second to last dimension\n    average = tf.reduce_mean(x, axis=-1)\n    # calculate the average of the non-zero elements in the second to last dimension\n    # by dividing the average by the sum of the non-zero elements\n    result = tf.divide(average, sum_non_zero)\n    ### END SOLUTION\n    return result\n\nprint(f(example_x))\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\n# Create a session\nsess = tf.compat.v1.Session()\n\n# Create two random matrices\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Calculate the sum of the matrix multiplication\nresult = tf.reduce_sum(tf.matmul(A, B))\n\n# Run the session\nprint(sess.run(result))\n\n# Close the session\nsess.close()\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "# Find the maximum value in each row\nmax_values = tf.argmax(a, axis=1)\n# Count the number of times the maximum value in each row is equal to the actual value\ncount = tf.math.count_nonzero(max_values == a, axis=1)\n# Calculate the accuracy\naccuracy = tf.math.divide(count, tf.math.count_nonzero(a, axis=1))\n# Print the accuracy\nprint(accuracy)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "# Find the maximum value in each column\nmax_values = tf.argmax(a, axis=1)\n# Convert the maximum value to the index of the maximum value in each column\nresult = tf.where(a == max_values[:, None], max_values, tf.constant(0))\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": " ###\n    # find the maximum value in each row\n    max_values = tf.argmax(a, axis=1)\n    # count the number of same values\n    count = tf.math.count_nonzero(a == max_values, axis=1)\n    # return the count\n    return count\n\nprint(f(example_a))\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "result = tf.argmin(a, axis=1)", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n\n# Save the model\nmodel.save('my_model')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "# generate 10 random integers from the uniform distribution\nresult = tf.random.uniform([10], minval=1, maxval=4, dtype=tf.int32)\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "seed_x = 10\ntf.random.set_seed(seed_x)\n\nrandom_integers = tf.random.uniform([114], minval=2, maxval=5, dtype=tf.int32)\nresult = tf.reshape(random_integers, [114, 1])\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": " ###\n    tf.random.set_seed(seed_x)\n    random_integers = tf.random.uniform([10], minval=1, maxval=4, dtype=tf.int32)\n    return random_integers\n\n# call the function\nprint(f())\n### END SOLUTION ###\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "print(tf.__version__)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "# Fit the data using the polyfit function\nresult = scipy.optimize.polyfit(x, y, 1)\nresult = np.array([result[0], result[1]])\nprint(result)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "# Fit the data using a polynomial of order 2\npoly_order = 2\npoly_coeffs = scipy.optimize.polyfit(x, y, poly_order)\n\n# Create a function to calculate the line\ndef line(x, A, B):\n    return A + B * x\n\n# Calculate the line coefficients\nA = poly_coeffs[0]\nB = poly_coeffs[1]\n\n# Create a line function\nline_func = lambda x: line(x, A, B)\n\n# Calculate the line coefficients\nresult = np.array([A, B])\n\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\nresult = np.array([result[0], result[1], result[2]])\nprint(result)", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "statistic, p_value = stats.ks_2samp(x, y)\nprint(statistic, p_value)", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.kstest(x, y)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    a, b, c = x\n    return ((a + b - c)**2 + (3*a - b - c)**2 + sin(b) + cos(b) + 4)\n\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.zscore(z_scores)\np_values = scipy.stats.t(z_scores, alpha=0.05)\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = scipy.stats.t.pdf(z_scores, scipy.stats.t.shape(scipy.stats.t.rvs(0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = scipy.stats.zscore(p_values)", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(total,mu,stddev)\nexpected_value = dist.mean()\nmedian = dist.median()\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": " ###\n    result = sA * sB\n    return result\n\nprint(f(example_sA, example_sB))\n<code> \n\nThe solution is to use the numpy array to create a sparse CSR matrix and then multiply it with the numpy array. The result will be a sparse CSR matrix.\n\n# Create a sparse CSR matrix from a numpy array\nsA = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],[7,8,9]]))\n\n# Create a numpy array\nc = np.array([0,1,2])\n\n# Multiply the sparse CSR matrix with the numpy array\nresult = sA * c\n\nprint(result)\n<code> \n\nThe solution is to use the numpy array to create a sparse CSR matrix and then multiply it with the numpy array. The result will be a sparse CSR matrix.\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid from the points\nx, y, z = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the points\nx_grid, y_grid, z_grid = np.meshgrid(points[:, 0], points[:, 1], points[:, 2])\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values = np.meshgrid(V, V, V)\n\n# Create a 3D grid from the values\nx_grid_values, y_grid_values, z_grid_values =", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a 3D grid of V values\nv = np.random.rand(10, 10, 10)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a 3D grid of V values\nv = np.random.rand(10, 10, 10)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a 3D grid of V values\nv = np.random.rand(10, 10, 10)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a 3D grid of V values\nv = np.random.rand(10, 10, 10)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a 3D grid of V values\nv = np.random.rand(10, 10, 10)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a 3D grid of V values\nv = np.random.rand(10, 10, 10)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a 3D grid of V values\nv = np.random.rand(10, 10, 10)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n# Create a 3D grid of points\nx, y, z = np.mgrid[0:100:10, 0:100:10, 0:100:10]\n\n# Create a 3D grid of V values\nv = np.random.rand(10, 10, 10)\n\n# Create a 3D grid of request points\nrequest = np.array([[25, 20, -30], [27, 20, -32]])\n\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle, reshape=False)\nxrot = x0 + np.cos(np.radians(angle)) * (x0 - x0)\nyrot = y0 + np.sin(np.radians(angle)) * (y0 - y0)\nxrot, yrot = np.round(xrot), np.round(yrot)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = np.diagonal(M.toarray())", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": " ###\n    # Use scipy's kstest function to perform the K-S test\n    # The result should be a tuple (statistic, pvalue)\n    # The statistic is the K-S test statistic\n    # The pvalue is the p-value for the hypothesis that the distribution of the times is uniform\n    # The function should return a tuple (statistic, pvalue)\n    ### END SOLUTION ###\n    # return result\n\n# print the result\nprint(f())", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = stats.kstest(times, \"uniform\")\nprint(result)", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "# calculate the distance between points\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# assign points2 to points1\nresult = points2[min_indices]\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "# calculate the distance between points\ndistances = scipy.spatial.distance.cdist(points1, points2)\n# find the minimum distance for each point in points2\nmin_distances = np.min(distances, axis=1)\n# find the index of the minimum distance for each point in points2\nmin_indices = np.argmin(distances, axis=1)\n# assign points2 to points1\nresult = points2[min_indices]\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b = b.toarray()\nb[np.arange(b.shape[0]), np.arange(b.shape[1])] = 0\nb = sparse.csr_matrix(b)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "region_count = np.sum(img > threshold)\nprint(region_count)\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "region_count = np.sum(img < threshold)", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": " ###\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number of regions of cells which value exceeds a given threshold\n    # Use numpy's logical operations to count the number", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "# Find the regions of cells which value exceeds a given threshold\nregion_values = np.where(img > threshold)\n\n# Find the center of mass of these regions\ncenter_of_mass = np.mean(region_values, axis=1)\n\n# Find the distance between the center of mass of these regions and the top left corner\ndistance = np.sqrt((center_of_mass[0] - 0)**2 + (center_of_mass[1] - 0)**2)\n\n# Print the distance\nprint(distance)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "M = (M + M.T)/2\nM = M + M.T\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": " ###\n    # make the matrix symmetric\n    sA = sA.tocsr()\n    sA = sA.tocsr()\n    ### END SOLUTION ###\n    return sA\n\n# call the function\nsA = f(example_sA)\nprint(sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "# erode the square by 1\neroded = scipy.ndimage.erosion(square, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# dilate the eroded square by 1\ndilated = scipy.ndimage.dilation(eroded, structure=np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]))\n# remove the isolated single cells\nresult = np.where(dilated == 0, 0, square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "# erode the square array\neroded = scipy.ndimage.erosion(square, structure=np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]]))\n\n# dilate the eroded array\ndilated = scipy.ndimage.dilation(eroded, structure=np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]]))\n\n# find the indices of the non-zero elements in the dilated array\nindices = np.where(dilated != 0)\n\n# set the non-zero elements in the dilated array to 0\ndilated[indices] = 0\n\n# find the indices of the non-zero elements in the square array\nindices = np.where(square != 0)\n\n# set the non-zero elements in the square array to 0\nsquare[indices] = 0\n\n# find the indices of the non-zero elements in the square array\nindices = np.where(square != 0)\n\n# set the non-zero elements in the square array to 0\nsquare[indices] = 0\n\n# print the square array\nprint(square)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col.toarray())\nstandard_deviation = np.std(col.toarray())", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_val = np.max(col.toarray())\nmin_val = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "# Convert the sparse matrix to a dense array\ndA = sA.toarray()\n# Get the median and mode of the dense array\nmedian = np.median(dA)\nmode = np.mode(dA)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *a):\n    return sum(a[i]*np.cos(np.pi*x/(2*tau)*i) for i in range(1, degree+1))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "# Convert the array to a 1D array\narray_1d = np.ravel(example_array)\n# Calculate pairwise Euclidean distances\ndistances = scipy.spatial.distance.pdist(array_1d, 'euclidean')\n# Convert distances tometres\ndistances_m = distances * 0.001\n# Convert distances to a 2D array\ndistances_2d = np.array(distances_m).reshape(len(array_1d), len(array_1d))\n# Find the minimum distance separating the nearest edges of each raster patch\nmin_distance = np.min(distances_2d)\n# Print the result\nprint(min_distance)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "# Convert the array to a 1D array\narray_1d = np.ravel(example_array)\n# Calculate pairwise Manhattan distances\ndistances = scipy.spatial.distance.pdist(array_1d, 'manhattan')\n# Convert the pairwise distances to a 2D array\ndistances_2d = np.reshape(distances, (len(array_1d), len(array_1d)))\n# Find the minimum distance between each pair of regions\nmin_distances = np.min(distances_2d, axis=1)\n# Convert the minimum distances to a 2D array\nmin_distances_2d = np.reshape(min_distances, (len(array_1d), len(array_1d)))\n# Create a 2D array with the minimum distances and the corresponding regions\nresult = np.stack((array_1d, min_distances_2d), axis=1)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": " ###\n    # Calculate pairwise Euclidean distances between all regions\n    # to obtain the minimum distance separating the nearest edges of each raster patch\n    # Convert distances from cells tometres\n    # Return the result in the form of \"from ID, to ID, distance\"\n    ### END SOLUTION ###\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    # return result\n    #", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(x.shape[1]):\n    tck = interpolate.splrep(x[:, i], y[:, i], k=3, s=4)\n    result[i] = interpolate.splev(x_val, tck, der=0)\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array([38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0])\nx2 = np.array([39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8])\nx3 = np.array([34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0])\nx4 = np.array([34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8])\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\nprint(statistic)\nprint(critical_values)\nprint(significance_level)", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\n\nresult = ss.anderson_ksamp(x1, x2)\nprint(result)", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(A['A']) # keep one column fix and run it in the other two\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\nA['AB'] = pd.rolling_apply(A['B'], 3, lambda x: tau1(x))\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nresult = is_csr_matrix_only_zeroes(sa)\nprint(result)", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return len(my_lil_matrix.nonzero()[0]) == 0\n\nresult = is_lil_matrix_only_zeroes(sa)\nprint(result)\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": " ###\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    return result.pvalue\n    ### END SOLUTION ###\n\nprint(f(pre_course_scores, during_course_scores))\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "import scipy.stats\nkurtosis_result = scipy.stats.kurtosis(a)\nprint(kurtosis_result)", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=True)\nprint(kurtosis_result)", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "def f(s, t):\n    return (s + t) * np.exp(-6.0 * (s * s + t * t))\n\nx, y = np.ogrid[-1:1:10j,-2:0:10j]\nz = (x + y)*np.exp(-6.0 * (x * x + y * y))\n\ninterpolated = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interpolated(x, y)", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    # Create a 2D array with the values of s and t\n    s_t = np.array([[s, t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in example_s for t in example_t])\n    # Create a 2D array with the values of z\n    z_values = np.array([[z for s in example_s for t in example_t] for s in", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "# find the region of each extra point\nregion_indices = [vor.point_to_region(point) for point in extraPoints]\n# count the number of points in each region\nregion_counts = np.bincount(region_indices)\nresult = region_counts<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "# find the voronoi region for each extra point\nregion_indices = []\nfor point in extraPoints:\n    region_indices.append(vor.point_to_region(point))\n\n# convert the region indices to a numpy array\nregion_indices = np.array(region_indices)\n\n# count the number of points in each region\nregion_counts = np.bincount(region_indices)\n\n# print the result\nprint(region_counts)\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "max_size = max([len(v) for v in vectors])\nresult = sparse.csc_matrix(np.zeros((len(vectors), max_size)))\nfor i, v in enumerate(vectors):\n    result[i, :len(v)] = v\nresult = result.tocsr()\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, origin=[1, 1])", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = np.array([M[row[i], column[i]] for i in range(len(row))])", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, j])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "dev = abs((x-u)/o2)\nP_inner = scipy.integrate.integrate(NDfx,-dev,dev)\nP_outer = 1 - P_inner\nP = P_inner + P_outer/2\nprint(P)", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n\nprint(f(x = 2.5, u = 1, o2 = 3))\n<code> \n\n# END SOLUTION #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "# Create a DCT matrix\ndct_matrix = np.zeros((N, N))\nfor i in range(N):\n    for j in range(N):\n        dct_matrix[i, j] = sf.dctn(np.ones(N), 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "result = sparse.diags(matrix, [-1,0,1], (5, 5)).toarray()\nprint(result)\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "def binomial_distribution(n, p):\n    return scipy.stats.binomial(n, p).pmf()\n\nresult = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        result[i, j] = binomial_distribution(i, p)\nresult[i, j] = scipy.stats.binomial(i, p).pmf()\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "result = df.apply(lambda x: stats.zscore(x, axis=0))\nresult", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "result = df.apply(lambda x: stats.zscore(x, axis=0))\nresult", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=1)\n\n# create a new dataframe with zscore and data\nresult = pd.DataFrame(zscore, index=df.index, columns=df.columns)\nresult.index = result.index.map(lambda x: x.split('_')[0])\nresult.columns = ['data', 'zscore']\nresult\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "# calculate zscore\nzscore = stats.zscore(df.values, axis=0)\n# round zscore to 3 decimal places\nzscore = np.round(zscore, 3)\n# create a new dataframe with zscore and original data\nresult = pd.DataFrame(np.c_[zscore, df.values], index=df.index, columns=['zscore', 'sample1', 'sample2', 'sample3'])\nresult", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "y, x = np.mgrid[0:6, 0:6]\ncenter = np.array([3, 3])\nresult = distance.cdist(np.dstack((y, x)), center, 'euclidean')\nprint(result)", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "y, x = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\ny = y - shape[0] // 2\nx = x - shape[1] // 2\nresult = distance.cdist(np.dstack((y, x)), np.array([[5., 4., 3., 3., 4., 5.],\n                                                      [4., 3., 2., 2., 3., 4.],\n                                                      [3., 2., 1., 1., 2., 3.],\n                                                      [3., 2., 1., 1., 2., 3.],\n                                                      [4., 3., 2., 2., 3., 4.],\n                                                      [5., 4., 3., 3., 4., 5.]]))\nprint(result)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": " ###\n    y, x = np.mgrid[0:shape[0], 0:shape[1]]\n    center = np.array([(shape[0] - 1) / 2, (shape[1] - 1) / 2])\n    return distance.cdist(np.dstack((y, x)), center)\n    ### END SOLUTION ###\n\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "result = scipy.ndimage.zoom(x, shape, order=1)\nprint result", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(pars, a, y):\n    x = pars['x']\n    model = func(x, a)\n    return (y - model) ** 2\n\ndef main():\n    fit_params = scipy.optimize.Parameters()\n    fit_params.add('x', value=x0)\n    out = scipy.optimize.minimize(residual, fit_params, args=(a, y))\n    print(out.x)\n\nif __name__ == '__main__':\n    main()\nEND SOLUTION\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return (y - func(x)) ** 2\n\ndef main():\n    out = scipy.optimize.minimize(residual, x0, bounds=[x_lower_bounds, x_lower_bounds + x_true])\n    print(out.x)\n\nif __name__ == '__main__':\n    main()\nEND SOLUTION\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t)) if 0 < t < 2*np.pi else 2*np.pi\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "sa = sa.toarray()\nsb = sb.toarray()\nresult = sparse.vstack((sa, sb))", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "sa = sa.toarray()\nsb = sb.toarray()\nresult = sparse.csr_matrix(numpy.hstack((sa, sb)))", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\n\ndef integrand(x, c):\n    return 2*c*x\n\nc_values = np.array([5, 10, 15])\nresults = np.zeros(len(c_values))\n\nfor i, c in enumerate(c_values):\n    result, error = scipy.integrate.quad(integrand, 0, 1, args=(c,))\n    results[i] = result\n\nprint(results)", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    def integrand(x, c):\n        return 2*c*x\n    result, error = scipy.integrate.quad(lambda x: integrand(x, c), low, high)\n    return result\n\n# call the function with different values of c\nc = [1, 2, 3, 4, 5]\nfor i in range(len(c)):\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n    print(f(c[i]))\n    # print the result\n   ", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = x.to_dense()\nV += x", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "x = x.reshape(1, 1)\nV += x", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "x = x.reshape(1, 1)\ny = y.reshape(1, 1)\nV += x\nV += y", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(np.square(column)))\n    # normalize the column\n    column_normalized = column / length\n    # update the original column of the matrix\n    sa[:, col] = column_normalized\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# iterate through columns\nfor col in range(sa.shape[1]):\n    # get the column\n    column = sa[:, col].data\n    # calculate the length of the column\n    length = np.sqrt(np.sum(np.square(column)))\n    # normalize the column\n    column_normalized = column / length\n    # update the original column of the matrix\n    sa[:, col] = column_normalized\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "a = np.where(a > 0, 1, 0)", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "a = np.where(a == 0, 0, 1)\na = np.where(a == 1, 0, 1)\na = np.where(a == 2, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest element to each centroid\nresult = np.argmin(distances, axis=1)\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# calculate the distance to each centroid\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the index of the closest centroid\nclosest_centroid_index = np.argmin(distances, axis=1)\n# find the closest point to each cluster\nresult = data[closest_centroid_index]", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# calculate the distance matrix\ndistances = scipy.spatial.distance.cdist(data, centroids)\n# find the k-th closest element to the centroid of each cluster\nclosest_elements = np.argpartition(distances, k, axis=0)[:k]\n# find the index of the k-th closest element in original data for each cluster\nresult = np.argwhere(closest_elements == np.argmin(distances, axis=0))\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result = fsolve(lambda x: eqn(x, a, b), x0=x)\n    print(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for i in range(xdata.shape[0]):\n    result = fsolve(lambda x: eqn(x, xdata[i], adata[i]), x0=xdata[i])\n    print(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Define the function to be tested\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\n# Fit the function to the sample data\nparams, params_cov = sp.optimize.curve_fit(bekkers_func, range_start, range_end, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\n# Calculate the K-S test\nstatistic, pvalue = stats.kstest(sample_data, bekkers_func, params)\n\n# Print the result\nprint(f\"Statistic: {statistic}, P-value: {pvalue}\")", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Fit the function to the data\npopt, pcov = sp.optimize.curve_fit(bekkers, range_start, range_end, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n# Calculate the K-S test\nkstest_result = stats.kstest(sample_data, bekkers(range_start, *popt))\n# Check the p-value at 95% confidence level\npvalue = kstest_result.pvalue\nresult = pvalue < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def rolling_integral(x, y):\n    return integrate.trapz(x, y)\n\nrolling_integral_df = df.groupby(pd.Grouper(freq='25S')).apply(rolling_integral)\nrolling_integral_df", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "x = np.array([(2,2), (1,2), (2,3), (3,2), (2,1)])\ny = np.array([5,7,8,10,3])\neval = np.array([(2.7, 2.3)])\n\n# Create a grid of points\nx_grid = np.linspace(x.min(), x.max(), 100)\ny_grid = np.linspace(y.min(), y.max(), 100)\n\n# Create a griddata object\ngriddata = scipy.interpolate.GridData(x_grid, y_grid, x, y, method='linear')\n\n# Use griddata to find the value at the given point\nresult = griddata(eval)\nprint(result)", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "# calculate the weights\nweights = sciopt.maximum_likelihood_solver(a.values, np.arange(a.values.shape[1]))\nweights = weights / np.sum(weights)\nprint(weights)", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "# find the indices of the elements that are less or equal to the neighbouring n elements\nindices = np.where((arr[:-n] <= arr[n:]) | (arr[:-n] <= arr[-n:]))\n# find the indices of the elements that are less or equal to the neighbouring n elements forwards\nindices_forward = np.where((arr[n:] <= arr[-n:]) | (arr[n:] <= arr[-n-n:-n]))\n# find the indices of the elements that are less or equal to the neighbouring n elements backwards\nindices_backward = np.where((arr[-n:] <= arr[-n-n:-n]) | (arr[-n:] <= arr[-n-n-n:-n]))\n# combine the indices of the elements that are less or equal to the neighbouring n elements forwards and backwards\nindices = np.concatenate((indices, indices_forward, indices_backward))\n# sort the indices in ascending order\nindices = np.sort(indices)\nresult = indices", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "# Find the absolute values of the array\nabs_arr = np.abs(arr)\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_arr, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np.roll(abs_neighbour_elements, 1, axis=1))\n# Find the absolute values of the neighbouring elements\nabs_neighbour_elements = np.abs(np", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "num_cols = df.select_dtypes(include=[np.float64, np.int64]).columns\noutlier_cols = [col for col in num_cols if np.abs(stats.zscore(df[col])) > 3]\ndf = df[outlier_cols]", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data.data, columns=data.feature_names)\ndata1['target'] = data.target\nprint(data1)", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": " ###\n    # Convert the data from a Bunch object to a DataFrame\n    data_df = pd.DataFrame(data.data, columns=data.feature_names)\n    data_df['target'] = data.target\n    return data_df\n    ### END SOLUTION ###\n\n# Call the function with the data from the Bunch object\nsolve(data)", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "# Create a list of unique elements in the column\nunique_elements = np.unique(df['Col3'])\n\n# Create a dataframe with one-hot-encoded values\nout_df = pd.DataFrame(np.zeros((len(unique_elements), len(df))), columns=unique_elements, index=df.index)\n\n# Iterate over each row in the dataframe\nfor i, row in df.iterrows():\n    # Iterate over each element in the list\n    for element in row['Col3']:\n        # Set the value of the one-hot-encoded value to 1\n        out_df.loc[i, element] = 1\n\n# Print the output dataframe\nprint(out_df)", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "# Convert the list of strings to a one-hot-encoded array\ndf_out = pd.get_dummies(df['Col3'].apply(lambda x: x.split(',')))\n\n# Remove the first column (all zeros)\ndf_out = df_out.iloc[:, 1:]\n\n# Add a new column for the unique names\ndf_out['unique_names'] = df['Col3'].apply(lambda x: x.split(','))\n\n# Convert the unique names to a one-hot-encoded array\ndf_out = pd.get_dummies(df_out['unique_names'])\n\n# Remove the unique names column\ndf_out = df_out.drop(['unique_names'], axis=1)\n\n# Print the output\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[:, -1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over the rows and set the value to 1 if the element exists in the last column\nfor i in range(df.shape[0]):\n    new_df.loc[i, unique_elements[np.where(unique_elements == df.iloc[i, -1])[0][0]]] = 1\n\n# Set the last column to the original last column\nnew_df.iloc[:, -1] = df.iloc[:, -1]\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "# Get unique elements from the last column\nunique_elements = np.unique(df.iloc[-1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over the rows and set the value to 1 if the element exists in the last column\nfor i in range(df.shape[0]):\n    new_row = np.zeros(unique_elements.shape[0])\n    new_row[np.where(unique_elements == df.iloc[i, -1])] = 1\n    new_df.iloc[i] = new_row\n\n# Drop the last column\nnew_df = new_df.drop(columns=[unique_elements[-1]])\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "# Get unique elements in the last column\nunique_elements = np.unique(df.iloc[-1])\n\n# Create a new dataframe with unique elements as columns\nnew_df = pd.DataFrame(np.zeros((df.shape[0], unique_elements.shape[0])), columns=unique_elements)\n\n# Iterate over the rows of the original dataframe\nfor i in range(df.shape[0]):\n    # Iterate over the unique elements\n    for j in range(unique_elements.shape[0]):\n        # If the element exists in the last column, set the value to 1\n        if unique_elements[j] in df.iloc[i, -1]:\n            new_df.iloc[i, j] = 1\n\n# Drop the last column\nnew_df = new_df.drop(columns=[unique_elements[-1]])\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "svmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\nproba = [1 / (1 + np.exp(-x)) for x in svmmodel.decision_function(x_test)]\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "model.fit(X, y)\nproba = model.predict_proba(x_predict)\nproba = np.max(proba, axis=1)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "# Convert the transform_output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a new dataframe with the original data and the transform_output\nnew_df = pd.concat([df_origin, transform_output], axis=1)\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "# Convert the transform output to a sparse matrix\ntransform_output = csr_matrix(transform_output)\n\n# Create a DataFrame from the transform output\ntransform_output_df = pd.DataFrame(transform_output.toarray())\n\n# Merge the original data and the transform output into a new DataFrame\nmerged_df = pd.concat([df_origin, transform_output_df], axis=1)\n\n# Print the merged DataFrame\nprint(merged_df)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": " ###\n    # transform the transform_output to a sparse matrix\n    transform_output = csr_matrix(transform_output)\n    # merge the transform_output and df\n    result = pd.concat([df, transform_output], axis=1)\n    return result\n    ### END SOLUTION ###\n\n# call the function\nsolve(df_origin, transform_output)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\n# Delete the first step\ndel clf.steps[0]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline\nprint(clf)", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\ndel clf.steps[1]\n# Insert a new step\nclf.steps.append(('new_step', SVC()))\n# Print the pipeline steps\nprint(clf.steps)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a new step\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.append(new_step)\n\n# Insert a new step at a specific position\nposition = 1\nnew_step = ('new_step', PolynomialFeatures())\nclf.steps.insert(position, new_step)\n\n# Delete a step\nposition = 0\ndel clf.steps[position]\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\n# Insert a new step\nnew_step = ('new_step', SVC())\nclf.steps.append(new_step)\n\n# Insert a new step at a specific position\nposition = 1\nnew_step = ('new_step', SVC())\nclf.steps.insert(position, new_step)\n\n# Delete a step\nposition = 0\ndel clf.steps[position]\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\n# Insert ('t1919810', PCA()) right before 'svdm'\nclf.steps.insert(1, ('t1919810', PCA()))\n\n# Print the steps\nprint(clf.steps)\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"early_stopping_rounds\": [42, 50, 60],\n    \"eval_metric\" : [\"mae\"],\n    \"eval_set\" : [[testX, testY]]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Perform grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the best score\nbest_score = gridsearch.best_score_\n\n# Get the best iteration\nbest_iteration = gridsearch.best_iteration_\n\n# Get the best model\nbest_model = gridsearch.best_estimator_\n\n# Predict on the test set\nc = best_model.predict(testX)\n\n# Score on the test set\nb = best_model.score(testX, testY)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\n# Define the grid of parameters\nparamGrid = {\n    \"learning_rate\" : [0.01, 0.1, 0.001],\n    \"max_depth\" : [3, 4, 5],\n    \"n_estimators\" : [100, 200, 300],\n    \"objective\" : [\"reg:squarederror\"],\n    \"n_jobs\" : [1, 2, 3],\n    \"iid\" : [True, False],\n    \"early_stopping_rounds\" : [10, 20, 30],\n    \"eval_metric\" : [\"mae\"],\n    \"eval_set\" : [[testX, testY]]\n}\n\n# Define the model\nmodel = xgb.XGBRegressor()\n\n# Define the grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid)\n\n# Fit the grid search\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nbest_params = gridsearch.best_params_\n\n# Get the score\nscore = gridsearch.best_score_\n\n# Get the prediction\nprediction = gridsearch.predict(testX)\n\n# Print the score and prediction\nprint(\"Score: \", score)\nprint(\"Prediction: \", prediction)\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "for train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": " ###\n    # predict t'\n    t_predicted = scaler.inverse_transform(scaled[:, 0])\n    # check score\n    score = np.mean((t_predicted - data[:, 0])**2)\n    return t_predicted, score\n\nt_predicted, score = solve(scaled, scaler, scaled)\nprint(t_predicted)\nprint(score)\n<!! END SOLUTION !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = np.mean(scores)\nprint(f'Name model: {model_name}, Mean score: {mean_score}')", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = np.mean(scores)\nprint(f'Name model: {model_name}, Mean score: {mean_score}')", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[0][1].transform(data)", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\n# Define the GridSearchCV object\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV object\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n# Create a new DecisionTreeClassifier with the best parameters\ndt = DecisionTreeClassifier(max_depth=best_params['base_estimator__max_depth'], max_samples=best_params['max_samples'])\n\n# Create a new BaggingClassifier with the new DecisionTreeClassifier and the best parameters\nbc = BaggingClassifier(dt, n_estimators=20, max_samples=best_params['max_samples'], max_features=best_params['max_features'])\n\n# Fit the BaggingClassifier\nbc.fit(X_train, y_train)\n\n# Get the probabilities\nproba = bc.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nrgr = regressor.fit(X,y)\npredict = rgr.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef solve():\n    tfidf = TfidfVectorizer(preprocessor=preprocess)\n    tfidf.fit(['This is a test', 'This is another test'])\n    print(tfidf.transform(['This is a test', 'This is another test']))\n\nsolve()\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# example variable\ntexts = [\"This is a test.\", \"This is another test.\", \"Another test.\"]\n\n# create a dataframe\ndf = pd.DataFrame(texts, columns=[\"text\"])\n\n# apply tfidf\ntfidf.fit(df[\"text\"])\n\n# print the tfidf vector\nprint(tfidf.transform(df[\"text\"]))", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "# Apply preprocessing.scale to the data\nscaled_data = preprocessing.scale(data)\n\n# Create a new DataFrame from the scaled data\nscaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)\n\n# Print the scaled data\nprint(scaled_data_df)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "# Apply preprocessing.scale to the data\nscaled_data = preprocessing.scale(data)\n\n# Create a new DataFrame with the scaled data\nscaled_data_df = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n\n# Print the scaled data\nprint(scaled_data_df)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "import numpy as np\n\n# Get the feature names from the model\nfeature_names = np.array(X.columns)\n\n# Get the indices of the selected features\nselected_indices = np.where(model.feature_importances_ > 0.05)[0]\n\n# Get the selected feature names\nselected_feature_names = feature_names[selected_indices]\n\n# Save the selected feature names\ncolumn_names = selected_feature_names\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "# get selected feature names\nselected_feature_names = [feature_names[i] for i in np.where(model.get_support())[0]]\nprint(selected_feature_names)\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "column_names = np.array([X.columns[i] for i in np.where(clf.feature_importances_ > 0.05)[0]])", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_100_samples = km.cluster_centers_[p]", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": " ###\n    km.fit(X)\n    centers = km.cluster_centers_\n    # get the 50 samples closest to the p^th center\n    # you can use numpy's sort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance to the center of the p^th cluster\n    # then select the first 50 samples from the sorted centers\n    # you can use numpy's argsort function to sort the centers by their distance", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"rbf\")\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"rbf\")\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model with a polynomial kernel of degree 2\nsvr = SVR(kernel='poly', degree=2)\n\n# fit the model\nsvr.fit(X, y)\n\n# predict the values\npredict = svr.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\n# create a SVM regression model\nsvm_reg = SVR(kernel=\"poly\", degree=2)\nsvm_reg.fit(X, y)\npredict = svm_reg.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarities(query, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    matrix = np.zeros((len(documents), len(documents)))\n    for i, doc in enumerate(documents):\n        matrix[i, :] = np.dot(query_tfidf.toarray(), tfidf.transform([doc]).toarray())\n    return matrix\n\ncosine_similarities_of_queries = get_cosine_similarities(queries, documents)\nprint(cosine_similarities_of_queries)", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarities(query, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit_transform(documents)\n    query_tfidf = tfidf.transform([query])\n    cosine_similarities = np.dot(query_tfidf, tfidf.transform(documents).T)\n    return cosine_similarities\n\ncosine_similarities_of_queries = np.array([get_cosine_similarities(query, documents) for query in queries])\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    tfidf_matrix = tfidf.transform(documents)\n    query_matrix = tfidf.transform(queries)\n    cosine_similarities_of_queries = np.dot(query_matrix, tfidf_matrix.T)\n    return cosine_similarities_of_queries\n    ### END SOLUTION ###\n\n# Test the function\nsolve(queries, documents)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "# Convert the features to a 2D array\nnew_features = np.array([[1 if feature in sample else 0 for feature in feature_list] for feature_list in features])\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0], 1))], axis=1)\n\n# Convert the 2D array to a 2D array with one-hot encoding\nnew_features = np.concatenate([new_features, np.ones((new_features.shape[0],", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "f = np.array(f)\nnew_f = np.zeros((f.shape[0], f.shape[1]))\nfor i in range(f.shape[0]):\n    for j in range(f.shape[1]):\n        new_f[i, j] = f[i, j]\nnew_f = new_f.astype(int)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "# Convert the features to a 2D array\nnew_features = np.array([[0, 0, 0, 1, 1, 1],\n                         [1, 0, 1, 0, 0, 0],\n                         [0, 0, 1, 1, 1, 1]])\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": " ###\n    # Convert the features to a 2D array\n    features_array = np.array(features)\n    # Convert the features to one-hot encoding\n    one_hot_encoded_features = np.zeros((features_array.shape[0], features_array.shape[1], features_array.shape[1]))\n    for i in range(features_array.shape[0]):\n        for j in range(features_array.shape[1]):\n            one_hot_encoded_features[i, j, j] = 1\n    # Return the one-hot encoded features\n    return one_hot_encoded_features\n    ### END SOLUTION ###\n\n# Test the function\nsolve(features)\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "# Convert the list of features to a 2D array\nnew_features = np.array(features).astype(int)\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape(new_features, (-1, new_features.shape[-1]))\n\n# Convert the 2D array to a 2D array with one column for each feature\nnew_features = np.reshape", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the data matrix\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2)\n\n# Fit the data to the cluster\ncluster.fit(data_matrix)\n\n# Get the cluster labels\ncluster_labels = cluster.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndist_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage_method='average')\n\n# Fit the clustering model\nclustering.fit(dist_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create a 2-d array from the similarity matrix\nsimM = np.array([[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]])\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage_method='average')\n\n# Fit the clustering model\ncluster.fit(simM)\n\n# Get the cluster labels\ncluster_labels = cluster.labels_\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage\n\n# calculate the linkage matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# get the cluster labels\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maximum')\ncluster_labels = cluster_labels.tolist()\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage\n\n# calculate the linkage matrix\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# get the cluster labels\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maximum')\ncluster_labels = cluster_labels.tolist()\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage\n\n# calculate the linkage matrix\nlinkage_matrix = linkage(simM, 'ward')\n\n# get the cluster labels\ncluster_labels = scipy.cluster.hierarchy.fcluster(linkage_matrix, 2, criterion='maximum')\ncluster_labels = cluster_labels.tolist()\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from sklearn.preprocessing import BoxCoxTransformer\n\ntransformer = BoxCoxTransformer()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import YeoJohnson\n\ntransformer = YeoJohnson()\ntransformer.fit(data)\ntransformed_data = transformer.transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnson\n\n# Create a YeoJohnson object\ntransformer = YeoJohnson()\n\n# Apply the transformer to the data\ntransformed_data = transformer.transform(data)\n\n# Convert the transformed data to a pandas DataFrame\ntransformed_data = pd.DataFrame(transformed_data, columns=data.columns)\n\n# Print the transformed data\nprint(transformed_data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer()\ntransformed_text = vectorizer.fit_transform(text)\ntransformed_text = transformed_text.toarray()\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_api import train_test_split\n\n# split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop([-1], axis=1), dataset[-1], test_size=0.2, random_state=42)\n\n# define x and y\nx_train_all = x_train.values\ny_train_all = y_train.values\nx_test_all = x_test.values\ny_test_all = y_test.values\n\n# split each of those sets so that we can define x and y\nx_train_all = x_train_all[:, :-1]\ny_train_all = y_train_all[:, -1]\nx_test_all = x_test_all[:, :-1]\ny_test_all = y_test_all[:, -1]\n\n# define x and y\nx_train = x_train_all\ny_train = y_train_all\nx_test = x_test_all\ny_test = y_test_all\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Define the features and the target\nx = data.drop(['target'], axis=1)\ny = data['target']\n\n# Split the data into a training set and a testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_api import train_test_split\n\n# split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(dataset.drop([-1], axis=1), dataset[-1], test_size=0.3, random_state=42)\n\n# define x and y\nx_train_all = x_train.values\ny_train_all = y_train.values\nx_test_all = x_test.values\ny_test_all = y_test.values\n\n# split each of those sets so that we can define x and y\nx_train_all = x_train_all[:, :-1]\ny_train_all = y_train_all[:, -1]\nx_test_all = x_test_all[:, :-1]\ny_test_all = y_test_all[:, -1]\n\n# define x and y\nx_train = x_train_all\ny_train = y_train_all\nx_test = x_test_all\ny_test = y_test_all\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": " ###\n    # split the data into training and testing sets\n    train_data, test_data = np.split(data, [int(data.shape[0]*0.8)])\n    # define x and y\n    x_train = train_data[:, :-1]\n    y_train = train_data[:, -1]\n    x_test = test_data[:, :-1]\n    y_test = test_data[:, -1]\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION ###\n\n# call the function\nsolve(dataset)\n# print the solution\nprint(x_train, y_train, x_test, y_test)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "mse_values = df['mse'].values\nlabels = KMeans(n_clusters=2, n_init=10).fit(mse_values.reshape(-1, 1)).predict(mse_values.reshape(-1, 1))\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "f1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2, n_init=10).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "# Fit the LinearSVC model\nsvc = LinearSVC(penalty='l1', loss='l1', max_iter=10000, random_state=42)\nsvc.fit(X, y)\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[svc.get_support()]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "# Create a linear SVC\nsvc = LinearSVC(penalty='l1', loss='l1', max_iter=10000)\n\n# Fit the model\nsvc.fit(X, y)\n\n# Get the selected feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[svc.get_support()]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": " ###\n    # Use LinearSVC with penalty='l1'\n    svc = LinearSVC(penalty='l1', loss='l1', max_iter=10000, random_state=42)\n    # Fit the model\n    svc.fit(X, y)\n    # Get the feature names\n    feature_names = np.asarray(vectorizer.get_feature_names())[svc.get_support()]\n    return feature_names\n    ### END SOLUTION ###\n\n# Test the function\nsolve(corpus, y, vectorizer, X)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "slopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    npMatrix = np.matrix(df2[['Time', col]])\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\nslopes = slopes.tolist()\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "slopes = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time',col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes = np.concatenate((slopes, m), axis = 0)\nslopes = slopes.tolist()\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column\ndf['Sex'] = le.transform(df['Sex'])\n\n# Print the transformed data\nprint(df)\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n\n# Fit the LabelEncoder object to the 'Sex' column\nle.fit(df['Sex'])\n\n# Transform the 'Sex' column\ndf['Sex'] = le.transform(df['Sex'])\n\n# Print the transformed data\nprint(df)\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": " ###\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    ### END SOLUTION ###\n    return df\n\ndef load_data():\n    df = pd.DataFrame({\n        'Sex': ['male', 'female', 'male', 'female'],\n        'Age': [20, 25, 30, 35],\n        'PassengerId': [1, 2, 3, 4],\n        'PassengerClass': [1, 2, 3, 4]\n    })\n    return df\n\ntransformed_df = Transform(load_data())\nprint(transformed_df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\nprint (\"R^2 for training set:\"),\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\"),\nprint (ElasticNet.score(X_test, y_test))\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(np_array)", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": " ###\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION ###\n\n# Test the function\nnp_array = np.array([[1, 2], [3, 4]])\nprint(Transform(np_array))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "close_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict([close_buy1, m5, m10, ma20])\nprint(predict)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = np.array([['asdf', '1'], ['asdf', '0']])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = np.array([['dsa', '2'], ['sato', '3']])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# array = dataframe.values\n# Data split\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[-1:].astype(float)\ny = dataframe.iloc[:,-1]\n# print(X)\n# print(y)\n\nlogReg = LogisticRegression()\nlogReg.fit(X[:None],y)\n# logReg.fit(dataframe.iloc[-1:],dataframe.iloc[:,-1])\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the data to numpy array\nX = dataframe.iloc[-1:].astype(float)\n# Get the last column as the target variable\ny = dataframe.iloc[:, -1]\n# Create a logistic regression model\nlogReg = LogisticRegression()\n# Fit the model\nlogReg.fit(X[:None], y)\n# Predict the target variable\npredict = logReg.predict(X[:None])\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "train_size = 0.2\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size, shuffle=False)\ntrain_dataframe = train_dataframe.sort_values([\"date\"])\ntest_dataframe = test_dataframe.sort_values([\"date\"])\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "train_dataframe = features_dataframe[features_dataframe['date'] <= np.mean(features_dataframe['date'])]\ntest_dataframe = features_dataframe[features_dataframe['date'] > np.mean(features_dataframe['date'])]\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": " ###\n    # sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(by=\"date\")\n    # calculate the number of rows in the dataframe\n    n_rows = features_dataframe.shape[0]\n    # calculate the number of rows to be used for the test set\n    test_size = n_rows * 0.8\n    # split the dataframe into train and test set\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=test_size, shuffle=False)\n    # return the train and test set\n    return train_dataframe, test_dataframe\n    ### END SOLUTION ###\n\n# call the function\nsolve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "scaler = MinMaxScaler()\n\ncols = df.columns[2:4]\ndf[cols + '_scale'] = scaler.fit_transform(df[cols])\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "myData['new_A2'] = scaler.fit_transform(myData[['Month', 'A2']])\nmyData['new_A3'] = scaler.fit_transform(myData[['Month', 'A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names())\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nprint(count.get_feature_names_out())\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = GridSearch_fitted.fit(X, y)\nfull_results.best_score_\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = GridSearch_fitted.fit(X, y)\nfull_results.best_score_\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nfrom sklearn.ensemble import IsolationForest\n\n# Load the data\ndata = load_data()\n\n# Fit the model\nif not hasattr(fitted_model, 'weights_'):\n    fitted_model = IsolationForest().fit(data)\n\n# Save the model\nnp.save('sklearn_model', fitted_model)\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "# tokenize each description\ndf['description'] = df['description'].apply(lambda x: x.split())\n\n# transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(df['description'])\n\n# calculate cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = np.ones((len(df), len(df)))\nfor i in range(len(df)):\n    for j in range(i + 1, len(df)):\n        cosine_similarity_matrix[i, j] = 1 - sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])\n\n# print the result\nprint(cosine_similarity_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\noptim.step()\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n# Define the optimizer\noptimizer = torch.optim.SGD(optim.parameters(), lr=0.01)\n\n# Define the scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define the loss function\nloss_func = torch.nn.CrossEntropyLoss()\n\n# Define the training data\ntrain_data = np.random.rand(100, 10)\ntrain_labels = np.random.randint(0, 10, (100,))\n\n# Define the epoch count\nepochs = 100\n\n# Training loop\nfor epoch in range(epochs):\n    # Update the learning rate\n    scheduler.step()\n\n    # Perform the training\n    optimizer.zero_grad()\n    output = model(train_data)\n    loss = loss_func(output, train_labels)\n    loss.backward()\n    optimizer.step()\n\n    # Print the loss\n    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\noptim.step()\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n# Define the optimizer\noptimizer = torch.optim.SGD(optim.parameters(), lr=0.005)\n\n# Define the scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Define the loss function\nloss_func = torch.nn.CrossEntropyLoss()\n\n# Define the training data\ntrain_data = np.random.rand(100, 10)\ntrain_labels = np.random.randint(0, 10, (100,))\n\n# Define the epoch\nepochs = 100\n\n# Training loop\nfor epoch in range(epochs):\n    # Get the loss\n    loss = loss_func(optim.parameters(), train_data, train_labels)\n\n    # Update the learning rate\n    scheduler.step()\n\n    # Print the loss\n    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Convert the word2vec model to a PyTorch embedding layer\nembedding_weights = torch.tensor(word2vec.wv.vectors, dtype=torch.float32)\nembedding_weights = embedding_weights.t()\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], -1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100, 1)\nembedding_weights = embedding_weights.reshape(embedding_weights.shape[0], 100)\n\n# Create a PyTorch embedding layer with the weights from the word2vec model\nembedding_layer = torch.nn.Embedding.from_pretrained(embedding_weights)\n\n# Embed the input data using the embedding layer\nembedded_input = embedding_layer(input_Tensor)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": " ###\n    # get the embedding weights from gensim\n    embedding_weights = word2vec.wv.vectors\n    # convert the embedding weights to PyTorch tensor\n    embedding_weights_tensor = torch.tensor(embedding_weights, dtype=torch.float32)\n    # return the embedding weights tensor\n    return embedding_weights_tensor\n    ### END SOLUTION ###\n\n# call the function\nembedding_weights_tensor = get_embedded_input(input_Tensor)\nprint(embedding_weights_tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "x = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\nprint(px)", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "x = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "x = torch.rand(6,6)\npx = pd.DataFrame(x.numpy())\nprint(px)", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "A_logical = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_logical]\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "A_log = torch.ByteTensor([1, 1, 0]) # the logical index\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "A_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = B[:, A_log] # Throws error\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": " ###\n    # Use the logical index to slice the columns of B\n    C = B[:, A_log]\n    ### END SOLUTION ###\n    return C\n\ndef load_data():\n    A_log = torch.ByteTensor([1, 0, 1])\n    B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n    return A_log, B\n\nsolve(A_log, B)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "A_log = torch.ByteTensor([0, 0, 1])\nB = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\nC = B[:, A_log]\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "# Convert the index to a numpy array\nidx = idx.numpy()\n# Use numpy's index_select to get the desired output\nC = np.array([np.take(B, idx[i]) for i in range(len(idx))])\n# Convert the numpy array to a PyTorch tensor\nC = torch.from_numpy(C)\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "x_tensor = torch.tensor(x_array.tolist())\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = torch.tensor(x_array, dtype=torch.double)\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": " ###\n    # Convert the numpy array to torch tensor\n    t = torch.tensor(a, dtype=torch.float16)\n    return t\n    ### END SOLUTION ###\n\n# Test the function\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=np.object)\n\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones_like(lens)\nmask[lens == 0] = 0\nmask[lens > 0] = 1\nmask", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "lens = torch.tensor([1, 9, 3, 5])\nmask = torch.ones(lens.shape[0], 10)\nmask[torch.arange(lens.shape[0]), lens - 1] = 0\nmask", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "lens = torch.tensor([3, 5, 4])\nmask = torch.ones_like(lens)\nmask[lens] = 0\nmask = torch.where(lens > 0, 1, 0)\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": " ###\n    mask = torch.zeros(len(lens), max(lens)).long()\n    for i in range(len(lens)):\n        mask[i, :lens[i]] = 1\n    return mask\n    ### END SOLUTION ###\n\n# Test the function\nlens = [3, 5, 4]\nprint(get_mask(lens))\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "# Create a 2D tensor with diagonal elements\ndiag_ele = torch.ones(len(Tensor_2D))\n\n# Create a 2D tensor with index_in_batch\nindex_in_batch = torch.arange(len(Tensor_2D))\n\n# Create a 2D tensor with drag_ele\ndrag_ele = torch.ones(len(Tensor_2D))\n\n# Create a 3D tensor with index_in_batch and drag_ele\nindex_in_batch_and_drag_ele = torch.stack([index_in_batch, drag_ele], dim=1)\n\n# Create a 3D tensor with index_in_batch and diag_ele\nindex_in_batch_and_diag_ele = torch.stack([index_in_batch, diag_ele], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix (who is a diagonal matrix)\nindex_in_batch_and_matrix = torch.stack([index_in_batch, torch.diag(diag_ele)], dim=1)\n\n# Create a 3D tensor with index_in_batch and Matrix", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    # Create a 2D tensor with the same shape as t\n    t_2D = torch.zeros_like(t)\n    # Create a 3D tensor with the same shape as t\n    t_3D = torch.zeros(t.shape[0], t.shape[1], t.shape[2])\n    # Iterate over the 2D tensor and fill the 3D tensor with the diagonal elements of t\n    for i in range(t.shape[0]):\n        for j in range(t.shape[1]):\n            t_3D[i, j, :] = t[i, j, :]\n    # Return the 3D tensor\n    return t_3D\n    ### END SOLUTION ###\n\n# Call the function with the 2D tensor\nresult = Convert(Tensor_2D)\nprint(result)\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nb = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n\nab = torch.stack((a, b), 0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = torch.tensor(a)\nb = torch.tensor(b)\nab = torch.stack((a, b), 0)\nprint(ab.shape) # (138, 514)", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": " ###\n    # stack the tensors a and b along the first axis\n    # then stack the result with a and b along the second axis\n    # finally stack the result with a and b along the third axis\n    # ### END SOLUTION ###\n    return torch.stack((a, b), 0)\n\n# call the function with a and b as inputs\nsolve(a, b)\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[ : , lengths : , : ] = 0", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[ : , lengths : , : ] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[ : , : lengths , : ] = 0", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[ : , : lengths , : ] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.tensor(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    return torch.tensor(lt)\n    ### END SOLUTION ###\n\n# Call the function\nlist_of_tensors = Convert(list_of_tensors)\nprint(list_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "result = t[idx]", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "result = t[idx]", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "result = t[idx]", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "scores = x.max(1)\nids = torch.argmax(scores, 1)\nresult = x.gather(1, ids.unsqueeze(-1)).squeeze(1)", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "scores = x.max(1)\nids = torch.argmax(scores, 1)\nresult = x.gather(1, ids.unsqueeze(-1)).squeeze(1)", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "# Find the index of the maximum score in each row\nmax_score_index = np.argmax(x, axis=1)\n\n# Create a mask to select the rows with the maximum score\nmask = np.arange(x.shape[0]) == max_score_index\n\n# Gather the selected rows\nresult = x[mask]\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "# Convert the softmax output to a numpy array\nsoftmax_output = softmax_output.numpy()\n\n# Find the maximum value in each row\nmax_values = np.max(softmax_output, axis=1)\n\n# Create a tensor with the maximum value\nmax_tensor = torch.tensor(max_values, dtype=torch.int64)\n\n# Create a tensor with the class index of the maximum value\nclass_index = torch.argmax(softmax_output, axis=1)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int64)\n\n# Print the class index\nprint(class_tensor)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "# Convert the softmax output to a numpy array\nsoftmax_output = softmax_output.numpy()\n\n# Find the maximum value in each row\nmax_values = np.max(softmax_output, axis=1)\n\n# Create a tensor with the maximum value\nmax_tensor = torch.tensor(max_values, dtype=torch.int64)\n\n# Create a tensor with the class index of the maximum value\nclass_index = torch.argmax(softmax_output, axis=1)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int64)\n\n# Print the class index\nprint(class_tensor)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "# Find the maximum value in the softmax output\nmax_value = np.max(softmax_output, axis=1)\n\n# Create a tensor with the maximum value\nmax_tensor = torch.tensor(max_value, dtype=torch.int32)\n\n# Create a tensor with the class index of the maximum value\nclass_index = np.argmax(softmax_output, axis=1)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\nclass_tensor = torch.tensor(class_index, dtype=torch.int32)\n\n# Create a tensor with the class index\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": " ###\n    # Find the maximum value in each row\n    max_values = np.max(softmax_output, axis=1)\n    # Find the index of the maximum value\n    max_index = np.argmax(softmax_output, axis=1)\n    # Create a tensor with the maximum value\n    max_tensor = torch.tensor(max_values, dtype=torch.int32)\n    # Create a tensor with the maximum index\n    max_index_tensor = torch.tensor(max_index, dtype=torch.int32)\n    # Return the maximum index tensor\n    return max_index_tensor\n    ### END SOLUTION ###\n\nsolve(softmax_output)\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "# Find the maximum value in the softmax output\nmax_value = torch.max(softmax_output, 1)[0]\n\n# Create a tensor with the same shape as the softmax output, with all values set to 0\ny = torch.zeros_like(softmax_output)\n\n# Set the values in the tensor to the maximum value\ny[torch.arange(y.size(0)), max_value] = 1\n\n# Print the result\nprint(y)\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "images = Variable(torch.randn(5, 3, 4, 4))\nlabels = Variable(torch.LongTensor(5, 4, 4).random_(3))\nloss = cross_entropy2d(images, labels)\nprint(loss)\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = np.sum(A == B)", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = np.sum(A == B)", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = np.sum(A != B)", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": " ###\n    # Use numpy to find the number of elements that are equal in the two tensors\n    cnt_equal = np.sum(A == B)\n    ### END SOLUTION ###\n    return cnt_equal\n\n# Call the function\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "x = A.shape[0]\ncnt_equal = np.sum(A[-x:] == B[-x:])", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "x = A.shape[0]\ncnt_not_equal = np.sum(A[-x:] != B[-x:])", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\n# Create a tensor with shape (1, 3, 10, 40, 1)\na = torch.randn(1, 3, 10, 40, 1)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_3 = a[:, :, :, 0, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_2 = a[:, :, :, 1, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_1 = a[:, :, :, 2, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_0 = a[:, :, :, 3, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_31 = a[:, :, :, 4, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_30 = a[:, :, :, 5, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_29 = a[:, :, :, 6, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_28 = a[:, :, :, 7, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_27 = a[:, :, :, 8, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_26 = a[:, :, :, 9, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_25 = a[:, :, :, 10, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_24 = a[:, :, :, 11, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_23 = a[:, :, :, 12, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_22 = a[:, :, :, 13, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_21 = a[:, :, :, 14, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_20 = a[:, :, :, 15, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_19 = a[:, :, :, 16, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_18 = a[:, :, :, 17, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_17 = a[:, :, :, 18, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_16 = a[:, :, :, 19, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_15 = a[:, :, :, 20, :]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_14 = a[:, :, :, 21, :]\n\n# Create a tensor with shape (1, 3, 10, 10", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\n# Create a tensor with shape (1, 3, 10, 40, 1)\na = torch.randn(1, 3, 10, 40, 1)\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_1 = a[:, :, :, :, 0]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_2 = a[:, :, :, :, 1]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_3 = a[:, :, :, :, 2]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_4 = a[:, :, :, :, 3]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_5 = a[:, :, :, :, 4]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_6 = a[:, :, :, :, 5]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_7 = a[:, :, :, :, 6]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_8 = a[:, :, :, :, 7]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_9 = a[:, :, :, :, 8]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_10 = a[:, :, :, :, 9]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_11 = a[:, :, :, :, 10]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_12 = a[:, :, :, :, 11]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_13 = a[:, :, :, :, 12]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_14 = a[:, :, :, :, 13]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_15 = a[:, :, :, :, 14]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_16 = a[:, :, :, :, 15]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_17 = a[:, :, :, :, 16]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_18 = a[:, :, :, :, 17]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_19 = a[:, :, :, :, 18]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_20 = a[:, :, :, :, 19]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_21 = a[:, :, :, :, 20]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\ntensor_22 = a[:, :, :, :, 21]\n\n# Create a tensor with shape (1, 3, 10, 10, 1)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output[mask == 1] = clean_input_spectrogram[mask == 1]", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output[mask == 0] = clean_input_spectrogram[mask == 0]", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_x = torch.min(torch.abs(x), torch.abs(y))\nmin_y = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.where(x < y, sign_x * min_x, sign_y * min_y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "sign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_x = torch.max(torch.abs(x))\nmax_y = torch.max(torch.abs(y))\nsigned_max = torch.where(x > y, max_x, max_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": " ###\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    min_x = torch.min(torch.abs(x), torch.abs(y))\n    min_y = torch.min(torch.abs(x), torch.abs(y))\n    min_x_sign = torch.where(x < y, sign_x, sign_y)\n    min_y_sign = torch.where(x < y, sign_y, sign_x)\n    min_x_sign = torch.where(x < y, min_x_sign, min_y_sign)\n    return min_x_sign\n\n# solve(x, y)\n<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\nprint(f\"Confidence score: {confidence_score}, Class name: {class_name}\")\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "a = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\na = a[:, :-1]\nb = b[:, 1:]\n\nresult = torch.stack([a, b], dim=1)\n\nresult = result.mean(dim=1)\n\nprint(result)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": " ###\n    a = a.reshape(-1, 1)\n    b = b.reshape(-1, 1)\n    a = a.repeat(1, 2)\n    b = b.repeat(1, 2)\n    a = a.reshape(-1, 2)\n    b = b.reshape(-1, 2)\n    a = a.sum(axis=1)\n    b = b.sum(axis=1)\n    result = np.concatenate([a, b], axis=1)\n    return result\n    ### END SOLUTION ###\n\n# Test the function\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\nprint(solve(a, b))\n", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[0., 0., 0., 0.]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "t = torch.tensor([[1, 2], [3, 4]])\nnew = torch.tensor([[0., 0., 0.,0.]])\nr = torch.stack([t,new])\nprint(r)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "t = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\nnew = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1]])\nr = torch.stack([t, new])\nprint(r)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\nprint(result)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
