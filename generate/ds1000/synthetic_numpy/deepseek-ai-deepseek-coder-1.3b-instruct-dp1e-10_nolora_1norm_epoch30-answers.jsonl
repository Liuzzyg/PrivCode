{"id": 0, "code": "df['Type'] = df['Type'].map(lambda x: List[x])\nresult = df\n", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "df_shuffled = df.iloc[List]\ndf_shuffled['Type'] = df['Type'].iloc[List]\nresult = df_shuffled\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\ndef solution(df):\n    df['Qu1'] = df['Qu1'].apply(lambda x: 'other' if x in ['apple', 'banana', 'cheese', 'egg'] else x)\n    df['Qu2'] = df['Qu2'].apply(lambda x: 'other' if x in ['sausage', 'banana', 'apple', 'cheese'] else x)\n    df['Qu3'] = df['Qu3'].apply(lambda x: 'other' if x in ['apple', 'potato', 'sausage', 'cheese'] else x)\n    return df\n\ndf = solution(df)\n\nresult = df\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\ndef solution(df):\n    for col in df.columns[1:]:\n        df[col] = np.where(df[col].value_counts() >= 3, df[col], 'other')\n    return df\n\ndf = solution(df)\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "\n    df_new = df.copy()\n    for col in df.columns[1:]:\n        df_new[col] = df[col].apply(lambda x: 'other' if x < 2 else x)\n    df_new.drop(df_new.columns[1:], axis=1, inplace=True)\n    df_new.columns = ['Qu1', 'Qu2']\n    df_new['Qu3'] = df['Qu3']\n    return df_new\n    ### END SOLUTION\n\ndf_new = f(example_df)\nprint(df_new)\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\ndef solution(df):\n    df['Qu1'] = np.where(df['Qu1'].isin(['cheese', 'potato']), 'other', df['Qu1'])\n    df['Qu2'] = np.where(df['Qu2'].isin(['banana', 'apple']), 'other', df['Qu2'])\n    df['Qu3'] = np.where(df['Qu3'].isin(['cheese', 'potato']), 'other', df['Qu3'])\n    return df\n\ndf = solution(df)\n\nresult = df\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\ndef solution(df):\n    df['Qu1'] = np.where(df['Qu1'].value_counts() >= 3, 'other', df['Qu1'])\n    df['Qu2'] = np.where(df['Qu2'].value_counts() >= 2, 'other', df['Qu2'])\n    df['Qu3'] = np.where(df['Qu3'].value_counts() >= 2, 'other', df['Qu3'])\n    return df\n\ndf = solution(df)\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "df = df.groupby('url').filter(lambda x: x.keep_if_dup.mode().iat[0] == 'Yes')\ndf = df.drop(columns=['keep_if_dup'])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.reset_index(drop=True)\ndf\n", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "df = df.groupby('url').filter(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('Yes').any())\ndf = df.groupby('url').apply(lambda x: x.drop_if_dup.eq('No').any())\ndf = df.", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "df['dup_url'] = df.groupby('url')['keep_if_dup'].transform(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.loc[x['keep_if_dup'] == 'Yes'])\ndf = df.drop(['keep_if_dup'], axis=1)\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x: x.iloc[0])\ndf = df.groupby('url').apply(lambda x:", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "df_dict = df.groupby('name')['v1'].apply(lambda x: {x: {y: df[df['v1'] == x][df['v2'] == y].max()[df['v3'] == df[df['v1'] == x]['v3'].max()] for y in x})\ndf_dict = {k: v for k, v in df_dict.items() if df_dict[k]}\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": "\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    ### END SOLUTION\n    return df\n\nexample_df = f(example_df)\nexample_df\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "df['message'] = df['message'].apply(lambda x: eval(x))\ndf['job'] = df['message'].apply(lambda x: x.split(': ')[-1])\ndf['wife'] = df['message'].apply(lambda x: x.split(': ')[-2])\ndf['money'] = df['message'].apply(lambda x: x.split(': ')[-3])\ndf['group'] = df['message'].apply(lambda x: x.split(': ')[-4])\ndf['kids'] = df['message'].apply(lambda x: x.split(': ')[-5])\ndf\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df.apply(lambda row: row.score * 10 if row.product in products else row.score, axis='columns')\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df.apply(lambda row: row.score if row.product in products else 0, axis='columns')\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "df['score'] = df.apply(lambda row: row.score * 10 if row.product.isin(products) else row.score, axis='columns')\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "df['score'] = df.groupby(df['product']).transform('min')\ndf['score'] = df['score'] * (df['product'].apply(lambda x: 1 if x in products else 0))\ndf['score'] = df['score'] / df.groupby(df['product']).transform('min')\ndf['score'] = df['score'].fillna(0)\ndf['score'] = df['score'].apply(lambda x: 1 if x > 0 else 0)\ndf['score'] = df['score'].apply(lambda x: x / (df['score'].max()))\ndf\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.iloc[:, ::-1].sum(axis=1)\ndf\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df = (df.iloc[:, ::-1].sum(axis=0) > 0).astype(int).astype(df.dtypes).where(df.dtypes==object).dropna().astype(df.dtypes) + df\ndf", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "df['E'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['F'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['G'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['H'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['I'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['J'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['K'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['L'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['M'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['N'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['O'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['P'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['Q'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['R'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['S'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['T'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['U'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['V'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['W'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['X'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['Y'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['Z'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['AA'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['BB'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['CC'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['DD'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['EE'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['FF'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['GG'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['HH'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['II'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['JJ'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['KK'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['LL'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['MM'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['NN'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['OO'] = df.iloc[:, ::-1].sum(axis=0).astype(int)\ndf['PP'] = df.iloc[:, ::-1].sum(axis=0).astype", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'].dt.to_period(\"M\")\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'].between('2019-01-17', '2019-02-20')\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "df = df.iloc[:, ::-1].reset_index(drop=True)\ndf.columns = ['#1', '#2']\ndf\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "df = df.iloc[:, ::-1].reset_index(drop=True)\ndf.columns = ['#1', '#2']\ndf['#1'] = df['#1'].shift(1, axis=0).fillna(0)\ndf = df.groupby(df.index // 1).apply(lambda g: g.iloc[:-1])\ndf = df.groupby(df.index // 1).apply(lambda g: g.iloc[-1])\ndf.loc[df.index.repeat(2).diff(df.index).ne(0) | df.index.repeat(2).diff(df.index).ne(0).cumsum() == 1, '#1'] = df.loc[df.index.repeat(2).diff(df.index).ne(0) | df.index.repeat(2).diff(df.index).ne(0).cumsum() == 1, '#1'] + df.loc[df.index.repeat(2).diff(df.index).ne(0) | df.index.repeat(2).diff(df.index).ne(0).cumsum() == 1, '#1'].shift(-1).fillna(0)\ndf.loc[df.index.repeat(2).diff(df.index).ne(0) | df.index.repeat(2).diff(df.index).ne(0).cumsum() == 1, '#2'] = df.loc[df.index.repeat(2).diff(df.index).ne(0) | df.index.repeat(2).diff(df.index).ne(0).cumsum() == 1, '#2'] + df.loc[df.index.repeat(2).diff(df.index).ne(0) | df.index.repeat(2).diff(df.index).ne(0).cumsum() == 1, '#2'].shift(-1).fillna(0)\ndf", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "df = df.iloc[:, ::-1].reset_index(drop=True)\ndf.columns = ['#1', '#2']\ndf['#1'] = df['#1'].shift(1, axis=0).fillna(0)\ndf['#2'] = df['#2'].shift(1, axis=0).fillna(0)\ndf = df.groupby(df.index // 1).apply(lambda g: g.iloc[::-1])\ndf\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "df = df.iloc[1:]\ndf.columns = ['#2', '#1']\ndf = df[::-1]\ndf.columns = ['#2', '#1']\ndf\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'A' for col in df.columns]\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df = df.add_prefix('X')\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "df.columns = [col+'A' if col.endswith('X') else col for col in df.columns]\ndf.columns = [col+'X' if col.startswith('Header') else 'Header'+col for col in df.columns]\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "df.groupby('group').apply(lambda g: g.mean() if 'val' in g.columns.tolist() else g)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "df.groupby('group').apply(lambda g: g.sum() if 'val' in g.columns else g)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "df['val32'] = df.groupby('group')['val1','val2','val42'].transform('sum')\ndf = df.groupby('group')[['group_color','val1','val2','val32']].sum()\ndf", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "\nfor row in row_list:\n    df_subset = df.iloc[row_list]\n    mean_value = df_subset[column_list].mean()\n    result[row] = mean_value\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "\nfor row in row_list:\n    df.loc[row, column_list] = df.loc[row, column_list].sum()\nresult = df\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "\ndef sum_df(df, row_list, column_list):\n    df_sum = df.loc[row_list].sum()\n    df_sum[column_list] = df_sum[column_list].sum()\n    return df_sum\n\ndf_sum = sum_df(df, row_list, column_list)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum[column_list].values)].index, inplace=True)\ndf_sum.drop(df_sum[df_sum[column_list]==max(df_sum", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "df_grouped = df.groupby(['id', 'temp', 'name']).size().reset_index(name='counts')\ndf_grouped.columns = ['id', 'temp', 'name', 'counts']\ndf_grouped.loc[df_grouped['counts'] == 1, 'counts'] = 0\ndf_grouped.loc[df_grouped['counts'] > 1, 'counts'] = 1\ndf_grouped.loc[df_grouped['counts'] == 2, 'counts'] = 2\ndf_grouped.loc[df_grouped['counts'] > 2, 'counts'] = 3\ndf_grouped.loc[df_grouped['counts'] == 3, 'counts'] = 4\ndf_grouped.loc[df_grouped['counts'] > 4, 'counts'] = 5\ndf_grouped.loc[df_grouped['counts'] == 5, 'counts'] = 6\ndf_grouped.loc[df_grouped['counts'] > 6, 'counts'] = 7\ndf_grouped.loc[df_grouped['counts'] > 7, 'counts'] = 8\ndf_grouped.loc[df_grouped['counts'] > 8, 'counts'] = 9\ndf_grouped.loc[df_grouped['counts'] > 9, 'counts'] = 10\ndf_grouped.loc[df_grouped['counts'] > 10, 'counts'] = 11\ndf_grouped.loc[df_grouped['counts'] > 11, 'counts'] = 12\ndf_grouped.loc[df_grouped['counts'] > 12, 'counts'] = 13\ndf_grouped.loc[df_grouped['counts'] > 13, 'counts'] = 14\ndf_grouped.loc[df_grouped['counts'] > 14, 'counts'] = 15\ndf_grouped.loc[df_grouped['counts'] > 15, 'counts'] = 16\ndf_grouped.loc[df_grouped['counts'] > 16, 'counts'] = 17\ndf_grouped.loc[df_grouped['counts'] > 17, 'counts'] = 18\ndf_grouped.loc[df_grouped['counts'] > 18, 'counts'] = 19\ndf_grouped.loc[df_grouped['counts'] > 19, 'counts'] = 20\ndf_grouped.loc[df_grouped['counts'] > 20, 'counts'] = 21\ndf_grouped.loc[df_grouped['counts'] > 21, 'counts'] = 22\ndf_grouped.loc[df_grouped['counts'] > 22, 'counts'] = 23\ndf_grouped.loc[df_grouped['counts'] > 23, 'counts'] = 24\ndf_grouped.loc[df_grouped['counts'] > 24, 'counts'] = 25\ndf_grouped.loc[df_grouped['counts'] > 25, 'counts'] = 26\ndf_grouped.loc[df_grouped['counts'] > 26, 'counts'] = 27\ndf_grouped.loc[df_grouped['counts'] > 27, 'counts'] = 28\ndf_grouped.loc[df_grouped['counts'] > 28, 'counts'] = 29\ndf_grouped.loc[df_grouped['counts'] > 29, 'counts'] = 30\ndf_grouped.loc[df_grouped['counts'] > 30, 'counts'] = 31\ndf_grouped.loc[df_grouped['counts'] > 31, 'counts'] = 32\ndf_grouped.loc[df_grouped['counts'] > 32, 'counts'] = 33\ndf_grouped.loc[df_grouped['counts'] > 33, 'counts'] = 34\ndf_grouped.loc[df_grouped['counts'] > 34, 'counts'] = 35\ndf_grouped.loc[df_grouped['counts'] > 35, '", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "df_temp = df[df['temp'].isnull()]\ndf_name = df[df['name'].isnull()]\n\nresult = pd.concat([df_temp, df_name])\n", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "\ndef solution(df):\n    df_counts = df.groupby(df.columns[0]).count()\n    df_counts.reset_index(drop=True, inplace=True)\n    df_counts.columns = ['column_name']\n    df_counts['count'] = df.groupby(df.columns[0]).count().groupby(level=0).count()\n    df_counts.reset_index(drop=True, inplace=True)\n    df_counts.columns = ['column_name']\n    return df_counts\n\ndf_counts = solution(df)\ndf_counts\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "df = df.groupby(['Nanonose','Unnamed: 1']).first()\ndf = df.groupby(['Nanonose','Unnamed: 1']).last()\ndf = df.groupby(['Nanonose','Unnamed: 1']).sum()\ndf = df.groupby(['Nanonose','Unnamed: 1']).count()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mean()\ndf = df.groupby(['Nanonose','Unnamed: 1']).std()\ndf = df.groupby(['Nanonose','Unnamed: 1']).min()\ndf = df.groupby(['Nanonose','Unnamed: 1']).max()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mul()\ndf = df.groupby(['Nanonose','Unnamed: 1']).div()\ndf = df.groupby(['Nanonose','Unnamed: 1']).sum()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mean()\ndf = df.groupby(['Nanonose','Unnamed: 1']).std()\ndf = df.groupby(['Nanonose','Unnamed: 1']).min()\ndf = df.groupby(['Nanonose','Unnamed: 1']).max()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mul()\ndf = df.groupby(['Nanonose','Unnamed: 1']).div()\ndf = df.groupby(['Nanonose','Unnamed: 1']).sum()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mean()\ndf = df.groupby(['Nanonose','Unnamed: 1']).std()\ndf = df.groupby(['Nanonose','Unnamed: 1']).min()\ndf = df.groupby(['Nanonose','Unnamed: 1']).max()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mul()\ndf = df.groupby(['Nanonose','Unnamed: 1']).div()\ndf = df.groupby(['Nanonose','Unnamed: 1']).sum()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mean()\ndf = df.groupby(['Nanonose','Unnamed: 1']).std()\ndf = df.groupby(['Nanonose','Unnamed: 1']).min()\ndf = df.groupby(['Nanonose','Unnamed: 1']).max()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mul()\ndf = df.groupby(['Nanonose','Unnamed: 1']).div()\ndf = df.groupby(['Nanonose','Unnamed: 1']).sum()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mean()\ndf = df.groupby(['Nanonose','Unnamed: 1']).std()\ndf = df.groupby(['Nanonose','Unnamed: 1']).min()\ndf = df.groupby(['Nanonose','Unnamed: 1']).max()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mul()\ndf = df.groupby(['Nanonose','Unnamed: 1']).div()\ndf = df.groupby(['Nanonose','Unnamed: 1']).sum()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mean()\ndf = df.groupby(['Nanonose','Unnamed: 1']).std()\ndf = df.groupby(['Nanonose','Unnamed: 1']).min()\ndf = df.groupby(['Nanonose','Unnamed: 1']).max()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mul()\ndf = df.groupby(['Nanonose','Unnamed: 1']).div()\ndf = df.groupby(['Nanonose','Unnamed: 1']).sum()\ndf = df.groupby", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "df = df.groupby(['Nanonose','Unnamed: 1']).first()\ndf = df.groupby(['Nanonose','Unnamed: 1']).last()\ndf = df.groupby(['Nanonose','Unnamed: 1']).sum()\ndf = df.groupby(['Nanonose','Unnamed: 1']).count()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mean()\ndf = df.groupby(['Nanonose','Unnamed: 1']).std()\ndf = df.groupby(['Nanonose','Unnamed: 1']).min()\ndf = df.groupby(['Nanonose','Unnamed: 1']).max()\ndf = df.groupby(['Nanonose','Unnamed: 1']).mul()\ndf = df.groupby(['Nanonose','Unnamed: 1']).div()\ndf = df.groupby(['Nanonose','Unnamed: 1']).dropna()\ndf = df.groupby(['Nanonose','Unnamed: 1']).fillna()\ndf = df.groupby(['Nanonose','Unnamed: 1']).transform()\ndf = df.groupby(['Nanonose','Unnamed: 1']).apply()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applymap()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfilter()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyregex()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applykw()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applymap()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\ndf = df.groupby(['Nanonose','Unnamed: 1']).applyfunc()\n", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "df.apply(lambda x: x.fillna(x.mean().round(2)), axis=0)\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "df = df.apply(lambda x: x.fillna(x.mean().astype(int)), axis=0)\ndf = df.apply(lambda x: x.fillna(x.mode().iloc[0]), axis=0)\ndf = df.apply(lambda x: x.fillna(x.median().iloc[0]), axis=0)\ndf = df.apply(lambda x: x.fillna(x.mean().iloc[0]), axis=0)\ndf = df.apply(lambda x: x.fillna(x.mode().iloc[0]), axis=1)\ndf = df.apply(lambda x: x.fillna(x.mean().iloc[0]), axis=2)\ndf", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "df = df.apply(lambda x: x.fillna(x.mean().astype(int)), axis=0)\ndf = df.apply(lambda x: x.fillna(x.mode().iloc[0]), axis=0)\ndf = df.apply(lambda x: x.fillna(x.median().iloc[0]), axis=0)\ndf = df.apply(lambda x: x.fillna(x.mean().iloc[0]), axis=0)\ndf = df.apply(lambda x: x.fillna(x.mode().iloc[0]), axis=1)\ndf = df.apply(lambda x: x.fillna(x.mean().iloc[0]), axis=2)\ndf", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "df_grouped = df.groupby(df.index // thresh + 1).sum()\ndf_grouped.index = df_grouped.index * thresh\ndf_grouped.loc['X'] = df.loc[df.value < thresh].sum()\ndf_grouped.loc['X'] = df_grouped.loc['X'] + df.loc[df.value >= thresh].sum()\nresult = df_grouped\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "df_grouped = df.groupby(df.index // thresh + 1).sum()\ndf_grouped.index = df_grouped.index // thresh + 1\ndf_grouped.index.name = 'lab'\ndf_grouped['value'] = df_grouped.index // thresh + 1\nresult = df_grouped\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# filter rows in the section\ndf_section = df[df['value'].between(section_left, section_right)]\n\n# calculate the average of the values in the section\naverage_value = df_section['value'].mean()\n\n# replace the values in the section with the average\ndf[df['value'].between(section_left, section_right)] = average_value\n\n# drop the original section\ndf.drop(df[df['value'].between(section_left, sectionordinate_right)].index, inplace=True)\n\n# set the index back to 'lab'\ndf.set_index('lab')\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\ndef inv_func(x):\n    return 1/x\n\n\ndf_inv = df.apply(inv_func)\ndf_inv.columns = [f'inv_{col}' for col in df.columns]\n\nresult = pd.concat([df, df_inv], axis=1)\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\n# Calculate the exponents of each column in the DataFrame\ndf_exp = df.iloc[:, ::-1].apply(lambda x: x**(len(df)-1))\n\n# Add the original columns to the DataFrame\ndf_exp['A'] = df['A']**(len(df)-1)\ndf_exp['B'] = df['B']**(len(df)-1)\n\n# Drop the original columns\ndf_exp.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the new columns to the original DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(df)-1)\ndf['B'] = df['B']**(len(df)-1)\n\n# Drop the new columns\ndf.drop(['A', 'B'], axis=1, inplace=True)\n\n# Add the original columns to the DataFrame\ndf['A'] = df['A']**(len(", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\ndef inverse(x):\n    return 1 / x\n\ndf_inv = df.copy()\nfor col in df.columns:\n    df_inv[f'inv_{col}'] = df[col].apply(inverse)\n\ndf_inv['A'] = df['A'].apply(inverse)\ndf_inv['B'] = df['B'].apply(inverse)\n\nresult = df_inv\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\ndf_sigmoid = df.apply(sigmoid)\n\nresult = pd.concat([df, df_sigmoid], axis=1)\n\nprint(result)\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "df.idxmin()\ndf.groupby('a').apply(lambda x: x.idxmax() if x.max() == x.max() else np.nan)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "max_locations = df.idxmax()\nmax_locations_first_occurance = max_locations[max_locations.groupby(df.groupby().idxmax().cumcount()).cumcount() == 0].groupby(df.groupby().idxmax().cumcount()).first_valid()\nmax_locations_first_occurance\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "df_grouped = df.groupby(['user','dt'])['val'].sum().reset_index()\ndf_pivot = df_grouped.pivot_table(index='dt',columns='user',values='val')\ndf_pivot.fillna(0,inplace=True)\ndf_pivot.reset_index(drop=True, inplace=True)\ndf_pivot = df_pivot.merge(df_grouped, how='left')\ndf_pivot.sort_values(by=['user','dt'])\n", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "df['dt'] = df.groupby('user')['dt'].transform('first')\ndf['val'] = df.groupby('user')['val'].transform('first')\ndf = df.groupby(['user','dt'])['val'].apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby(level=0).apply(lambda x: x.fill_value).reset_index(drop=True)\ndf = df.groupby('user')['val'].groupby", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "df['dt'] = df.groupby('user')['dt'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('first')\ndf = df.groupby(['user','dt'])['val'].sum().unstack().fillna(0).astype(int)\ndf = df.groupby('user')['val'].sum().reset_index().rename(columns={'val':'val_sum'})\ndf = df.merge(df.groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().groupby('user')['val_sum'].sum().reset_index().group", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "df_grouped = df.groupby(['user','dt'])\ndf_grouped['val'].max().reset_index().rename(columns={'val': 'max_val'})\ndf_grouped_max = df_grouped.groupby('user').apply(lambda x: x.loc[x['val'] == x['val'].max()])\ndf_grouped_max = df_grouped_max.merge(df_grouped, how='left')\ndf_grouped_max.sort_values(['user','dt'])\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "df['dt'] = df.groupby('user')['dt'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].transform('min')\ndf = df.groupby('user')['val'].transform('max", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "df['name'] = df.groupby('name')['a'].transform('first')\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first')\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform('first') + 1\ndf['name'] = df.groupby('name')['a'].transform", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "df['a'] = df.groupby('name')['a'].transform('first')\nresult = df\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "\n    df['name_id'] = range(1, df.groupby('name')['name'].transform('count')+1)\n    df = df.sort_values(['name', 'a'])\n    df.drop_duplicates(subset='name_id', keep='first', inplace=True)\n    df.drop(columns=['name_id'], inplace=True)\n    df.rename(columns={'name': 'name_new'}, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.drop(columns=['a'], inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.rename(columns={'name_new': 'name'}, inplace=True)\n    df.insert(loc=len(df.columns), column='a', value=[1]*len(df))\n    df.sort_values(['name', 'a'])\n    df.reset_index(drop=True, inplace=True)\n    df.ren", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "df['ID'] = df.groupby('name')['a'].cumsum()\ndf = df.drop('name', axis=1)\ndf = df.rename(columns={'ID': 'ID'})\ndf = df.groupby('ID').apply(lambda g: g.assign(b=g.b.sum(), c=g.c.sum()) if g.a.nunique()>1 else g)\ndf = df.reset_index(drop=True)\ndf\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df_pivot = df.pivot_table(index='user', columns='date', values='value', aggfunc='first')\ndf_pivot['someBool'] = df['someBool']\ndf_pivot.reset_index(drop=True, inplace=True)\ndf_pivot\n", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df_pivot = df.pivot_table(index='user', columns='01/12/15',\n                          values='02/12/15',\n                          aggfunc='first')\n\ndf_pivot['others'] = df[['user', '01/12/15']].drop_duplicates()\ndf_pivot['value'] = df[['user', '02/12/15']].drop_duplicates()\n\ndf_pivot = df_pivot[['user', 'others', 'value']]\n", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df_pivot = df.pivot_table(index='user', columns='date', values='value', aggfunc='first')\ndf_pivot['someBool'] = df['someBool']\ndf_pivot.reset_index(drop=True, inplace=True)\ndf_pivot\n", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "df_sub = df[df['c'] > 0.5][columns]\nresult = df_sub.values\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "df_sub = df[df.c > 0.45][columns]\nresult = df_sub.values\n", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    df_sub = df[df['c'] > 0.5][['b', 'e']]\n    ### END SOLUTION\n    return df_sub\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ndf['c'] = df['c'] * 2\ndf['d'] = df['d'] * 2\n\ndf_sub = f(df)\nprint(df_sub)\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    df_ = df[df['c'] > 0.5][['b', 'e']]\n    df_.columns = ['b', 'e']\n    df_['sum'] = df_['b'] + df_['e']\n    return df_\n    ### END SOLUTION\n\ndf = pdseemed.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ndf = f(df)\nprint(df)\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    df_ = df[df.c > 0.5][['b', 'e']]\n    return df_\n    ### END SOLUTION\n\ndf_ = f(df)\nprint(df_)\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\n\ndf['mask'] = (df.groupby(df.index.date - df.index.date.dt.days // X).groups.groups != df.index.date.dt.days // X\ndf['mask'] = df['mask'].astype(int)\n\ndf = df[df['mask'] == 1]\n\ndf.set_index('date', inplace=True)\ndf = df[df.index.date.dt.days // X != 1]\n\ndf.index = df.index.date\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day = df.index.day // X\n\ndf.index.day = df.index.day // X\ndf.index.day", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\n\nmask = (df.groupby(df.index[::X]).date.isin(df.groupby(df.index[::X]).date.rolling(window=X).min()))\n\ndf = df[mask]\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\n\nmask = (df.groupby(df.index[::-1].diff().dt.days // X).cumsum() != 1).rolling(X).sum()\ndf['mask'] = mask\n\ndf[mask != 1]\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "df['col1'] = df['col1'].rolling(3).mean().apply(lambda x: x.round(1))\ndf\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "df['col1'] = df.groupby((df.index+1)//3).sum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index // 3).sum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index // 3).sum()\ndf\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "df['col1'] = df.groupby(df['col1'].groupby().rolling(4).sum().index).cumsum()\ndf\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "df['col1'] = df['col1'].rolling(3).mean().dropna()\ndf['col1'] = df['col1'].rolling(3).mean().dropna()\ndf['col1'] = df['col1'].rolling(3).mean().dropna()\ndf['col1'] = df['col1'].rolling(3).mean().dropna()\ndf['col1'] = df['col1'].rolling(3).mean().dropna()\n\ndf\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\ndf['col1'] = df.groupby((df.index+1)//3).sum()\ndf['col1'] = df.groupby((df.index+1)//3).mean()\ndf['col1'] = df.groupby((df.index+1)//3).sum()\ndf['col1'] = df.groupby((df.index+1)//3).mean()\n", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\n\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\n\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\n\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\n\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\n\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\n\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(2).mean().index).cumsum()\n\ndf['col1'] = df['col1'].groupby(df['col1'].groupby().rolling(3).sum().index).cumsum()\ndf", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df['A'] = df['A'].where(df['A'].ne(0), df['A'].shift(-1))\ndf['A'].fillna(0, inplace=True)\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(0).mean()\ndf.loc[df.eq(0).groupby().ne(0).index] = df.loc[df.eq(0).groupby().ne(0).index].groupby().ne(", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "df['A'] = df['A'].where(df['A'].ne(0), df['A'].rolling(3).max())\ndf['A'].fillna(df['A'].rolling(3).max(), inplace=True)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['time'] = df['duration'].str.split().str[0]\ndf['time_days'] = df['duration'].str.split().str[1]\ndf['time_days'] = df['time_days'].apply(lambda x: int(x) if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*7 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*30 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*365 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if x.isdigit() else x)\ndf['time_days'] = df['time_days'].apply(lambda x: x*1 if", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['time_day'] = df['duration'].str.split(' ').str[1].astype(int)\ndf\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "\n    df['duration'] = df['duration'].str.split(' ').str[0]\n    df['duration'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    df['time_days'] = df['duration'].map({'year': 365, 'day': 1, 'week': 7, 'month': ", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['time_day'] = df['duration'].str.split(' ').str[1].astype(int)\ndf\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "\ncheck_conditions = [(df1[column] == df2[column]) for column in columns_check_list]\nresult = np.all(check_conditions, axis=1)\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "check = [df1[column].equals(df2[column]) for column in columns_check_list]\nresult = [True if x else False for x in check]\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index.levels[1] = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index.levels[1] = pd.to_datetime(df.index.levels[1])\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index(['id','date'],inplace=True)\n    df = df.sort_values(['id','date'])\n    return df\n    ### END SOLUTION\n\ndf = f(df)\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "\n    df['date'] = pdquis.to_datetime(df['date'])\n    df.set_index(['id','date'], inplace=True)\n    df = df.groupby(levels=['id','date'])\n    df.groupby(levels=['id','date'])\n    return df\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df_long = df.groupby(['Country','Variable'])['2000','2001','2002','2003','2004','2005'].apply(lambda x: x.groupby(level=0).apply(lambda x: x.reset_index(drop=True)))\ndf_long = df_long.reset_index(drop=True)\ndf_long\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df = df.groupby(['Country','Variable','year']).apply(lambda x: x.iloc[:,::-1])\ndf = df.reset_index(drop=True)\ndf\n", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "df = df.loc[df.apply(tuple, axis=1).abs().sum(axis=1) < 1]\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "df = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1)]\n", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "df = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc[df.apply(lambda x: x.abs().max() > 1, axis=1, result=True)]\ndf = df.loc", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df.replace('&LT;', '<', regex=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "\n    df.replace('&AMP;', '&', regex=True, inplace=True)\n    ### END SOLUTION\n    return df\n\nprint(f(example_df))\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\ndf['A'] = df['A'].str.replace('&LT;', '<')\ndf['A'] = df['A'].str.replace('&GT;', '>')\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df.replace('&AMP;', '&')\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "df['first_name'], df['last_name'] = df['name'].str.split(' ', 1).str[0], df['name'].str.split(' ', 1).str[1]\ndf\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "df = df.apply(lambda x: x.split(' ') if len(x.split(' ')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('_') if len(x.split('_')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('.') if len(x.split('.')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('-') if len(x.split('-')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('/') if len(x.split('/')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('@') if len(x.split('@')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('#') if len(x.split('#')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('$') if len(x.split('$')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('^') if len(x.split('^')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('&') if len(x.split('&')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('*') if len(x.split('*')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('=') if len(x.split('=')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('+') if len(x.split('+')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('-') if len(x.split('-')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('_') if len(x.split('_')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split(':') if len(x.split(':')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split(';') if len(x.split(';')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('=') if len(x.split('=')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('<') if len(x.split('<')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('>') if len(x.split('>')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('?') if len(x.split('?')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('~') if len(x.split('~')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('$') if len(x.split('$')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('^') if len(x.split('^')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('&') if len(x.split('&')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('*') if len(x.split('*')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('=') if len(x.split('=')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('+') if len(x.split('+')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('-') if len(x.split('-')) > 1 else x, axis=1)\ndf = df.apply(lambda x: x.split('_') if len(x.split('_')) > 1 else x, axis=1)\ndf = df.apply(lambda x:", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "df['first_name'] = df['name'].str.split().str[0]\ndf['middle_name'] = df['name'].str.split().str[1] if len(df['name'].str.split()) > 2 else None\ndf['last_name'] = df['name'].str.split().str[2] if len(df['name'].str.split()) > 2 else None\ndf", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "df1.set_index('Timestamp')\ndf2.set_index('Timestamp')\n\ndf_merged = pd.merge(df1, df2, how='outer', on='Timestamp')\n\ndf_merged.sort_values('Timestamp')\n", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "df1.merge(df2, on='Timestamp', how='left')\n", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= 50, df['col1'], df['col3'].max())\ndf\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = np.where(df['col2']>50, df['col1'], df['col1']+df['col3'])\ndf", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, int) else 'and')\ndf\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, int) else int(x))\ndf['Field1'].apply(lambda x: x if isinstance(x, int) else x)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "\n    df_new = pd.DataFrame()\n    for i in df['Field1']:\n        if isinstance(i, str) or i.isnumeric():\n            df_new = df_new.append(i)\n        else:\n            df_new = df_new.append('and')\n    return df_new\n    ### END SOLUTION\n\ndf_new = f(example_df)\nprint(df_new)\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "df['val1'] = df['val1'] / df.groupby('cat')['val1'].transform('sum')\ndf['val2'] = df['val2'] / df.groupby('cat')['val2'].transform('sum')\ndf['val3'] = df['val3'] / df.groupby('cat')['val3'].transform('sum')\ndf['val4'] = df['val4'] / df.groupby('cat')['val4'].transform('sum')\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1_sum'] = df.groupby('cat')['val1'].transform('sum')\ndf['val1_prop'] = df['val1'] / df['val1_sum']\ndf", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]\n", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]\n", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "df.drop(test, inplace=True)\nresult = df\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": "\n    df_new = df.loc[test]\n    return df_new\n\ndf = pdply(df, f)\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\ndf['euclidean_distance'] = df.groupby('time')['x_y_diff'].apply(lambda g: g.apply(lambda row: np.sqrt((row[0])**2 + (row[1])**2), axis=1).mean())\ndf\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\ndf['farmost_neighbour'] = df.groupby('time')['car'].transform('nearest')\ndf['euclidean_distance'] = ((df['x']-df['x'].groupby(df['time']).transform('nearest')['x'])**2 + (df['y']-df['y'].groupby(df['time']).transform('nearest')['y'])**2)**0.5\ndf\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df.apply(lambda x: ','.join(x.dropna().astype(str)), axis=1)\ndf\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df.apply(lambda x: '-'.join(x.dropna().astype(str).tolower()), axis=1)\ndf\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().astype(str)), axis=1)\ndf\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().astype(str)), axis=1)\ndf\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "df['Quantity'] = df.groupby(df['UserId'])['Quantity'].transform('count')\ndf['Quantity'] = df['Quantity'] > 0.2\ndf['Quantity'] = df['Quantity'].apply(lambda x: 0 if x else 1)\ndf['Quantity'] = df.groupby(df['UserId'])['Quantity'].transform('count')\ndf['Quantity'] = df['Quantity'].apply(lambda x: 0 if x else 1)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else 0)\ndf['Quantity'] = df['Quantity'].apply(lambda x: x if x else", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "df['ProductId'] = df.groupby(df['UserId'])['ProductId'].apply(lambda x: x.apply(lambda x: x.sample(frac=1).index[0]) if len(x) > 1 else x)\ndf", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "df['Quantity'] = df.groupby(df['UserId'])['Quantity'].transform('count')\ndf['Quantity'] = df['Quantity'] > 0.2\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(int)\ndf['UserId'] = df['UserId'].astype(str)\ndf['UserId'] = df['UserId'].astype(", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "df['index_original'] = np.where(df.duplicated(subset=['col1','col2'], keep='first') == True, df.index, np.nan)\ndf\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False) * 1\ndf\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "\n    df_dup = df[df.duplicated(subset=['col1','col2'], keep=False)]\n    df_dup_grouped = df_dup.groupby(['col1','col2']).count()\n    df_dup_grouped['index_original'] = range(len(df_dup_grouped))\n    df_dup_grouped['index_dup'] = range(len(df_dup_grouped))\n    df_dup_grouped['duplicate_index'] = df_dup_grouped.index\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = dfipage_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_grouped['duplicate_bool'] = df_dup_grouped.duplicated", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "df['index_original'] = df.duplicated(subset=['col1','col2', '3col'], keep=False)\ndf\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\ndf\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "df_max = df.groupby(['Sp','Mt'])['count'].max().reset_index().groupby('Sp')['count'].transform('max')\ndf_max_df = df[df['count'] == df_max]\nresult = df_max_df\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "df_max = df.groupby(['Sp','Mt'])['count'].max().reset_index().groupby('Sp')['count'].transform('max')\ndf_max_gt_zero = df[df['count']>0]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero.duplicated(['Sp','Mt']) == False]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_gt_zero['count'].max()]\ndf_max_gt_zero = df_max_gt_zero[df_max_gt_zero['count']==df_max_", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "df_grouped = df.groupby(['Sp','Mt'])\nmin_count_df = df_grouped['count'].transform('min')\ndf_grouped_min = df_grouped[df_grouped['count'] == min_count_df]\nresult = df_grouped_min\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "df['max_count'] = df.groupby(['Sp','Value'])['count'].transform('max')\ndf_max_count = df[df['count'] == df.groupby(['Sp','Value'])['count'].transform('max')]\nresult = df_max_count\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "df.loc[df['Category'].isin(filter_list)]\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "df.query(\"Category not in @filter_list\")\n", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "df_melt = df.melt(value_vars=['A', 'B', 'E', 'F', 'G', 'H', 'I', 'J'])\ndf_melt.columns = [list('ABCDEFGHIJ')]\ndf_melt\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "df_melt = df.melt(df.columns.tolist())\ndf_melt['variable_0'] = df_melt['variable'].str[0].astype(str)\ndf_melt['variable_1'] = df_melt['variable'].str[1].astype(str)\ndf_melt['variable_2'] = df_melt['variable'].str[2].astype(str)\ndf_melt = df_melt.drop(columns=['variable'])\ndf_melt = df_melt.groupby(['variable_0','variable_1','variable_2'])['value'].sum().reset_index()\ndf_melt.columns = ['variable_0','variable_1','variable_2','value']\ndf_melt\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby(df['id'])['val'].cumsum()\ndf\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cumsum().reset_index(drop=True)\ndf", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "df['v'] = df.groupby('l')['v'].apply(lambda x: x.sum() if np.isnan(x.sum()) else x.sum())\ndf['v'].groupby(df['l']).sum()\n", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "df['v'] = df.groupby('r')['v'].apply(lambda x: x.sum() if np.isnan(x.sum()) else x.sum())\ndf['v'].groupby(df['r']).sum()\n", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "df['v'] = df.groupby('l')['v'].apply(lambda x: x.sum() if np.isnan(x.sum()) else x.sum())\ndf.groupby('l')['v'].sum()\n", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "def relationship_type(column1, column2):\n    if len(set(column1)) == len(column1):\n        if len(set(column2)) == len(column2):\n            return 'many-to-many'\n        else:\n            return 'many-to-one'\n    else:\n        if len(set(column2)) == len(column2):\n            return 'one-to-many'\n        else:\n            return 'one-to-one'\n\n\ndef solve():\n    for i in range(len(df.columns)):\n        for j in range(i+1, len(df.columns)):\n            result.append([df.columns[i], df.columns[j], relationship_type(df[df.columns[i]], df[df.columns[j]])])\n    return result\n\n\nsolve()\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "def relationship_type(df):\n    df_grouped = df.groupby(list(df.columns)[:-1]).apply(lambda g: list(g.dtypes))\n    df_grouped.columns = ['Column']\n    df_grouped['Type'] = df_grouped.apply(lambda x: 'many-2-many' if x.all()=='object' else 'one-2-many' if x.nunique()==2 else 'one-2-one' if x.nunique()==1 else 'many-2-one' if x.nunique()==3 else 'many-2-many', axis=1)\n    return df_grouped\n\ndf_grouped = df.groupby(list(df.columns)[:-1]).apply(lambda g: list(g.dtypes))\ndf_grouped.columns = ['Column']\ndf_grouped['Type'] = df_grouped.apply(lambda x: 'many-2-many' if x.all()=='object' else 'one-2-many' if x.nunique()==2 else 'one-2-one' if x.nunique()==1 else 'many-2-one' if x.nunique()==3 else 'many-2-many', axis=1)\n\ndf_grouped\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "def relationship(df):\n    df_rel = pd.DataFrame(index=df.columns, columns=df.columns)\n    for col in df.columns:\n        for col2 in df[col].unique():\n            df_temp = df[df[col]==col2]\n            if len(df_temp.columns)>1:\n                df_rel.loc[col, col2] = 'one-to-many'\n            else:\n                df_rel.loc[col, col2] = 'one-to-one'\n    return df_rel\n\ndf_rel = relationship(df)\ndf_rel\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "def relationship_type(df):\n    df_grouped = df.groupby(list(df.columns)[:-1]).apply(lambda g: g.apply(lambda x: 'one-2-many' if len(x.unique()) > 1 else 'one-2-one'))\n    df_grouped = df_grouped.groupby(list(df.columns)[:-1]).apply(lambda g: 'many-2-many' if len(g.unique()) > 1 else 'many-2-one')\n    return df_grouped\n\ndf_grouped = df.groupby(list(df.columns)[:-1]).apply(lambda g: 'many-2-one' if len(g.unique()) > 1 else 'many-2-many')\n\ndf_grouped.index = df.columns\ndf_grouped.columns = df.columns\n\ndf_grouped\n", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "df.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='last', inplace=True)\ndf.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first', inplace=True)\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n\n# print function\nprint(\"hello world\")\n\n# input function\nname = input(\"Enter your name: \")\nprint(\"Hello \" + name)\n\n# variable\nname = \"John\"\nprint(\"Hello \" + name)\n\n# string\nname = \"John\"\nprint(\"Hello \" + name)\n\n# list\nname = [\"John\", \"Doe\"]\nprint(\"Hello \" + name[0])\n\n# dictionary\nname = {\"firstname\":\"John\", \"lastname\":\"Doe\"}\nprint(\"Hello \" + name[\"firstname\"])\n\n# tuple\nname = (\"John\", \"Doe\")\nprint(\"Hello \" + name[0])\n\n# set\nname = {\"John\", \"Doe\"}\nprint(\"Hello \" + name[0])\n\n# function\ndef greet(name):\n  return \"Hello \" + name\n\nname = input(\"Enter your name: \")\nprint(greet(name))\n\n# loop\nnames = [\"John\", \"Doe\", \"Smith\"]\nfor name in names:\n  print(greet(name))\n\n# if\nif name:\n  print(\"Hello \" + name)\nelse:\n  print(\"Not found\")\n\n# else if\nif name:\n  print(\"Hello \" + name)\nelse:\n  print(\"Not found\")\n\n# else\nif name:\n  print(\"Hello \" + name)\nelse:\n  print(\"Not found\")\n\n# and\nif name and name:\n  print(\"Hello \" + name)\n\n# or\nif name or name:\n  print(\"Hello \" + name)\n\n# in\nif name in [\"John\", \"Doe\"]:\n  print(\"Hello \" + name)\n\n# not\nif not name:\n  print(\"Not found\")\n\n# tuples\ntuple_name = (\"John\", \"Doe\")\nprint(\"Hello \" + tuple_name[0])\n\n# dictionary\ndict_name = {\"firstname\":\"John\", \"lastname\":\"Doe\"}\nprint(\"Hello \" + dict_name[\"firstname\"])\n\n# list\nlist_name = [\"John\", \"Doe\"]\nprint(\"Hello \" + list_name[0])\n\n# set\nset_name = {\"John\", \"Doe\"}\nprint(\"Hello \" + set_name[0])\n\n# f-string\nname = \"John\"\nprint(f\"Hello {name}\")\n\n# format\nname = \"John\"\nprint(\"Hello \" + format(name, \"s\"))\n\n# rjust\nname = \"John\"\nprint(\"Hello \" + name.rjust(5))\n\n# ljust\nname = \"John\"\nprint(\"Hello \" + name.ljust(5))\n\n# center\nname = \"John\"\nprint(\"Hello \" + name.center(5))\n\n# strip\nname = \"John\"\nprint(\"Hello \" + name.strip())\n\n# lower\nname = \"John\"\nprint(\"Hello \" + name.lower())\n\n# upper\nname = \"John\"ips\nprint(\"Hello \" + name.upper())\n\n# isalpha\nname = \"John\"\nprint(name.isalpha())\n\n# isdigit\nname = \"John\"\nprint(name.isdigit())\n\n# islower\nname = \"John\"\nprint(name.islower())\n\n# isupper\nname = \"John\"\nprint(name.isupper())\n\n# startswith\nname = \"John\"\nprint(name.startswith(\"J\"))\n\n# endswith\nname = \"John\"\nprint(name.endswith(\"J\"))\n\n# count\nname = \"John\"\nprint(name.count(\"o\"))\n\n# find\nname = \"John\"\nprint(name.find(\"o\"))\n\n# replace\nname = \"John\"\nprint(name.replace(\"o\", \"a\"))\n\n# split\nname = \"John\"\nprint(name.split(\" \"))\n\n# strip\nname = \"John\"\nprint(name.strip())\n\n# join\nname = [\"John\", \"Doe\"]\nprint(\" \".join(name))\n\n# title\nname = \"John Doe\"\nprint(name.title())\n\n# capfirst\nname = \"john doe\"\nprint(name.capfirst())\n\n# swallow\nname = \"john doe\"\nprint(name.swallow())\n\n# isprintable\nname = \"john doe", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "df['Has_Family'] = np.where(df['SibSp']>0 | df['Parch']>0, 1, 0)\ndf['No_Family'] = np.where((df['SibSp']==0) & (df['Parch']==0), 1, 0)\n\ngrouped = df.groupby('Has_Family')['Survived'].mean()\nresult = grouped\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "df['Group'] = (df['Survived'] > 0) | (df['Parch'] > 0)\ndf['Group'] = df['Group'].map({0:'Has Family', 1:'No Family'})\nmean_group_sibsp = df.groupby('Group')['SibSp'].mean()\nmean_group_sibsp\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "df['Family'] = (df['SibSp'] == 1) & (df['Parch'] == 1)\ndf['New Family'] = (df['SibSp'] == 0) & (df['Parch'] == 0)\ndf['Old Family'] = (df['SibSp'] == 1) & (df['Parch'] == 0)\ndf['No Family'] = (df['SibSp'] == 0) & (df['Parch'] == 0)\n\ngrouped = df.groupby('Survived')['Family'].mean()\nprint(grouped)\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "df.groupby('cokey').apply(lambda x: x.sort_values('A'))\n", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.tuples().values[::-1])\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])\ndf\n", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])\ndf\n", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "df = pd.DataFrame(list(someTuple), columns=['birdType', 'birdCount'])\ndf.birdType = df.birdType.astype('category')\ndf.birdCount = df.birdCount.astype('int')\ndf.birdCount = df.birdCount.groupby(df.birdType).cumsum()\ndf\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "df['mean'] = df.groupby('a')['b'].transform('mean')\ndf['std'] = df.groupby('a')['b'].transform('std')\ndf['std_div'] = df['b'] - df['mean']/df['std']\ndf['std_div'] = df['std_div'].groupby(df['a'].groupby()['a'].mean()\ndf", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "df['a'] = df.groupby('b')['a'].transform('mean')\ndf['std'] = df.groupby('b')['a'].rolling(window=3).std()\ndf['std'] = df.groupby('b')['a'].rolling(window=3).mean() / df.groupby('b')['a'].rolling(window=3).std()\ndf['std'] = df.groupby('b')['a'].rolling(window=3).std()\ndf\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "df['softmax'] = df.groupby(df['a'])['b'].transform('count') / df.groupby(df['a'])['b'].count()\ndf['min_max'] = df.groupby(df['a'])['b'].transform('min') / df.groupby(df['a'])['b'].transform('max')\ndf", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "df_no_zero = df[df.sum(axis=0)>0]\ndf_no_zero = df_no_zero.dropna(axis=0, how='any')\ndf_no_zero = df_no_zero.dropna(axis=1, how='any')\n\ndf_no_zero = df_no_zero.reset_index(drop=True)\n\ndf_no_zero.columns = ['A','B','D']\ndf_no_zero\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "df_no_zero = df[df.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_noseemed.dropna(subset=['A','B'])\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=0) != 0]\ndf_no_zero = df_no_zero[df_no_zero.sum(axis=1) != 0]\n\ndf_no_zero = df_", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "max_val = df.max().max()\ndf = df[df.max() == max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() != max_val]\ndf = df[df.max() !", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "max_val = df.max().max()\ndf[df.max().index == df.max().max()] = 0\nresult = df\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "sorted_s = s.sort_values()\nsorted_s.index = range(len(sorted_s))\nresult = sorted_s\n", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.groupby(s.index).apply(lambda x: x.sort_values(ascending=True)).reset_index(drop=True)\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.index = df.index.astype(str)\ndf.index = df.index.astype(int)\ndf.index += 1\ndf.", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "df['A'] = df['A'].astype('int')\ndf = df[df['A'].astype(str) == df['A'].astype(str)]\nresult = df\n", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "df_result = df[df['A'].isin(['s', 'b'])]\nresult = df_result\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "df_max = df.groupby(['Sp','Mt'])['count'].max().reset_index().groupby('Sp')['count'].transform('max')\ndf_max_df = df[df['count'] == df_max]\nresult = df_max_df\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "df['max_count'] = df.groupby(['Sp','Mt'])['count'].transform('max')\ndf_max_count = df[df['count'] == df.groupby(['Sp','Mt'])['count'].transform('max')]\nresult = df_max_count\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "df_grouped = df.groupby(['Sp','Mt'])\nmin_count_df = df_grouped['count'].transform('min')\ndf_grouped_min = df_grouped[df_grouped['count'] == min_count_df]\nresult = df_grouped_min\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "df['max_count'] = df.groupby(['Sp','Value'])['count'].transform('max')\ndf_max_count = df[df['count'] == df.groupby(['Sp','Value'])['count'].transform('max')]\nresult = df_max_count\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\ndf", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    df.loc[df['Group'] == 'A', 'Date'] = dict.get('abc')\n    df.loc[df['Group'] == 'B', 'Date'] = dict.get('def')\n    ### END SOLUTION\n    return df\n\ndf = f()\nprint(df)\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = pd.to_datetime(df['Date'])\ndf", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year])['Val'].transform('count')\ndf", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year])['Val'].transform('count')\ndf['Count_Val'] = df.groupby(['Date','Val'])['Val'].transform('count')\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year])['Val'].transform('count')\ndf['Count_w'] = df.groupby([df['Date'].dt.weekofyear, df['Date'].dt.weekday])['Val'].transform('count')\ndf['Count_Val'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, df['Date'].dt.weekday])['Val'].transform('count')\ndf", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\n\ndf_zero = df[df.eq(0).any(axis=1)]\ndf_non_zero = df[df.ne(0).any(axis=1)]\n\ndf_zero.groupby(df_zero.index // 1).count()\ndf_non_zero.groupby(df_non_zero.index // 1).count()\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "df['Date'] = pd.to_datetime(df['Date']).dt.date\ndf_even = df[df['B'] % 2 == 0]\ndf_odd = df[df['B'] % 2 != 0]\n\ndf_even_grouped = df_even.groupby(['Date', 'B']).sum()\ndf_odd_grouped = df_odd.groupby(['Date', 'B']).sum()\n\ndf_even_grouped.to_csv('even_values.csv')\ndf_odd_grouped.to_csv('odd_values.csv')\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.sum)\nresult = pdseemed to be the solution for the above problem.\n\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.mean)\nresult = pd seemed to be the solution for the above problem.\n\nEND SOLUTION\n<code>\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nresult_mean = pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=npipage.mean)\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.sum)\nresult = pdseemed to be the solution to the problem.\n\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.mean)\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.max)\nresult = pdipage.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.min)\n", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "df_new = df.join(df['var2'].str.split(',',expand=True))\ndf_new.drop(['var2'], axis=1, inplace=True)\ndf_new.columns = ['id','var1','var2']\ndf_new\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "df_new = df.join(df['var2'].str.split(',',expand=True).stack().groupby(level=0).apply(lambda x: x.str.split(',',expand=True).apply(lambda x: pd.Series(x.str.strip())))\ndf_new.index = df_new.index.repeat(df['var2'].str.len())\ndf_new.columns = ['var2']\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new = df_new.groupby(level=0).apply(lambda x: x.str.strip())\ndf_new =", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "df_new = df.join(df.var2.str.split(', ',expand=True))\ndf_new.drop(['var2'], axis=1, inplace=True)\ndf_new.reset_index(drop=True, inplace=True)\ndf_new\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df['fips'] = df['row'].str.split(' ', 1).str[0]\ndf['row'] = df['row'].str.split(' ', 1).str[1]\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df['fips'] = df['row'].str.split(' ', 1).str[0]\ndf['row'] = df['row'].str.split(' ', 1).str[1]\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df['fips'] = df['row'].str.split(' ').str[0]\ndf['medi'] = df['row'].str.split(' ').str[1]\n", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "df = df.groupby('Name')['2001','2002','2003','2004','2005','2006'].apply(lambda x: x.cumsum()/x.sum() if x.sum()>0 else 0)\ndf\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "df = df.set_index('Name').groupby(level=0).apply(lambda x: x.cumsum() / x.sum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df.groupby(level=0).apply(lambda x: x.fillna(0))\ndf = df.groupby(level=0).apply(lambda x: x.cumsum())\ndf = df", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "\n    df_mean = df.mean(axis=1).dropna()\n    df_mean = df_mean.astype(int)\n    df_mean = df_mean.groupby(df.columns[0]).apply(lambda x: x/sum(x))\n    df_mean.columns = df.columns[0]\n    df_mean.index = df.columns[0]\n    df_mean = df_mean.groupby(level=0).apply(lambda x: x*sum(x.dropna().astype(int)))\n    df_mean = df_mean.groupby(level=0).apply(lambda x: x.fillna(x.mean()))\n    return df_mean\n    ### END SOLUTION\n\nf(example_df)\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "df = df.set_index('Name').groupby(level=0).apply(lambda x: x.cumsum() / x.sum())\ndf\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['Label'] = (df['Close'] - df['Close'].shift(1)) > 0\ndf['DateTime'] = df['DateTime']\ndf\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: -1 if x<0 else (0 if x==0 else 1)))\ndf\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'zero')\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = df.departure_time.diff().apply(lambda x: x.days*24*60*60 + x.seconds)\ndf\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = df.departure_time.diff().dt.total_seconds()\ndf\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['Duration'] = df.departure_time.diff().apply(lambda x: x.total_seconds())\ndf\n", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "df_grouped = df.groupby('key1')['key2'].apply(lambda x: x.value_counts().loc[x.value_counts() == 'one'])\ndf_grouped.reset_index(drop=True, inplace=True)\ndf_grouped.columns = ['count']\ndf_grouped['key1'] = df['key1']\nresult = df_grouped\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "df_grouped = df.groupby('key1')['key2'].apply(lambda x: x.value_counts().loc[x.value_counts() == 'two'].size())\ndf_grouped.name = 'count'\ndf_grouped", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "df['end_with_e'] = df['key2'].str.endswith('e')\ndf.groupby('key1')['key2'].apply(lambda x: x.sum() if x.sum() > 0 else 0)\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df[df['value']>df['value'].mean()].max()\nmin_result = df[df['value']<df['value'].mean()].min()\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.mode(axis=0)\nmedian_result = df.median()\nprint(mode_result)\nprint(median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "df = df[~(99 <= df['closing_price'] <= 101)]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "df.groupby(\"item\")[\"diff\"].min()\n", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n    return df\n\ndf = f(example_df)\nprint(df)\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "df['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace=True)\ndf['Column_x'] = df['Column_x'].apply(lambda x: 0 if x==1 else 1)\ndf['Column_x'] = df['Column_x'].astype(int)\ndf['Column_x'] = df['Column_x'].replace([np.nan], 1).astype(int)\ndf['Column_x'] = df['Column_x'].replace([np.nan], 0).astype(int)\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "df['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace=True)\ndf['Column_x'] = df['Column_x'].apply(lambda x: 0 if x==1 else x)\ndf['Column_x'] = df['Column_x'].apply(lambda x: 0.5 if x==0 else x)\ndf['Column_x'] = df['Column_x'].apply(lambda x: 1 if x==1 else x)\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "df['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace= True)\ndf['Column_x'] = df['Column_x'].map({0:'0', 1:'1'})\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "\nfor i in range(len(a)):\n    a_b = pd.DataFrame([(a.iloc[i, 0], b.iloc[i, 0])], columns=['one', 'two'])\n    result = result.append(a_b)\n\nresult\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "\ndef solve_it(a, b, c):\n    result = []\n    for i in range(len(a)):\n        result.append((a.iloc[i, 0], b.iloc[i, 0], c.iloc[i, 0]))\n    return pd.DataFrame(result, columns=['a', 'b', 'c'])\n\nresult = solve_it(a, b, c)\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\ndef solve_it(a, b):\n    a_b = []\n    for i in range(len(a)):\n        a_b.append((a.iloc[i, 0], b.iloc[i, 0]))\n    return pd.DataFrame(a_b, columns=['one', 'two'])\n\nresult = solve_it(a, b)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "df['bin'] = pd.cut(df.views, bins)\nresult = df.groupby('username').bin.value_counts()\n", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "df['bin'] = pd.cut(df.views, bins)\ndf.groupby('username')['bin'].value_counts().unstack().fillna(0)\n", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "df['bin'] = pd.cut(df.views, bins)\nresult = df.groupby('username').bin.value_counts()\n", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "df['text'] = df['text'].apply(lambda x: ','.join(x.split(',')))\nresult = df\n", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "df['text'] = '-'.join(df['text'])\nresult = df\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "df['text'] = df['text'].str.split(', ').apply(' '.join)\nresult = df\n", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "df['text'] = df['text'].apply(lambda x: ','.join(x))\ndf['text'] = df['text'].str.strip()\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip('\\n')\ndf['text'] = df['text'].str.strip('\\t')\ndf['text'] = df['text'].str.strip('\\r')\ndf['text'] = df['text'].str.strip('\\'').str.strip(' ')\ndf['text'] = df['text'].str.strip('\"')\ndf['text'] = df['text'].str.strip('\"')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['text'] = df['text'].str.strip(' ')\ndf['", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "df['text'] = df['text'].apply(lambda x: ''.join(x.split('-')))\ndf = df.groupby('text')['text'].apply(lambda x: ' - '.join(x)).reset_index(drop=True)\ndf", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "df1['city'] = df2.groupby('id')['value'].transform(list)\ndf1.drop('value', axis=1, inplace=True)\ndf1.sort_values('id', inplace=True)\ndf1\n", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "df1['date'] = df1.groupby('id')['date'].transform('first')\ndf = pd.concat([df1, df2], ignore_index=True)\ndf.sort_values(['id', 'date'], inplace=True)\ndf\n", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "df1['id'] = df1.groupby('id')['value'].transform(min)\ndf2['id'] = df2.groupby('id')['value'].transform(min)\n\ndf = pd.concat([df1, df2], ignore_index=True)\ndf.sort_values(['id', 'date'], inplace=True)\ndf.drop_duplicates(subset=['id', 'date'], keep='last', inplace=True)\ndf\n", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "merged_df = pd.merge(C, D, on='A', how='outer')\nresult = merged_df\n", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "merged_df = pd.merge(C, D, on='A', how='outer')\nresult = merged_df\n", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "merged_df = pd.merge(C, D, on='A', how='outer')\nmerged_df['dulplicated'] = merged_df.duplicated(subset='A')\nmerged_df\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "df['time'] = df.groupby('user')['time'].transform(list)\ndf['amount'] = df.groupby('user')['amount'].transform(list)\nresult = df\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "df['amount-time'] = df.groupby('user')['amount', 'time'].apply(lambda x: list(zip(x, x.shift(-1))))\ndf['amount-time'] = df['amount-time'].apply(lambda x: [i[0] for i in x])\ndf['time-amount'] = df.groupby('user')['time', 'amount'].apply(lambda x: list(zip(x, x.shift(-1))))\ndf['time-amount'] = df['time-amount'].apply(lambda x: [i[0] for i in x])\ndf\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "df['time'] = df.groupby('user')['time'].transform(list)\ndf['amount'] = df.groupby('user')['amount'].transform(list)\nresult = df\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "df_concatenated = pd.concat([series[i] for i in series.index])\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "df_concatenated = pd.concat([series.apply(pd.Series.groupby, axis=1).groupby(level=0).apply(lambda x: pd.Series(x.values.tolist())) for name, group in series.groupby(level=0)], ignore_index=True)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "\ndef find_col_with_pattern(df, pattern):\n    cols_with_pattern = [col for col in df.columns if pattern in str(col)]\n    return cols_with_pattern\n\nresult = find_col_with_pattern(df, s)\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "\nfor i in range(len(df.columns)):\n    if s in df.columns[i]:\n        result = df.columns[i]\n        break\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "\ndef find_spike(df):\n    for col in df.columns:\n        if isinstance(df[col][0], str):\n            if s in col:\n                return df[col]\n\ndf[find_spike(df)] = df[find_spike(df)]\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "df['codes'] = df['codes'].apply(lambda x: pd.Series(x, index=range(len(x))))\ndf = df.join(df['codes'].apply(pd.Series).rename('code_'+str(df.index)))\ndf = df.drop(columns='codes')\ndf = df.fillna(pd.Series(np.nan, index=range(len(df.columns)), df.columns))\ndf\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "df['codes'] = df['codes'].apply(lambda x: pd.Series(x, index=range(1, len(x) + 1)))\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\ndf = df.groupby('codes').apply(lambda x: x.fillna(0))\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "df['codes'] = df['codes'].apply(lambda x: pd.Series(x, index=range(1, len(x) + 1)))\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\ndf = df.groupby('codes').apply(lambda x: x.dropna())\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\ndf['col1'] = df['col1'].apply(str)\ndf['col1'] = df['col1'].apply(eval)\n\nresult = df['col1'].apply(lambda x: [item for sublist in x for item in sublist])\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\ndf['col1'] = df['col1'].apply(lambda x: ','.join(map(str, x)))\ndf['col1'] = df['col1'].str.strip(',')\nresult = df['col1'].tolist()\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\ndf['col1'] = df['col1'].apply(str)\ndf['col1'] = df['col1'].apply(lambda x: ','.join(x))\n\nresult = df['col1'].str.cat(sep=',')\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "df['Time'] = df['Time'].dt.floor_divide(pd.Timedelta(minutes=2))\ndf['Value'] = df.groupby(df['Time'].dt.floor_divide(pd.Timedelta(minutes=2))).mean()['Value'].groupby(df['Time'].dt.floor_divide(pdipage=2)).mean()\ndf\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "df['Time'] = df['Time'].dt.floor_divide(pd.Timedelta(minutes=3))\ndf['bins'] = df['Time'].groupby(df['Time'].dt.floor_divide(pd.Timedelta(minutes=3))).sum()\ndf\n", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\ndf\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[filt]\n", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "result = df[filt]\n", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "df_row_0 = df.iloc[0]\ndf_row_8 = df.iloc[8]\n\ndiff_columns = set(df_row_0.index) ^ set(df_row_8.index)\n\nresult = df_row_0[diff_columns]\nresult\n", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "df[df.index.isin(df[['c2', 'c5']].index)]\n", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "df_row_0 = df.loc[0]\ndf_row_8 = df.loc[8]\n\ndiff_columns = set(df_row_0.index) ^ set(df_row_8.index)\n\nresult = list(diff_columns)\n", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "df_row_0 = df.iloc[0]\ndf_row_8 = df.iloc[8]\n\ndiff_cols = [(col, df_row_0[col], df_row_8[col]) for col in df_row_0.index if df_row_0[col] != df_row_8[col]]\n\nresult = diff_cols\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df_new = df.groupby(df.index // 1).first()\n", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df_new = df.iloc[0].repeat(df.shape[0]).reset_index(drop=True)\ndf_new = pd.concat([df[df.index % 2 == 0], df[df.index % 2 != 0]], axis=1)\ndf_new.columns = df.columns.repeat(df.columns.size // 2).apply(lambda x: x.str.replace('_0$','') if x.startswith('_0') else x)\ndf_new.index = df_new.index.repeat(df.shape[0] // df.shape[0]).reset_index(drop=True)\ndf_new", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: x.round(2) if pd.notna(x) else x)\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: x if pd.notna(x) else x.round(2))\ndf['cats'] = df['cats'].apply(lambda x: x if pdseemed(x) else x.round(2))\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "df[list_of_my_columns] = df[list_of_my_columns].groupby(level=0).transform('mean')\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "df.sort_values(by=['treatment','time'])\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "df.sort_values(by=['treatment','time','VIM'])\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "\ndays_to_delete = ['2020-02-17', '2020-02-18']\ndf = df[~df.Date.isin(days_to_delete)]\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "\n# Get the weekday of the date\ndf['Weekday'] = df['Date'].dt.weekday\n\n# Filter the dataframe based on the weekday\ndf_filtered = df[df['Weekday'].isin([1, 2])]\n\n# Reset the index of the filtered dataframe\ndf_filtered.reset_index(drop=True, inplace=True)\n\n# Print the filtered dataframe\nprint(df_filtered)\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "\n# Filtering the correlation matrix to get the correlation above 0.3\nabove_0_3_corr = corr.where(corr.abs() > 0.3)\n\n# Getting the top 10 correlations above 0.3\ntop_10_above_0_3 = above_0_3_corr.sort_values(ascending=False).groupby('Pearson Correlation Coefficient').head(10)\n\n# Getting the top 10 correlations above 0.3\ntop_10_above_0_3_df = top_10_above_0_3[['Pearson Correlation Coefficient']]\n\n# Getting the top 10 correlations above 0.3\ntop_10_above_0_3_df.columns = ['Col2']\ntop_10_above_0_3_df['Col1'] = top_10_above_0_3_df.index\n\n# Getting the top 10 correlations above 0.3\ntop_10_above_0_3_df\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "\nthreshold = 0.3\nmask = corr.abs() > threshold\nmasked_corr = corr.where(mask)\n\ndesired_series = masked_corr.stack()\ndesired_series.index = df.columns[masked_corr.isnull().any(axis=1)]\ndesired_series.name = 'threshold' + str(threshold)\n\nresult = desired_series\n", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "df.columns[-1] = 'Test'\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.columns.set_names('A', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'A')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.columns.set_names('A', 'A')\ndf.columns.set_names('B', 'B')\ndf.", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndf['frequent'] = df.apply(lambda x: x.value_counts().index[0], axis=1)\ndf['freq_count'] = df.apply(lambda x: x.value_counts()[x.value_counts().index[0]], axis=1)\ndf\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndf['frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().sum(axis=1), axis=1)\n\ndf\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndf['frequent'] = df.apply(lambda x: list(set(x.value_counts().index)), axis=1)\ndf['freq_count'] = df.apply(lambda x: len(set(x.value_counts().index)), axis=1)\ndf\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "df['bar'] = df.groupby(['id1','id2'])['foo'].transform('mean')\ndf['foo'] = df.groupby(['id1','id2'])['foo'].transform('mean')\ndf\n", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "df['bar'] = df.groupby(['id1','id2'])['foo'].transform('mean')\ndf['bar'] = df.groupby(['id1','id2'])['bar'].apply(lambda x: x.fillna(x.mean()))\ndf\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum')\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "df_c = pd.merge(df_a, df_b, on='EntityNum')\ndf_c.set_index('EntityNum', inplace=True)\ndf_c = df_c[['foo','a_col','b_col']]\ndf_c.reset_index()\n", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "dimensions = a.shape\n", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[~np.isnan(x)]\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x[np.isnan(x)] = np.inf\n", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "x = x.view(np.structures.PyArrayObject).view(np.array)\nx[x==np.nan] = 0\nx = x.tolist()\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.eye(max(a))\nb = np.array([(i in a) for i in range(len(b))]).astype(int)\nb = np.array([(i in a) for i in range(len(b))]).astype(int).reshape(-1,1) * b\nb = np.array([(i in a) for i in range(len(b))]).astype(int).reshape(1,-1) * b\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.eye(max(a))\nb = np.array([(i in a) for i in range(len(b))]).astype(int)\nb = np.array([(i in a) for i in range(len(b))]).astype(int)\nb = npumerate(b, 1)\nb = np.array([(i in a) for i in range(len(b))]).astype(int)\nb = np.eye(max(a))\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.eye(max(a), dtype=int)[a-min(a)]\n", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.eye(max(a)).astype(int)\nb[np.arange(len(a)), a] = 1\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "\ndef encode_one_hot(a):\n    smallest_element = a[0][0]\n    largest_element = a[0][-1]\n    a_encoded = (a - smallest_element + 1) * np.ones_like(a)\n    max_range = len(a) * (largest_element - smallest_element + 1)\n    a_encoded[a < smallest_element] = 0\n    a_encoded[a > largest_element] = 0\n    a_encoded = a_encoded / max_range\n    return a_encoded\n\nb = encode_one_hot(a)\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)\n", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.array_split(A, ncol)\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.array(A[np.arange(0, len(A), nrow)])\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.array_split(A, ncol)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.array_split(A, ncol)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "result = np.roll(a, shift)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=0)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "result = np.roll(a, shift, axis=0)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "max_val = a.max()\nmax_indices = np.where(a == max_val)\nresult = max_indices\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "min_val = a.min()\nmin_pos = np.where(a == min_val)\nresult = min_pos\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "max_val = np.amax(a)\nmax_indices = np.unravel_index(np.argmax(a), a.shape)\nresult = max_indices\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "result = np.unraveled_index(a.argmax(), a.shape)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": "\n    max_val = np.amax(a)\n    max_pos = np.where(a == max_val)\n    max_pos_raveled = max_pos[0][0] + max_pos[1][0]*a.shape[1]\n    return max_pos_raveled\n    ### END SOLUTION\n\nprint(f(example_a))\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "\n# get the maximum of the flattened array\nmax_val = a.flatten().max()\n\n# find the indices of the maximum value\nmax_indices = np.where(a.flatten() == max_val)\n\n# get the maximum of the flattened array\nmax_val_2nd = a.flatten()[max_indices[1]].max()\n\n# get the indices of the maximum value in the 2nd dimension\nmax_2nd_indices = npordinate[max_indices[0]].max()\n\n# get the indices of the maximum value in the 3rd dimension\nmax_3rd_indices = max_indices[2]\n\n# get the position of the 2nd largest value in the 3rd dimension\nresult = max_3rd_indices\n\n# END SOLUTION\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "b = np.delete(a, np.where(np.isnan(a).any(axis=1)))\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]\n", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = np.transpose(a, permutation)\n", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.array([a[p] if i%2==0 else a[p][::-1] for i in range(len(a))])\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_val = np.min(a)\nmin_pos = np.argwhere(a == min_val)\nresult = tuple(min_pos[0])\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_index = np.unravel_index(a.argmax(), a.shape)\nresult = max_index\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "min_val_indices = np.argmin(a, axis=0)\nmin_val_matrix = a[min_val_indices]\n\n# to get the indices of the minimum value in the first axis\nmin_val_indices_first_axis = np.argmin(a, axis=0)\nmin_val_first_axis = a[min_val_indices_first_axis]\n\n# to get the indices of the minimum value in the second axis\nmin_val_indices_second_axis = npordinate\nmin_val_second_axis = a[min_val_indices_second_axis]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get the minimum value in the second axis\nmin_val_second_axis_value = a[min_val_indices_second_axis][1]\n\n# to get the minimum value in the first axis\nmin_val_first_axis_value = a[min_val_indices_first_axis][0]\n\n# to get", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "\n# Convert the degree to radian\nradian = np.degrees(degree)\n\n# Calculate the sine of the angle\nsine_angle = np.sin(radian)\n\n# Store the result in the variable 'result'\nresult = sine_angle\n", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "\n# Convert the degree to radian\nradian = np.degrees(np.radians(degree))\n\n# Use the formula of cos(\u03b8/2) to get the cosine of an angle\ncos_angle = np.cos(radian/2)\n\n# Store the result in the variable 'result'\nresult = cos_angle\n", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "sine_value = np.sin(np.radians(number))\nif sine_value > 0.9999:\n    result = 1\nelse:\n    result = 0\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "\n# Calculate the cosine of the input value\ncos_val = np.cos(np.radians(value))\n\n# Calculate the sine of the input value\nsin_val = np.sin(npordinate(value))\n\n# Calculate the tangent of the input value\ntan_val = np.tan(np.radians(value))\n\n# Calculate the cotangent of the input value\ncot_val = 1 / tan_val\n\n# Calculate the secant of the input value\nsec_val = 1 / cos_val\n\n# Calculate the cosecant of the input value\ncosec_val = 1 / sec_val\n\n# Calculate the value of the input value\nval_val = np.arccos(sec_val)\n\n# Convert the value of the input value from radians to degrees\nval_val_degrees = val_val * 180 / np.pi\n\n# Print the value of the input value\nprint(val_val_degrees)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "def pad(A, length):\n    if len(A) >= length:\n        return A\n    else:\n        return A + [0]*(length - len(A))\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    if len(A) >= length:\n        return A\n    else:\n        return A + [0]*(length - len(A))\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.linalg.matrix_power(a, power)\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.power(a, power)\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(example_a, 2))\nprint(f(example_a, 3))\nprint(f(exampleordinate_a, 2))\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = (numerator//denominator, denominator//numerator)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.divide(numerator, denominator)\n    return result\n    ### END SOLUTION ###\n\n# Test the function\nprint(f(98, 42))\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "\n# first, we need to check if the denominator is zero\nif denominator == 0:\n    result = (np.nan, np.nan)\nelse:\n    result = (numerator / denominator, denominator / denominator)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3\n", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.where(a > b, a, b)\nresult = np.where(c > result, c, result)\n", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "def custom_diagonal(a):\n    n = len(a)\n    return a[np.arange(n), np.arange(n)]\n\nresult = custom_diagonal(a)\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "def custom_diagonal(a):\n    n = a.shape[0]\n    return a[np.arange(n), np.arange(n)]\n\nresult = custom_diagonal(a)\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "def get_diagonal(a):\n    n = len(a)\n    return a[np.arange(n), np.arange(n)]\n\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\n\nresult = get_diagonal(a)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "def get_diagonal(a):\n    n = a.shape[0]\n    return a[np.arange(n), np.arange(n)]\n\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n\nresult = get_diagonal(a)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "for i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        result.append(X[i][j])\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": "\n    result = [j for i in X for j in i]\n    ### END SOLUTION\n    return result\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "\ndef to_fortran_order(arr):\n    return arr.view(np.ctypeslib.ndarray)\n\ndef to_python_order(arr):\n    return arr.view(np.ctypeslib.ndarray)\n\ndef to_string(arr):\n    return ' '.join(map(str, arr))\n\ndef to_array(s):\n    return np.fromstring(s, sep=' ')\n\ndef solve(X):\n    X_f = to_fortran_order(X)\n    X_p = to_python_order(X)\n    X_s = toordinate(X_p)\n    X_a = to_array(X_s)\n    X_r = X_a.astype(int)\n    X_t = X_r.T\n    X_f.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X_t.flat[::-1] = X_r\n    X", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.fromstring(mystr, dtype=int, sep='')\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "\n# Multiply the specific column by the given number\na[:, col] = a[:, col] * multiply_number\n\n# Calculate the cumulative sum of the specific column\ncumulative_sum = np.cumsum(a[:, col])\n\n# Store the result in the variable 'result'\nresult = cumulativeordinate\n", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "\n# Select the specific row\nrow_data = a[row]\n\n# Multiply each element by the given number\nrow_data = row_data * multiply_number\n\n# Calculate the cumulative sum of the elements\ncumulative_sum = np.cumsum(row_data)\n\n# Store the result in the solution variable\nresult = cumulativeordinate\n", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "\n# get the specific row\nrow_data = a[row]\n\n# divide the specific row by a number\nrow_data = row_data / divide_number\n\n# calculate the multiplication of the numbers in that row\nresult = np.prod(row_data)\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "def find_max_independent_set(a):\n    n = len(a)\n    dp = [0]*(1<<n)\n    for mask in range(1<<n):\n        for i in range(n):\n            if ((mask>>i)&1)==1:\n                for j in range(i):\n                    if ((mask>>j)&1)==1:\n                        dp[mask|(1<<i)] = max(dp[mask|(1<<i)], dp[mask]+a[i][j])\n    return dp\n\ndp = find_max_independent_set(a)\nmax_independent_set = [i for i in range(len(dp)) if dp[i]==max(dp)]\nmax_independent_set\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "\nfor i in range(a.shape[0]):\n    result += len(a[i])\n\nprint(result)\n", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "p_value = scipy.stats.ttest_ind(a, b, equal_var=False, usecols='all')[1]\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "p_value = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the standard deviation of each sample\nsample1_std_dev = np.sqrt(bvar)\nsample2_std_dev = np.sqrt(avenar)\n\n# calculate the t-test statistic\nt_stat = (sample1_mean - sample2_mean) / np.sqrt((sample1_std_dev / n1) + (sample2_std_dev / n2))\n\n# calculate the p-value\np_value = scipy.stats.ttest_ind(sample1_mean, sample2_mean, equal_var=True, nan_policy='omit')\n\n# print the p-value\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "\n# find elements in A that are in B\nin_A_but_not_B = np.unique(np.intersect1d(np.unique(A, return_indices=True), np.unique(B, return_indices=True)))\n\n# find elements in B that are in A\nin_B_not_A = np.unique(np.intersect1d(np.unique(B, return_indices=True), np.unique(A, return_indices=True)))\n\n# find elements in A that are in B and not in A\nin_A_and_not_B = np.intersect1d(np.intersect1d(np.unique(A, return_indices=True), np.unique(B, return_indices=True)), np.unique(A, return_indices=True))\n\n# find elements in B that are in A and not in B\nin_B_and_not_A = np.intersect1d(np.intersect1d(np.unique(B, return_indices=True), np.unique(A, return_indices=True)), np.unique(B, return_indices=True))\n\n# find elements in A that are in B and in B\nin_A_in_B = np.intersect1d(np.intersect1d(np.unique(A, return_indices=True), np.unique(B, return_indices=True)), np.unique(B, return_indices=True))\n\n# find elements in B that are in A and in A\nin_B_in_A = np.intersect1d(np.intersect1d(np.unique(B, return_indices=True), np.unique(A, return_indices=True)), np.unique(A, return_indices=True))\n\n# find elements in A that are in B and in A and in B\nin_A_in_B_and_in_A = np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(np.intersect1d(", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "\ndef set_diff(A, B):\n    return [i for i in A if i not in B] + [j for j in B if j not in A]\n\noutput = set_diff(A, B)\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.argsort(a.sum(axis=2)) + np.arange(3*3*3) * 3\nresult = result.reshape((3, 3, 3))\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=0)\n", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, 0)\n", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = a[:, :2]\n", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "result = np.delete(a, del_col, axis=1)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a[pos] = element\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a[pos] = np.array(element)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": " ###\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION ###\n    print(a)\nf(example_a, 2, 66)\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "\n# Create a copy of each array\ncopied_arrays = [a.copy() for a in array_of_arrays]\n\n# Interact with each element in the copied arrays\nfor i in range(len(copied_arrays)):\n    copied_arrays[i][0] = 1\n\n# Check the original and the modified arrays\nprint(array_of_arrays[0])\nprint(copied_arrays[0])\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "np.array_equal(a[0], a[1])\n", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "np.array_equal(a[0], a[1])\n", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": "\n    return np.all(a == a[0], axis = 0)\n    ### END SOLUTION\n\n# call the function\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "\n# Create a grid of x, y\nX, Y = np.meshgrid(x, y)\n\n# Calculate the function\nZ = (np.cos(X)**4) + (np.sin(Y)**2)\n\n# Integrate the function\nresult = np.trapz(Z, (X, Y))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": "\n    return (np.cos(x))**4 + (np.sin(y))**2\n    ### END SOLUTION\nexample_z = f(example_x, example_y)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "\ndef ecdf(data):\n    x = np.sort(data)\n    y = np.arange(1, len(x)+1)\n    return x, y\n\nresult = ecdf(grades)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n  x_sorted = np.sort(x)\n  y_sorted = np.sort(x_sorted * len(x))\n  return y_sorted\n\nx_prop = np.mean(eval)\n\ndef ecdf_prop(x):\n  x_sorted = np.sort(x)\n  y_sorted = npordinate(x_sorted * len(x))\n  return y_sorted\n\nx_prop = np.mean(eval)\n\ndef ecdf_prop(x):\n  x_sorted = np.sort(x)\n  y_sorted = np.cumsum(x_sorted) / x_sorted\n  return y_sorted\n\nresult = ecdf_prop(eval)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def longest_interval(low, high):\n    intervals = []\n    for i in range(low, high):\n        intervals.append(i)\n    return max(intervals, key=lambda x: x[::-1].count(x))\n\nlow, high = longest_interval(low, high)\n\ndef ecdf(x):\n    x = np.array(x)\n    x_mean = np.mean(x)\n    x_std = npordinate(x)\n    x_std_dev = x_std / np.sqrt(len(x))\n    x_std_dev_adj = x_std * np.sqrt(len(x))\n    x_std_adj = x_std_dev_adj / np.sqrt(len(x))\n    x_std_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj = x_std_adj * np.sqrt(len(x))\n    x_std_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_adj_", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.randint(2, size=(int(one_ratio*size), size))\n", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "\n# Get the indices of the elements in the original array in decreasing order\nindices = np.argsort(a)[::-1]\n\n# Get the elements in the original array in the decreasing order\na_sorted = a[indices]\n\n# Get the indices of the elements in the original array in the decreasing order\nresult = indices\n\n# Get the elements in the original array in the decreasing order\nresult = a_sorted\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n# Get the elements in the original array in the decreasing order\nresult = a[result]\n\n#", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a)\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "result = np.argsort(a)[::-1][:N]\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.linalg.matrix_power(A, n)\n", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "\ndef extract_patches(a, patch_size):\n    # get the shape of the input array\n    shape = a.shape\n\n    # get the number of rows in the input array\n    rows = shape[0] // patch_size\n\n    # get the number of columns in the input array\n    cols = shape[1] // patch_size\n\n    # create a list to store the patches\n    patches = []\n\n    # iterate over the rows\n    for i in range(rows):\n        # iterate over the columns\n        for j in range(cols):\n            # extract a patch of the given size\n            patch = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patchordinate]\n            patches.append(patch)\n\n    return patches\n\npatches = extract_patches(a, 2)\n\n# print the patches\nfor patch in patches:\n    print(patch)\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "\ndef sliding_window(a, window_size):\n    # Calculate the number of rows and columns\n    rows = a.shape[0]\n    cols = a.shape[1]\n\n    # Calculate the number of rows and columns of the window\n    window_rows = window_size[0]\n    window_cols = window_size[1]\n\n    # Calculate the number of windows\n    num_windows = rows // window_rows\n    num_windows = cols // window_cols\n\n    # Create a list to store the windows\n    windows = []\n\n    # Loop over the windows\n    for i in range(num_windows):\n        for j in range(cols):\n            # Extract the window\n            window = a[i:i+window_rows, j:j+window_cols]\n            windows.append(window)\n\n    return windows\n\n# Apply the function to the array\nwindows = sliding_window(a, window_size=(2, 2))\n\n# Print the windows\nfor window in windows:\n    print(window)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "\ndef extract_patches(a, patch_size):\n    # get the shape of the input array\n    shape = a.shape\n\n    # get the number of rows in the input array\n    rows = shape[0] // patch_size\n\n    # get the number of columns in the input array\n    cols = shape[1] // patch_size\n\n    # create a list to store the patches\n    patches = []\n\n    # iterate over the rows\n    for i in range(rows):\n        # iterate over the columns\n        for j in range(cols):\n            # extract a patch of the given size\n            patch = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patchordinate]\n            patches.append(patch)\n\n    return patches\n\npatches = extract_patches(a, 2)\n\n# print the patches\nfor patch in patches:\n    print(patch)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "\ndef extract_patches(a, patch_size):\n    # get the shape of the array\n    shape = a.shape\n    # get the number of rows in the array\n    rows = 2*shape[0]//patch_size\n    # get the number of columns in the array\n    cols = 2*shape[1]//patch_size\n    # get the number of patches in the rows\n    patches_rows = shape[0]//patch_size\n    # get the number of patches in the columns\n    patches_cols = shape[1]//patchordinate\n    # create a list to store the patches\n    patches = []\n    # iterate over the rows\n    for i in range(patches_rows):\n        # iterate over the columns\n        for j in range(patches_cols):\n            # get the patch\n            patch = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n            # append the patch to the list\n            patches.append(patch)\n    return patches\n\n# call the function\npatches = extract_patches(a, patch_size)\n\n# print the patches\nfor patch in patches:\n    print(patch)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "\ndef solution(a, h, w):\n    # Your solution here\n    pass\n\n# Test the function\na = np.array([[[ 0,  1,  2],\n        [ 6,  7,  8]],    \n       [[ 3,  4,  5],\n        [ 9, 10, 11]], \n       [[12, 13, 14],\n        [18, 19, 20]],    \n       [[15, 16, 17],\n        [21, 22, 23]]])\nh = 4\nw = 6\n\n# Call the function\nresult = solution(a, h, w)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "\ndef extract_patches(a, patch_size):\n    # get the shape of the array\n    shape = a.shape\n    # get the number of rows in the array\n    rows = 2*shape[0]//patch_size\n    # get the number of columns in the array\n    cols = 2*shape[1]//patch_size\n    # get the number of patches in the rows\n    patches_rows = shape[0]//patch_size\n    # get the number of patches in the columns\n    patches_cols = shape[1]//patchordinate\n    # create a list to store the patches\n    patches = []\n    # iterate over the rows\n    for i in range(patches_rows):\n        # iterate over the columns\n        for j in range(patches_cols):\n            # get the patch\n            patch = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n            # append the patch to the list\n            patches.append(patch)\n    return patches\n\n# call the function\npatches = extract_patches(a, patch_size)\n\n# print the patches\nfor patch in patches:\n    print(patch)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high+1]\n", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high+1]\n", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "\n# Check if the range is valid\nif low < 0 or high > 6:\n    print(\"Invalid range\")\n    return None\n\n# Extract the columns\nresult = a[:, low:high+1]\n", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.array(eval(string))\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "\n# Generate samples from a log-uniform distribution\nresult = np.random.uniform(min, max, n)\n\n# Apply the log function to each sample\nresult = np.log(result)\n\n# Scale the samples to the desired range\nresult = (result - np.mean(result)) / np.std(result)\nresult = result * (max - min) + min\n\n# Apply the exp function to each sample\nresult = np.exp(result)\n\n# Scale the samples to the desired range\nresult = (result - npseemed to be the first sample in the range of 0 to 1\n", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "\n# Generate samples from the log-uniform distribution\nresult = np.random.uniform(np.log(min), np.log(max), n)\n\n# Convert the samples to the original scale\nresult = np.exp(result)\n\n# Now you have a sample of size n from the log-uniform distribution\nprint(result)\n", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": "\n    return np.random.loguniform(min, max, n)\n    ### END SOLUTION\n\n# Test the function\nprint(f(min=1, max=10, n=10))\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = a * A.iloc[0]\nB += b * A.iloc[1]\nB = a * A.iloc[0] + b * A.iloc[1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = [a*A[0]]\nfor i in range(1, len(A)):\n    B.append(a*A[i] + b*B[i-1])\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "\n# Create an empty numpy array with shape = (0,)\nresult = np.array([], dtype=int)\n\n# Check the result\nprint(result)\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "\n# Create an empty 3x0 matrix\nresult = np.empty([3,0])\nprint(result)\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.ravel_copy(index) + np.ravel_copy(np.argwhere(np.triu(a == np.roll(a, 1))))\n", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "\ndef sub2ind(ind, N) :\n    return (ind[0]*N + ind[1])*N + ind[2]\n\nresult = sub2ind(index, np.prod(dims))\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "df = pd.DataFrame(np.zeros((2,3)), index=index, columns=columns)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "def accum_np(accmap, a):\n    return np.cumsum(a[accmap])\n\nresult = accum_np(accmap, a)\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = a[index]\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.bincount(accmap, weights=a)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = a[index]\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "def elementwise_function(element_1,element_2):\n    return (element_1 + element_2)\n\nz = np.apply_along_axis(elementwise_function, axis=0, arr1=x, arr2=y)\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "np.random.choice(lista_elegir, samples, probabilit)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "\n# Create a mask of zeros with the same shape as `a`\nmask = np.zeros_like(a)\n\n# Create a boolean mask of `a` with True at the desired indices\nmask[low_index:high_index] = True\n\n# Apply the mask to `a`\nresult = a * mask\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "result = x[x >= 0]\n", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.abs(x) != np.around(x.real)].tolist()\n", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "def bin_data(data, bin_size):\n    return [data[i:i+bin_size] for i in range(0, len(data), bin_size)]\n\ndef calculate_mean(data):\n    return sum(data) / len(data)\n\nbins = bin_data(data, bin_size)\nbin_data_mean = [calculate_mean(bin) for bin in bins]\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "def bin_data(data, bin_size):\n    return [data[i:i+bin_size] for i in range(0, len(data), bin_size)]\n\ndef max_in_bins(data):\n    return [max(bin) for bin in data]\n\nbins = bin_data(data, bin_size)\nmax_bins = max_in_bins(bins)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "\ndef bin_data(data, bin_size):\n    return (data.reshape(-1,bin_size) == np.repeat(np.arange(data.shape[1]//bin_size),bin_size).reshape(1,-1))\n\ndef mean_bins(data):\n    return np.mean(data,axis=1)\n\nbin_data_mean = mean_bins(np.concatenate([data[i] for i in range(data.shape[0]) if bin_data(data, 3)[i]],axis=0))\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "def bin_data(data, bin_size):\n    return [data[i:i+bin_size] for i in range(0, len(data), bin_size)]\n\ndef calculate_mean(data):\n    return sum(data) / len(data)\n\nbins = bin_data(data, bin_size)\nbin_data_mean = [calculate_mean(bin) for bin in bins]\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "def bin_data(data, bin_size):\n    # get the last part of the array\n    last_part = data[-1:-bin_size:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef calculate_mean(data):\n    # return the mean of each element in the array\n    return np.mean(data)\n\ndef solve(data, bin_size):\n    # get the last part of the array\n    last_part = data[-1:-bin_size:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return npuncher.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[:(len(data)-len(last_part))*-1:-1]\n    # return the combined array\n    return np.concatenate((last_part, rest))\n\ndef solution(data):\n    # get the last part of the array\n    last_part = data[-1:-2:-1]\n    # get the rest of the array\n    rest = data[", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "def bin_data(data, bin_size):\n    # get the length of the data\n    n = len(data)\n    # get the number of bins\n    n_bins = n // bin_size\n    # get the rest of the elements\n    rest = n % bin_size\n    # get the bins\n    bins = [data[i:i+n//bin_size][::-1] for i in range(0, n//bin_size)]\n    # get the rest of the elements\n    rest = data[n//bin_size*bin_size:(n//bin_size+1)*binuncher]\n    # return the bins\n    return [bins[i]+rest for i in range(n//bin_size)]\n\ndef get_mean(data):\n    # get the length of the data\n    n = len(data)\n    # get the mean of the elements\n    mean = sum(data) / n\n    # return the mean\n    return mean\n\ndef solve(data, bin_size):\n    # get the bins\n    bins = bin_data(data, bin_size)\n    # get the means of the bins\n    means = [get_mean(b) for b in bins]\n    # return the means\n    return means\n\n# solve the problem\nmeans = solve(data, bin_size)\nprint(means)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x):\n    x_scaled = (x - x_min) / (x_max - x_min)\n    x_smooth = 3 * (2 * (x_scaled ** 2) - 1)\n    return x_min + x_max * np.tanh(3 * x_scaled) * (np.sqrt(1 - x_smooth ** 2) / x_smooth)\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothstep(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0, 1)\n    t = (t * N) / (N - 1)\n    t = t * 2 - 1\n    return (t * t) * (t - 1) / (t * 2)\n\ndef smoothclamp(x, x_min, x_max):\n    return x_max * smoothstep(x, x_min, x_max)\n\ndef clamp(x):\n    return smoothclamp(x, x_min, x_max)\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\ndef circular_correlation(a, b):\n    # get the length of the arrays\n    len_a = len(a)\n    len_b = len(b)\n\n    # get the maximum length of the two arrays\n    max_len = max(len_a, len_b)\n\n    # get the circular version of the arrays\n    a_circular = a[:max_len]\n    b_circular = b[:max_len]\n\n    # get the circular cross-correlation\n    cross_corr = np.convolve(a_circular, b_circular, 'same')\n\n    # get the indices of the maximum value in the cross-correlation\n    max_index = cross_corr.argmax()\n\n    # get the circular version of the maximum value\n    max_value = cross_corr[max_index]\n\n    # get the circular version of the arrays\n    a_circular_max = a_circular[:max_index+1]\n    b_circular_max = b_circular[:maxordinate(max_index)]\n\n    # get the circular cross-correlation of the maximum value\n    cross_corr_max = np.convolve(a_circular_max, b_circular_max, 'same')\n\n    # get the circular version of the maximum value\n    max_value_circular = cross_corr_max[max_index]\n\n    return max_value_circular\n\n# test the function\nprint(circular_correlation(a, b))\n", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "df.groupby(level=0).apply(lambda x: x.values.reshape(1, 1, 5))\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "df_agg = df.groupby('major').apply(lambda x: x.groupby('timestamp').mean())\ndf_agg.index = pd.MultiIndex.from_product([names, times], names=['major','timestamp'])\ndf_agg.columns = list('colu')\ndf_agg.index.name = 'major'\ndf_agg.columns.name = 'timestamp'\nresult = df_agg.values\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "\ndef int_to_binary(a, m):\n    binary_array = np.array([(np.binary_repr(i, width=m)) for i in a])\n    binary_array = binary_array.astype(int)\n    return binary_array\n\nresult = int_to_binary(a, m)\nprint(result)\n", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "\ndef int_to_binary(x):\n    return np.array([(x >> i) & 1 for i in range(m)][::-1], dtype=int)\n\nresult = np.array([int_to_binary(a[i]) for i in range(len(a))])\n", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "\ndef binary_to_int(binary):\n    return int(''.join(str(i) for i in binary), 2)\n\ndef int_to_binary(n):\n    return [int(i) for i in np.binary_repr(n, width=m)]\n\ndef solution(a):\n    global m\n    a = a.astype(int)\n    binary_a = np.array([int_to_binary(i) for i in a])\n    binary_xor = [binary_a[i][:-m] ^ binary_a[i][-m:-1] for i in range(len(binary_a))]\n    xor_a = [binary_to_int(binary) for binary in binary_xor]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xorumerate(xor_a)]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i ^ a[i] for i in xor_a]\n    xor_a = [i", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "mean_a = np.mean(a)\nstd_dev_a = np.std(a)\nstart_of_interval = mean_a - 3 * std_dev_a\nend_of_interval = mean_a + 3 * std_dev_a\nresult = (start_of_interval, end_of_interval)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "mean_a = np.mean(a)\nstd_a = np.std(a)\nstd_interval = (mean_a - 2*std_a, mean_a + 2*std_a)\nresult = std_interval\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": "\n    mean_a = np.mean(a)\n    std_dev_a = np.std(a)\n    start_a = mean_a - 3 * std_dev_a\n    end_a = mean_a + 3 * std_dev_a\n    return (start_a, end_a)\n    ### END SOLUTION\n\nf(example_a)\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "\n# calculate the mean of the array\nmean = np.mean(a)\n\n# calculate the standard deviation of the array\nstd_dev = np.std(a)\n\n# calculate the 2nd standard deviation\nsecond_std_dev = std_dev * 2\n\n# calculate the lower limit of the 2nd standard deviation interval\nlower_limit = mean - second_std_dev\n\n# calculate the upper limit of the 2nd standard deviation interval\nupper_limit = mean + second_std_dev\n\n# create a boolean mask for the outliers\nmask = (a >= lower_limit) & (a <= upper_limit)\n\n# apply the mask to the original array\nresult = a[~mask]\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less_than(DataArray, 0)\nprob = np.percentile(masked_data.data, percentile)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows] = 0\na[:,zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, zero_cols] = 0\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, :] = 0\na[:, 0] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "max_value = np.amax(a, axis=1)\nmask = np.where(a == max_value, True, False)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "min_val = a.min(axis=1)[0]\nmask = (a < min_val).astype(int)\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "\ndef calculate_pearson_correlation(post, distance):\n    # calculate the mean of the two lists\n    mean_post = np.mean(post)\n    mean_distance = np.mean(distance)\n\n    # calculate the sum of squares\n    sum_post_squared = np.sum([(i - mean_post) ** 2 for i in post])\n    sum_distance_squared = npordinate\n    sum_product = np.sum([(i - mean_distance) * (j - mean_distance) for i, j in zip(post, distance)])\n\n    # calculate the Pearson correlation\n    r_value = sum_product / np.sqrt(sum_post_squared * sum_distance_squared)\n\n    return r_value\n\n# calculate the Pearson correlation\nresult = calculate_pearson_correlation(post, distance)\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "M = X.shape[1]\nresult = np.eye(M, dtype=np.int).dot(X).dot(np.eye(M, dtype=npordinate))\n", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.dot(Y, np.linalg.inv(np.eye(Y.shape[0])))\n", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a\n", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "C = [i for i in A if i not in B]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[~np.in1d(A, B)]\n", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "mask = np.in1d(A, B)\nC = A[mask]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "ranked_a = rankdata(a)\nranked_a = len(a) - ranked_a\nresult = ranked_a\n", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "ranked_a = rankdata(a)\nranked_a = ranked_a[::-1]\nresult = ranked_a.astype(int)\nprint(result)\n", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": "\n    return np.flip(rankdata(a), 0)\n    ### END SOLUTION\n\nprint(f(example_a))\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "\ndef perm_2D(x):\n    x_shape = x.shape\n    x = x.reshape(x_shape[0], x_shape[1]*x_shape[2])\n    x_perm = np.roll(np.eye(x_shape[0]), x.argmax(axis=0), 0)\n    x_perm = x_perm.reshape(x_shape[0], x_shape[1]*x_shape[2]).T @ x_perm.T\n    return x_perm.reshape(x_shape[0], x_shape[1], x_shape[2])\n\nx_dists_perm = perm_2D(x_dists)\ny_dists_perm = perm_2D(y_dists)\n\ndists = np.dstack(x_dists_perm, y_dists_perm)\n", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "\ndef perm_2D(x):\n    x_shape = x.shape\n    x = x.ravel().tolist()\n    for i in range(len(x)):\n        x[i] = x[i] + x_shape[0]\n    return x\n\ndef perm_1D(x):\n    x_shape = x.shape\n    x = x.ravel().tolist()\n    for i in range(len(x)):\n        x[i] = x[i] + x_shape[0]\n    return x\n\ndef perm_2D_list(x):\n    x_shape = x.shape\n    x = x.ravel().tolist()\n    for i in range(len(x)):\n        x[i] = x[i] + x_shape[0]\n    return x\n\ndef perm_1D_list(x):\n    x_shape = x.shape\n    x = x.ravel().tolist()\n    for i in range(len(x)):\n        x[i] = x[i] + x_shape[0]\n    return x\n\ndef perm_2D_array(x):\n    x_shape = x.shape\n    x = x.ravel().tolist()\n    for i in range(len(x)):\n        x[i] = x[i] + x_shape[0]\n    return x\n\ndef perm_1D_array(x):seemed to be the solution\n", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = a[:][second][third]\n", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))\n", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = X.sum(axis=1)\nprint(l1)\nX_norm = X / l1.reshape(5,1)\nprint(X_norm)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "def L_norm(X):\n    X_norm = np.sqrt(np.sum(X**2))\n    return X / X_norm\n\nX_norm = L_norm(X)\n\nprint(X_norm)\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].str.contains(target)\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "def euclidean_distance(a, b):\n    distance = 0\n    for i in range(len(a)):\n        distance += (a[i][0] - b[i][0]) ** 2\n        distance += (a[i][1] - b[i][1]) ** 2\n        distance += (a[i][2] - b[i][2]) ** 2\n    return distance\n\ndef calculate_distance(a):\n    distance_matrix = np.zeros((len(a), len(a)))\n    for i in range(len(a)):\n        for j in range(i+1, len(a)):\n            distance_matrix[i][j] = euclidean_distance(a[i], a[j])\n            distance_matrix[j][i] = euclidean_distance(a[j], a[i])\n    return distance_matrix\n\nresult = calculate_distance(a)\n", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "def euclidean_distance(a, b):\n    distance = 0\n    for i in range(len(a)):\n        distance += (a[i] - b[i]) ** 2\n    return distance\n\ndef calculate_distance(a):\n    distance_matrix = np.zeros((len(a), len(a)))\n    for i in range(len(a)):\n        for j in range(i+1, len(a)):\n            distance_matrix[i, j] = euclidean_distance(a[i], a[j])\n            distance_matrix[j, i] = euclidean_distance(a[j], a[i])\n    return distance_matrix / len(a)\n\nresult = calculate_distance(a)\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "def euclidean_distance(a, b):\n    distance = 0\n    for i in range(len(a)):\n        distance += (a[i] - b[i]) ** 2\n    return distance\n\ndef calculate_distance(a):\n    distance_matrix = np.zeros((len(a), len(a)))\n    for i in range(len(a)):\n        for j in range(i+1, len(a)):\n            distance_matrix[i, j] = euclidean_distance(a[i], a[j])\n            distance_matrix[j, i] = distance_matrix[i, j]\n    return distance_matrix\n\nresult = calculate_distance(a)\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# First, find the indices of the non-zero elements\ninds = np.where(a != 0)[0]\n\n# Then, find the unique elements in the array\nuniques = np.unique(a[inds])\n\n# Finally, create a new array with the unique elements\nresult = a[inds][np.isin(a[inds], uniques)]\n\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "mask = (a[:-1] != a[1:]) & (a[1:] != 0)\nresult = a[mask]\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame(np.column_stack((lat[0], lon[0], val[0])), columns=['lat', 'lon', 'val'])\n\nfor i in range(1, len(lat)):\n    df = pd.concat([df, pd.DataFrame(np.column_stack((lat[i], lon[i], val[i])), columns=['lat', 'lon', 'val'])])\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "\n    df = pd.DataFrame(columns=['lat', 'lon', 'val'])\n    for i in range(len(lat)):\n        df = df.append(pd.Series([lat[i], lon[i], val[i]]), ignore_index=False)\n    return df\n    ### END SOLUTION\ndf = f(example_lat, example_lon, example_val)\nprint(df)\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame(np.column_stack((lat[0], lon[0], val[0])), columns=['lat', 'lon', 'val'])\n\nfor i in range(1, len(lat)):\n    df = pd.concat([df, pd.DataFrame(np.column_stack((lat[i], lon[i], val[i])), columns=['lat', 'lon', 'val'])])\n\nmax_val = val.max(axis=1)\ndf['maximum'] = max_val\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "\ndef moving_window(a, size):\n    # Calculate the strides of the window\n    stride_row = a.strides[1]\n    stride_col = a.strides[2]\n\n    # Calculate the shape of the window\n    window_shape = (a.shape[0] - 1) // 2, (a.shape[1] - 1) // 2\n\n    # Slide the window across the array\n    result = np.empty(window_shape)\n    for i in range(window_shape[0]):\n        for j in range(window_shape[1]):\n            # Slide the window\n            row_slice = slice(a[i, :])\n            col_slice = slice(a[:, j])\n            # Slide the window across the array\n            result[i, j] = a[i:i+size[0], j:j+size[1]]\n            result[i, j] = a[i:i+size[0], j:j+size[1]]\n    return result\n\nresult = moving_window(a, size)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "\ndef moving_window(a, size):\n    # Calculate the strides of the window\n    stride_row = a.strides[1]\n    stride_col = a.strides[2]\n\n    # Calculate the shape of the window\n    window_shape = (a.shape[0] - 1) // 2, (a.shape[1] - 1) // 2\n\n    # Slide the window across the array\n    result = np.empty(window_shape)\n    for i in range(window_shape[0]):\n        for j in range(window_shape[1]):\n            # Slide the window\n            row_slice = slice(a[i, :])\n            col_slice = slice(a[:, j])\n            # Slide the window across the array\n            result[i, j] = a[i:i+size[0], j:j+size[1]]\n            result[i, j] = a[i:i+size[0], j:j+size[1]]\n    return result\n\nresult = moving_window(a, size)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "mean_re = np.mean(a.real)\nmean_im = npipage.mean(a.imag)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": "\n    return np.mean(a)\n    ### END SOLUTION\n\n# test the function\nprint(f(a))\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "\n# Slicing with unknown dimension\n# Suppose we have a 2D array X of unknown dimension\n\n# First, we need to find the maximum dimension of X\nmax_dim = max(np.ndim(X) for X in [X, Y])\n\n# Then, we can slice X using this maximum dimension\nresult = X[..., :-1]\n", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "\n# Slicing with unknown dimension\na[-1:, :]\n", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "def is_member(c, cnts):\n    for cnt in cnts:\n        if np.array_equal(c, cnt):\n            return True\n    return False\n\nresult = any(is_member(c, CNTS) for c in [c1, c2, c3])\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "def is_member(c, CNTS):\n    for contour in CNTS:\n        if np.array_equal(c, contour):\n            return True\n    return False\n\nresult = is_member(c, CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = np.interp(x_new, np.linspace(0, 2, 4), np.linspace(0, 2, 4))\nf = np.interp(y_new, np.linspace(0, 2, 4), f)\nf = np.interp(f, np.linspace(0, 2, 4), np.linspace(0, 2, 4))\nresult = f\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df['Q_cum'] = np.cumsum(df.Q[df.D==df.D.iloc[0]])\ndf\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "import numpy as np\na = np.matrix([[3, 4, 3, 1],[1,3,2,6],[2,4,1,5],[3,3,5,2]])\nU, i, V = np.linalg.svd(a,full_matrices=True)\n\n# i is a 1x4 matrix, we need to create a 4x4 identity matrix\ni_diag = np.zeros((4,4))\ni_diag[np.tril(np.ones(4)) == 1] = 1\ni_diag = i_diag.T\n\ni = i_diag\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "mask = a.T != a\na[mask] = 0\n", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "\ndt_start = pd.to_datetime(start)\ndt_end = pdordinate(end)\n\ndt_range = pd.date_range(start=dt_start, end=dt_end, freq='S')\n\n# get the timestamps\ntimestamps = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n//2)]\ntimestamps = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n)]\ntimestamps_same_freq = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps_same_freq]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq_2 = [dt_start + i*pdseemedi(seconds=50) for i in range(n)]\ntimestamps_same_freq_2 = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps_same_freq_2]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq_3 = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n)]\ntimestamps_same_freq_3 = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps_same_freq_3]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq_4 = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n)]\ntimestamps_same_freq_4 = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps_same_freq_4]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq_5 = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n)]\ntimestamps_same_freq_5 = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps_same_freq_5]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq_6 = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n)]\ntimestamps_same_freq_6 = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps_same_freq_6]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq_7 = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n)]\ntimestamps_same_freq_7 = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps_same_freq_7]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq_8 = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n)]\ntimestamps_same_freq_8 = [dt.strftime('%d-%b-%Y %H:%M:%S.%f') for dt in timestamps_same_freq_8]\n\n# get the timestamps with the same frequency\ntimestamps_same_freq_9 = [dt_start + i*pd.Timedelta(seconds=50) for i in range(n)]\ntimestamps_same_freq_9 = [dt.strftime('%d-%b-%Y %H:%M", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "\ndef find_index(x, y, a, b):\n    for i in range(len(x)):\n        if x[i] == a and y[i] == b:\n            return i\n    return -1\n\nresult = find_index(x, y, a, b)\nprint(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "\ndef find_indices(x, y, a, b):\n    indices_x = [i for i in range(len(x)) if x[i] == a]\n    indices_y = [i for i in range(len(y)) if y[i] == b]\n    return indices_x, indices_y\n\nresult = find_indices(x, y, a, b)\nprint(result)\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "def f(a, b, x):\n    return a * x[0]**2 + b * x[0] + a\n\ndef fit(x, y):\n    a = y[0]\n    b = y[1]\n    for i in range(2, len(y)):\n        a = f(a, b, x)\n        b = a * x[i] + b\n    return a, b\n\nresult = fit(x, y)\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "def fit_func(params, x):\n    a, b, c = params\n    return a + b * x + c * x ** 2\n\ndef residuals(params, x, y):\n    return y - fit_func(params, x)\n\ndef sum_of_squares(params, x, y):\n    return np.sum(residuals(params, x, y) ** 2)\n\ndef fit(params, x, y):\n    return np.linalg.solve(residuals(params, x, y) ** 2, params)\n\ndef fit_with_degree(params, x, y):\n    return fit(params, x, y)\n\nparams = [0, 0, 0]\nresult = fit_with_degree(params, x, y)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "df.apply(lambda x: x-a[0], axis=1)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)\n", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled\nEND SOLUTION\nEND SOLUTION\n<code>\nprint(result)\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler(axis=1)\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "min_max_scaler = MinMaxScaler()\na_scaled = min_max_scaler.fit_transform(a)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "\narry_temp = arr.copy()\nmask = arry_temp < -10\nmask2 = arry_temp >= 15\nmask3 = mask ^ mask3\n\narr[mask] = 0\narr[mask2] = arr[mask2] + 5\narry[mask3] = 30\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "\narry_temp = arr.copy()\nmask = arry_temp < n1\nmask2 = arry_temp < n2\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arry[mask3] + 5\narry[~mask2] = 30 \n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "# get the indices of the elements that are different\ndiff_indices = np.nonzero(s1 != s2)[0]\n# get the number of different elements\nresult = len(diff_indices)\nprint(result)\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "# find the elements that are not equal\ndiff_elements = np.logical_and(s1 != s2, np.isnan(s1) | np.isnan(s2))\n# count the number of elements that are not equal\nresult = np.sum(diff_elements)\nprint(result)\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a[1]) and np.array_equal(a[0], a[2])\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(np.isnan(arr).all() for arr in a)\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "\n# First, find the longest shape of the two arrays\nmax_shape = max(a.shape, shape)\n\n# Then, create a new array of zeros with the max shape\npadded_array = np.zeros(max_shape)\n\n# Copy the original array into the new array\npadded_array[:a.shape[0], :a.shape[1]] = a\n\n# Now, the new array should be the right padded version of the original array\nresult = padded_array\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "\n# find the maximum of the two dimensions\nmax_dim = max(shape[0], shape[1])\n\n# create a mask of zeros with the max_dim shape\nmask = np.ones(shape) * np.nan\nmask = npordinate_mask(mask, max_dim)\n\n# apply the mask to the original array\nresult = np.where(mask != 0, a, np.nan)\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "\n# First, find the longest dimension of the original array and the desired padded array\nmax_dim = max(a.shape[0], element)\nmax_dim = max(max_dim, shape[0])\nmax_dim = max(max_dim, shape[1])\n\n# Then, create a mask of zeros with the same shape as the original array\nmask = np.zeros_like(a)\nmask = np.pad(mask, [(max_dim - shape[0]) // 2, (max_dim - shape[1]) // 2], mode='constant')\n\n# Apply the mask to the original array\nresult = a * mask\n\n# Now apply the desired element to the padded array\nresult += element * mask\n\n# Repeat this process for each row in the original array\nresult = np.array([(a[i] * mask + element * mask) for i in range(a.shape[0])])\n\n# Now the result is the padded array with the desired element\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": "\n    padded_arr = np.pad(arr, ((0, 0), (0, shape[1] - arr.shape[1])), 'constant')\n    return padded_arr\n    ### END SOLUTION\n\npadded_arr = f(example_arr)\nprint(padded_arr.shape)\n", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "\n# find the longest shape\nmax_shape = max(a.shape, shape)\n\n# create a mask of zeros of the max_shape\nmask = np.ones(max_shape) / max_shape[0]\n\n# apply the mask to the original array\na_padded = a * mask\n\n# find the indices of the original array in the masked version\nindices = np.transpose(np.where(np.logical_not(mask)))\n\n# apply the mask to the original array\na_padded[indices] = a[indices]\n\n# find the indices of the longest shape\nlongest_indices = npumerate(max_shape) - 1\n\n# apply the mask to the longest shape\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longestseemed]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices]\n\n# apply the mask to the original array\na_padded[longest_indices] = a[longest_indices", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a.reshape(3, 4)\n", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "\nmask = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n\nresult = np.array([a[i][j][mask[i][j]==1] for i,j in enumerate(b)]).reshape(a.shape[:-1])\n\nprint(result)\n", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "\n# First, we need to create a mask using the indices in b\nmask = np.zeros_like(a, dtype=bool)\nmask[np.arange(a.shape[0]), np.arange(a.shape[1]), b] = True\n\n# Now we can use this mask to index into a\nresult = a[mask]\n\n# The result should be the same as the desired output\n", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "\n# solution:\n\n# first, we need to get the indices of a from b\nindices = np.array([[0, 1, 2], [2, 1, 3], [1, 0, 3]])\n\n# then, we can use these indices to select the elements of a\nresult = a[np.arange(a.shape[0]), indices]\n\n# the result should be the same as the desired output\n", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "\n# first, we need to get the indices of a from b\nindices = np.array([[0, 1, 2], [2, 1, 3], [1, 0, 3]])\n\n# then, we can use these indices to get the elements of a\nresult = np.sum(a[tuple(indices[:,0])] * b[tuple(indices[:,1])])\n", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "\nmask = np.isin(b, np.arange(len(b)))\nresult = np.sum(a[mask] * np.power(2, b[mask]))\n", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "df['c'] = np.where(df['a'] > 1, df['b'], np.nan)\nresult = df['c'].values\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "mask = np.array([[0,1,1,1],[1,1,0,1],[0,0,1,1]])\nresult = np.array([i for i in range(im.shape[0]) if np.array_equal(im[i], mask)])\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "mask = A.any(axis=0)\nmasked_A = np.ma.masked_array(A, mask==0)\n\n# Now apply the mask to the array\nmasked_A.set_data(masked_A*1)\n\n# Draw the masked array\nplt.imshow(masked_A.filled_like(A), cmap='gray')\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\ndef remove_peripheral_zeros(im):\n    mask = np.zeros_like(im, dtype=bool)\n    mask[1:-1, 1:-1] = True\n    mask[2:, 1:-1] = True\n    mask[3:, 1:-1] = True\n    mask[4:, 1:-1] = True\n    mask[5:, 1:-1] = True\n    mask[6:, 1:-1] = True\n    mask[7:, 1:-1] = True\n    mask[8:, 1:-1] = True\n    mask[9:, 1:-1] = True\n    mask[10:, 1:-1] = True\n    mask[11:, 1:-1] = True\n    mask[12:, 1:-1] = True\n    mask[13:, 1:-1] = True\n    mask[14:, 1:-1] = True\n    mask[15:, 1:-1] = True\n    mask[16:, 1:-1] = True\n    mask[17:, 1:-1] = True\n    mask[18:, 1:-1] = True\n    mask[19:, 1:-1] = True\n    mask[20:, 1:-1] = True\n    mask[21:, 1:-1] = True\n    mask[22:, 1:-1] = True\n    mask[23:, 1:-1] = True\n    mask[24:, 1:-1] = True\n    mask[25:, 1:-1] = True\n    mask[26:, 1:-1] = True\n    mask[27:, 1:-1] = True\n    mask[28:, 1:-1] = True\n    mask[29:, 1:-1] = True\n    mask[30:, 1:-1] = True\n    mask[31:, 1:-1] = True\n    mask[32:, 1:-1] = True\n    mask[33:, 1:-1] = True\n    mask[34:, 1:-1] = True\n    mask[35:, 1:-1] = True\n    mask[36:, 1:-1] = True\n    mask[37:, 1:-1] = True\n    mask[38:, 1:-1] = True\n    mask[39:, 1:-1] = True\n    mask[40:, 1:-1] = True\n    mask[41:, 1:-1] = True\n    mask[42:, 1:-1] = True\n    mask[43:, 1:-1] = True\n    mask[44:, 1:-1] = True\n    mask[45:, 1:-1] = True\n    mask[46:, 1:-1] = True\n    mask[47:, 1:-1] = True\n    mask[48:, 1:-1] = True\n    mask[49:, 1:-1] = True\n    mask[50:, 1:-1] = True\n    mask[51:, 1:-1] = True\n    mask[52:, 1:-1] = True\n    mask[53:, 1:-1] = True\n    mask[54:, 1:-1] = True\n    mask[55:, 1:-1] = True\n    mask[56:, 1:-1] = True\n    mask[57:, 1:-1] = True\n    mask[58:, 1:-1] = True\n    mask[59:, 1:-1] = True\n    mask[60:, 1:-1] = True\n    mask[61:, 1:-1] = True\n    mask[62:, 1:-1] = True\n    mask[63:, 1:-1] = True\n    mask[64:, 1:-1] = True\n    mask[65:, 1:-1] = True\n    mask[66:, 1:-1] = True\n    mask[67:, 1:-", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "\ndef remove_border(im):\n    mask = np.ones_like(im)\n    mask[1:-1,1:-1] = im\n    mask[2:] = im[1:-1]\n    mask[:-2] = im[2:]\n    mask[-2:] = im[-1:-2]\n    return mask\n\nresult = remove_border(im)\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.figure(figsize=(10, 6))\npltuncher_plot = sns.scatterplot(x, y, hue=y, palette='viridis')\nplt.legend(title='y')\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.gca().yaxis.set_minor_locator(plt.NullLocator())\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.gca().set_xticks(np.arange(min(x), max(x)+1, 1), minor=True)\npltseemed.gca().set_yticks(np.arange(min(y), max(y)+1, 1), minor=True)\nplt.grid(which='minor', axis='both')\nplt.show()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.gca().xaxis.set_minor_locator(plt.NullLocator())\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.figure(figsize=(10, 6))\npltseemedcool.plot(x, x, '--', label='line 1')\nplt.plot(x, x, '--', label='line 2')\npltseemedcool.legend()\nplt.show()\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.figure(figsize=(10, 6))\npltseemedcool.plot(x, x, '--', label='line 1')\nplt.plot(x, x, '--', label='line 2')\npltseemedcool.legend()\nplt.show()\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.figure(figsize=(10, 7))\nplt.plot(x, y, marker='D')\npltseemedio()\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, marker='D', linestyle='-', color='red')\npltseemediental.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, 'r')\npltordinate = x[2:4]\nplt.plot(pltordinate, 'r')\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "\n# 1. Create a DataFrame\ndf = pdseemedu.DataFrame({\n    'Age': [20, 21, 22, 23, 24],\n    'Salary': [5000, 5500, 6000, 6500, 7000]\n})\n\n# 2. Plot the DataFrame\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='Age', y='Salary', data=df)\nplt.show()\n\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.linspace(0, 1, 100)\ny = 2*x + 1\nplt.figure(figsize=(10, 6))\npltseemedcool.plot(x, y, '--')\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "\nplt.figure(figsize=(10, 7))\nplt.title(\"Heights vs Weights\")\nsns.violinplot(\n    x=\"Gender\",\n    y=\"Height (cm)\",\n    hue=\"Gender\",\n    data=df,\n    split=True,\n    palette=\"desaturate\",\n    scale=\"width\",\n    cut=0,\n    linewidth=1.5,\n    edgecolor=\"black\",\n)\nplt.show()\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "plt.figure(figsize=(10, 7))\nsns.set_style('whitegrid')\npltseemedly.hist(x, y, color='purple')\nplt.title('Histogram of y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "\ndf = pd.DataFrame(x, columns=['x'])\ndf['y'] = y\n\nsns.lineplot(x='x', y='y', data=df)\nplt.show()\n\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', markersize=7, label='line')\npltseemed to be wrong\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\npltseemed.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='xyz')\nplt.title('xyz')\nplt.legend(loc='xyz', title='", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), x, 'g^', alpha=0.2, label='y')\npltseemed to be too long\nplt.plot(range(10), y, 'g^', alpha=0.2, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, ms=10, color='black')\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), \"r--o\", lw=5, markersize=30)\npltseemed.show()\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "plt.xticks = [0, 2, 4]\npltseemed to solve this problem\nplt.show()\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc=\"upper center\")\npltseemedient()\nplt.show()\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 7))\nplt.imshow(H, cmap='hot')\npltipage()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\npltseemeda.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\npltseemed.show()\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "plt.xticks(rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "plt.title(myTitle, fontsize=16, fontproperties='Comic Sans MS', color='red')\npltordinate.set_title(myTitle, fontsize=16, fontproperties='Comic Sans MS', color='red')\nplt.show()\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "plt.ylim(0, 2 * max(y))\npltseemed to add the solution\nplt.show()\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5])\npltordinate_x = plt.xticks\nplt.xticks([0, 1.5])\nplt.xticks(rotation=45)\nplt.show()\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks = np.arange(-1, 2, 0.2)\npltordinate()\nplt.show()\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(12, 6))\nplt.plot(x, '--', label='x')\nplt.plot(y, '--', label='y')\nplt Together.plot(z, '--', label='z')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, edgecolor='black', facecolor='blue')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "plt.plot(x, y)\npltordinate.set_xlim(0, 10)\nplt.set_ylim(0, 20)\nplt.show()\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().set_ylmt(format='g')\nplt.show()\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "\n# Create a DataFrame\ndf = pdordinate.DataFrame()\ndf['x'] = x\ndf['y'] = y\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(df['x'], df['y'], '--')\nplt.show()\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, sharex=axs)\nfig.suptitle('Sine and Cosine Waves')\n\naxs[0].plot(x, y1, 'r')\naxs[0].set_title('Sine Wave')\naxs[1].plot(x, y2, 'g')\naxs[1].set_title('Cosine Wave')\n\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(x, y1, 'r')\nplt.title('Sine Wave')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y2, 'g')\nplt.title('Cosine Wave')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.gca().spines['x']s.remove()\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks(rotation=90)\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks([3, 4])\npltseemed.grid(axis='x')\nplt.show()\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(3, 'y')\npltseemed.grid(axis='y', which='major', linestyle='-', color='r', linestyle='--')\nplt.grid(axis='y', which='minor', linestyle=':', color='b', linestyle='--')\nplt.show()\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.xticks([3, 4])\npltseemed to close()\nplt.yticks([3, 4])\nplt.grid(True, which='both')\nplt.xticks(rotation=90)\nplt.grid(True, which='both')\nplt.show()\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\npltseemed to be off\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\npltordinate_plot()\nplt.show()\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.show()\nplt.clf()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt Together.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.xaxis.tick_top()\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.figure(figsize=(10, 10))\npltseemedc()\nplt.plot(x, y, label='X')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('X vs Y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().spines['right'].set_color('none')\npltmanysa.title('y over x')\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y, label='Y')\nplt.ylabel('Y')\npltseemed.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=tips['total_bill'], y=tips['tip'], color='green', kind='reg')\npltordinate = plt.gca()\nplt.setp(pltordinate.get_yaxis(), color='green')\nplt.setp(pltordinate.get_xaxis(), color='green')\nplt.show()\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=tips['total_bill'], y=tips['tip'], color='green')\npltordinate = plt.gca()\npltordinate.set_color('blue')\nplt.show()\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x=tips['total_bill'], y=tips['tip'])\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df['celltype'], df['s1'])\nplt.xlabel('celltype')\npltseemed.xticks(rotation=90)\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df['celltype'], df['s1'])\nplt.xticks(rotation=45)\npltseemed.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\npltseemed to be wrong\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, 'r')\nplt.xlabel('X')\npltquismodus\nplt.ylabel('Y')\nplt.title('Graph of X and Y')\nplt.legend(['Y'])\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y)\npltseemedc()\nplt.xticks(rotation=90)\nplt.yticks(rotation=90)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'r--')\npltordinate(x, y, 'r--')\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "plt.imshow(rand_mat, extent=(0, 4, 4, 0), cmap='hot')\npltordinate.setp(plt.gca().get_xticklabels(), rotation=90)\nplt.xlabel('A', rotation=90)\nplt.ylabel('C', rotation=0)\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=\"best\")\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 20))\n\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Subplot 1')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Subplot 2')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(df['bill_length_mm'], df['bill_depth_mm'], s=30)\npltordinate.setp(plt.gca().get_xticklabels(), rotation=45)\nplt.show()\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "plt.scatter(a, b, c)\npltply.show()\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\npltseemedient()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\npltseemedient()\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, bins=10, color='skyblue', outlier_key='outliers')\npltseemediolot.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(2)\nfig.suptitle('Wider subplot')\naxs[0].plot(x, y, label='y')\naxs[0].set_title('First subplot')\naxs[1].plot(x, y, label='y')\naxs[1].set_title('Second subplot')\npltseemed.show()\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, color='r', label='x')\nplt.hist(y, bins, alpha=0.5, color='b', label='y')\npltquisition_plt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist([x, y], bins=10, color=['r', 'b'],\n         label=['x', 'y'])\npltseemed.legend(loc='upper center')\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.xlim(0, 5)\npltseemed.ylim(0, 5)\nplt.plot([a, c], [b, d], 'r')\nplt.show()\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].imshow(x, cmap='viridis')\naxs[0].set_title('x')\naxs[1].imshow(y, cmap='viridis')\naxs[1].set_title('y')\n\n# create a colorbar for each subplot\naxs[0].get_xaxis().set_visible(False)\naxs[0].get_yaxis().set_visible(False)\naxs[1].get_xaxis().set_visible(False)\naxs[1].get_yaxis().set_visible(False)\n\n# create a colorbar for each subplot\ncbar_x = fig.colorbar(axs[0].colors[0], ax=axs[0])\ncbar_y = fig.colorbar(axs[1].colors[0], ax=axs[1])\n\n# add the colorbar to the subplots\naxs[0].figure.axes.get_children().extend([cbar_x, cbar_y])\naxs[1].figure.axes.get_children().extend([cbar_x, cbaruncher_y])\n\nplt.show()\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 5))\npltseemedc()\nplt.plot(x[:, 0], 'r--', label='a')\nplt.plot(x[:, 1], 'g--', label='b')\npltseemedc()\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y)\naxs[0].set_title('Y')\naxs[1].plot(a, z)\naxs[1].set_title('Z')\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.figure(figsize=(10, 6))\npltseemedy.subplot(2, 1, 1)\nplt.plot(points, 'o-')\nplt.yscale('log')\nplt.title('Log Scale')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o-')\npltmanys.title('y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(np.arange(1, 11, 1))\nax.set_yticks(np.arange(1, 11, 1))\nax.set_xticklabels(np.arange(1, 11, 1))\nax.set_yticklabels(npseemedu.arange(1, 11, 1))\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "\nplt.figure(figsize=(5, 5))\nplt.plot([(0, 1), (1, 1)], 'r')\npltordinate = [(0, 1), (1, 1)]\nplt.plot(pltordinate, 'r')\nplt.plot([(2, 3), (3, 3)], 'b')\nplt.plot(pltordinate, 'b')\nplt.plot([(1, 2), (1, 3)], 'g')\nplt.plot(pltordinate, 'g')\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y)\npltseemedcian()\nplt.title('y over x on a log-log plot')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\naxs[0, 0].plot(df.A)\naxs[0, 0].set_title('A')\naxs[0, 1].plot(df.B)\naxs[0, 1].set_title('B')\naxs[1, 0].plot(df.C)\naxs[1, 0].set_title('C')\naxs[1, 1].plot(df.D)\naxs[1, 1].set_title('D')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "plt.hist(data, bins=10, edgecolor='black')\npltseemedediary.ylabel('Frequency')\nplt.xticks(np.arange(10), 'Interval')\nplt.yticks(np.arange(10), 'Interval')\nplt.title('Histogram of Data')\nplt.show()\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=12, markeredgecolor='r', markerfacecolor='w', alpha=0.5)\npltseemediaries.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 20))\n\naxs[0].plot(x, y, label='y')\naxs[0].set_title('y')\naxs[0].legend(loc='upper center')\n\naxs[1].plot(a, z, label='a')\naxs[1].set_title('a')\naxs[1].legend(loc='upper center')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 20))\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\npltuncher.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "plt.ylabel(\"Second\")\nplt.yticks(range(1, 10))\nplt.show()\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend(loc='best')\npltseemedow()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(range(0, 10, 2))\npltseemed to work\nplt.show()\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\npltseemed to do this\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\npltseemed_to_be_rotated = plt.gca().get_xticklabels()\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=0)\nplt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=-60)\nplt.gca().set_xt", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().spines['top'].set_color('none')\npltseemed_to_be_gone_by_2020_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0_5_0", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(0.01)\npltseemed to solve the problem\nplt.show()\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(0.01)\npltseemed to solve the problem\nplt.show()\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(2)\n\naxs[0].plot(x, y)\naxs[0].set_title('Figure 1')\n\naxs[1].plot(x, y)\naxs[1].set_title('Figure 2')\n\nplt.suptitle('Figure')\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.figure(figsize=(10, 5))\nplt.plot(df, marker='o')\npltseemedcool(plt.gca().get_xticklabels(), loc='center')\nplt.title(\"Line plot of the data\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='///')\nplt.show()\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, edgecols='k', linewidths=2)\npltseemediaru.show()\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*')\nplt.show()\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, marker='^hv')\nplt.show()\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.imshow(data, extent=(1, 5, 1, 4))\npltuncher.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(y, x, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(range(len(d)), d.values(), color=c[next(iter(d))])\npltquis.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.axvline(3, color='r', linestyle='--')\nplt.legend(['cutoff'])\npltquisition_df['cutoff'] = plt.ylabel('cutoff')\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(5, 5))\nplt.pie(height, labels=labels, autopct='%1.1f%%')\npltquis.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedge=0.4)\npltquis.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, '--b')\nplt.grid(color='gray', linestyle='--', linewidth=1, axis='y', alpha=0.1)\npltseemed to be too slow\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.grid(axis='both', on=True, linestyle='-', color='gray', linewidth=0.5, which='major')\npltseemed.grid(axis='y', on=True, linestyle='--', color='r', linewidth=1, which='minor')\nplt.show()\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\npltquisition_df['Talking']\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\npltquisition_df['Talking']\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, '--o', alpha=0.5, edgecolors='none')\npltseemediaries.show()\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color='g')\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "plt.figure(figsize=(10, 5))\nplt.bar(range(1, 5), blue_bar, color='b', align='center')\npltordinate_x = 1\nplt.bar(range(1, 5), orange_bar, color='orange', align='center')\nplt.xticks(np.arange(1, 5, 1))\nplt.ylabel('Scores')\nplt.title('Blue and Orange Bar')\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='y')\naxs[0].set_title('First Subplot')\naxs[0].legend()\naxs[1].plot(a, z, label='z')\naxs[1].set_title('Second Subplot')\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\npltseemediaries.show()\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o-')\npltseemedcay.set_title('y over x')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "\nsns.catplot(\n    data=df,\n    x=\"bill_length_mm\",\n    y=\"species\",\n    hue=\"sex\",\n    col=\"species\",\n    facet_kws=dict(sharex=False, sharey=False),\n    height=6,\n    aspect=1.5\n)\npltuncher.show()\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(5, 5))\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\npltordinate = plt.Circle((0.5, 0.5), 0.2, edgecolor='g', facecolor='none')\nplt.gca().set_aspect(1)\nplt.gca().add_artist(plt.Circle((0.5, 0.5), 0.2, edgecolor='g', facecolor='none'))\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5, '^', markersize=20, color='red')\nplt.plot(0.5, 0.5, 'o', markersize=20, color='red')\nplt.plot(0.5, 0.5,", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.title(r'$\\phi$')\nplt.plot(x, y, '--bo')\nplt.show()\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.legend(x, y, labelspacing=0.1)\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.legend(loc='center left', bbox_to_anchor=(0.3, 0, 0.5, 0), ncol=3, title_rotation=45)\npltseemed.legend(loc='center left', bbox_to_anchor=(0.3, 0, 0.5, 0), ncol=3, title_rotation=45)\nplt.show()\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='best')\npltseemedg('best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc='best')\nplt.legend(loc=", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='best')\npltseemedg('best')\nplt.legend(loc='upper right')\nplt.legend(loc='lower left')\nplt.legend(loc='lower right')\nplt.legend(loc='upper left')\nplt.legend(loc='center')\nplt.legend(loc='center right')\nplt.legend(loc='center left')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc='center')\nplt.legend(loc='lower center')\nplt.legend(loc='upper center')\nplt.legend(loc='lower center')\nplt.legend(loc=", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "fig, axs = plt.subplots(2, 5, figsize=(20, 15))\n\nfor ax in axs:\n    c = ax.matshow(data, cmap='viridis')\n    fig.colorbar(c, ax=ax)\n\npltquis.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label='Figure 1')\npltseemedy=plt.legend(loc='best')\nplt.title('Figure 1')\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.set(style=\"whitegrid\")\nsns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(y, x)\nplt.xlabel('y')\npltseemed.ylabel('x')\nplt.title('Plot of y vs x')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.scatter(x, y, alpha=0.5, clip_on=True)\npltseemed to be wrong\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.show()\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, edgecolors='black')\nplt.show()\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor ax in axs:\n    ax.plot(x, y)\n\nplt.show()\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=range(0, 12), edgecolor='black', width=2)\npltquis.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 5))\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.axhline(0, color='white')\npltseemed.axvline(0, color='white')\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 20))\naxs[0].plot(x, y)\naxs[0].set_title('Y')\naxs[1].plot(a, z)\naxs[1].set_title('Z')\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(2, 2, figsize=(5, 5), tight_layout=True)\n\naxs[0, 0].plot(x, y)\naxs[0, 0].set_title('Subplot 1')\n\naxs[0, 1].plot(x, y)\naxs[0, 1].set_title('Subplot 2')\n\naxs[1, 0].plot(x, y)\naxs[1, 0].set_title('Subplot 3')\n\naxs[1, 1].plot(x, y)\naxs[1, 1].set_title('Subplot 4')\n\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "fig, ax = plt.subplots(figsize=(8, 8))\nax.matshow(d, cmap='viridis')\npltquisit.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.figure(figsize=(10, 10))\nax = pltordinate_table(df, bbox=[0, 0, 1, 1])\nplt.show()\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\npltseemedc()\nplt.xticks(rotation=90)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.figure(figsize=(10, 5))\nplt.plot(x, y)\npltseemedc()\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xticks(x, rotation=90)\npltseemed.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "sns.set(style=\"whitegrid\")\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=.7)\nplt enthus.title(\"Group: Fat vs No Fat\")\nplt.show()\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=0.7)\nplt enthus.title(\"Exercise Time vs Pulse\")\nplt.show()\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=.7)\nplt competing_df.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=5, aspect=.7)\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\npltseemedc()\nplt.show()\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\npltseemedy.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='best')\npltseemed.show()\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "fig, ax = plt.subplots(3, figsize=(7, 12))\nax[0].plot(t, a, 'r')\nax[0].set_title('Sine Wave')\nax[0].set_xlabel('Time')\nax[0].set_ylabel('Amplitude')\nax[1].plot(t, b, 'g')\nax[1].set_xlabel('Time')\nax[1].set_ylabel('Amplitude')\nax[2].plot(t, c, 'b')\nax[2].set_xlabel('Time')\nax[2].set_ylabel('Amplitude')\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "df.drop(['species'], axis=1, inplace=True)\ndf.drop(['sex'], axis=1, inplace=True)\n\nstrip_plot = sns.stripplot(x=\"bill_length_mm\", y=\"species\", data=df, hue=\"sex\", jitter=.1, palette=\"Set2\")\npltuncher.show()\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "\nfig, axs = plt.subplots(3, figsize=(10, 10))\n\nfor ax, b in zip(axs, df['b'].unique()):\n    sns.FaceGrid(df[df['b'] == b]).map(sns.scatterplot, 'a', 'c', size='c')\n    ax.set_title(b)\n\nplt enthus\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<jupyter>\n<", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z, c='r', s=50)\nax.set_title('3D Scatter Plot of x, y, z')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\npltseemed.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.title(\"Line Plot\")\npltseemed.ylabel(\"Y\")\nplt.xlabel(\"X\")\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, i * ncol + j + 1)\n        ax.imshow(x[i][j])\npltquis.show()\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "\n# create a function to change the value of x\ndef change_x_value(new_value):\n    global x\n    x.assign(new_value)\n\n# call the function to change the value of x\nchange_x_value(1)\n\n# print the value of x\nprint(x.numpy())\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "\n# change the value of x to 114514\nx.assign_add(114514)\n\n# print the value of x\nprint(x)\n", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\ndef one_hot(input_tensor):\n    input_tensor = tf.cast(input_tensor, tf.int32)\n    tensor_shape = tf.shape(input_tensor)\n    size = tensor_shape[0]\n    return tf.cast(\n        tf.one_hot(input_tensor, depth=10), tf.int32\n    )\n\nresult = tf.map_fn(lambda x: one_hot(x), labels)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\ndef get_class_indices(labels):\n    class_indices = tf.constant([0, 6, 5, 4, 2])\n    class_indices_tensor = tf.constant(class_indices)\n    return class_indices_tensor\n\ndef get_class_indices_tensor(labels):\n    class_indices_tensor = tfordinate.get_class_indices(labels)\n    return class_indices_tensor\n\nclass_indices_tensor = get_class_indices_tensor(labels)\n\nprint(class_indices_tensor)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "\ndef get_class_tensor(labels):\n    class_tensor = tf.constant(0, dtype=tf.int32)\n    for label in labels:\n        class_tensor = tf.where(class_tensor < label, class_tensor, label)\n    return class_tensor\n\nclass_tensor = get_class_tensor(labels)\nprint(class_tensor)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": "\n    ###\n    # Create a 2D tensor of zeros with the same shape as your labels\n    # Then, use the `tf.one_hot` function to create a one-hot tensor\n    # Finally, use `tf.boolean_tensor_to_int` to convert the boolean tensor to an int tensor\n    ### SOLUTION\n    ###\n    return tf.one_hot(labels, 10)\n\n# Apply the function to your labels\nresult = f(example_labels)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\ndef get_class_tensor(labels):\n    class_tensor = tf.constant(0, dtype=tf.int32)\n    for label in labels:\n        class_tensor = tf.where(class_tensor < label, class_tensor, label)\n    return class_tensor\n\nclass_tensor = get_class_tensor(labels)\nprint(class_tensor)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return [[i, i+1, i+2]]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.compat.v1.py_func(\n  func=my_map_func, inp=[input], Tout=[tf.int64]\n))\nelement = tf.compat.v1ordinate.get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": "\n    return [i+1 for i in input]\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(lambda x: tf.py_function(f, [x], [tf.int32]))\n\nelement = tf.compat.v1ordinate(ds).get_next()\n\nwith tf.compat.v1.Session() as sess:\n    for _ in range(3):\n        print(sess.run(element))\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\ndef create_mask(lengths):\n    max_length = max(lengths)\n    mask_shape = [1] * len(lengths) + [max_length]\n    mask = tf.constant(0, dtype=tf.int32)\n    for i in range(len(lengths)):\n        mask += tf.sequence.pad_right(tf.ones_like(tf.range(lengths[i]), dtype=tf.int32), max_length)\n    mask = tf.cast(mask, tf.int32)\n    mask = tf.cast(mask, tfordinate.dtype)\n    return mask\n\nmask = create_mask(lengths)\nprint(mask)\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\ndef create_padded_mask(lengths):\n    max_length = tf.reduce_max(lengths)\n    mask_shape = [max_length, max_length]\n    mask_dtype = tf.int32\n    mask = tf.map_fn(lambda x: x * 0, dtype=mask_dtype)\n    padded_mask = tf.pad_right(mask, 1)\n    return tf.map_fn(lambda x: x * 0, dtype=mask_dtype)\n\n\nmask = tf.map_fn(lambda x: x * 0, dtype=mask_dtype)\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\ndef create_mask(lengths):\n    max_length = max(lengths)\n    mask_shape = [1] * len(lengths) + [max_length]\n    mask = tf.constant(0, dtype=tf.int32)\n    for i in range(len(lengths)):\n        mask += tf.sequence.pad_right(\n            tf.range(mask_shape[i]),\n            mask_shape[i],\n            name=f\"mask_{i}\"\n        )\n    mask = tf.cast(mask, tf.int32)\n    mask = tf.where(mask >= 1, 1, 0)\n    return mask\n\nmask = create_mask(lengths)\nprint(mask)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": "\n    max_length = tf.reduce_max(lengths)\n    lengths_tensor = tf.constant(lengths, dtype=tf.int32)\n    mask_tensor = tf.where(lengths_tensor <= max_length, 1, 0)\n    padded_tensor = tf.pad_right(mask_tensor, [(0, max_length - tf.shape(mask_tensor)[0])])\n    return tf.cast(padded_tensor, tf.int32)\n    ### END SOLUTION\n\nprint(f(example_lengths))\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\ndef create_mask(lengths):\n    max_length = max(lengths)\n    mask_shape = [1] * len(lengths) + [max_length]\n    mask = tf.constant(0, dtype=tf.int32)\n    mask_1 = tf.cast(mask, dtype=tfordinate_tensor.dtype)\n    for i in range(len(lengths)):\n        mask_1 = tf.where(mask_1 < lengths[i], 1, mask_1)\n        mask_1 = tf.where(mask_1 >= lengths[i], 0, mask_1)\n    mask_1 = tf.pad_right(mask_1, [(max_length // 2)])\n    return tf.expand_dims(mask_1, 0)\n\nmask = create_mask(lengths)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack(tf.map_fn(lambda x, y: [x, y], a, b))\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "\n    return tf.stack(\n        [\n            tf.stack([a, b], 1),\n            tf.stack([b, a], 1),\n        ],\n        1,\n    )\n\n# Run the function\nprint(f(example_a, example_b).numpy())\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "\n# Reshape the tensor to drop the third dimension\na = tf.squeeze(a, [2])\n\n# Print the shape of the tensor\nprint(a.shape)\n", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "\n# Add a new dimension\na = tf.expand_dims(a, axis=2)\n\n# Reshape the tensor\na = tf.reshape(a, [50, 100, 1, 512])\n\n# Print the shape of the tensor\nprint(a.shape)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "\n# Reshape the tensor\nreshaped_tensor = tf.reshape(a, [1, 50, 100, 1, 512])\n\n# Adding a new dimension\nnew_dimension = tf.expand_dims(a, 0)\n\n# Combining the tensors\nresult = tf.concat([new_dimension, reshaped_tensor], 0)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "\n# Calculate the sum of each row\nsum_A = tf.reduce_sum(A, axis=1)\n\n# Create a tensor with the result\nresult = tf.constant(sum_A.numpy())\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "\n# Calculate the product of each element in the array\nprod_A = tf.reduce_prod(A, axis=1)\n\n# Create a placeholder for the result\nresult = tf.placeholder(tf.float32, shape=[], name='result')\n\n# Define the function that will be used to compute the result\ndef compute_result(A):\n    # Calculate the product of each element in the array\n    prod_A = tf.reduce_prod(A, axis=1)\n    # Return the result\n    return prod_A\n\n# Compute the result\nwith tf.Session() as sess:\n    sess.run(result, feed_dict={A: np.array([[1, 2, 3], [4, 5, 6]])})\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "reciprocal_A = 1 / A\nresult = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "l2_norm_a_b = tf.sqrt(tf.reduce_sum(tf.square(a - b)))\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "l2_norm_a_b = tf.sqrt(tf.reduce_sum(tf.square(a - b), axis=-1))\nresult = tf.reduce_sum(l2_norm_a_b, axis=-1)\nprint(result)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": "\n    return tf.sqrt(tf.reduce_sum(tf.square(A - B)))\n    ### END SOLUTION\n\nresult = f(example_a, example_b)\nprint(result)\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = x[y,z]\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = x[row][col]\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "\n    return x[y,z]\n\nm = f()\nprint(m)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\ndef dot_product(A, B):\n    return tf.reduce_sum(A * tf.tile(B, [tf.shape(A)[0], 1, 1]), axis=2)\n\nC = dot_product(A, B)\n\nprint(C)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "\ndef dot_product(A, B):\n    return tf.reduce_sum(A * B, axis=-1)\n\nC = dot_product(A, B)\n\nprint(C.shape) # should print (10, 20, 1)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\n# Convert bytes to string\nx_str = [str(i, 'utf-8') for i in x]\n\n# Create a TensorFlow dataset from the string list\ndataset = tf.data.Dataset.from_tensor_slices(x_str)\n\n# Print the first 10 elements of the dataset\nfor i in dataset.take(10):\n    print(i)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": "\n    x = tf.constant(x)\n    x = tf.strings.regex_split(x, '\\\\x')\n    x = tf.strings.regex_replace(x, '[^a-zA-Z]', '')\n    x = tf.strings.regex_replace(x, '[^0-9]', '')\n    x = tfipage(x, 'utf-8')\n    return x\n    ### END SOLUTION\n\nprint(f(example_x))\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "\nmask = tf.cast(x != 0, tf.float32)\nmask_sum = tf.reduce_sum(mask)\nmask_sum_dtype = tf.dtype.float\nmask_sum = tfipage.astype(mask_sum.numpy(), dtype=mask_sum_dtype)\nmask_sum_not_zero = tf.boolean_tensor_cast(mask_sum > 0, dtype=mask_sum_dtype)\nmask_sum_not_zero_inv = 1 / mask_sum_not_zero\nmask_sum_not_zero_inv_sqrt = tf.sqrt(mask_sum_not_zero_inv)\nmask_sum_not_zero_inv_sqrt_mul_x = mask_sum_not_zero_inv_sqrt * x\nresult = tf.reduce_sum(mask_sum_not_zero_inv_sqrt_mul_x, axis=-1)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = tf.cast(tf.not_equal(x, 0), tf.float32)\nx_padded = tf.pad_with(x, [[0, 0, 0, 0, 1, 1]], 'constant')\nmask_padded = tf.pad_with(mask, [[0, 0, 0, 0, 1, 1]], 'constant')\n\nx_padded_masked = x_padded * mask_padded\nx_masked = x_padded / x_padded_masked\n\nx_masked_squared = x_masked ** 2\nx_masked_cube = x_masked ** 3\n\nx_masked_cube_sum = tf.reduce_sum(x_masked_cube)\nx_masked_cube_mean = tf.reduce_mean(x_masked_cube)\n\nx_masked_cube_variance = x_masked_cube_squared - x_masked_cube_cube\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    mask = tf.cast(x != 0, tf.float32)\n    masked_x = x * mask\n    return tf.reduce_sum(masked_x, axis=-1) / tf.reduce_sum(mask, axis=-1)\n    ### END SOLUTION\n\nresult = f(example_x)\nprint(result)\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\n# Create a TensorFlow session\nsession = tf.Session()\n\n# Create two placeholders for input\nA = tf.placeholder(tf.float32, shape=[100, 100])\nB = tf.placeholder(tf.float32, shape=[100, 100])\n\n# Create a variable for the output\n@tf.function.experimental.v1.enable_v2_behavior\ndef matmul(A, B):\n    return tf.matmul(A, B)\n\n# Create a function that computes the sum of the matrix multiplication\n@tf.function.experimental.v1.enable_v2_behavior\ndef reduce_sum(matmul):\n    return tf.reduce_sum(matmul)\n\n# Compute the matrix multiplication and the sum\nmatmul_op = matmul(A, B)\nsum_op = reduce_sum(matmul_op)\n\n# Run the operations in the session\nresult = session.run(sum_op)\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\ndef get_max_index(tensor):\n    return tf.argmax(tensor, axis=1)\n\nresult = get_max_index(a)\n\nprint(result)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\ndef get_max_index(tensor):\n    return tf.argmax(tensor, axis=1)\n\nresult = get_max_index(a)\n\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": "\n    max_indices = tf.argmax(a, axis=1)\n    return max_indices\n\nmax_indices = f(example_a)\nprint(max_indices)\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "\ndef get_min_index(tensor):\n    return tf.argmin(tensor, axis=0)\n\nresult = get_min_index(a)\n\nprint(result)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save(\"my_model\")\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\n# set the seed\ntf.random.set_seed(seed_x)\n\n# generate 10 random integers from the range 1 to 4\nresult = tf.random.uniform(minval=1, maxval=4, dtype=tf.int32, seed=seed_x)\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "\n# set the seed\ntf.random.set_seed(seed_x)\n\n# generate 114 random integers from the range 2 to 5\nresult = tf.random.uniform(minval = 2, maxval = 6, size = (114,))\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    x = tf.random.uniform(minval=1, maxval=4, dtype=tf.int32)\n    y = tf.random.uniform(minval=1, maxval=4, dtype=tfordinate.float32)\n    return x, y\n    ### END SOLUTION\n\n# call the function\nx, y = f(10)\nprint(x, y)\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "\n# get the version of tensorflow\nversion = tf.__version__\n\n# print the version\nprint(version)\n\n# check if version is compatible with your requirement\nif version >= '2.0':\n    print('TensorFlow version 2.0 or above is installed')\nelse:\n    print('TensorFlow version 2.0 or below is installed')\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "coefficients = np.polyfit(x, y, 1)\nresult = coefficients\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "coefficients = np.polyfit(x, y, 1)\nresult = coefficients\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, popt = scipy.optimize.curve_fit(func, x, y, p0=p0)\nprint(result)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "test_stat = stats.ks_test(x, y, alternative='two-sided')\n", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "\ndef two_sample_KS_test(x, y, alpha):\n    D = np.std(x)\n    d = np.std(y)\n    r = np.sqrt(np.std(x-y)**2)\n    n = len(x)\n    z = (D/np.sqrt(n**2 - 1))*(1/np.sqrt(n**2/((n-1)**2) - 1))\n    p_value = 2*(1-stats.norm.cdf(z))\n    reject_null = np.abs(z) < (1/np.sqrt(n**2/((n-1)**2) - 1))\n    return reject_null, p_value\n\nreject_null, p_value = two_sample_KS_test(x, y, alpha)\nresult = p_value < alpha\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(x):\n    return sqrt((sin(pi/2) + sin(0) + sin(x[0]) - 2)**2 + (cos(pi/2) + cos(0) + cos(x[0]) - 1)**2)\n\ndef g(x):\n    return x[1]**2 + x[2]**2\n\ndef h(x):\n    return g(x) - f(x)\n\ndef minimize_with_multiple_variables(x):\n    global result\n    result = x\n    return h(x)\n\nresult = optimize.minimize(minimize_with_multiple_variables, initial_guess, method='BFGS')\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.norm.cdf(z_scores, loc=0, scale=1)\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [2 * max(0, z_score * sigma + mu) for z_score in z_scores]\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = [scipy.stats.norm.ppf(1-p) for p in p_values]\n", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm(stddev, scale=mu)\nresult = dist.cdf(x)\n", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = lognorm(mu/stddev)\n\n# Generate 1000 samples from the distribution\nsamples = dist.rvs(size=1000)\n\n# Calculate the expected value and median\nexpected_value = np.mean(samples)\nmedian = npordinate(samples, 0.5)\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "\nsa * sb\n", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": "\n    return sA.dot(sB)\n    ### END SOLUTION\n\nf(example_sA, example_sB)\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "interp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "interp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle)\nxrot = x0 + data_rot.shape[1]*np.cos(angle*npseemed.pi/180)\nyrot = y0 - data_rot.shape[0]*np.sin(angle*np.pi/180)\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "\n# get the main diagonal of the sparse matrix\nresult = M.diagonal_r()\n", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "# Applying the Kolmogorov-Smirnov test\nks_result = stats.kstest(times, 'uniform')\nprint(ks_result)\n", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": "\n    ks_test_result = stats.kstest(times, 'uniform')\n    return ks_test_result\n    ### END SOLUTION\nf(example_times, example_rate, example_T)\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "# Applying the Kolmogorov-Smirnov test\nD = [(i+1)/T for i in times]\nDf = stats.kstest(D, 'uniform')\npvalue = Df.pvalue\nalpha = 0.05\nresult = pvalue < alpha\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = c1 + c2\n", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = c1 + c2\n", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = sparse.vstack([c1, c2])\n", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "def objective(points1, points2):\n    distances = scipy.spatial.distance.cdist(points1, points2, 'euclidean')\n    return np.sum(distances)\nscipy.optimize.minimize(objective, (points2,), method='TWO_POINT_CROSSOVER')\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "def objective(points1, points2):\n    distances = scipy.spatial.distance.cdist(points1, points2, 'euclidean')\n    return np.sum(distances)\nscipy.optimize.minimize(objective, (points2,), method='TWO_POINT_CROSSOVER')\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "mask = img > threshold\nresult = np.sum(mask.astype(int))\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "mask = img < threshold\nresult = np.sum(mask.astype(int))\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": "\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimageordinate.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[img<=threshold] = 0\n    labels, num_regions = ndimage.label(img)\n    return num_regions\n    ### END SOLUTION\n    # return the solution in this function\n    # result = f(img)\n    ### BEGIN SOLUTION\n    img[img>threshold] = 1\n    img[", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\n# Find the cells with the value greater than the threshold\nmask = img > threshold\n\n# Find the coordinates of the cells with the value greater than the threshold\ncoords = np.where(mask)\n\n# Calculate the distance from the center of mass of the regions to the top left corner\ndistances = np.sqrt(np.sum((coords[0], coords[1])**2))\n\n# Print the distances\nprint(distances)\n\n# The solution to the problem\nresult = distances\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "\n# Create a copy of the matrix\nM_copy = M.copy()\n\n# Iterate over the rows of the matrix\nfor i in range(M.shape[0]):\n    # Iterate over the columns of the matrix\n    for j in range(M.shape[1]):\n        # Check if the elements are the same\n        if M[i, j] == M_copy[j, i]:\n            # If the elements are the same, assign the value to the diagonal of the matrix\n            M[i, j] = M_copy[j, i]\n\n# Convert the matrix to a dense matrix\nM_dense = M.todense()\n\n# Convert the dense matrix to a sparse matrix\nM_sparse = sparse.csr_matrix(M_dense)\n\n# Print the sparse matrix\nprint(M_sparse)\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": "\n    for i in range(len(sA)):\n        for j in range(len(sA[i])):\n            if sA[i][j].data != 0:\n                sA[j][i].data = sA[i][j].data\n    ### END SOLUTION\n    return sA\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\ndef erosion(img, structuring_element):\n    return scipy.ndimage.minimum_filter(img, structuring_element, mode='constant')\n\ndef dilate(img, structuring_element):\n    return scipy.ndimage.maximum_filter(img, structuring_element, mode='constant')\n\ndef remove_isolated_cells(img):\n    structuring_element = np.ones((3, 3))\n    eroded_img = erosion(img, structuring_element)\n    dilated_img = dilate(img, structuring_element)\n    isolated_cells_mask = np.where(np.logical_and(img != eroded_img, img != dilated_img), 1, 0)\n    return isolated_cells_mask\n\nisolated_cells_mask = remove_isolated_cells(square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\ndef erosion(img):\n    return scipy.ndimage.minimum_filter(img, 1) == img\n\ndef dilation(img):\n    return scipy.ndimage.maximum_filter(img, 1) == img\n\ndef remove_isolated_cells(img):\n    return erosion(img) & dilation(img)\n\ndef main():\n    img = square.copy()\n    img[remove_isolated_cells(img)] = 0\n    return img\n\nif __name__ == \"__main__\":\n    print(main())\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "\n# Convert the sparse matrix to a dense matrix\nX = np.array(col.todense())\n\n# Calculate the mean and standard deviation\nmean = np.mean(X)\nstandard_deviation = npordinate.std(X)\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_val = np.max(col.A)\nmin_val = np.min(col.A)\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "\n# Convert the sparse matrix to a dense matrix\ndA = np.array(col.todense())\n\n# Calculate the median and mode\nMedian = np.median(dA)\nMode = npipage.mode(dA)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *args):\n    return sum([a*np.cos(b*np.pi/tau*x) for a, b in zip(args[::2], args[1::2])])\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "def find_min_distance(raster):\n    # Flatten the raster into a 1D array\n    flat_raster = raster.flatten()\n    # Compute the pairwise Euclidean distance\n    distances = scipy.spatial.distance.pdist(flat_raster, 'euclidean')\n    # Find the minimum distance\n    min_distance = distances.min()\n    return min_distance\n\nmin_distance = find_min_distance(example_array)\nprint(min_distance)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "def find_min_distance(example_array):\n    distances = np.zeros((len(example_array)**2)//2)\n    for i in range(len(example_array)):\n        for j in range(len(example_array[i])):\n            for k in range(len(example_array[i][j])-1):\n                distances[example_array[i][j][:len(example_array[i][j])-k-1]] = example_array[i][j][:len(exampleordinate[i][j])-k-1]\n                distances[example_array[i][j][len(example_array[i][j])-k-1:]] = example_array[i][j][len(example_array[i][j])-k:]\n    return np.min(distances)\n\ndef find_min_distance_2D(example_array):\n    distances = np.zeros((len(example_array)**2)//2)\n    for i in range(len(example_array)):\n        for j in range(len(example_array[i])):\n            for k in range(len(example_array[i][j])-1):\n                distances[example_array[i][j][:len(example_array[i][j])-k-1]] = example_array[i][j][:len(example_array[i][j])-k-1]\n                distances[example_array[i][j][len(example_array[i][j])-k-1:]] = example_array[i][j][len(example_array[i][j])-k:]\n    return np.min(distances)\n\ndef find_min_distance_3D(example_array):\n    distances = np.zeros((len(example_array)**2)//2)\n    for i in range(len(example_array)):\n        for j in range(len(example_array[i])):\n            for k in range(len(example_array[i][j])-1):\n                distances[example_array[i][j][:len(example_array[i][j])-k-1]] = example_array[i][j][:len(example_array[i][j])-k-1]\n                distances[example_array[i][j][len(example_array[i][j])-k-1:]] = example_array[i][j][len(example_array[i][j])-k:]\n    return np.min(distances)\n\ndef find_min_distance_4D(example_array):\n    distances = np.zeros((len(example_array)**2)//2)\n    for i in range(len(example_array)):\n        for j in range(len(example_array[i])):\n            for k in range(len(example_array[i][j])-1):\n                distances[example_array[i][j][:len(example_array[i][j])-k-1]] = example_array[i][j][:len(example_array[i][j])-k-1]\n                distances[example_array[i][j][len(example_array[i][j])-k-1:]] = example_array[i][j][len(example_array[i][j])-k:]\n    return np.min(distances)\n\ndef find_min_distance_5D(example_array):\n    distances = np.zeros((len(example_array)**2)//2)\n    for i in range(len(example_array)):\n        for j in range(len(example_array[i])):\n            for k in range(len(example_array[i][j])-1):\n                distances[example_array[i][j][:len(example_array[i][j])-k-1]] = example_array[i][j][:len(example_array[i][j])-k-1]\n                distances[example_array[i][j][len(example_array[i][j])-k-1:]] = example_array[i][j][len(example_array[i][j])-k:]\n    return np.min(distances)\n\ndef find_min_distance_6D(example_array):\n    distances = np.zeros((len(example_array", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": "\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "for i in range(5):\n    x_int = np.interp(x_val, x[:, i], y[:, i])\n    plt.plot(x_val, x_int, linestyle = '', marker = 'o')\nplt.show()\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1_list = [x1, x2, x3, x4]\nfor x in x1_list:\n    statistic, critical_values, significance_level = ss.anderson_ksamp(x)\n    print(f'For x = {x}:')\n    print(f'Statistic: {statistic}')\n    print(f'Critical values: {critical_values}')\n    print(f'Significance level: {significanceIMARY_LEVEL}')\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\n\n# Apply the Anderson-Darling method\nstat, p_value = ss.anderson_ksamp([x1, x2])\n\n# Check if the p_value is less than 0.05\nif p_value < 0.05:\n    result = True\nelse:\n    result = False\n", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def ktau(x):\n    y = x['B'] # keep one column fixed and run it in the other two\n    tau, p_value = stats.kendalltau(x['B'], y)\n    return tau\ndf['AB'] = df['B'].rolling(3).apply(ktau, raw=True)\ndf\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return(len(my_csr_matrix.nonzero()[0]) == 0)\n\nresult = is_csr_matrix_only_zeroes(sa)\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_empty_lil_matrix(sa):\n    return len(sa.data) == 0\nresult = is_empty_lil_matrix(sa)", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(*a)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "p_value = stats.ranksums(pre_course_scores, during_course_scores)\np_value\n", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": "\n    t_stat, p_value = stats.ranksums(pre_course_scores, during_course_scores)\n    return t_stat, p_value\n    ### END SOLUTION\nf(example_pre_course_scores, example_during_course_scores)\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "\ndef calculate_kurtosis(a):\n    n = len(a)\n    mean = np.mean(a)\n    var = np.var(a)\n    std_dev = npseemed.sqrt(var)\n    skewness = (3 * mean) / (n ** 2)\n    kurtosis = (3 * var) / (n ** 3) - 3 * skewness\n    return kurtosis\n\nkurtosis_result = calculate_kurtosis(a)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "f = scipy.interpolate.interp2d(s, t, z, 'linear')\nresult = f(x, y)\n", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    ### SOLUTION ###\n    ### END SOLUTION ###\n    return z\n\ninterp = scipy.interpolate.interp2d(example_s, example_t, f(example_s, example_t))\n\n# Now you can use the interpolation function to get the values at any point\ns_new = 0.5\nt_new = 1.5\n\n# Get the value at the new point\nz_new = interp(s_new, t_new)\n\nprint(z_new)\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "def get_vor_region(point):\n    hull = scipy.spatial.ConvexHull(point)\n    for i in range(len(hull.points)):\n        if hull.points[i] in point:\n            return i\n\ndef get_vor_cell_regions(point):\n    cell_regions = set()\n    for p in point:\n        cell_regions.add(get_vor_region(p))\n    return cell_regions\n\ndef get_vor_cell_containing_point(point):\n    cell_regions = get_vor_cell_regions(point)\n    cell_count = dict()\n    for cell in cell_regions:\n        cell_count[cell] = cell_count.get(cell, 0) + 1\n    return cell_count\n\ndef get_vor_cell_containing_points(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = getipage(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(points):\n    cell_count = dict()\n    for p in points:\n        cell_count[p] = get_vor_cell_containing_point(p)\n    return cell_count\n\ndef get_vor_cell_containing_points_all(", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "def get_vor_region(point):\n    \"\"\"\n    This function returns the region of the Voronoi diagram for a given point\n    \"\"\"\n    # get the region of the Voronoi diagram for the given point\n    region = vor.point_region(point)\n    return region\n\ndef count_points_in_region(region):\n    \"\"\"\n    This function counts the number of points in a given region\n    \"\"\"\n    # count the number of points in the given region\n    count = len(region)\n    return count\n\ndef solve():\n    \"\"\"\n    This function solves the problem\n    \"\"\"\n    # get the region of the Voronoi diagram for each extra point\n    regions = [get_vor_region(point) for point in extraPoints]\n    # count the number of points in each region\n    counts = [count_points_in_region(region) for region in regions]\n    return counts\n\nresult = solve()\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "\n# Create a sparse matrix using the vectors\nsparse_matrix = sparse.csc_matrix(vectors)\n\n# Padding zeros to the end of elements for the vectors which are smaller than the maximum size\nfor i in range(len(vectors)):\n    if len(vectors[i]) < max_vector_size:\n        diff = np.pad(vectors[i][:max_vector_size-len(vectors[i])],(0,max_vector_size-len(vectors[i])),mode='constant')\n        vectors[i] = np.concatenate([vectors[i],diff])\n\n# Now the vectors should be of the same length\nassert len(vectors[0]) == len(vectors[1]) == len(vectors[2]) == max_vector_size\n\n# Now you can use the sparse_matrix\nresult = sparse_matrix\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant')\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "M[row, column].data\n", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = [M[r, c] for r, c in zip(row, column)]\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "\ninterp = scipy.interpolate.interp1d(x, array, kind='linear')\nnew_array = interp(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u, o2, x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n\nprob = NormalDistro(u, o2, x)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n    ### BEGIN SOLUTION ###\n    ### END SOLUTION ###\n   ", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "\n# Create a 2D array of zeros with the same shape as the input\ndct_matrix = np.zeros_like(input_matrix)\n\n# Compute the DCT of each column of the input matrix\nfor i in range(input_matrix.shape[1]):\n    column = input_matrix[:, i]\n    dct_matrix[:, i] = sf.dct(column)\n\n# Normalize each column of the DCT matrix by its length\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct enthus_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the length of the column\ndct_matrix /= len(column)\n\n# Replace each element of the input matrix with its DCT divided by the length of the column\ninput_matrix /= len(column)\n\n# Replace each element of the DCT matrix with its DCT divided by the", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "offsets = [0, 1, 2]\noffsets = [-1, 0, 1]\ntridi_matrix = sparse.diags(matrix, offsets, format='csr')\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "\ndef binomial_matrix(N, p):\n    \"\"\"\n    Generate a 2D binomial distribution probability matrix\n    \"\"\"\n    M = np.zeros((N+1, N+1))\n    for i in range(N+1):\n        for j in range(i+1):\n            M[i,j] = scipy.stats.binomial.pmf(j, N-i, p)\n    return M\n\nM = binomial_matrix(N, p)\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "\n# Calculate z-scores\ndf_z_scores = df.apply(lambda x: stats.zscore(x))\n\n# Combine the original dataframe with the z-scores\ndf_combined = df.join(df_z_scores)\n\n# Print the result\nprint(df_combined)\n", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "\n# Calculate z-score for each column\nz_scores = df.apply(lambda x: stats.zscore(x))\n\n# Combine the original dataframe and the z-scores\ndf_with_z_scores = pd.concat([df, z_scores], axis=0)\n\n# Print the result\nprint(df_with_z_scores)\n", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "\n# calculate zscore\ndf_zscore = df.apply(lambda x: stats.zscore(x))\n\n# combine original df with zscore\ndf_combined = df.join(df_zscore)\n\n# set the index of df_combined to probegenes\ndf_combined.index = df.index\n\n# set the column names of df_combined to sample1, sample2, sample3\ndf_combined.columns = ['sample1_zscore', 'sample2_zscore', 'sample3_zscore']\n\ndf_combined\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "\n# First, calculate the z-scores for each column\nfor col in df.columns[1:]:\n    df[col+'_zscore'] = stats.zscore(df[col])\n\n# Then, create the final dataframe with the original data and the z-scores\ndf_final = df.loc['probegenes'].apply(lambda x: pd.Series(x))\ndf_final.columns = ['sample1_zscore', 'sample2_zscore', 'sample3_zscore']\ndf_final.index = df.loc['probegenes'].index\n\n# Finally, join the original dataframe with the z-scores\ndf_final = df_final.join(df.loc['probegenes'])\n\n# Now, df_final should contain the original data and the z-scores\ndf_final\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "def callback(x):\n    global result\n    result = x\n    return x\n\nresult = None\nsp.optimize.line_search(test_func, test_grad, starting_point, direction, callback=callback)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "def get_distance_2(y, x):\n    y_flat = y.flatten()\n    x_flat = x.flatten()\n    dist_matrix = np.sqrt(np.sum(y_flat**2, axis=1) + np.sum(x_flat**2, axis=1))\n    return dist_matrix.reshape(shape)\n\ny = np.array([[1, 2, 3], [4, 5, 6]])\nx = np.array([[1, 2], [3, 4]])\n\nresult = get_distance_2(y, x)\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "def get_distance_2(y, x):\n    y_range = range(y.shape[0])\n    x_range = range(x.shape[0])\n    y_grid, x_grid = np.meshgrid(y_range, x_range)\n    y_grid = y_grid - y[0, 0]\n    x_grid = x_grid - x[0, 0]\n    distances = np.sqrt(y_grid**2 + x_grid**2)\n    return distances\n\ny = np.array([[5., 4., 3., 3., 4., 5.],\n       [4., 3., 2., 2., 3., 4.],\n       [3., 2., 1., 1., 2., 3.],\n       [3., 2., 1., 1., 2., 3.],\n       [4., 3., 2., 2., 3., 4.],\n       [5., 4., 3., 3., 4., 5.]])\n\nx = npumerate(y)\n\nresult = get_distance_2(y, x)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": "\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    center = np.array(shape) // 2\n    dist = np.sqrt((y - center)**2 + (x - center)**2)\n    return dist\n    ### END SOLUTION\n\n# Test the function\nprint(f(shape = (6, 6)))\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "\ndef resample_array(x, shape):\n    # calculate the new shape\n    new_shape = tuple(np.round(np.array(x.shape) * np.array(shape) / np.array(x.shape)))\n    # use the nearest neighbour method to interpolate\n    result = scipy.ndimage.zoom(x, new_shape, order=1)\n    return result\n\n# apply the function to your array\nresult = resample_array(x, shape)\nprint(result.shape)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return np.sum((a.dot(x ** 2) - y) ** 2)\n\ndef residual(pars, a):\n    x = pars['x']\n    return func(x, a)\n\nout = scipy.optimize.minimize(residual, x0, method='BFGS')\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x, a):\n    return np.sum((a.dot(x ** 2) - y) ** 2)\n\ndef residual(pars, a, y):\n    x = pars['x']\n    return (y - a.dot(x ** 2)) ** 2\n\ndef main():\n    fit_params = Parameters()\n    fit_params.add('x', value=x0)\n    out = scipy.optimize.minimize(residual, fit_params, args=(a, y))\n    return out\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    if 0 < t < 2*np.pi:\n        return -100 * N1 + t * 2 * np.sin(t)\n    else:\n        return 2 * np.pi * 100 * N1\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "merged_matrix = sa + sb\n", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "merged_matrix = sa + sb\n", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "I = []\nfor c in [1, 2, 3]:\n    eqn = lambda x: x * c\n    integral, error = scipy.integrate.quad(eqn, low, high)\n    I.append(integral)\nI = np.array(I)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": " ###\n    def eqn(x):\n        return 2*x*c\n    result, error = scipy.integrate.quad(eqn, low, high)\n    return result\nI = [f(c=c[n]) for n in range(len(c))]\nI = np.array(I)\n", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "V[V.keys()] = x\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V += x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V += x\nV += y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "def update_matrix(sa):\n    for Col in xrange(sa.shape[1]):\n        Column = sa[:,Col].data\n        List = [x**2 for x in Column]\n        Len = math.sqrt(sum(List))\n        sa[:,Col] = (1/Len)*Column\n    return sa\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "def normalize_matrix(matrix):\n    for i in range(matrix.shape[1]):\n        column = matrix[:,i].toarray().squeeze()\n        norm = np.sqrt(np.dot(column, column))\n        matrix[:,i] = column / norm\n    return matrix\n\nsa = normalize_matrix(sa)\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "\n# Convert the matrix to binary matrix\nb = np.where(a==0, 0, 1)\n\n# Create a mask to remove the zeros\nmask = a.sum(axis=0)>0\nmask = np.where(mask==0, 0, 1)\n\n# Create a binary matrix\nb = npordinate.dot(mask, b)\n\n# Remove the zeros\nb = np.where(b==0, 0, b)\n\n# Reshape the matrix\nb = b.reshape(a.shape[0], a.shape[1])\n\n# Print the binary matrix\nprint(b)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "\n# Convert the matrix to a binary matrix\na_binary = np.where(a==0, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "distances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\nclosest_indices = np.argmin(distances, axis=0)\nresult = closest_indices\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "distances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\nclosest_points = np.argmin(distances, axis=0)\nresult = data[closest_points]\n", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "distances = scipy.spatial.distance.cdist(data, centroids, 'euclidean')\nclosest_points = np.argpartition(distances, k)[:k]\nresult = closest_points\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    sol = fsolve(eqn, x0=x, args=(a, b))\n    result.append(sol[0])\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    sol = fsolve(eqn, x0=x)\n    result.append(sol)\n\ndef sort_roots(roots):\n    return [min(root, key=lambda x: abs(x-3)) for root in roots]\n\nsorted_roots = sort_roots(result)\n\nprint(sorted_roots)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "def func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\nparams_est, params_cov = sp.optimize.curve_fit(func, range_start, sample_data, p0=[estimated_a, estimated_m, estimated_d])\n\ndef bekkers_func(x, a, m, d):\n    return bekkers(x, a, m, d)\n\ndef kstest(sample_data, func):\n    x = np.linspace(range_start, range_end, 100)\n    y_fit = func(x, *params_est)\n    y_obs = sample_data\n    ks_stat, p_value = stats.kstest(y_obs, y_fit, 'norm')\n    return ks_stat, p_value\n\nks_stat, p_value = kstest(sample_data, func)\nprint(f'KS stat: {ks_stat:.2f}, p-value: {p_value:.2f}')", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "def residual_chi_square(sample_data, estimated_a, estimated_m, estimated_d):\n    x = np.linspace(range_start, range_end, 100)\n    y_hat = bekkers(x, estimated_a, estimated_m, estimated_d)\n    y_true = sample_data\n    chi_square_residual = np.sum((y_true - y_hat)**2)\n    return chi_square_residual\n\ndef ks_test(sample_data, estimated_a, estimated_m, estimated_d):\n    x = np.linspace(range_start, range_end, 100)\n    y_hat = bekkers(x, estimated_a, estimated_m, estimated_d)\n    y_true = sample_data\n    ks_test_statistic, p_value = stats.kstest(y_true, y_hat, 'norm')\n    return ks_test_statistic, p_value\n\ndef test_ks_test(sample_data, estimated_a, estimated_m, estimated_d):\n    ks_test_statistic, p_value = ks_test(sample_data, estimated_a, estimated_m, estimated_d)\n    if p_value < 0.05:\n        result = True\n    else:\n        result = False\n    return result\n\nresult = test_ks_test(sample_data, estimated_a, estimated_m, estimated_d)\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "df['Time'] = pd.to_datetime(df['Time'])\ndf.sort_values('Time', inplace=True)\ndf['A'] = df['A'].diff()\ndf['A'].apply(lambda x: x.loc[x.abs().argsort()[-2]])\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "interp = scipy.interpolate.LinearInterpolation(x, y)\nresult = interp(eval[0])\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "def multinomial_log_likelihood(params):\n    # params should be the weights of the categories\n    # we return the negative of the log likelihood\n    # this is because scipy.optimize.minimize solves for minimizing a function\n    # we want to maximize the likelihood of the data given the parameters\n    # so we minimize the negative of the likelihood\n    likelihoods = np.array([1/w**abs(a-c) for c,w in zip(params[:-1],params[1:])])\n    return -np.sum(np.log(likelihoods))\n\n# we want to maximize the likelihood of the data given the parameters\n# so we minimize the negative of the likelihood\nparams_guess = np.ones_like(a.columns[1:], dtype=float)\nparams_guess[0] = 0\nparams_guess[1:] = 0.1\nparams_guess[1:] = 0.2\nparams_guess[1:] = 0.3\nparams_guess[1:] = 0.4\nparams_guess[1:] = 0.5\nparams_guess[1:] = 0.6\nparams_guess[1:] = 0.7\nparams_guess[1:] = 0.8\nparams_guess[1:] = 0.9\nparams_guess[1:] = 1.0\nparams_guess[1:] = 1.1\nparams_guess[1:] = 1.2\nparams_guess[1:] = 1.3\nparams_guess[1:] = 1.4\nparams_guess[1:] = 1.5\nparams_guess[1:] = 1.6\nparams_guess[1:] = 1.7\nparams_guess[1:] = 1.8\nparams_guess[1:] = 1.9\nparams_guess[1:] = 2.0\nparams_guess[1:] = 2.1\nparams_guess[1:] = 2.2\nparams_guess[1:] = 2.3\nparams_guess[1:] = 2.4\nparams_guess[1:] = 2.5\nparams_guess[1:] = 2.6\nparams_guess[1:] = 2.7\nparams_guess[1:] = 2.8\nparams_guess[1:] = 2.9\nparams_guess[1:] = 3.0\nparams_guess[1:] = 3.1\nparams_guess[1:] = 3.2\nparams_guess[1:] = 3.3\nparams_guess[1:] = 3.4\nparams_guess[1:] = 3.5\nparams_guess[1:] = 3.6\nparams_guess[1:] = 3.7\nparams_guess[1:] = 3.8\nparams_guess[1:] = 3.9\nparams_guess[1:] = 4.0\nparams_guess[1:] = 4.1\nparams_guess[1:] = 4.2\nparams_guess[1:] = 4.3\nparams_guess[1:] = 4.4\nparams_guess[1:] = 4.5\nparams_guess[1:] = 4.6\nparams_guess[1:] = 4.7\nparams_guess[1:] = 4.8\nparams_guess[1:] = 4.9\nparams_guess[1:] = 5.0\nparams_guess[1:] = 5.1\nparams_guess[1:] = 5.2\nparams_guess[1:] = 5.3\nparams_guess[1:] = 5.4\nparams_guess[1:] = 5.5\nparams_guess[1:] = 5.6\nparams_guess[1:] = 5.7\nparams_guess[1:] = 5.8\nparams_guess[1:] = 5.9\nparams_guess[1:] = 6.0\nparams_guess[1:] = 6.1\nparams_guess[1:] = 6.2\nparams_guess[1:] = 6.3\nparams_guess[1:] = 6.4\nparams_guess[1:] = 6.5\nparams_guess[1:] = 6.6\nparams_guess[1:] = 6.7\nparams_guess[1:] = 6.8\nparams_guess[1:] = 6.9", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fminbound(e, pmin, pmax, args=(x,y))\nresult = popt\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "\n# find the indices of the elements in the array that are less than the nth element in the array\n# and greater than the (n-1)th element in the array\n# then find the minimum of these indices\n\n# find the indices of the elements in the array that are less than the nth element in the array\n# and greater than the (n-1)th element in the array\n# then find the maximum of these indices\n\n# find the minimum of these indices\nmin_index = np.min(min_indices)\n\n# find the maximum of these indices\nmax_index = np.max(max_indices)\n\n# find the elements at the minimum and maximum indices\nmin_element = arr[min_index]\nmax_element = arr[max_index]\n\n# find the elements in the array that are less than the max_element and greater than the min_element\n# then find the minimum of these elements\n\n# find the elements in the array that are less than the max_element and greater than the min_element\n# then find the maximum of these elements\n\n# find the minimum of these elements\nmin_element_2 = np.min(elements_2)\n\n# find the maximum of these elements\nmax_element_2 = np.max(elements_2)\n\n# find the elements in the array that are less than the max_element_2 and greater than the min_element_2\n# then find the minimum of these elements\n\n# find the elements in the array that are less than the max_element_2 and greater than the min_element_2\n# then find the maximum of these elements\n\n# find the minimum of these elements\nmin_element_3 = np.min(elements_3)\n\n# find the maximum of these elements\nmax_element_3 = np Together, you have to find the minimum and maximum of the following elements in the array:\n\n# find the minimum of these elements\nmin_element_4 = np.min(elements_4)\n\n# find the maximum of these elements\nmax_element_4 = np.max(elements_4)\n\n# find the elements in the array that are less than the max_element_4 and greater than the min_element_4\n# then find the minimum of these elements\n\n# find the elements in the array that are less than the max_element_4 and greater than the min_element_4\n# then find the maximum of these elements\n\n# find the minimum of these elements\nmin_element_5 = np.min(elements_5)\n\n# find the maximum of these elements\nmax_element_5 = np.max(elements_5)\n\n# find the elements in the array that are less than the max_element_5 and greater than the min_element_5\n# then find the minimum of these elements\n\n# find the elements in the array that are less than the max_element_5 and greater than the min_element_5\n# then find the maximum of these elements\n\n# find the minimum of these elements\nmin_element_6 = np.min(elements_6)\n\n# find the maximum of these elements\nmax_element_6 = np.max(elements_6)\n\n# find the elements in the array that are less than the max_element_6 and greater than the min_element_6\n# then find the minimum of these elements\n\n# find the elements in the array that are less than the max_element_6 and greater than the min_element_6\n# then find the maximum of these elements\n\n# find the minimum of these elements\nmin_element_7 = np.min(elements_7)\n\n# find the maximum of these elements\nmax_element_7 = np.max(elements_7)\n\n# find the elements in the array that are less than the max_element_7 and greater than the min_element_7\n# then find the minimum of these elements\n\n# find the elements in the array that are less than the max_element_7 and greater than the min_element_7\n# then find the maximum of these elements\n\n# find the minimum of these elements\nmin_element_8 = np.min(elements_8)\n\n# find the maximum of these elements\nmax_element_8 = np.max(elements_8)\n\n# find the elements in the array that are less than the max_element_8 and greater than the min_element_8\n# then find the minimum of these elements\n\n# find the elements in the array that are less than the max_element_8 and greater than the min_element_8\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "\ndef find_relative_extrema(arr, n):\n    # get the max of the current element and its neighbours\n    max_val = max(arr[0][:n] + arr[0][-n:])\n    max_indices = [i for i in range(len(arr[0])) if arr[0][i] == max_val]\n\n    # get the min of the current element and its neighbours\n    min_val = min(arr[1][:n] + arr[1][-n:])\n    min_indices = [i for i in range(len(arr[1])) if arr[1][i] == min_val]\n\n    # return the indices of the max and min elements\n    return [max_indices, min_indices]\n\nresult = find_relative_extrema(arr, n)\nprint(result)\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "df = df.select_dtypes(include=[np.numberc])\ndf = df.apply(lambda x: x.transform(), axis=0)\ndf = df.apply(lambda x: x.drop_duplicates(), axis=0)\ndf = df.apply(lambda x: x.dropna(), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y<=3 else 'A'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y>=-3 else 'A'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='A' else 'B'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='B' else 'C'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='C' else 'D'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='D' else 'E'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='E' else 'F'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='F' else 'G'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='G' else 'H'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='H' else 'I'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='I' else 'J'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='J' else 'K'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='K' else 'L'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='L' else 'M'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='M' else 'N'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='N' else 'O'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='O' else 'P'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='P' else 'Q'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='Q' else 'R'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='R' else 'S'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='S' else 'T'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='T' else 'U'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='U' else 'V'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='V' else 'W'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='W' else 'X'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='X' else 'Y'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='Y' else 'Z'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='Z' else '10'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if y=='10' else '11'), axis=0)\ndf = df.apply(lambda x: x.apply(lambda y: y if", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "\n# Convert Bunch object to Pandas DataFrame\ndata_df = pd.DataFrame(data=np.array(data.data), columns=data.feature_names)\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "\n# Convert the Bunch object to a Pandas DataFrame\ndata_df = pd.DataFrame(data=np.array(data.data), columns=data.feature_names)\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "\n# Assuming the data is a dictionary of lists\ndata_dict = data.data\n\n# Convert it into a pandas DataFrame\ndf = pd.DataFrame(data_dict)\n\n# Now df is a pandas DataFrame with the same data as the original Bunch object\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": "\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']], columns=np.append(data['feature_names'], 'target'))\n    return df\n    ### END SOLUTION\nsolve(data)\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# First, we need to create a dictionary to map each unique element in the list to a unique column\nelement_dict = {}\nfor i in range(len(df.Col3)):\n    for j in range(len(df.Col3[i])):\n        if df.Col3[i][j] not in element_dict:\n            element_dict[df.Col3[i][j]] = len(element_dict) + 1\n\n# Now we can create the new columns\nfor i in element_dict.keys():\n    df[i] = 0\n\n# And finally, we can map each element to its column\nfor i in range(len(df)):\n    for j in element_dict.keys():\n        if j in df.Col3[i]:\n            df[element_dict[j]][i] = 1\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# First, we need to flat the list of strings into a long format\ndf_long = pd.DataFrame(df['Col3'].apply(pd.Series).stack().reset_index(drop=True))\n\n# Then we can use pivot_table to get the one-hot-encoded values\ndf_out = df_long.pivot_table(index='Col1', columns='Col2', aggfunc='size')\n\n# Now we need to fill the NaN values with 0\ndf_out.fillna(0, inplace=True)\n\n# And finally, we can drop the Col2 because we don't need it anymore\ndf_out.drop('Col2', axis=1, inplace=True)\n\ndf_out\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# First, we need to get the unique elements in the last column\ncol_last = df.columns[-1]\nunique_elements = df[col_last].unique()\n\n# Then we create a new DataFrame with the same shape as df but with zeros\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(unique_elements))), columns=df.columns)\n\n# We then loop over each row in df\nfor i in range(df.shape[0]):\n    # We get the elements in the last column of the row\n    elements = df.loc[i, col_last]\n    # We loop over each unique element in the last column\n    for j in range(len(unique_elements)):\n        # If the element is in the last column, we set the value to 1\n        if elements[j] in unique_elements:\n            df_out.loc[i, unique_elements[j]] = 1\n\n# We return the DataFrame df_out\ndf_out\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\ndef one_hot_encode(df):\n    df_out = pd.get_dummies(df.Col3)\n    df_out.columns.name = 'Col3'\n    df_out.columns = [f'Col3_{i}' for i in range(len(df.Col3.unique()))]\n    return df_out\n\ndf_out = one_hot_encode(df)\ndf_out.index = df.index\ndf_out.columns.name = 'Col1'\ndf_out = df_out.groupby(level='Col1').sum()\ndf_out.columns.name = 'Col2'\ndf_out = df_out.groupby(level='Col2').sum()\ndf_out.columns.name = 'Col3'\ndf_out = df_out.groupby(level='Col3').sum()\n\ndf_out\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# First, we need to get the unique elements from the last column\nlast_col_unique = df.loc[0, 'Col3']\n\n# Then we create a dictionary to map each unique element to its corresponding column\ncol_map = {element: column for column, element in enumerate(last_col_unique)}\n\n# Now we apply this dictionary to each element in the last column\ndf['Col3'] = df['Col3'].apply(lambda x: [col_map.get(element, 0) for element in x])\n\n# Finally, we convert the list of lists into a DataFrame\ndf_out = pdumerate(df['Col3'].apply(pd.Series))\n\n# Now df_out has the one-hot-encoded columns\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "predicted_test= svmmodel.fit(X, y).predict(x_test)\npredicted_test_scores= svmmodel.decision_function(x_test)\nproba = [1 / (1 + np.exp(-x)) for x in predicted_test_scores]\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "\nmodel.fit(X, y)\npredicted_test = model.predict(x_predict)\npredicted_test_scores = model.decision_function(x_predict)\nproba = [1 / (1 + np.exp(-x)) for x in predicted_test_scores]\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "\n# create a mask to select the non-zero elements of the sparse matrix\nmask = transform_output.data != 0\n\n# apply the mask to the original dataframe\ndf_transformed = df_origin[mask]\n\n# create a sparse matrix with the same shape as the original dataframe\ntransform_output = csr_matrix((transform_output.data, df_transformed.shape), shape=df_transformed.shape)\n\n# concatenate the original dataframe and the sparse matrix\ndf = pdordinate(df_transformed, transform_output)\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "\n# Convert the csr_matrix to a DataFrame\ndf_transformed = pd.DataFrame(np.zeros(transform_output.shape), columns=df_origin.columns)\ndf_transformed.iloc[:, :] = df_origin\n\n# Merge the two DataFrames\ndf = pdordinate.merge(df_origin, df_transformed, how='outer')\n\n# Now df is your final DataFrame\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": "\n    df_new = pd.concat([df, transform_output], axis=1)\n    return df_new\n    ### END SOLUTION\nsolve(df_origin, transform_output)\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\nclf.steps.append(('delete_step', None))\nclf.steps.remove(('reduce_dim', PCA()))\nclf.steps.remove(('poly', PolynomialFeatures()))\nclf.steps.remove(('svm', SVC()))\nclfestudi.com/2016/06/06/how-to-delete-a-step-in-a-sklearn-pipeline-object/\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.append(('new_step', None))\nclf.steps.append(('insert_step', None))\nclf.steps.", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.append(('delete_step', None))\nclf.steps.remove(('reduce_poly', None))\nclf.steps.remove(('dim_svm', None))\nclf.steps.remove(('sVm_233', None))\nclfordinate_var_clf.steps.append(('new_step', 'new_estimator'))\nclf.fit(X_train, y_train)\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\nclf.steps.append(('new_step', SomeCustomTransformer()))\nclf\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\nclf.fit(X_train, y_train)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PolynomialFeatures(2)))\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# define the model\nmodel = xgb.XGBRegressor()\n\n# define the grid of parameters to search\nparamGrid = {\n    'objective': ['binary:logistic', 'reg:linear', 'binary:logistic'],\n    'eta': [0.0, 0.1, 0.2, 0.3],\n    'min_child_weight': [1, 2, 3, 4, 5],\n    'subsample': [0.5, 0.75, 1],\n    'colsample_bylevel': [0.3, 0.5, 0.75],\n    'colsample_bytree': [0.3, 0.5, 0.75],\n    'colsample_bylevel': [0.3, 0.5, 0 competing_cols_bias=None,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel': 0.3,\n    'eta': 0.0,\n    'min_child_weight': 1,\n    'subsample': 0.5,\n    'colsample_bylevel': 0.3,\n    'colsample_bytree': 0.3,\n    'colsample_bylevel", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nmodel = xgb.XGBRegressor()\n\n# Define the grid of parameters to search\nparamGrid = {\n    'objective': ['binary:logistic', 'reg:linear', 'binary:logistic'],\n    'eta': [0.0, 0.1, 0.2, 0.3],\n    'min_child': [1, 2, 3, 4, 5],\n    'subsample': [0.5, 1.0, 1.25, 1.5, 1.75, 2.0],\n    'colsample_bylevel': [0.3, 0.5, 0.7, 1.0],\n    'colsample_bytree': [0.3, 0.5, 0.7, 1.0],\n    'colsample_bylevel': [0.3, 0.5, 0 competing_cols_to_drop, 1.0],\n    'colsample_bynode': [0.3, 0.5, 0.7, 1.0],\n    'colsample_byspread': [0.3, 0.5, 0.7, 1.0],\n    'colsample_byspace': [0.3, 0.5, 0.7, 1.0],\n    'colsample_byarch': [0.3, 0.5, 0.7, 1.0],\n    'colsample_bycard': [0.3, 0.5, 0.7, 1.0],\n    'colsample_bychannel': [0.3, 0.5, 0.7, 1.0],\n    'colsample_bychannel': [0.3, 0estudi, 0.5, 0.7, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5, 3.75, 4.0, 4.25, 4.5, 4.75, 5.0, 5.25, 5.5, 5.75, 6.0, 6.25, 6.5, 6.75, 7.0, 7.25, 7.5, 7.75, 8.0, 8.25, 8.5, 8.75, 9.0, 9.25, 9.5, 9.75, 10.0, 10.25, 10.5, 10.75, 11.0, 11.25, 11.5, 11.75, 12.0, 12.25, 12.5, 12.75, 13.0, 13.25, 13.5, 13.75, 14.0, 14.25, 14.5, 14.75, 15.0, 15.25, 15.5, 15.75, 16.0, 16.25, 16.5, 16.75, 17.0, 17.25, 17.5, 17.75, 18.0, 18.25, 18.5, 18.75, 19.0, 19.25, 19.5, 19.75, 20.0, 20.25, 20.5, 20.75, 21.0, 21.25, 21.5, 21.75, 22.0, 22.25, 22.5, 22.75, 23.0, 23.25, 23.5, 23.75, 24.0, 24.25, 24.5, 24.75, 25.", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "for train, test in cv:\n    logreg.fit(X[train], y[train])\n    proba.append(logreg.predict_proba(X[test]))\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train, test in cv:\n    logreg.fit(X[train], y[train])\n    proba.append(logreg.predict_proba(X[test]))\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": "\n    ...\n    return inversed\nsolve(data, scaler, scaled)\n", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "\n# fit the model\nmodel.fit(X, y)\n\n# get the name of the model\nmodel_name = model.__class__.__name__\n\n# print the model name\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')\n", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "\n# fit the model\nmodel.fit(X, y)\n\n# get the name of the model\nmodel_name = model.__class__.__name__\n\n# print the model name\nprint(f'Name model: {model_name} , Mean score: {model.score(X, y)}')\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "scores = cross_val_score(model, X, y, cv=5)\nmean_score = scores.mean()\nmodel_name = model\nprint(f'Name model: {model_name}, Mean score: {mean_score}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"]\n", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transformer_\n", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "\n# fit the pipeline\npipe.fit(data, target)\n\n# get the intermediate data state\nselect_out = pipe.steps[0][1].transform(data)\n", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\nclf = GridSearchCV(bc, param_grid, cv=5)\nclf.fit(X_train, y_train)\nproba = clf.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\n# Fit the model\nregressor.fit(X, y)\n\n# Predict\ny_pred = regressor.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\n# Fit the model\nregressor.fit(X, y)\n\n# Predict\ny_pred = regressor.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ndef main():\n    df = pd.read_csv('your_file.csv')\n    df['column_name'] = df['column_name'].apply(preprocess)\n\n    tfidf = TfidfVectorizer(preprocessor=preprocess)\n    X = tfidf.fit_transform(df['column_name'])\n\n    return X\n\nX = main()\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Apply the preprocessing.scale function to the DataFrame\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=df.columns, index=df.index)\n\n# Check if the DataFrame is the same as the original\nassert np.all(df == df_out)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Apply the preprocessing.scale function to the DataFrame\ndf_out = pd.DataFrame(preprocessing.scale(data), columns=df.columns, index=df.index)\n\n# Check if the output is the same as the input\nassert np.all(np.abs(df_out - df) < 1e-9)\n\n# Print the output\nprint(df_out)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nselected_columns = X.columns[model.get_support() == True]\ncolumn_names = selected_columns\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nselected_columns = X.columns[model.get_support()==True]\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\ndistances = km.transform(X)\nclosest_50_samples = X[np.argpartition(distances, 50)[:50]]\n", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\ndistances = km.transform(X)\nclosest_50_samples = X[np.argpartition(distances, 50)[:50]]\n", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\ndistances = km.transform(X)\nclosest_100_samples = X[np.argpartition(distances, 100)[:100]]\n", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": "\n    km.fit(X)\n    centers = km.cluster_centers_\n    sorted_centers = np.argsort(centers)[::-1]\n    closest_centers = centers[sorted_centers[:p]]\n    return closest_centers\n    ### END SOLUTION\n\nget_samples(p, X, km)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# apply one hot encoding\nX_train = pd.get_dummies(X_train)\n\n# fit the model\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# One hot encoding\nX_train = pd.get_dummies(X_train)\n\n# Fit the classifier\nclf = GradientBoostingClassifier(learning_rate=0.01, max_depth=8, n_estimators=50).fit(X_train, y_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\nsvr = SVR(kernel='rbf')\nsvr.fit(X, y)\npredict = svr.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn.svm import SVC\nsvc = SVC(kernel='rbf')\nsvc.fit(X, y)\npredict = svc.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\nsvr = SVR(kernel='rbf')\nsvr.fit(X, y)\npredict = svr.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\nsvr = SVR(kernel='poly', degree=2)\nsvr.fit(X, y)\npredict = svr.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarity(queries, documents):\n    tfidf_matrix = tfidf.fit_transform(documents)\n    cosine_similarities_of_queries = []\n    for i in range(len(queries)):\n        query_vector = tfidf_matrix[i]\n        cosine_similarity = np.sqrt(np.einsum(np.multiply(query_vector, query_vector), np.multiply(tfidf_matrix[i], tfidf_matrix[i])))\n        cosine_similarities_of_queries.append(cosine_similarity)\n    return cosineumerate(cosine_similarities_of_queries)\n\ncosine_similarities_of_queries = get_cosine_similarity(queries, documents)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarity_of_queries(queries, documents):\n    tfidf_matrix = TfidfVectorizer().fit_transform(documents)\n    cosine_similarities_of_queries = []\n    for i in range(len(queries)):\n        query_vector = tfidf_matrix[i]\n        cosine_similarity = np.sqrt(np.einsum(query_vector, query_vector))\n        cosine_similarities_of_queries.append(cosine_similarity)\n    return cosineumerate(cosine_similarities_of_queries)\n\ncosine_similarity_matrix = get_cosine_similarity_of_queries(queries, documents)\ncosine_similarity_matrix = np.array(cosine_similarity_matrix).reshape(3,5)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": " ###\n    cosine_similarities_of_queries = []\n    for i in range(len(queries)):\n        cosine_similarity = np.sqrt(np.sum(np.square(tfidf.transform([queries[i]]))))\n        cosine_similarities_of_queries.append(cosine_similarity)\n    return cosineumerate(cosine_similarities_of_queries)\n    ### END SOLUTION ###\nsolve(queries, documents)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# First, we need to flatten the list of lists into a 2D array\ndf = pd.DataFrame(new_features)\n\n# Then, we can use the OneHotEncoder from sklearn to convert the categorical variables into binary dummy/indicator variables\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False)\ndf_encoded = encoder.fit_transform(df)\n\n# Now, we need to convert the list of lists into a 2D array\ndf_encoded = np.array(df_encoded)\n\n# Now, df_encoded is a 2D array where each row represents a sample and each column represents a feature. The value in the cell represents the presence of the feature in the sample.\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# First, we need to flatten the list of lists into a 2D array\nf_array = np.array(f).flatten()\n\n# Then, we can use the OneHotEncoder from sklearn to convert the categorical variables into a binary format\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False)\nf_array_encoded = encoder.fit_transform(f_array.reshape(-1,1))\n\n# Now, we need to convert the list of lists into a DataFrame\nf_df = pdumerate(f_array_encoded.T)\n\n# Now, we can use the `get_dummies` function from pandas to create the binary format\nf_df = pd.get_dummies(f_df)\n\n# Now, `f_df` is a DataFrame where each column is a feature and each row is a sample\n\n# Now, we can use this DataFrame to perform feature selection\nfrom sklearn.feature_selection import SelectKBest\n\nselector = SelectKBest(k=2)\nf_df_selected = selector.fit_transform(f_df, y)\n\n# Now, `f_df_selected` is a DataFrame where each column is a feature and each row is a sample\n# And `y` is the target variable\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# First, we need to flatten the list of lists into a 2D array\nflat_features = np.array(features).flatten()\n\n# Then, we need to one-hot encode the categorical variables\nnew_features = pd.get_dummies(flat_features)\n\n# Now, new_features is a 2D array where each row is a sample and each column is a feature\n# The value in the cell is 1 if the feature is in the sample and 0 otherwise\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": "\n    df = pdply(features, function(x) {\n        x <- as.matrix(x)\n        x[is.na(x)] <- 0\n        return(x)\n    })\n    return(df)\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# First, we need to flatten the list of lists into a 1D list of features\nflat_features = [item for sublist in features for item in sublist]\n\n# Then, we need to create a dictionary where the keys are the unique items in the list and the values are the boolean values indicating whether the item is in the list\nitem_dict = {item: True for item in flat_features}\n\n# Now, we can convert the list of lists into a 2D numpy array\nnew_features = np.array(list(item_dict.keys())*len(features))\n\n# Now, we can use this new_features to create a feature selection utility\nfrom sklearn.feature_selection import SelectKBest\nnew_features_selected = SelectKBest(k=2).fit_transform(new_features, features)\n\n# Now, new_features_selected is a 2D array where each row is a sample and each column is a feature\n# We can also use this new_features_selected to create a feature selection utility\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "\ndef agglomerative_clustering(data_matrix, n_clusters):\n    # Initialize the distance matrix\n    distance_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n    # Perform hierarchical clustering\n    agglomerative_clustering = sklearn.cluster.AgglomerativeClustering(n_clusters=n_clusters)\n    agglomerative_clustering.fit_transform(distance_matrix)\n\n    # Get the cluster labels\n    cluster_labels = agglomerative_clustering.labels_\n\n    return cluster_labels\n\n# Test the function\nn_clusters = 2\ncluster_labels = agglomerative_clustering(data_matrix, n_clusters)\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "\n# Assuming that the data_matrix is the distance matrix between the professors\n# and we want to determine the number of clusters\n\n# Flatten the 2D array into a 1D array\ndata_matrix_flattened = data_matrix.flatten()\n\n# Apply agglomerative clustering\nclf = sklearn.cluster.AgglomerativeClustering(n_clusters=2)\nclusters = clf.fit_predict(data_matrix_flattened)\n\n# Create a dictionary to map the cluster labels to the original data_matrix indices\ncluster_map = {label: index for index, label in enumerate(clusters)}\n\n# Map the cluster labels to the original data_matrix indices\ncluster_labels = [cluster_map[label] for label in clusters]\n\n# Replace the original labels with the cluster labels\ndata_matrix[cluster_labels, :] = np.nan\n\n# Now, the data_matrix is the distance matrix between the professors, with the\n# professors labeled by their cluster\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "\ndef agglomerative_clustering(simM, n_clusters):\n    # Initialize the distance matrix\n    dist_matrix = pd.DataFrame(simM)\n\n    # Create a list of zeros to store the cluster labels\n    cluster_labels = np.zeros(len(dist_matrix))\n\n    # Perform agglomerative clustering\n    for i in range(len(dist_matrix)):\n        for j in range(i+1, len(dist_matrix)):\n            if dist_matrix[i][j] < dist_matrix[j][i]:\n                cluster_labels[i] = j\n                cluster_labels[j] = i\n            else:\n                cluster_labels[j] = i\n\n    return cluster_labels\n\n# Perform agglomerative clustering\ncluster_labels = agglomerative_clustering(simM, 2)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster import hierarchy\n\nZ = hierarchy.linkage(data_matrix, 'complete')\ncluster_labels = hierarchy.fcluster(Z, 2, criterion='max')\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster import hierarchy\n\nZ = hierarchy.linkage(data_matrix, 'complete')\ncluster_labels = hierarchy.fcluster(Z, 2, criterion='distance')\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "\n# Flatten the 2D matrix into a 1D array\nsimM_flat = np.array(simM).flatten()\n\n# Perform hierarchical clustering\ncluster_labels = scipy.cluster.hierarchy.fcluster(simM_flat, 2, criterion='distance')\n\n# Create a dictionary to map the cluster labels back to the original matrix\ncluster_map = {label: i for i, label in enumerate(cluster_labels)}\n\n# Apply the cluster map to the original matrix\nsimM_clusters = np.array([cluster_map[label] for label in cluster_labels])\n\n# Reshape the matrix into the original shape\nsimM_reshaped = simM_clusters.reshape(len(simM), len(simM[0]))\n\n# Check if the clustering is correct\nassert np.all(simM_reshaped == simM_clusters)\n\n# Print the clusters\nprint(simM_clusters)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from scipy.stats import boxcox\n\n# Apply Box-Cox transformation\nbox_cox_data, _ = boxcox(data, 0)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pd.DataFrame(pt.fit_transform(data), columns=data.columns)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "\n# Split the text into words\nwords = text.split()\n\n# Count the words\nword_counts = words.count(' ')\n\n# Create a dictionary of words and their counts\nword_dict = dict(words.split())\n\n# Create a function to replace words with their counts\ndef replace_words_with_counts(text):\n    for word, count in word_dict.items():\n        text = text.replace(word, str(count))\n    return text\n\n# Apply the function to the text\ntransformed_text = replace_words_with_counts(text)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "\n# Splitting the dataset into 80% training set and 20% testing set\nfrom sklearn.model_selection import train_test_split\n\nx = dataset.iloc[:,:-1] # x contains all columns except last one\ny = dataset.iloc[:,-1] # y contains last column\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "\n# Splitting the data\nnp.random.seed(42)\nmask = np.random.rand(len(data)) < 0.8\nx_train = data[mask]\nx_test = data[~mask]\n\n# Splitting the target\ny_train = x_train.pop('target')\ny_test = x_test.pop('target')\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "\n# Splitting the dataset into training set and testing set\nfrom sklearn.model_selection import train_test_split\n\nx = dataset.iloc[:,:-1] # x contains all columns except last one\ny = dataset.iloc[:,-1] # y contains last column\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    from sklearn.model_selection import train_test_split\n    x = data.iloc[:,:-1]\n    y = data.iloc[:,-1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, x_test, y_train, y_test\n    ### END SOLUTION\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n    ##\n   ", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "df['mse'] = df['mse'].apply(lambda x: x if x > 0 else 0) # this line is to filter out the negative values\ndf['mse'] = df['mse'].apply(lambda x: x if x > 0 else 0) # this line is to filter out the negative values\n\n# Reshape the data\nX = df['mse'].values.reshape(-1,1)\n\n# Apply KMeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n\n# Assign the labels to the original data\ndf['cluster'] = kmeans.labels_\n\n# Print the labels\nprint(df['cluster'].value_counts())\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\n# assuming df is your DataFrame and it has 'mse' as your target\ndf['mse'] = df['mse'].apply(lambda x: x.replace('[', '').replace(']', ''))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\"'\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\",\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\" \", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\"/\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\"-\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\":\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\" \", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse'].apply(lambda x: x.replace(\".\", \"\"))\ndf['mse'] = df['mse", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "\n# Apply the linear model with L1 penalty\nclassifier = LinearSVC(C=1.0, penalty='l1', dual=False)\nclassifier.fit(X, y)\n\n# Get the support vector\nsupport_vector = classifier.coef_\nsupport_vector_index = np.where(support_vector != 0)\n\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[support_vector_index]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "\n# Select the best features\nselector = 'lbfgs' # 'lbfgs' stands for least biases framework gradient boosting\n\n# Select the best features\nclf = LinearSVC(C=1.0, dual=True, penalty='l1', tol=0.1, random_state=42)\nclf.fit(X, y)\n\n# Get the best features\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[featureSelector.get_support()]\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": "\n    clf = LinearSVC(C=1.0, penalty='l1', dual=False, random_state=None)\n    clf.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[clf.coef_ != 0]\n    return selected_feature_names\n    ### END SOLUTION\nsolve(corpus, y, vectorizer, X)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\n\n# Get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# Create a mask to filter out the words that are not in the vocabulary\nmask = np.array([word in vocabulary for word in vectorizer.get_feature_names_out()])\n\n# Create a masked version of X\nX_masked = X[..., mask]\n\n# Apply the mask to X\nX_masked = np.where(mask, X_masked, 0)\n\n# Print the result\nprint(X_masked)\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\n\n# Get the vocabulary in the same order as the original\nvocabulary = vectorizer.get_feature_names_out()\n\n# Create a mask to keep only the words in the original vocabulary\nmask = np.array([word in vocabulary for word in vectorizer.get_feature_names_out()])\n\n# Create a sparse matrix with the words in the original vocabulary\n# and the words in the vocabulary of the original vectorizer\nX_sparse = sparse.csr_matrix((np.ones(X.shape[1]), np.where(mask)[0]), dtype=np.int8)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pdumerate(zip(vectorizer.get_feature_names_out(), X.toarray().T[0]), columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\ndf = pd.DataFrame(X.toarray().T[0], columns=vocabulary)\n\n# Create a DataFrame with the words in the original vocabulary\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\n\n# get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# get the counts\ncounts = X.toarray().sum(axis=0)\n\n# get the indices of the vocabulary in the original corpus\nindices = [vocabulary.index(word) for word in vocabulary if word in corpus[0]]\n\n# get the counts of the words in the vocabulary\nvocabulary_counts = counts[indices]\n\n# get the counts of the words in the original corpus\noriginal_counts = counts[np.where(X.toarray() > 0)]\n\n# get the word counts in the original corpus\nword_counts = original_counts[indices]\n\n# get the word counts in the vocabulary\nvocabulary_word_counts = vocabulary_counts[indices]\n\n# get the word counts in the original corpus that are in the vocabulary\nvocabulary_word_counts_in_original_corpus = word_counts[np.where(vocabulary_word_counts > 0)]\n\n# get the word counts in the original corpus that are in the vocabulary and in the original corpus\nvocabulary_word_counts_in_original_corpus_in_vocabulary = vocabulary_word_counts_in_original_corpus[np.where(vocabulary_word_counts_in_original_corpus > 0)]\n\n# get the word counts in the original corpus that are in the vocabulary and in the original corpus that are in the vocabulary\nvocabulary_word_counts_in_original_corpus_in_vocabulary_in_original_corpus = vocabulary_word_counts_in_original_corpus[npipage(vocabulary_word_counts_in_original_corpus_in_vocabulary > 0)]\n\n# get the word counts in the original corpus that are in the vocabulary and in the original corpus that are in the vocabulary that are in the original corpus\nvocabulary_word_counts_in_original_corpus_in_vocabulary_in_vocabulary_in_original_corpus = vocabulary_word_counts_in_original_corpus[np.where(vocabulary_word_counts_in_original_corpus_in_vocabulary_in_original_corpus > 0)]\n\n# get the word counts in the original corpus that are in the vocabulary and in the original corpus that are in the vocabulary that are in the original corpus that are in the original corpus\nvocabulary_word_counts_in_original_corpus_in_vocabulary_in_vocabulary_in_vocabulary_in_original_corpus = vocabulary_word_counts_in_original_corpus[np.where(vocabulary_word_counts_in_original_corpus_in_vocabulary_in_vocabulary_in_original_corpus > 0)]\n\n# get the word counts in the original corpus that are in the vocabulary and in the original corpus that are in the vocabulary that are in the original corpus that are in the original corpus that are in the original corpus\nvocabulary_word_counts_in_original_corpus_in_vocabulary_in_vocabulary_in_vocabulary_in_original_corpus = vocabulary_word_counts_in_original_corpus[np.where(vocabulary_word_counts_in_original_corpus_in_vocabulary_in_vocabulary_in_vocabulary_in_original_corpus > 0)]\n\n# get the word counts in the original corpus that are in the vocabulary and in the original corp", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\n\n# get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# get the binary representation of X\nX_binary = np.array(X.toarray() > 0, dtype=int)\n\n# get the vocabulary in the same order\nvocabulary_sorted = [v for v in vocabulary if v in vectorizer.get_params()['vocabulary']]\n\n# get the binary representation of the vocabulary\nvocabulary_binary = [v in vocabulary_sorted for v in vocabulary_sorted]\n\n# get the binary representation of X in the same order\nX_binary_sorted = [X_binary[i] for i in np.argsort(vectorizer.get_params()['vocabulary'])]\n\n# get the binary representation of X in the original order\nX_binary_original = X_binary\n\n# get the original vocabulary\nvocabulary_original = [v for v in vocabulary if v not in vectorizer.getumerate_params()['vocabulary']]\n\n# get the original binary representation of X\nX_original_binary = [X_binary[i] for i in np.argsort(vocabulary_original)]\n\n# get the original vocabulary\nvocabulary_original_sorted = [v for v in vocabulary if v in vocabulary_original]\n\n# get the original binary representation of X in the original order\nX_original_binary_sorted = [X_original_binary[i] for i in npumerate(vocabulary_original_sorted)]\n\n# get the original vocabulary in the original order\nvocabulary_original_sorted = [v for v in vocabulary if v in vocabulary_original]\n\n# get the original binary representation of X in the original order\nX_original_binary_sorted = [X_original_binary[i] for i in np.argsort(vocabulary_original_sorted)]\n\n# get the original vocabulary in the original order\nvocabulary_original_sorted = [v for v in vocabulary if v in vocabulary_original]\n\n# get the original binary representation of X in the original order\nX_original_binary_sorted = [X_original_binary[i] for i in np.argsort(vocabulary_original_sorted)]\n\n# get the original vocabulary in the original order\nvocabulary_original_sorted = [v for v in vocabulary if v in vocabulary_original]\n\n# get the original binary representation of X in the original order\nX_original_binary_sorted = [X_original_binary[i] for i in np.argsort(vocabulary_original_sorted)]\n\n# get the original vocabulary in the original order\nvocabulary_original_sorted = [v for v in vocabulary if v in vocabulary_original]\n\n# get the original binary representation of X in the original order\nX_original_binary_sorted = [X_original_binary[i] for i in np.argsort(vocabulary_original_sorted)]\n\n# get the original vocabulary in the original order\nvocabulary_original_sorted = [v for v in vocabulary if v in vocabulary_original]\n\n# get the original binary representation of X in the original order\nX_original_binary_sorted = [X_original_binary[i] for i in np.argsort(vocabulary_original_sorted)]\n\n# get the original vocabulary in the original order\nvocabulary_original_sorted = [v for v in vocabulary if v in vocabulary_original]\n\n# get the original binary representation of X in the original order\nX_original_binary_sorted = [X_original_binary[i] for i in np.argsort(vocabulary_original_sorted)]\n\n# get the original vocabulary in the original order\nvocabulary_original_sorted = [v for v in", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = []\n\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time','col']]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    slopes.append(slope.coef_[0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time','col']]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\ntransformed_df = df\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\ntransformed_df = df\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "\n    df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n    ### END SOLUTION\n    return df\ndf['Sex'] = Transform(df['Sex'])\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "\n# ElasticNet Regression\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(ElasticNet.coef_) # print out the coefficients\n# print (ElasticNet.intercept_) # print out the coefficients\n\n# calculate the R^2 score for the training set\ntraining_set_score = ElasticNet.score(X_train, y_train)\n\n# calculate the R^2 score for the test set\ntest_set_score = ElasticNet.score(X_test, y_test)\n\nprint (\"R^2 for training set:\", training_set_score)\nprint (\"R^2 for test set:\", test_set_score)\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "\nmin_max_scaler = MinMaxScaler()\nnp_array_scaled = min_max_scaler.fit_transform(np_array)\n\ntransformed = np_array_scaled\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "\nmin_max_scaler = MinMaxScaler()\nnp_array_scaled = min_max_scaler.fit_transform(np_array)\n\ntransformed = np_array_scaled\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": "\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION\n\nnp_array_transformed = Transform(np_array)\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "close_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\n\npredict = clf.predict(b)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = [['asdf', '1'], ['asdf', '0']]\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = [['asdf', '1'], ['asdf', '0']]\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = [['dsa', '2'], ['sato', '3']]\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# Fit the model\nlogReg.fit(X, y)\n\n# Predict\ny_pred = logReg.predict(X)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Split the data into X and y\nX = dataframe.iloc[:,:-1]\ny = dataframe.iloc[:,-1]\n\n# Fit the model\nlogReg.fit(X, y)\n\n# Predict\ny_pred = logReg.predict(X)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# First, we need to find the cut-off point for the test set\nn_train = int(len(features_dataframe) * 0.2)\n\n# Then we can split the dataframe into two parts\ntrain_dataframe = features_dataframe[0:n_train]\ntest_dataframe = features_dataframe[n_train:]\n\n# Now we can shuffle the test_dataframe\nnp.random.seed(0)\nnp.random.shuffle(test_dataframe)\n\n# And finally, we can split the dataframe into two parts\ntrain_dataframe = features_dataframe[0:n_train]\ntest_dataframe = features competing_dataframe[n_train:]\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# First, we need to sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by=\"date\")\n\n# Then, we calculate the cutoff for the test set\nn = len(features_dataframe)\ncutoff = int(0.2 * n)\n\n# We split the dataframe into two parts: the first part will be used for training the model, and the second part will be used for testing the model\ntrain_dataframe = features_dataframe[cutoff:]\ntest_dataframe = features_dataframe[:cutoff]\n\n# Now we can use the train_dataframe and the test_dataframe to train the model\n# And the test_dataframe to test the model\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size)\n    return train_dataframe, test_dataframe\nsolve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2_scale', 'X3_scale']] = scaler.fit_transform(df[['X2', 'X3']])\ndf", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "\nmyData[['new_A2', 'new_A3']] = scaler.fit_transform(myData[['A2', 'A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\n\n# Get the feature names\nfeature_names = count.get_feature_names()\n\n# Get the vocabulary\nvocabulary = count.get_feature_names()\n\n# Print the feature names\nprint(feature_names)\n\n# Print the vocabulary\nprint(vocabulary)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "\ncount = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\n\n# Get the feature names\nfeature_names = count.get_feature_names_out()\n\n# Get the vocabulary\nvocabulary = count.get_feature_names_out()\n\n# Get the vocabulary with the words in the vocabulary\nvocabulary_with_words = [word for word in vocabulary if word in words]\n\n# Get the unique words in the vocabulary\nunique_words = [word for word in words if word not in vocabulary_with_words]\n\n# Get the unique words in the vocabulary with the words in the words list\nunique_words_with_words = [word for word in unique_words if word not in words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique enthus_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_words_with_words]\n\n# Get the unique words in the words list\nunique_words_in_words_list = [word for word in words if word not in vocabulary_with_words and word not in unique_", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results.sort_values(by='rank_test_score', inplace=True)\nfull_results\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results.sort_values(by='mean_fit_time')\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nfrom sklearn.externals.six import StringIO\nimport pkg_resources\nimport sys\nimport pickle\n\ndef save_model(model, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump(model, f)\n\ndef load_model(filename):\n    with open(filename, 'rb') as f:\n        return pickle.load(f)\n\ndef train_model(X, y):\n    model = IsolationForest()\n    model.fit(X)\n    save_model(model, \"model.pkl\")\n    return model\n\ndef load_model_and_predict(X):\n    model = load_model(\"model.pkl\")\n    y_pred = model.predict(X)\n    return y_pred\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "\n# Tokenizing each description\ndf['description_token'] = df['description'].str.split()\n\n# Creating TF-IDF matrix\ntfidf_matrix = tfidf.fit_transform(df['description_token'])\n\n# Calculating cosine similarity\ncosine_similarity_matrix = 1 - sklearn.metrics.pairwise.cosine_distances(tfidf_matrix, metric='cosine')\n\n# Saving the result into a csv file\ncosine_similarity_matrix.to_csv('cosine_similarity_matrix.csv')\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n\ndef adjust_learning_rate(optimizer, epoch):\n    if epoch > 5:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.001\n\noptim.step()\nadjust_learning_rate(optim, epoch)\noptim.step()\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n\ndef adjust_learning_rate(optimizer, epoch):\n    lr = 0.05 if epoch < 5 else 0.0005\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\noptim.zero_gradients()\noptim.step()\nadjust_learning_rate(optim, epoch)\noptim.step()\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "\nclass Word2VecEmbedding(torch.nn.Module):\n    def __init__(self, word2vec):\n        super(Word2VecEmbedding, self).__init__()\n        self.embedding = torch.nn.Embedding(len(word2vec.wv.vocab), len(word2vec[0]))\n        self.embedding.weights.data.copy_(word2vec.wv.vectors)\n\n    def forward(self, text):\n        return self.embedding(text)\n\nmodel = Word2VecEmbedding(word2vec)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": "\n    ###\n    # First, we need to convert the input into a PyTorch tensor\n    input_Tensor = torch.tensor(input_Tensor)\n\n    # Then, we need to create a weight matrix for the embedding layer\n    # This will be of the same size as your vocabulary\n    # We'll use the word2vec model to get the weights\n    weights = torch.zeros(len(word2vec.wv.vocab), word2vec.wv.vector_size)\n\n    # We'll iterate over each word in the vocabulary\n    for i, word in enumerate(word2vec.wv.vocab):\n        # If the word is in the vocabulary, we add the word's vector to the weight matrix\n        weights[i] = word2vec.wv[word]\n\n    # Finally, we create the embedding layer\n    # This layer will take the input word and return the word's vector\n    # We'll use the weights we created earlier\n    embedding_layer = torch.nn.Embedding.from_pretrained(weights)\n\n    # Now, we can use this layer to embed our input\n    # We'll use the input tensor as the input to this layer\n    # This will give us the word embeddings for the words in the input\n    embedded_input = embedding_layer(input_Tensor)\n\n    return embedded_input\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "\npx = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(np.array(x))\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "\nC = B[:, A_log]\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "\n# Create a ByteTensor of the same length as B\nA_logical_byte = torch.ByteTensor(A_logical.shape[0]).bool()\n\n# Create a mask of the same shape as A_logical\nmask = torch.zeros_like(A_logical_byte, dtype=torch.bool)\nmask[A_logical] = True\n\n# Apply the mask to B\nC = B.masked(mask)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "\nC = B[:, A_log]\n", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "\nC = B[:, A_log]\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": "\n    C = B[:, A_log]\n    return C\nC = solve(A_log, B)\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "\n# Create a ByteTensor with the same length as your input tensor\nA_log = torch.ByteTensor(np.ones(len(B), dtype=torch.bool))\n\n# Create a mask using the logical index\nmask = torch.ByteTensor(A_log.bool())\n\n# Apply the mask to the input tensor\nC = B[mask]\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "\ndef apply_indices(indices):\n    return torch.index_select(B, 0, indices)\n\nC = apply_indices(idx)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "\nx_tensor = torch.tensor(np.array(x_array))\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "\nx_tensor = torch.tensor(np.array(x_array))\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": "\n    return torch.tensor(np.array(a))\n    ### END SOLUTION\n\n# Test the function\nprint(Convert(x_array))\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\nmask = torch.where(lens.unsqueeze(1) >= torch.arange(len(lens)).unsqueeze(1), 1, 0)\nmask = mask.long()\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\nmax_len = max(lens)\nmask = torch.BoolTensor(max_len, max_len)\n\nfor i, j in enumerate(lens):\n    mask[i][j-1] = True\n\nmask = mask.bool()\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\nmask = torch.where(lens.unsqueeze(1) >= torch.arange(len(lens)).unsqueeze(1), 1, 0)\nmask = mask.long()\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": "\n    max_len = max(lens)\n    max_len = torch.tensor(max_len)\n    max_len = torchordinate.unsqueeze(max_len, 0)\n    max_len = torch.eq(max_len, max_len.T).long()\n    max_len = max_len.bool()\n    return max_len\n    ### END SOLUTION\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# get the diagonal elements of the 2D tensor\ndiag_ele = Tensor_2D[np.arange(Tensor_2D.shape[0]), np.arange(Tensor_2D.shape[1])]\n\n# create a 3D tensor of zeros with the same shape as the 2D tensor\nTensor_3D = torch.zeros_like(Tensor_2D)\n\n# set the diagonal elements of the 3D tensor to be the diagonal elements of the 2D tensor\nTensor_3D[np.arange(Tensor_2D.shape[0]), npipage.arange(Tensor_2D.shape[1])] = diag_ele\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": "\n    t_diag = torch.diag(t)\n    return t_diag\n    ### END SOLUTION\n\nTensor_2D_diag = Convert(Tensor_2D)\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "\na = torch.tensor(a)\nb = torchordinate(b)\n\nab = torch.stack((a, b), 0)\n\nprint(ab.shape) # should print: torch.Size([3, 11])\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "\na_tensor = torch.from_numpy(a)\nb_tensor = torchordinate(b)\n\nab = torch.stack([a_tensor, b_tensor], 0)\n\nprint(ab.shape) # should print (138, 514)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": "\n    c = torch.cat([a.unsqueeze(0), b.unsqueeze(0)], 0)\n    return c\n    ### END SOLUTION\n\na, b = solve(a, b)\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "\nmask = torch.arange(max(lengths)).repeat(10).unsqueeze(0).unsqueeze(2).to(device)\nindex = torch.arange(max(lengths)).unsqueeze(0).unsqueeze(1).repeat(10).to(device)\na[mask != index] = 0\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "\nmask = torch.arange(max(lengths)).repeat(10,1).unsqueeze(1).unsqueee(2).cuda()\nmask = (mask < lengths.unsqueee(1).unsqueee(2).cuda())\na[mask] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "\nmask = torch.arange(max(lengths)).repeat(a.shape[0], a.shape[1]).reshape(a.shape[0], a.shape[1]) < lengths\na[mask] = 0\n", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "\nmask = torch.arange(max(lengths)).repeat(10,1).to(device).long()\nmask = mask < lengths.unsqueeze(1)\na[mask] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "\ntensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "\n# assuming your list of tensors is named 'tensors'\nnew_tensors = torch.stack(tensors)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": "\n    tensor_of_tensors = torch.tensor(lt)\n    return tensor_of_tensors\n\n\n# call the function\ntensor_of_tensors = Convert(list_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "\ntensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "\n# Assuming that `t` is your tensor and `idx` is your indexing array\n\n# `t.shape` will give you the shape of your tensor\n# `idx.shape` will give you the shape of your indexing array\n\n# `t[idx]` will give you the element at the position specified by `idx`\n# `t[idx] = v` will replace the element at the position specified by `idx` with `v`\n\n# `t.view()` can be used to create a view of the tensor on a different device\n# `t.to()` can be used to move the tensor to a different device\n\n# `t.sum()` can be used to compute the sum of the tensor\n# `t.mean()` can be used to compute the mean of the tensor\n\n# `t.min()` and `t.max()` can be used to compute the minimum and maximum of the tensor\n\n# `t.abs()` can be used to compute the absolute value of the tensor\n\n# `t.clamp()` can be used to clamp the values of the tensor to a range\n\n# `t.norm()` can be used to compute the norm of the tensor\n\n# `t.sort()` can be used to sort the tensor\n\n# `t.sort(0)` can be used to sort the tensor in ascending order\n\n# `t.sort(-1)` can be used to sort the tensor in descending order\n\n# `t.unsqueeze(0)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(0)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(1)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(1)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(2)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(2)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(3)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(3)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(4)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(4)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(5)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(5)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(6)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(6)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(7)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(7)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(8)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(8)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(9)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(9)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(10)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(10)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(11)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(11)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(12)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(12)` can be used to remove a new dimension from the tensor\n\n# `t.unsqueeze(13)` can be used to add a new dimension to the tensor\n\n# `t.squeeze(13)` can be used to remove a new dimension from the", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "\n# get the first element of each row\nresult = t[range(len(t)), idx]\n\n# get the last element of each row\nresult = result[-1]\n\n# get the sum of each row\nresult = torch.sum(result, dim=0)\n\n# get the minimum of each row\nresult = torchordinate(result, dim=0)\n\n# get the maximum of each row\nresult = torch.max(result, dim=0)\n\n# get the mean of each row\nresult = torch.mean(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard deviation of each row\nresult = torch.std(result, dim=0)\n\n# get the standard", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "\n# First, we need to convert the torch tensor to a numpy array\nt_np = t.detach().numpy()\n\n# Now, we can use the indices from the numpy array to index into the torch tensor\nresult = t_np[idx]\n\n# Now, the result is a torch tensor with the desired values\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "scores = ... # your scores tensor\nids = torch.argmax(scores,1,True)\nresult = x.gather(1,ids)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "scores = ... # your scores tensor\nids = torch.argmax(scores,1,True)\nresult = x.gather(1,ids)\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# get the maximum score\nmax_score = x.max(axis=1)\n\n# get the indices of the maximum score\nmax_indices = np.where(max_score == x.max(axis=1))\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the indices of the maximum score\nmax_indices = npordinate(max_indices[0])\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_indices[1]]\n\n# get the maximum score\nmax_score = x[max_indices[0], max_", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "\ndef softmax(x):\n    e_x = torch.exp(x - torch.max(x))\n    return e_x / e_x.sum()\n\nclass Net(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.shape[0], -1)\n        x = softmax(x)\n        return x\n\nnet = Net()\ny = net(input_data)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "\ndef softmax(x):\n    e_x = torch.exp(x - torch.max(x))\n    return e_x / e_x.sum()\n\nclass Net(torch.nn.Module):\n    def forward(self, x):\n        x = x.view(x.shape[0], -1)\n        x = softmax(x)\n        return x\n\nnet = Net()\ny = net(input_data)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "\ndef softmax(x):\n    x_exp = np.exp(x - np.max(x))\n    return x_exp / x_exp.sum()\n\ndef softmax_class(x):\n    x_exp = np.exp(x - np.max(x))\n    return x_exp / x_exp.sum()\n\nclass Softmax(torch.nn.Module):\n    def forward(self, x):\n        x_exp = np.exp(x - np.max(x))\n        return x_exp / x_exp.sum()\n\nmodel = Softmax()\ny = model(softmax_output)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": "\n    max_val, max_index = torch.max(softmax_output, 1)\n    return max_index\n    ### END SOLUTION\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "\n# Get the maximum of the softmax_output\nmax_val = torch.max(softmax_output)\n\n# Create a mask to get the maximum value\nmask = softmax_output == max_val\n\n# Apply the mask to get the class with the maximum probability\ny = torchordinate(mask)\n\n# Return the class with the maximum probability\nreturn y\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "def cross_entropy2d(input, target):\n    input = input.view(-1, input.size(-1))\n    target = target.view(-1)\n    input = input[target >= 0]\n    target = target[target >= 0]\n    loss = F.nll_loss(input, target, size_average=False)\n    return loss\n\ndef class_wise_cross_entropy(input, target):\n    input = input.view(-1, input.size(-1))\n    target = target.view(-1)\n    input = input[target >= 0]\n    target = target[target >= 0]\n    loss = F.nll_loss(input, target, size_average=False)\n    return loss\n\ndef calculate_loss(input, target):\n    return cross_entropy2d(input, target) + class_wise_cross_entropy(input, target)\n\ndef train():\n    loss = calculate_loss(input, target)\n    loss.backward()\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = torch.sum(A==B)\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = torch.sum((A==B).bool())\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = torch.sum(A != B)\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": "\n    return torch.sum(A==B)\n    ### END SOLUTION\n\ncnt_equal = Count(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "cnt_equal = torch.sum((A[-x:] == B[-x:])).item()\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "cnt_not_equal = torch.sum((A[-x:] != B[-x:])).item()\n", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\n\na_split = torch.chunk(a, chunk_dim, dim=3)\ntensors_31 = [a_split[i] for i in range(len(a_split))]\n\nfor i in range(len(tensors_31)):\n    print(tensors_31[i].shape)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\n\na_split = torch.chunk(a, chunk_dim, dim=2)\ntensors_31 = [a_split[i] for i in range(len(a_split))]\n\nfor i in range(len(tensors_31)):\n    print(tensors_31[i].shape)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "\nmask = mask.bool() # Convert the mask to a boolean tensor\noutput[mask] = clean_input_spectrogram[mask] # Set the elements of output where the mask is True\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "\nmask = mask.bool() # Convert the mask to a boolean tensor\noutput[mask] = clean_input_spectrogram[mask] # Set the elements of output where the mask is True\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "\ndef get_min_signed_element(x, y):\n    x_abs = torch.abs(x)\n    y_abs = torch.abs(y)\n    min_element_x = torch.min(x_abs)\n    min_element_y = torch.min(y_abs)\n    min_element_xy = torch.min(torch.min(x_abs, y_abs))\n    return torch.where(x < y, min_element_x, min_element_y)\n\nsigned_min = get_min_signed_element(x, y)\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "\ndef get_max_signed_element(x, y):\n    x_abs = torch.abs(x)\n    y_abs = torch.abs(y)\n    max_x = torch.max(x_abs)\n    max_y = torch.max(y_abs)\n    max_xy = torch.max(x_abs, y_abs)\n    max_element_x = torch.where(x == max_x, max_x)\n    max_element_y = torch.where(y == max_y, max_y)\n    max_element_xy = torch.where(x == max_xy, max_xy)\n    return max_element_x, max_element_y, max_element_xy\n\nmax_element_x, max_element_y, max_element_xy = get_max_signed_element(x, y)\n\nsigned_max = torch.where(x >= y, max_element_x)\nsigned_max = torch.where(x < y, max_element_y)\nsigned_max = torch.where(x >= y, max_element_xy)\n\nEND SOLUTION\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": "\n    x_sign = torch.sign(x)\n    y_sign = torch.sign(y)\n    min_val = torch.min(torch.abs(x), torch.abs(y))\n    min_val_sign = torch.sign(min_val)\n    return x_sign * min_val_sign, y_sign * min_val_sign\n    ### END SOLUTION\nsolve(x, y)\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\ndef solve(a, b):\n    # your solution here\n    pass\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch competing_tensor([[5, 6, 7], [5, 6, 7]])\n\n# call the function\nsolve(a, b)\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": "\n    a_last_col = a[:, -1]\n    b_first_col = b[:, 0]\n    a_last_col = a_last_col.unsqueeze(0)\n    b_first_col = b_first_col.unsqueeze(1)\n    a_last_col = a_last_col.unsqueeze(2)\n    b_first_col = b_first_col.unsqueeze(3)\n    a_last_col = a_last_col.unsqueeze(4)\n    b_first_col = b_first_col.unsqueeze(5)\n    a_last_col = a_last_col.unsqueeze(6)\n    b_first_col = b_first_col.unsqueeze(7)\n    a_last_col = a_last_col.unsqueeze(8)\n    b_first_col = b_first_col.unsqueeze(9)\n    a_last_col = a_last_col.unsqueeze(10)\n    b_first_col = b_first_col.unsqueeze(11)\n    a_last_col = a_last_col.unsqueeze(12)\n    b_first_col = b_first_col.unsqueeze(13)\n    a_last_col = a_last_col.unsqueeze(14)\n    b_first_col = b_first_col.unsqueeze(15)\n    a_last_col = a_last_col.unsqueeze(16)\n    b_first_col = b_first_col.unsqueeze(17)\n    a_last_col = a_last_col.unsqueeze(18)\n    b_first_col = b_first_col.unsqueeze(19)\n    a_last_col = a_last_col.unsqueeze(20)\n    b_first_col = b_first_col.unsqueeze(21)\n    a_last_col = a_last_col.unsqueeze(22)\n    b_first_col = b_first_col.unsqueeze(23)\n    a_last_col = a_last_col.unsqueeze(24)\n    b_first_col = b_first_col.unsqueeze(25)\n    a_last_col = a_last_col.unsqueeze(26)\n    b_first_col = b_first_col.unsqueeze(27)\n    a_last_col = a_last_col.unsqueeze(28)\n    b_first_col = b_first_col.unsqueeze(29)\n    a_last_col = a_last_col.unsqueeze(30)\n    b_first_col = b_first_col.unsqueeze(31)\n    a_last_col = a_last_col.unsqueeze(32)\n    b_first_col = b_first_col.unsqueeze(33)\n    a_last_col = a_last_col.unsqueeze(34)\n    b_first_col = b_first_col.unsqueeze(35)\n    a_last_col = a_last_col.unsqueeze(36)\n    b_first_col = b_first_col.unsqueeze(37)\n    a_last_col = a_last_col.unsqueeze(38)\n    b_first_col = b_first_col.unsqueeze(39)\n    a_last_col = a_last_col.unsqueeze(40)\n    b_first_col = b_first_col.unsqueeze(41)\n    a_last_col = a_last_col.unsqueeze(42)\n    b_first_col = b_first_col.unsqueeze(43)\n    a_last_col = a_last_col.unsqueeze(44)\n    b_first_col = b_first_col.unsqu", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "\n# assuming your tensor t is of shape (n, 2)\nn = t.shape[0]\n\n# create a tensor of zeros with the same shape as t\nnew_t = torch.zeros_like(t)\n\n# stack the original tensor with the new tensor\nresult = torch.stack([t, new_t])\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "\n# assuming `t` is your tensor\nt = torch.tensor([[1, 2], [3, 4]])\n\n# you can stack tensors using torch.stack()\nt_stacked = torch.stack([t, torch.tensor([[0, 0, 0, 0]])])\n\n# or you can use torch.cat()\nt_cat = torch.cat([t, torch.tensor([[0, 0, 0, 0]])])\n\n# or you can use torch.cat() with a list comprehension\nt_cat = torch.cat([t] + [torch.tensor([[0, 0, 0, 0]])] * len(t))\n\n# print the result\nprint(t_stacked)\nprint(t_cat)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "\n# assuming your tensor t is of shape (n, 2)\nn = t.shape[0]\n\n# create a tensor of size (n, 1) with -1\nones = torch.ones(n, 1)\n\n# stack the tensor t and the ones\nresult = torch.cat([t, ones], dim=1)\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
