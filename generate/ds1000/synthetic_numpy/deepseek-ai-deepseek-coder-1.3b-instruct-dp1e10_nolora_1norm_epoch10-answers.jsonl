{"id": 0, "code": "result = df.loc[List]\n", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "result = df.loc[List]\nprint(result)\nprint(np.sum(df['Type'] != result['Type']))\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "import numpy as np\n\n# calculate value count for each column\ncounts = df.groupby(df.columns).size()\n\n# find columns with value count greater than or equal to 2\nneed_changes = counts[counts >= 2].index\n\n# create new values for columns with value count greater than or equal to 2\nfor col in df.columns:\n    if col in need_changes:\n        # create new values for columns with value count greater than or equal to 2\n        new_values = np.where(df[col].values == 'apple', 'other', 'sausage')\n        new_values = np.where(df[col].values == 'egg', 'other', new_values)\n        df[col] = new_values\n\n# replace values in columns with value count less than 2 with 'other'\nfor col in df.columns:\n    if col not in need_changes:\n        df[col] = np.where(df[col].values == 'apple', 'other', np.where(df[col].values == 'egg', 'other', df[col].values))\n\n# print result\nprint(df)\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "import numpy as np\n\n# Create a mask to select rows where values in Qu1 and Qu2 are not 'cheese'\nmask = (df['Qu1'] != 'cheese') & (df['Qu2'] != 'cheese')\n\n# Create a mask to select rows where values in Qu1 and Qu3 are 'cheese'\ncheese_mask = (df['Qu1'] == 'cheese') | (df['Qu3'] == 'cheese')\n\n# Create a mask to select rows where values in Qu2 are not 'banana'\nbanana_mask = df['Qu2'] != 'banana'\n\n# Create a mask to select rows where values in Qu2 are 'banana'\nbanana_cheese_mask = (df['Qu2'] == 'banana') & (df['Qu3'] == 'cheese')\n\n# Create a mask to select rows where values in Qu3 are 'sausage'\nsausage_mask = df['Qu3'] == 'sausage'\n\n# Create a mask to select rows where values in Qu3 are 'apple'\napple_mask = (df['Qu1'] == 'apple') & (df['Qu2'] != 'apple')\n\n# Create a mask to select rows where values in Qu3 are 'egg'\negg_mask = df['Qu3'] == 'egg'\n\n# Create a mask to select rows where values in Qu1 and Qu2 are not 'sausage'\nsausage_mask2 = (df['Qu1'] != 'sausage') & (df['Qu2'] != 'sausage')\n\n# Create a mask to select rows where values in Qu1 and Qu3 are not 'apple'\napple_mask2 = (df['Qu1'] != 'apple') & (df['Qu3'] != 'apple')\n\n# Create a mask to select rows where values in Qu2 and Qu3 are not 'banana'\nbanana_mask2 = (df['Qu2'] != 'banana') & (df['Qu3'] != 'banana')\n\n# Create a mask to select rows where values in Qu2 and Qu3 are 'banana'\nbanana_cheese_mask2 = (df['Qu2'] == 'banana') & (df['Qu3'] == 'cheese')\n\n# Create a mask to select rows where values in Qu1 and Qu3 are 'sausage'\nsausage_mask3 = (df['Qu1'] == 'sausage') & (df['Qu3'] == 'sausage')\n\n# Create a mask to select rows where values in Qu1 and Qu2 are 'sausage'\nsausage_mask4 = (df['Qu1'] == 'sausage') & (df['Qu2'] == 'sausage')\n\n# Create a mask to select rows where values in Qu1 and Qu3 are 'egg'\negg_mask4 = (df['Qu1'] == 'egg') & (df['Qu3'] == 'egg')\n\n# Create a mask to select rows where values in Qu2 and Qu3 are 'sausage'\nsausage_mask5 = (df['Qu2'] == 'sausage') & (df['Qu3'] == 'sausage')\n\n# Create a mask to select rows where values in Qu2 and Qu3 are 'apple'\napple_mask5 = (df['Qu2'] == 'apple') & (df['Qu3'] == 'apple')\n\n# Create a mask to select rows where values in Qu2 and Qu3 are 'potato'\npotato_mask = (df['Qu2'] == 'potato') & (df['Qu3'] == 'potato')\n\n# Create a mask to select rows where values in Qu1 and Qu3 are 'potato'\npotato_mask2 = (df['Qu1'] == 'potato') & (df['Qu3'] == 'potato')\n\n# Create a mask to select rows where values in Qu1 and Qu3 are 'cheese'\ncheese_mask2 = (df['Qu1'] == 'cheese') & (df['Qu3'] == 'cheese')\n\n# Create a mask to select rows where values in Qu1 and Qu3 are 'egg'\negg_mask3 = (df['Qu1'] == 'egg') & (df['Qu3'] == 'egg')\n\n# Create a mask to select rows where values in Qu1 and Qu", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "\n    for col in df.columns:\n        count = df[col].value_counts()\n        if len(count) >= 2:\n            count = count[count >= 2]\n            for i in count.index:\n                df[col] = np.where(df[col] == i, i + '", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "import numpy as np\n\n# calculate value counts for Qu1\ncounts_Qu1 = np.bincount(df['Qu1'].values)\n\n# create new values for Qu1 based on value counts\nnew_values_Qu1 = np.where(counts_Qu1 >= 3, 'other', 'cheese')\n\n# create new values for Qu2 and Qu3 based on value counts\nnew_values_Qu2 = np.where(counts_Qu1 >= 2, 'other', 'sausage')\nnew_values_Qu3 = np.where(counts_Qu1 >= 2, 'other', 'sausage')\n\n# create new dataframe\ndf_new = pd.DataFrame({'Qu1': new_values_Qu1,\n                       'Qu2': new_values_Qu2,\n                       'Qu3': new_values_Qu3})\n\n# replace values in original dataframe\ndf['Qu1'] = df_new['Qu1']\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "import numpy as np\n\n# calculate value counts for Qu1\ncounts_Qu1 = np.bincount(df['Qu1'].values)\n\n# create new values for Qu1\nnew_values_Qu1 = np.where(counts_Qu1 >= 3, 'other', df['Qu1'])\n\n# create new dataframe\ndf_new = DataFrame({'Qu1': new_values_Qu1,\n                    'Qu2': df['Qu2'],\n                    'Qu3': df['Qu3']})\n\n# print the result\nprint(df_new)\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "import numpy as np\n\n# Create a mask to identify duplicates\nmask = np.where(df['keep_if_dup'] == 'Yes', 1, 0)\n\n# Create a mask to identify first occurrence of each 'url'\nfirst_occurrence_mask = np.where(np.diff(mask) != 0, 1, 0)\n\n# Apply the mask to the 'keep_if_dup' column\ndf['keep_if_dup'] = df['keep_if_dup'].where(first_occurrence_mask)\n\n# Select rows where 'keep_if_dup' is 'Yes'\nresult = df[df['keep_if_dup'] == 'Yes']\n\n# Reset the mask to remove duplicates\nmask = np.where(df['keep_if_dup'] == 'Yes', 1, 0)\n\n# Apply the mask to the 'keep_if_dup' column\ndf['keep_if_dup'] = df['keep_if_dup'].where(mask)\n\n# Select rows where 'keep_if_dup' is 'Yes'\nresult_2 = df[df['keep_if_dup'] == 'Yes']\n\n# Combine results\nresult = np.concatenate((result, result_2))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "import numpy as np\n\n# Create a mask to identify duplicates\nmask = np.where(df['drop_if_dup'] == 'Yes', 1, 0)\n\n# Create a mask to identify first occurrence of each 'url'\nfirst_occurrence_mask = np.where(np.diff(mask) != 0, 1, 0)\n\n# Apply the mask to the 'url' column\ndf['first_occurrence'] = np.where(first_occurrence_mask, 1, 0)\n\n# Select rows where 'first_occurrence' is 1\nresult = df[df['first_occurrence'] == 1]\n\n# Reset the mask\nmask = np.where(df['drop_if_dup'] == 'Yes', 1, 0)\n\n# Apply the mask to the 'url' column\ndf['first_occurrence'] = np.where(mask, 1, 0)\n\n# Select rows where 'first_occurrence' is 1\nresult2 = df[df['first_occurrence'] == 1]\n\n# Combine results\nresult = np.concatenate((result, result2))\n\n# Sort by 'url'\nresult = result[np.argsort(result['url'])]\n\n# Reset the mask\nmask = np.where(df['drop_if_dup'] == 'Yes', 1, 0)\n\n# Apply the mask to the 'url' column\ndf['first_occurrence'] = np.where(mask, 1, 0)\n\n# Select rows where 'first_occurrence' is 1\nresult3 = df[df['first_occurrence'] == 1]\n\n# Combine results\nresult = np.concatenate((result, result3))\n\n# Sort by 'url'\nresult = np.sort(result, axis=0)\n\n# Print result\nprint(result)\n", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "import numpy as np\n\n# Create a mask to select the last occurrence of 'keep_if_dup'\nmask = np.where(df['keep_if_dup'] == 'Yes', np.arange(df.shape[0])[:, None], -1)\n\n# Find the indices of the last occurrence of each 'keep_if_dup' value\nidx = np.argmax(mask, axis=1)\n\n# Select the last occurrence of each 'keep_if_dup' value\ndf_last_occurrence = df.loc[idx]\n\n# Replace the original 'keep_if_dup' column with the last occurrence\ndf['keep_if_dup'] = df_last_occurrence['keep_if_dup']\n\n# Drop the original 'keep_if_dup' column\ndf = df.drop('keep_if_dup', axis=1)\n\n# Reorder the columns\ndf = df[['id', 'url', 'keep_if_dup']]\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "import numpy as np\n\ndef convert_to_nested_dict(df):\n    # Get unique values of 'name' and 'v1'\n    unique_names = np.unique(df['name'])\n    unique_v1 = np.unique(df['v1'])\n\n    # Initialize a dictionary to store the result\n    result = {}\n\n    # Iterate over unique names\n    for name in unique_names:\n        # Initialize a dictionary to store the values for each 'v1'\n        v1_dict = {}\n\n        # Iterate over unique 'v1'\n        for v1 in unique_v1:\n            # Get indices of current 'v1'\n            idx = np.where(df['v1'] == v1)[0]\n\n            # Initialize a dictionary to store the values for current 'v1'\n            v_dict = {}\n\n            # Iterate over indices\n            for i in idx:\n                # Get column names\n                col_names = df.columns[2:]\n\n                # Initialize a dictionary to store column values\n                col_values = {}\n\n                # Iterate over column names\n                for col in col_names:\n                    # Get column values\n                    col_values[col] = df.loc[i, col]\n\n                # Add column values to 'v_dict'\n                col_values = {k: v for k, v in col_values.items() if v != 0}\n\n                # Add column values to 'v_dict'\n                v_dict[col_names[0]] = col_values\n\n            # Add 'v_dict' to 'v1_dict'\n            v1_dict[v1] = v_dict\n\n        # Add 'v1_dict' to 'result'\n        result[name] = v1_dict\n\n    return result\n\n# Test the function\nresult = convert_to_nested_dict(df)\nprint(result)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": "\n    # Convert the datetime column to UTC\n    df['datetime'] = df['datetime'].dt.tz_convert('UTC')\n    # Convert the datetime column back to the original timezone\n    df['datetime'] = df['datetime'].dt.tz_convert(example_df['datetime'].dt.tz_localize('UTC').tz_convert('America/New_York'))\n    # Return the result\n    return df\n\nresult = f()\nprint(result)\n## END SOLUTION\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "import numpy as np\n\n# Extract key-value pairs from the message\nkey_values = []\nfor message in df['message']:\n    key_values.append(np.array([k.split(': ')[0], k.split(': ')[1] for k in message.split(', ')]))\n\n# Flatten the list of key-value pairs\nkey_values = [item for sublist in key_values for item in sublist]\n\n# Convert the list to a NumPy array\nkey_values = np.array(key_values)\n\n# Reshape the array to match the output structure\nkey_values = key_values.reshape(-1, 4)\n\n# Create a dataframe from the array\ndf_output = pd.DataFrame(key_values, columns=['key', 'value', 'status', 'message'])\n\n# Replace 'none' with None\ndf_output['value'] = df_output['value'].replace('none', None)\n\n# Replace 'yes' with True\ndf_output['value'] = df_output['value'].replace('yes', True)\n\n# Replace 'none' with None\ndf_output['status'] = df_output['status'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['message'] = df_output['message'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['group'] = df_output['group'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Replace 'none' with None\ndf_output['kids'] = df_output['kids'].replace('none', None)\n\n# Re", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "\n# Define the constant to multiply\nconstant = 10\n\n# Select products to multiply\nproducts_to_multiply = df[df['product'].isin(products)]\n\n# Multiply scores corresponding to products to multiply by constant\ndf.loc[df['product'].isin(products), 'score'] = df['score'] * constant\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "def multiply_scores(df, products):\n    # Create a mask to select rows where product is not in products\n    mask = ~df['product'].isin(products)\n    \n    # Multiply scores by 10\n    df.loc[mask, 'score'] *= 10\n    \n    return df\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "def multiply_product_scores(df, products):\n    product_mask = np.isin(df['product'], products)\n    product_mask = product_mask.reshape(-1, 1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    df['product'] = df['product'].astype(int)\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_mask.astype(bool)\n    product_mask = product_mask.reshape(-1)\n    product_mask = product_mask.astype(int)\n    product_mask = product_mask * np.ones((df.shape[0], 1))\n    product_mask = product_", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "\n# Select rows corresponding to the products to be normalized\nproduct_df = df[df['product'].isin(products)]\n\n# Calculate the minimum and maximum values for each product\nmin_values = product_df.groupby('product')['score'].min().values\nmax_values = product_df.groupby('product')['score'].max().values\n\n# Normalize the scores\nproduct_df['normalized_score'] = (product_df['score'] - min_values) / (max_values - min_values)\n\n# Replace the original scores with the normalized ones\nproduct_df['score'] = product_df['normalized_score']\n\n# Reset the index\nproduct_df = product_df.reset_index(drop=True)\n\nprint(product_df)\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "import numpy as np\n\n# Convert binary columns to categorical\ndf['category'] = np.where(df == 1, df.columns, '')\n\n# Remove the binary columns\ndf = df.drop(columns=['A', 'B', 'C', 'D'])\n\n# Add a new column to store the category\ndf['category'] = df['category'].apply(lambda x: x[0])\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n# Replace the binary columns with the new category\ndf = df.apply(lambda x: x.replace(0, ''))\n\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "import numpy as np\n\n# Create a mask to identify binary columns\nmask = np.where((df == 0) | (df == 1), 1, 0)\n\n# Create a new categorical column\ndf['category'] = np.where(mask == 1, df.columns, 'D')\n\n# Replace the binary columns with the new categorical column\ndf = df.replace(mask, df.columns)\n\n# Replace 0 with 'D'\ndf = df.replace(0, 'D')\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'0': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['A', 'B', 'C', 'D']})\n\n# Replace 1 with 'A', 'B', 'C', 'D'\ndf = df.replace(1, {'1': 'D', 1: ['", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "import numpy as np\n\ndef convert_binary_to_category(df):\n    # Get the indices of non-zero values in each column\n    non_zero_indices = np.where(df != 0)\n    \n    # Create a new column to store the category\n    df['category'] = np.empty(df.shape[0], dtype=object)\n    \n    # Iterate over each column\n    for col in df.columns:\n        # Get the indices of non-zero values in the current column\n        non_zero_col_indices = non_zero_indices[0][df[col] != 0]\n        \n        # Get the values of non-zero indices\n        non_zero_values = df[col][non_zero_col_indices]\n        \n        # Convert non-zero values to lists\n        non_zero_lists = [list(x) for x in non_zero_values]\n        \n        # Replace non-zero values in the column with the lists\n        df['category'][non_zero_col_indices] = non_zero_lists\n    \n    return df\n\n\ndf = convert_binary_to_category(df)\nprint(df)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.to_period(\"M\")\ndf['Date'] = df['Date'].apply(lambda x: x.strftime(\"%B-%Y\"))\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime(\"%d-%b-%Y\")\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "import numpy as np\n\ndef extract_date(df, list):\n    dates = np.array(list)\n    dates = np.sort(dates)\n    mask = np.in1d(df['Date'].dt.date, dates)\n    masked_df = df[mask]\n    masked_df['Date'] = masked_df['Date'].dt.strftime(\"%d-%b-%Y\")\n    return masked_df\n\n\ndf_result = extract_date(df, List)\nprint(df_result)\n", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "# shift down the first row of the first column\ndf_shifted = df.shift(1, axis=0)\n\n# replace the first row with NaN\ndf_shifted.loc[0] = np.nan\n\n# replace NaN with the first row\ndf_shifted = df_shifted.fillna(df.iloc[0])\n\n# shift up the last row of the last column\ndf_shifted = df.shift(-1, axis=1)\n\n# replace the last row with NaN\ndf_shifted.loc[-1] = np.nan\n\n# replace NaN with the last row\ndf_shifted = df_shifted.fillna(df.iloc[-1])\n\n# calculate R^2 for each shift\nr2_values = np.mean((df - df_shifted) ** 2 / np.var(df, axis=0), axis=0)\n\n# print R^2 values\nprint(r2_values)\n\n# print the original and shifted dataframes\nprint(df)\nprint(df_shifted)\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "# Shift the last row of the first column up\ndf_shifted = df.shift(1, axis=0)\n\n# Replace the first row of the first column with the last row of the shifted data\ndf_shifted.loc[0] = df.loc[-1]\n\n# Replace the last row of the first column with the first row of the shifted data\ndf_shifted.loc[-1] = df.loc[0]\n\n# Replace the last row of the last column with the first row of the shifted data\ndf_shifted.loc[-1, -1] = df.loc[0, -1]\n\n# Replace the first row of the last column with the last row of the shifted data\ndf_shifted.loc[0, -1] = df.loc[-1, -1]\n\n# Replace the first row of the first column with the last row of the shifted data\ndf_shifted.loc[0, 0] = df.loc[-1, -1]\n\n# Replace the last row of the first column with the first row of the shifted data\ndf_shifted.loc[-1, 0] = df.loc[0, -1]\n\n# Replace the last row of the last column with the first row of the shifted data\ndf_shifted.loc[-1, -1] = df.loc[0, -1]\n\n# Replace the first row of the last column with the last row of the shifted data\ndf_shifted.loc[0, -1] = df.loc[-1, -1]\n\n# Replace the first row of the first column with the last row of the shifted data\ndf_shifted.loc[0, 0] = df.loc[-1, -1]\n\n# Replace the last row of the first column with the first row of the shifted data\ndf_shifted.loc[-1, 0] = df.loc[0, -1]\n\n# Replace the last row of the last column with the first row of the shifted data\ndf_shifted.loc[-1, -1] = df.loc[0, -1]\n\n# Replace the first row of the last column with the last row of the shifted data\ndf_shifted.loc[0, -1] = df.loc[-1, -1]\n\n# Replace the first row of the first column with the last row of the shifted data\ndf_shifted.loc[0, 0] = df.loc[-1, -1]\n\n# Replace the last row of the first column with the first row of the shifted data\ndf_shifted.loc[-1, 0] = df.loc[0, -1]\n\n# Replace the last row of the last column with the first row of the shifted data\ndf_shifted.loc[-1, -1] = df.loc[0, -1]\n\n# Replace the first row of the last column with the last row of the shifted data\ndf_shifted.loc[0, -1] = df.loc[-1, -1]\n\n# Replace the first row of the first column with the last row of the shifted data\ndf_shifted.loc[0, 0] = df.loc[-1, -1]\n\n# Replace the last row of the first column with the first row of the shifted data\ndf_shifted.loc[-1, 0] = df.loc[0, -1]\n\n# Replace the last row of the last column with the first row of the shifted data\ndf_shifted.loc[-1, -1] = df.loc[0, -1]\n\n# Replace the first row of the last column with the last row of the shifted data\ndf_shifted.loc[0, -1] = df.loc[-1, -1]\n\n# Replace the first row of the first column with the last row of the shifted data\ndf_shifted.loc[0, 0] = df.loc[-1, -1]\n\n# Replace the last row of the first column with the first row of the shifted data\ndf_shifted.loc[-1, 0] = df.loc[0, -1]\n\n# Replace the last row of the last column with the first row of the shifted data\ndf_shifted.loc[-1, -1] = df.loc[0, -1]\n\n# Replace the first row of the last column with the last row of the shifted data", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "# shift down the first row of the first column\ndf_shifted_down = df.shift(1, axis=0)\n# replace the first row with NaN\ndf_shifted_down.loc[0] = np.nan\n# shift up the last row of the second column\ndf_shifted_up = df.shift(-1, axis=0)\n# replace the last row with NaN\ndf_shifted_up.loc[-1] = np.nan\n# combine the two shifts\ndf_combined = df_shifted_down.combine(df_shifted_up, np.where)\n# replace NaN with the mean of the two shifts\ndf_combined = df_combined.fillna(df_combined.mean())\nprint(df_combined)\n", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "# shift the first row of the first column down 1 row\ndf = df.shift(1)\n# replace the first row with the original value\ndf.loc[0] = df.loc[0]\n# shift the last row of the first column up 1 row\ndf = df.shift(-1)\n# replace the last row with the original value\ndf.loc[-1] = df.loc[-1]\n# print the result\nprint(df)\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [f'Header{c}X' for c in df.columns]\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col for col in df.columns]\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "# Create a list of column names\ncol_names = ['HeaderAX', 'HeaderBX', 'HeaderCX', 'HeaderX']\n\n# Create a new column names list with 'X' appended to each column\nnew_col_names = [f'{col}X' for col in col_names]\n\n# Create a new DataFrame with new column names\nnew_df = df.rename(columns=dict(zip(col_names, new_col_names)))\n\n# Print the new DataFrame\nprint(new_df)\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "import numpy as np\n\ndef get_mean(df):\n    # Get all column names\n    cols = df.columns.tolist()\n    # Remove 'group' and 'group_color' from column list\n    cols.remove('group')\n    cols.remove('group_color')\n    # Calculate mean for other columns\n    mean_cols = df[cols].mean(axis=0)\n    return mean_cols\n\n# Get mean of other columns\nmean_cols = get_mean(df)\n\n# Get mean of 'val1' and 'val2'\nmean_val = np.mean([df['val1'], df['val2']])\n\n# Combine mean of other columns and 'val1' and 'val2'\nresult = np.column_stack((mean_cols, mean_val))\n\nprint(result)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "import numpy as np\n\ndef sum_other_columns(df):\n    # Get all column names\n    cols = df.columns.tolist()\n    # Remove 'group' and 'group_color'\n    cols.remove('group')\n    cols.remove('group_color')\n    # Get the sum of other columns\n    other_cols_sum = np.sum(df[cols].values, axis=0)\n    return other_cols_sum\n\n# Use the function\nresult = sum_other_columns(df)\nprint(result)\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "import numpy as np\n\ndef get_mean(df, column):\n    return np.mean(df[column])\n\ndef get_sum(df, column):\n    return np.sum(df[column])\n\ndef get_mean_val2(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val3(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val4(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val5(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val6(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val7(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val8(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val9(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val10(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val11(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val12(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val13(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val14(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val15(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val16(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val17(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val18(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val19(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val20(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val21(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val22(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val23(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val24(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val25(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val26(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val27(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val28(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val29(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val30(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val31(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val32(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val33(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val34(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val35(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val36(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val37(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val38(df, column):\n    return np.mean(df[column])\n\ndef get_mean_val39(df, column):\n    return np.mean", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "def meanAdvance(row_list, column_list, axis=0):\n    return np.mean(df.loc[row_list, column_list], axis=axis)\n\n\nresult = meanAdvance(row_list, column_list)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "def sumAdvance(df, row_list, column_list, axis=0):\n    return df.loc[row_list, column_list].sum(axis=axis)\n\n\n# test the function\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\n\nresult = sumAdvance(df, row_list, column_list)\nprint(result)\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "# get column indices\ncol_indices = [df.columns.get_index(col) for col in column_list]\n# get sum of rows\nsum_rows = df.loc[row_list, col_indices].sum(axis=1)\n# get maximum value and its index\nmax_val = sum_rows.max()\nmax_idx = sum_rows.idxmax()\n# delete the maximum value\nsum_rows = sum_rows.drop(max_idx)\n# get sum of rows after deleting the maximum value\nsum_rows_after_del = sum_rows\n# print the result\nprint(sum_rows_after_del)\n", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "import numpy as np\n\n# Get unique values for each column\nunique_values = np.unique(df, axis=0)\n\n# Count the number of unique values for each column\ncounts = np.bincount(unique_values[:, 0])\n\n# Create a Series with column names as indices and counts as values\nseries = pd.Series(counts, index=df.columns)\n\n# Sort the series in descending order\nseries = series.sort_values(ascending=False)\n\nprint(series)\n", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "import numpy as np\n\n# Count the number of 'null' values in each column\ncounts = np.count_nonzero(df == 'null', axis=0)\n\n# Create a Series with the counts\nseries = pd.Series(counts, index=df.columns)\n\n# Replace 'null' values with NaN\nseries = series.replace('null', np.nan)\n\n# Print the series\nprint(series)\n", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "import numpy as np\n\nresult = np.array([\n    np.count_nonzero(df['temp'] == 'null'),\n    np.count_nonzero(df['name'] == 'mark')\n])\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "result = df.loc[0].concatenate(df.loc[1])\n", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "result = df.loc[0]\nresult = np.concatenate((result, df.loc[1]))\nprint(result)\n", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "def replace_nan(x):\n    return x.fillna(x.mean()).astype(float)\n\ndf = df.apply(replace_nan, axis=1)\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "def replace_nan(x):\n    return x.sort_values().drop_duplicates().values[0]\n\ndf = df.apply(replace_nan, axis=1)\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "def replace_nan_with_last_non_nan(x):\n    return np.where(x.isnull(), x.dropna().values[-1], x)\n\ndf = df.apply(replace_nan_with_last_non_nan, axis=1)\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "# Select rows smaller than the threshold\nsmaller_than_thresh = df['value'] < threshold\n# Select rows that are smaller than the threshold\nsmaller_than_thresh_df = df[smaller_than_thresh]\n# Sum the values of smaller_than_thresh_df\nresult = smaller_than_thresh_df.sum()\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "# Select rows smaller than the threshold\nsmaller_than_thresh = df['value'] < threshold\n# Select rows larger than or equal to the threshold\nlarger_than_thresh = df['value'] >= threshold\n# Calculate the average value of rows larger than the threshold\navg_larger_than_thresh = df[larger_than_thresh]['value'].mean()\n# Select rows smaller than the threshold\nsmaller_than_thresh = df['value'] < threshold\n# Replace values in the smaller_than_thresh dataframe with the average value\ndf.loc[smaller_than_thresh, 'value'] = avg_larger_than_thresh\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "# find the indices of the values in the given section\nidx_left = np.where(df['value'] <= section_left)[0]\nidx_right = np.where(df['value'] >= section_right)[0]\n\n# calculate the average of values in the given section\navg = np.mean(df['value'][idx_left:idx_right+1])\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace the values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the given section with the average\ndf['value'][idx_left:idx_right+1] = avg\n\n# replace values in the", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "import numpy as np\n\n# Create a copy of the original dataframe\ndf_copy = df.copy()\n\n# Create a dictionary to store the inverse values\ninv_values = {}\n\n# Iterate over each column in the original dataframe\nfor col in df.columns:\n    # Calculate the inverse of the current column\n    inv_values[f\"inv_{col}\"] = np.reciprocal(df[col])\n\n# Create a new dataframe with the inverse values\ndf_result = df.copy()\ndf_result.assign(**inv_values)\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "import numpy as np\n\n# Create a list of exponential values for each column\nexp_values = [np.exp(1), np.exp(2), np.exp(3)]\n\n# Create a list of column names with prefix\nprefixes = [\"exp_\", \"A\", \"B\"]\n\n# Create a new dataframe with exponential values\nnew_df = pd.DataFrame(np.column_stack((df, exp_values)), columns=np.concatenate((df.columns, prefixes)))\n\n# Print the new dataframe\nprint(new_df)\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "import numpy as np\n\n# calculate inverse of each column\ndf_inv = df.copy()\nfor col in df.columns:\n    df_inv[col + \"_inv\"] = 1 / df[col]\n\n# add a column with the original column names\ndf_inv = df_inv.add_column(df[df.columns], label=\"original\")\n\n# replace 0 with NaN\ndf_inv = df_inv.replace(0, np.nan)\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv.mean())\n\n# replace NaN with 0\ndf_inv = df_inv.fillna(0)\n\n# replace NaN with inverse of column\ndf_inv = df_inv.fillna(df_inv", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nfor col in df.columns:\n    df[f'sigmoid_{col}'] = sigmoid(df[col])\n\nprint(df)\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "import numpy as np\n\n# Find the index of the minimum value in each column\nmin_idx = np.argmin(df, axis=0)\n\n# Find the index of the maximum value in each column\nmax_idx = np.argmax(df, axis=0)\n\n# Find the index of the last occurrence of each column\nlast_idx = np.where(np.arange(df.shape[0]) == max_idx)[0]\n\n# Get the last occurrence of each column\nlast_val = df[last_idx]\n\n# Print the result\nprint(last_val)\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "import numpy as np\n\n# Find the index of the minimum value in each column\nmin_idx = np.argmin(df, axis=0)\n\n# Find the index of the maximum value in each column\nmax_idx = np.argmax(df, axis=0)\n\n# Find the index of the first occurrence of the column-wise maximum\nfirst_max_idx = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum\nfirst_min_idx = np.where(np.all(df == df[min_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum in each column\nfirst_min_idx_in_each_column = np.where(np.all(df == df[min_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise maximum in each column\nfirst_max_idx_in_each_column = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise maximum in each column\nfirst_max_idx_in_each_column = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum in each column\nfirst_min_idx_in_each_column = np.where(np.all(df == df[min_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise maximum in each column\nfirst_max_idx_in_each_column = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum in each column\nfirst_min_idx_in_each_column = np.where(np.all(df == df[min_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise maximum in each column\nfirst_max_idx_in_each_column = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum in each column\nfirst_min_idx_in_each_column = np.where(np.all(df == df[min_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise maximum in each column\nfirst_max_idx_in_each_column = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum in each column\nfirst_min_idx_in_each_column = np.where(np.all(df == df[min_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise maximum in each column\nfirst_max_idx_in_each_column = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum in each column\nfirst_min_idx_in_each_column = np.where(np.all(df == df[min_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise maximum in each column\nfirst_max_idx_in_each_column = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum in each column\nfirst_min_idx_in_each_column = np.where(np.all(df == df[min_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise maximum in each column\nfirst_max_idx_in_each_column = np.where(np.all(df == df[max_idx], axis=1))[0]\n\n# Find the index of the first occurrence of the column-wise minimum in each column\nfirst_min_idx_in_each_column = np.where(np.all(df == df[", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "# Find the minimum and maximum date\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\n# Create a range of dates between min and max\ndates = pd.date_range(start=min_dt, end=max_dt)\n\n# Create a new dataframe with dates and fill with zeros\nnew_df = pd.DataFrame({'dt': dates, 'val': 0})\n\n# Join the new dataframe with the original dataframe\nresult = pd.concat([df, new_df], axis=1)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n# Reorder the columns\nresult = result[['dt', 'user', 'val']]\n\n# Reorder the rows\nresult = result.sort_values('dt')\n\n# Reset the index\nresult = result.reset", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "# Find the minimum and maximum date\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\n# Expand the date column to have all dates there\ndt_range = pd.date_range(start=min_dt, end=max_dt)\n\n# Create a new dataframe with all dates and fill in 0 for the val column\ndf_expanded = pd.DataFrame({'dt': dt_range, 'val': 0}, index=dt_range)\n\n# Combine the original dataframe with the expanded one\ndf_combined = pd.concat([df, df_expanded])\n\n# Reorder the columns\ndf_combined = df_combined[['dt', 'user', 'val']]\n\n# Sort the dataframe by dt and user\ndf_combined = df_combined.sort_values(['dt', 'user'])\n\n# Reset the index\ndf_combined = df_combined.reset_index(drop=True)\n\n# Print the final dataframe\nprint(df_combined)\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "# Find the minimum and maximum date\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\n# Create a new dataframe with all dates\ndf_all = pd.DataFrame({'dt': pd.date_range(start=min_dt, end=max_dt), 'val': [233]*len(df['dt'].unique())})\n\n# Join the original dataframe with the new dataframe\ndf = df.merge(df_all, how='left', left_index=True, right_index=True)\n\n# Fill in 233 for the val column\ndf['val'] = df['val'].fillna(233)\n\n# Reorder columns\ndf = df[['dt', 'val', 'user']]\n\n# Reset index\ndf = df.reset_index(drop=True)\n\n# Reorder columns\ndf = df[['dt', 'val', 'user']]\n\n# Reorder columns\ndf = df.sort_values('dt')\n\n# Reset index\ndf = df.reset_index(drop=True)\n\n# Print the final dataframe\nprint(df)\n", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "import numpy as np\n\n# Find the minimum and maximum date\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\n# Expand the date column to have all dates there\ndt_range = pd.date_range(start=min_dt, end=max_dt)\n\n# Create a new dataframe with all dates\ndf_new = pd.DataFrame({'dt': dt_range, 'user': df['user'], 'val': df['val']}, index=dt_range)\n\n# Fill in the maximum val of the user for the val column\ndf_new['val'] = df_new.groupby('user')['val'].transform('max')\n\n# Replace the original dataframe with the new one\ndf['dt'] = df_new['dt']\n\n# Reshape the dataframe to have the minimum date as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the minimum date as the index\ndf = df.set_index('dt').sort_index()\n\n# Print the final dataframe\nprint(df)\nEND SOLUTION\n", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "# Find the minimum and maximum date\nmin_dt = df['dt'].min()\nmax_dt = df['dt'].max()\n\n# Expand the date column to have all dates there\ndates = pd.date_range(start=min_dt, end=max_dt)\n\n# Create a new dataframe with all dates\ndf_dates = pd.DataFrame({'dt': dates})\n\n# Join the original dataframe with the new dataframe\ndf = pd.concat([df, df_dates], axis=1)\n\n# Fill in the maximum val of the user for the val column\ndf['val'] = df.groupby('dt')['val'].transform('max')\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the user column as the index\ndf = df.set_index('user').sort_index()\n\n# Reshape the dataframe to have the val column as the index\ndf = df.set_index('val').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n# Reshape the dataframe to have the date column as the index\ndf = df.set_index('dt').sort_index()\n\n", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "import numpy as np\n\n# Create a unique ID for each name\nunique_ids = np.unique(df['name'])\n\n# Create a dictionary to map unique IDs to their corresponding indices\nid_to_index = {id: i for i, id in enumerate(unique_ids)}\n\n# Replace names with unique IDs\ndf['name'] = df['name'].map(id_to_index)\n\n# Reshape the dataframe to get the desired output\ndf = df.reshape(-1, 4)\n\n# Reorder the columns\ndf = df[:, [0, 1, 2, 3]]\n\n# Reshape the dataframe back to its original shape\ndf = df.reshape(-1, 4)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "import numpy as np\n\n# Create a unique ID for each unique a value\nunique_ids = np.unique(df['a'])\n\n# Create a new column 'id' to store the unique IDs\ndf['id'] = np.searchsorted(unique_ids, df['a'])\n\n# Replace 'a' with 'id'\ndf['a'] = df['id']\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\nprint(df)\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "\n    # Create a unique ID for each name\n    unique_ids = np.unique(df['name'])\n    # Create a dictionary to map unique IDs to their corresponding indices\n    id_to_idx = {id: idx for idx, id in enumerate(unique_ids)}\n    # Replace the names with unique IDs\n    df['name'] = df['name'].map(id_to_idx)\n    # Return the modified dataframe\n    return df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "import numpy as np\n\n# Create a unique ID for each row\nunique_ids = np.arange(1, len(df) + 1)\n\n# Combine name and a into a new column\ndf['new_id'] = df['name'].astype(str) + np.array([str(i) for i in unique_ids]).astype(str)\n\n# Replace 'a' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('a', str(unique_ids[0])))\n\n# Replace 'b' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('b', str(unique_ids[1])))\n\n# Replace 'c' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('c', str(unique_ids[2])))\n\n# Replace 'a' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('a', str(unique_ids[0])))\n\n# Replace 'b' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('b', str(unique_ids[1])))\n\n# Replace 'c' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('c', str(unique_ids[2])))\n\n# Replace 'a' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('a', str(unique_ids[0])))\n\n# Replace 'b' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('b', str(unique_ids[1])))\n\n# Replace 'c' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('c', str(unique_ids[2])))\n\n# Replace 'a' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('a', str(unique_ids[0])))\n\n# Replace 'b' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('b', str(unique_ids[1])))\n\n# Replace 'c' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('c', str(unique_ids[2])))\n\n# Replace 'a' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('a', str(unique_ids[0])))\n\n# Replace 'b' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('b', str(unique_ids[1])))\n\n# Replace 'c' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('c', str(unique_ids[2])))\n\n# Replace 'a' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('a', str(unique_ids[0])))\n\n# Replace 'b' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('b', str(unique_ids[1])))\n\n# Replace 'c' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('c', str(unique_ids[2])))\n\n# Replace 'a' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('a', str(unique_ids[0])))\n\n# Replace 'b' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('b', str(unique_ids[1])))\n\n# Replace 'c' with unique ID\ndf['new_id'] = df['new_id'].apply(lambda x: x.replace('c', str(unique", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "import numpy as np\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15']\ndf['someBool'] = np.where(df['someBool'], df['01/12/15'], -df['01/12/15'])\n\n# Create a new column 'date' where the value is the second column of the original data\ndf['date'] = df['02/12/15']\n\n# Create a new column 'date' where the value is the second column of the original data\ndf['date'] = df['02/12/15']\n\n# Create a new column 'value' where the value is the first column of the original data\ndf['value'] = df['01/12/15']\n\n# Create a new column 'someBool' where the value is the boolean value of the original data\ndf['someBool'] = df['someBool']\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1, -1)\n\n# Create a new column 'value' where the value is the first column of the original data\n# and the boolean value is used to determine whether the value is positive or negative\ndf['value'] = df['01/12/15'] * np.where(df['someBool'], 1", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "import numpy as np\n\n# Create a new column 'others' to store the date\ndf['others'] = df['01/12/15'].apply(lambda x: np.where(x == -1, '02/12/15', x))\n\n# Create a new column 'value' to store the original value\ndf['value'] = df['02/12/15'].apply(lambda x: np.where(x == -1, 300, x))\n\n# Create a new column 'bool' to store the boolean value\ndf['bool'] = df['someBool'].apply(lambda x: np.where(x, 1, 0))\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf['others'] = df['others'].apply(lambda x: x * 2)\n\n# Create a new column 'value' to store the value\ndf['value'] = df['value'].apply(lambda x: x * 2)\n\n# Create a new column 'others' to store the value\ndf", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "import numpy as np\n\n# Create a new column 'value' where the value is the first non-None value in '01/12/15' and '02/12/15'\ndf['value'] = np.where(df['01/12/15'].notnull(), df['01/12/15'], df['02/12/15'])\n\n# Create a new column 'date' where the value is the first non-None value in '01/12/15' and '02/12/15'\ndf['date'] = np.where(df['01/12/15'].notnull(), df['01/12/15'], df['02/12/15'])\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool'] = df['someBool']\n\n# Create a new column 'someBool' where the value is the value in 'someBool'\ndf['someBool", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "result = df[df['c'] > 0.5][columns]\n", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "result = df[df['c'] > 0.45][columns]\n", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    return df[df['c'] > 0.5][columns]\n    ### END SOLUTION\n\ndf = pd.DataFrame(np.random.rand(4, 5), columns=['a', 'b', 'c', 'd', 'e'])\nprint(f(df, ['b', 'e']))\n## END\n\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    mask = df['c'] > 0.5\n    result = df[mask][columns]\n    result['sum'] = result['b'] + result['e']\n    return result\n    ### END SOLUTION\n\ndf = pd.DataFrame({'a': [0.945686, 0.919359, 0.976163, 0.459039],\n                   'b': [0.667057, 0.462478, 0.008204, 0.473096],\n                   'c': [0.976163, 0.621712, 0.208423, 0.980471],\n                   'd': [0.621712, 0.621712, 0.621712, 0.621712],\n                   'e': [0.667057, 0.667057, 0.667057, 0.667057]})\n\ndf = df.reset_index()\nprint(f(df))\n## END\n\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint f(df, ['b', 'e'])\n## END\n\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\n\ndef overlap_rows(df, X):\n    # Create a mask to identify overlapping rows\n    mask = np.abs(np.diff(df['date'].values)) <= X\n    # Create a mask to exclude rows with the same date\n    mask_unique = np.ones(mask.shape, dtype=bool)\n    for i in range(X):\n        mask_unique = np.logical_and(mask_unique, np.diff(mask, axis=1) <= i)\n    # Apply the mask to the original data\n    return df[mask | mask_unique]\n\n\n# Test the function\nresult = overlap_rows(df, X)\nprint(result)\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "def overlap_rows(df, X):\n    # Create a mask to identify overlapping rows\n    mask = np.zeros(df.shape[0], dtype=bool)\n    for i in range(df.shape[0]):\n        for j in range(i + 1, df.shape[0]):\n            # Calculate the difference in weeks\n            weeks_diff = np.abs((df['date'].astype(np.datetime64) - df['date'].astype(np.datetime64)).days / 1000000000)\n            if weeks_diff <= X:\n                mask[j] = True\n    return mask\n\n\n# Define the number of weeks\nX = 7\n\n# Find overlapping rows\noverlap_mask = overlap_rows(df, X)\n\n# Select rows that match the mask\nresult = df[overlap_mask]\n\nprint(result)\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "def overlap_rows(df, X):\n    # Convert date to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Create a mask to identify overlapping rows\n    mask = np.zeros(df.shape[0], dtype=bool)\n    for i in range(df.shape[0]):\n        for j in range(i + 1, df.shape[0]):\n            if np.abs(df['date'][i] - df['date'][j]) <= X * np.timedelta64(1, 'D'):\n                mask[j] = True\n                \n    # Select overlapping rows\n    overlapping_rows = df[mask]\n    \n    return overlapping_rows\n\n\n# Test the function\nX = 7\noverlap_rows_result = overlap_rows(df, X)\nprint(overlap_rows_result)\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "import numpy as np\n\n\ndf_bin = np.zeros(df.shape)\n\n\nfor i in range(df.shape[0]):\n    if i % 3 == 0:\n        df_bin[i] = df['col1'][i]\n    else:\n        df_bin[i] = df['col1'][i] / 2\n\n\nprint(df_bin)\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "import numpy as np\n\n\ndef bin_data(df, n):\n    bin_df = np.zeros((df.shape[0]//n + 1, 1))\n    for i in range(n):\n        bin_df = np.hstack((bin_df, np.zeros((df.shape[0]//n + 1, 1))))\n    return bin_df\n\n\ndef bin_data_2(df):\n    bin_df = np.zeros((df.shape[0]//3 + 1, 1))\n    for i in range(3):\n        bin_df = np.hstack((bin_df, np.zeros((df.shape[0]//3 + 1, 1))))\n    return bin_df\n\n\nresult = bin_data_2(df)\nprint(result)\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "import numpy as np\n\n\ndef bin_data(df):\n    n = len(df)\n    bin_size = 4\n    bin_data = np.zeros((n // bin_size + 1, bin_size))\n    for i in range(n):\n        bin_data[i // bin_size, i % bin_size] = df.loc[i, 'col1']\n    return bin_data\n\n\nresult = bin_data(df)\nprint(result)\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "import numpy as np\n\n\ndf_bin = np.zeros((df.shape[0]//3, 1))\nfor i in range(df.shape[0]//3):\n    df_bin[i] = np.mean(df.iloc[i*3:(i+1)*3, 0])\n\n\ndf_bin = np.append(df_bin, np.zeros((df.shape[0]%3, 1)))\n\n\ndf_bin = np.c_[df_bin, df.iloc[df.shape[0]//3:, 0]]\n\n\nprint(df_bin)\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "import numpy as np\n\n\ndef bin_data(df):\n    n = len(df)\n    bin_size = 3\n    for i in range(0, n, bin_size):\n        bin_df = df.iloc[i:i+bin_size]\n        sum_val = np.sum(bin_df['col1'])\n        avg_val = np.mean(bin_df['col1'])\n        print(f'Sum: {sum_val}, Avg: {avg_val}')\n\n\nbin_data(df)\n", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "import numpy as np\n\n\ndef bin_data(df):\n    n = len(df)\n    for i in range(n):\n        if i == 0:\n            print(df.sum())\n        elif i == 1:\n            print(df.mean())\n        elif i == 2:\n            print(df.sum())\n        elif i == n - 3:\n            print(df.mean())\n        elif i == n - 2:\n            print(df.sum())\n        elif i == n - 1:\n            print(df.mean())\n\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\nbin_data(df)\nEND SOLUTION\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df['A'] = df['A'].fillna(df['A'].where(df['A'] != 0, df['A'].shift()))\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df['A'] = df['A'].fillna(df['A'].shift(1))\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "import numpy as np\n\n# Find the indices where the values are not zero\nidx = np.where(df['A'] != 0)[0]\n\n# Find the maximum value before and after each non-zero value\nmax_before = np.maximum.accumulate(df['A'][idx])\nmax_after = np.maximum.accumulate(df['A'][idx][::-1])[::-1]\n\n# Find the maximum value between the two values before and after\nmax_value = np.maximum(max_before, max_after[::-1])\n\n# Replace zero values with the maximum value\ndf['A'][idx] = max_value[idx]\n\n# Reversing the order of the array to get the desired output\ndf['A'] = df['A'][::-1]\n\nprint(df)\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "import numpy as np\n\n# Create a new column 'duration' with integer values\ndf['duration'] = df['duration'].apply(lambda x: int(x.split(' ')[0]))\n\n# Create a new column 'time' with time values\ndf['time'] = df['duration'].apply(lambda x: x.split(' ')[1])\n\n# Create a new column 'number' from duration\ndf['number'] = df['duration'].apply(lambda x: int(x))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.timedelta64(1, 'D'))\n\n# Create a new column 'time_days' from time\ndf['time_days'] = df['time'].apply(lambda x: np.tim", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "import numpy as np\n\n# Create a new column 'time_day' based on the values of 'duration'\ndf['time_day'] = np.where(df['duration'].str.contains('year'), 365,\n                          np.where(df['duration'].str.contains('month'), 30,\n                                   np.where(df['duration'].str.contains('week'), 7,\n                                            np.where(df['duration'].str.contains('day'), 1,\n                                                     np.nan)))))\n\n# Create a new column 'number' based on the values of 'duration'\ndf['number'] = np.where(df['duration'].str.contains('year'), 1,\n                        np.where(df['duration'].str.contains('month'), 1,\n                                 np.where(df['duration'].str.contains('week'), 1,\n                                          np.where(df['duration'].str.contains('day'), 1,\n                                                   np.nan)))))\n\n# Create a new column 'duration' based on the values of 'duration'\ndf['duration'] = df['duration'].str.split(' ').str[0]\n\n# Create a new column 'time' based on the values of 'duration'\ndf['time'] = df['duration'].str.split(' ').str[1]\n\n# Convert 'time' column to integer\ndf['time'] = df['time'].apply(lambda x: int(x))\n\n# Create a new column 'time_day' based on the values of 'duration'\ndf['time_day'] = np.where(df['duration'].str.contains('year'), 365,\n                          np.where(df['duration'].str.contains('month'), 30,\n                                   np.where(df['duration'].str.contains('week'), 7,\n                                            np.where(df['duration'].str.contains('day'), 1,\n                                                     np.nan)))))\n\n# Create a new column 'number' based on the values of 'duration'\ndf['number'] = np.where(df['duration'].str.contains('year'), 1,\n                        np.where(df['duration'].str.contains('month'), 1,\n                                 np.where(df['duration'].str.contains('week'), 1,\n                                          np.where(df['duration'].str.contains('day'), 1,\n                                                   np.nan)))))\n\n# Create a new column 'duration' based on the values of 'duration'\ndf['duration'] = df['duration'].str.split(' ').str[0]\n\n# Create a new column 'time' based on the values of 'duration'\ndf['time'] = df['duration'].str.split(' ').str[1]\n\n# Convert 'time' column to integer\ndf['time'] = df['time'].apply(lambda x: int(x))\n\nprint(df)\n", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "\n    # Create a new column 'duration_days' from 'duration' column\n    df['duration_days'] = df['duration'].str.extract('(\\d+)', expand=False).astype(int)\n    # Create a new column 'time' from 'duration' column\n    df['time'] = df['duration'].str.extract('([a-z]+)', expand=False)\n    # Create a new column 'number' from 'duration' column\n    df['number'] = df['duration'].str.extract('(\\d+)', expand=False).astype(int)\n    # Create a new column 'time_days' from 'duration_days' column\n    df['time_days'] = df['duration_days'].astype(int)\n    return df\n\nprint(f())\n## END SOLUTION\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "import numpy as np\n\n\nduration = df['duration'].values\n\n\nduration_num = np.array([int(x) for x in duration if x.isdigit()])\nduration_time = np.array([x for x in duration if not x.isdigit()])\n\n\ndf['numer'] = duration_num\ndf['time'] = duration_time\n\n\ndf['time_day'] = np.where(df['duration'].str.contains('year'), 365,\n                          np.where(df['duration'].str.contains('month'), 30,\n                                   np.where(df['duration'].str.contains('week'), 7,\n                                            np.where(df['duration'].str.contains('day'), 1,\n                                                     np.nan)))))\n\n\ndf['time_day'] = df['time_day'].astype(int)\n\n\ndf['time_day'] = df['time_day'].astype(int)\n\n\ndf['number'] = df['duration'].str.extract('(\\d+)').astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['number'].astype(int)\n\n\ndf['number'] = df['", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "\n\ndef check_equal(df1, df2, columns_check_list):\n    return np.all(np.equal(df1[columns_check_list], df2[columns_check_list]))\n\n\nresult = check_equal(df1, df2, columns_check_list)\nprint(result)\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "\n\ndef check_equal(df1, df2, columns_check_list):\n    return np.all(np.equal(df1[columns_check_list], df2[columns_check_list]))\n\n\nresult = check_equal(df1, df2, columns_check_list)\n\n\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "import numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\n# Test the function\ndf = parse_date(df)\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_date(df):\n    # Convert date index to datetime\n    df.index = pd.to_datetime(", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "import numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.index = df.index.set_levels(parse_datetime(df.index.levels[1]))\n\n\nprint(df)\nEND SOLUTION\n<code>\nimport numpy as np\n\n\ndef parse_datetime(index):\n    return np.array([np.datetime64(datetime) for datetime in index])\n\n\ndf.", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "\n    # Convert date index to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Reshape the data to get the desired output\n    return df['date'].values, df['x'].values, df['y'].values\n    ### END SOLUTION\n\n# Test the function\ndf = pd.DataFrame({\n    'id': ['abc', 'abc', 'abc'],\n    'date': ['3/1/1994', '9/1/1994', '3/1/1995'],\n    'x': [100, 90, 80],\n    'y': [7, 8, 9]\n})\n\nprint(f(df))\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "\n    # Convert date index to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    # Swap the two levels\n    df = df.set_index(['id', 'date']).sort_index()\n    # Return the final output\n    return df\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "import numpy as np\n\n# Create a new column for variable name\ndf['Var'] = df['Variable'].apply(lambda x: x.split(' ')[0])\n\n# Create a new column for year\ndf['Year'] = df['Variable'].apply(lambda x: int(x.split(' ')[1]))\n\n# Reshape the data\ndf = df.pivot(index='Country', columns='Var', values='Year')\n\n# Reorder the columns\ndf = df.reindex(['Argentina', 'Brazil'], axis=1)\n\n# Replace NaN values with 0\ndf = df.fillna(0)\n\n# Reshape the data back to wide format\ndf = df.reset_index()\n\n# Reorder the columns\ndf = df[['Country', 'Argentina', 'Brazil']]\n\n# Replace NaN values with 0\ndf = df.fillna(0)\n\n# Print the final data\nprint(df)\nEND SOLUTION\n<code>\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "import numpy as np\n\n# Create a new column 'year' with values from 2005 to 2000 in descending order\ndf['year'] = np.arange(2005, 0, -1)\n\n# Reshape the data to long format\ndf = df.melt(id_vars=['Country', 'Variable', 'year'],\n            var_name='year',\n            value_name='var')\n\n# Reorder the columns\ndf = df[['Country', 'Variable', 'year', 'var']]\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='var')\n\n# Reorder the columns\ndf = df.reset_index()\n\n# Reorder the columns\ndf = df[['Country', 'Variable', 'year']]\n\n# Reorder the columns\ndf = df.sort_values(by=['Country', 'Variable', 'year'], ascending=[True, True, True])\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf = df.pivot(index='Country', columns='Variable', values='year')\n\n# Reshape the data to long format again\ndf", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "import numpy as np\n\n# Get the number of columns\nnum_cols = len(df.columns)\n\n# Create a mask to filter rows\nmask = np.abs(df.values) < 1\n\n# Apply the mask to the data frame\ndf_filtered = df[mask]\n\n# Reshape the mask to match the original shape\nmask = mask.reshape(-1, num_cols)\n\n# Create a mask to select columns\nmask_col = np.ones((df.shape[0], num_cols), dtype=bool)\n\n# Apply the mask to the data frame\ndf_filtered = df[mask_col]\n\n# Reshape the mask to match the original shape\nmask_col = mask_col.reshape(-1, num_cols)\n\n# Apply the mask to the data frame\ndf_filtered = df[mask_col]\n\n# Reshape the mask to match the original shape\nmask = mask.reshape(-1)\n\n# Apply the mask to the data frame\ndf_filtered = df[mask]\n\n# Print the filtered data frame\nprint(df_filtered)\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "import numpy as np\n\n# Get the number of columns\nnum_cols = len(df.columns)\n\n# Create a mask to filter rows\nmask = np.abs(df.values[:, 1:]) > 1\n\n# Create a mask to select columns\nmask_col = np.ones((num_cols,), dtype=bool)\nmask_col[0] = False\n\n# Apply the mask to the data\ndata = df.values[mask, :]\n\n# Create a new data frame\ndf_filtered = pd.DataFrame(data, columns=df.columns)\n\n# Print the result\nprint(df_filtered)\n", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "import numpy as np\n\n# Create a mask to select rows where any column's absolute value is greater than 1\nmask = np.any(np.abs(df.values) > 1, axis=1)\n\n# Use the mask to filter the data\nfiltered_df = df[mask]\n\n# Reshape the filtered data to match the original shape\nfiltered_df = filtered_df.reshape(-1, len(df.columns))\n\n# Reorder the columns to match the original order\nreordered_df = df[df.columns]\n\n# Combine the filtered and reordered data\nfinal_df = pd.concat([filtered_df, reordered_df], axis=1)\n\n# Reorder the columns to match the original order\nfinal_df = final_df.reorder_columns(df.columns)\n\nprint(final_df)\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df['A'] = df['A'].str.replace('&AMP;', '&')\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df['A'] = df['A'].str.replace('&LT;', '<')\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "\n    # replace 'AMP' with 'AMP'\n    df = df.replace({'AMP': 'AMP'}, regex=True)\n    # replace 'AMP' with 'AMP'\n    df = df.replace({'AMP': 'AMP'}, regex=True)\n    # return the result\n    return df\n\nf()\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df['A'] = df['A'].str.replace('&AMP;', '')\ndf['A'] = df['A'].str.replace('&LT;', '')\ndf['A'] = df['A'].str.replace('&GT;', '')\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df = df.apply(lambda x: x.replace('&AMP;', '&'))\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "import numpy as np\n\ndef split_name(name):\n    if ' ' in name:\n        return np.split(name, np.where(np.diff(np.where(np.diff(np.array(list(name))))))[0])\n    else:\n        return np.array([name])\n\ndf['first_name'] = np.vectorize(split_name)(df['name'])\ndf['last_name'] = np.where(df['name'].str.split().apply(lambda x: len(x) == 1), np.nan, df['name'].str.split().apply(lambda x: x[-1]))\n\nprint(df)\n", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "import numpy as np\n\ndef split_name(name):\n    if ' ' in name:\n        return np.split(name, 2)\n    else:\n        return np.full(2, name)\n\ndf['1_name'], df['2_name'] = np.split(df['name'], 2)\n\nprint(df)\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "import numpy as np\n\ndef split_name(name):\n    if np.isnan(name):\n        return np.nan, np.nan\n    else:\n        name_list = name.split()\n        if len(name_list) == 1:\n            return name_list[0], np.nan\n        elif len(name_list) == 2:\n            return name_list[0], name_list[1]\n        else:\n            return np.nan, np.nan\n\n\ndf['first_name'] = np.where(df['name'].apply(validate_single_space_name) == None, np.nan, df['name'].apply(lambda x: x.split()[0]))\ndf['middle_name'] = np.where(df['name'].apply(validate_single_space_name) == None, np.nan, df['name'].apply(lambda x: x.split()[1]))\ndf['last_name'] = np.where(df['name'].apply(validate_single_space_name) == None, np.nan, df['name'].apply(lambda x: x.split()[2]))\n\n\ndf = df.drop(columns=['name'])\n\n\ndf = df.apply(lambda x: x.astype(str), axis=0)\n\n\ndf = df.apply(lambda x: x.str.split(sep=' '), axis=1)\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y[0] if len(y) == 1 else y[1:] if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y if len(y) == 2 else y))\n\n\ndf = df.apply(lambda x: x.apply(lambda y: y if len(y) == 1 else y", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "result = df1.merge(df2, left_on='Timestamp', right_on='Timestamp', how='left')\nresult = result.drop(columns='Timestamp')\nresult = result.sort_values('Timestamp')\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'stuff': 'stuff_df1', 'data': 'data_df1'})\nresult = result.merge(df1, left_on='stuff_df1', right_on='stuff', how='left')\nresult = result.merge(df1, left_on='data_df1', right_on='data', how='left')\nresult = result.drop(columns=['stuff_df1', 'data_df1'])\nresult = result.sort_values('stuff')\nresult = result.sort_values('data')\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'stuff': 'stuff_df2', 'data': 'data_df2'})\nresult = result.merge(df1, left_on='stuff_df2', right_on='stuff', how='left')\nresult = result.merge(df1, left_on='data_df2', right_on='data', how='left')\nresult = result.drop(columns=['stuff_df2', 'data_df2'])\nresult = result.sort_values('stuff')\nresult = result.sort_values('data')\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'stuff': 'stuff_df3', 'data': 'data_df3'})\nresult = result.merge(df1, left_on='stuff_df3', right_on='stuff', how='left')\nresult = result.merge(df1, left_on='data_df3', right_on='data', how='left')\nresult = result.drop(columns=['stuff_df3', 'data_df3'])\nresult = result.sort_values('stuff')\nresult = result.sort_values('data')\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'stuff': 'stuff_df4', 'data': 'data_df4'})\nresult = result.merge(df1, left_on='stuff_df4', right_on='stuff', how='left')\nresult = result.merge(df1, left_on='data_df4', right_on='data', how='left')\nresult = result.drop(columns=['stuff_df4', 'data_df4'])\nresult = result.sort_values('stuff')\nresult = result.sort_values('data')\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'stuff': 'stuff_df5', 'data': 'data_df5'})\nresult = result.merge(df1, left_on='stuff_df5', right_on='stuff', how='left')\nresult = result.merge(df1, left_on='data_df5', right_on='data', how='left')\nresult = result.drop(columns=['stuff_df5', 'data_df5'])\nresult = result.sort_values('stuff')\nresult = result.sort_values('data')\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'stuff': 'stuff_df6', 'data': 'data_df6'})\nresult = result.merge(df1, left_on='stuff_df6', right_on='stuff', how='left')\nresult = result.merge(df1, left_on='data_df6', right_on='data', how='left')\nresult = result.drop(columns=['stuff_df6', 'data_df6'])\nresult = result.sort_values('stuff')\nresult = result.sort_values('data')\nresult = result.reset_index(drop=True)\nresult = result.rename(columns={'stuff': 'stuff_df7', 'data': 'data_df7", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "result = pd.merge(df1, df2, left_on='Timestamp', right_on='Timestamp', how='left')\nresult = result.sort_values('Timestamp')\n", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= 50, df['col1'], np.maximum(df['col1'], np.maximum(df['col2'], df['col3'])))\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = np.where((df['col2'] > 50) & (df['col3'] > 50), df['col1'], df['col1'] + df['col2'] + df['col3'])\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "import numpy as np\n\ndef check_integer(row):\n    values = row['Field1'].split(',')\n    errors = []\n    for value in values:\n        if not np.isnan(value):\n            if not np.issubdtype(type(value), np.integer):\n                errors.append(value)\n    return errors\n\ndf['Errors'] = df.apply(check_integer, axis=1)\n\n# Convert errors to string\ndf['Errors'] = df['Errors'].apply(lambda x: ', '.join(map(str, x)))\n\nprint(df)\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "import numpy as np\n\ndef get_integer_values(df):\n    mask = np.isnan(df[\"Field1\"])\n    mask = np.logical_not(mask)\n    integer_values = df[\"Field1\"][mask]\n    return integer_values.astype(int)\n\n\ndf[\"Field1\"] = get_integer_values(df)\n\nprint(df)\n", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "\n    result = []\n    for i in range(len(df)):\n        if not df.loc[i, \"Field1\"].isnumeric():\n            result.append(df.loc[i, \"Field1\"])\n    return result\n\nprint(f())\n## END SOLUTION\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "import numpy as np\n\n\ndef compute_percentage(df):\n    # Calculate the total number of values for each category\n    total_values = df.groupby('cat').sum()\n    \n    # Calculate the percentage of each value in each category\n    percentages = df.groupby('cat').apply(lambda x: x / total_values)\n    \n    # Replace NaN values with 0\n    percentages = percentages.fillna(0)\n    \n    return percentages\n\n\n# Compute the percentage of each value in each category\npercentages = compute_percentage(df)\n\n\n# Print the result\nprint(percentages)\nEND SOLUTION\n<code>\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "import numpy as np\n\n# compute the total value for each column\ntotal_val = df.sum()\n\n# compute the percentage of each value\npercentage = df / total_val * 100\n\n# replace the values in the dataframe with the percentage\ndf = df / total_val * 100\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column names with the percentage\ndf.columns = ['val' + str(i) + '\\%']\n\n# replace the column", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]\nprint(result)\n", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]\nprint(result)\n", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "result = df.drop(test)\nprint(result)\n", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": "\n    return df.loc[test]\n    ### END SOLUTION\n\ndf = pd.DataFrame({\n    'alleles': ['A/C', 'A/T', 'T/A', 'C/A', 'C/T'],\n    'chrom': [0, 0, 0, 0, 0],\n    'pos': [0, 0, 0, 0, 0],\n    'strand': ['+', '+', '+', '+', '+'],\n    'assembly': [np.nan, np.nan, np.nan, np.nan, np.nan],\n    'center': [np.nan, np.nan, np.nan, np.nan, np.nan],\n    'protLSID': [np.nan, np.nan, np.nan, np.nan, np.nan],\n    'assayLSID': [np.nan, np.nan, np.nan, np.nan, np.nan]\n})\n\ntest = ['TP3', 'TP12', 'TP18', 'TP3']\n\nprint(f(df, test))\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "import numpy as np\n\n# Calculate the distance between each pair of cars\ndistances = np.sqrt((df['x'].values[:, None] - df['x'].values[None, :])**2 + (df['y'].values[:, None] - df['y'].values[None, :])**2)\n\n# Find the nearest neighbour for each car\nneighbours = np.argmin(distances, axis=1)\n\n# Calculate the distance between each car and its nearest neighbour\neuclidean_distances = distances[np.arange(len(df))[:, None], neighbours]\n\n# Calculate the average distance for each car\naverage_distances = np.mean(euclidean_distances, axis=1)\n\n# Create a new dataframe with average distances\ndf_result = pd.DataFrame({'car': df['car'], 'average_distance': average_distances})\n\nprint(df_result)\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "import numpy as np\n\n# Calculate the distance between each pair of cars\ndistances = np.sqrt((df['x'].values[:, None] - df['x'].values[None, :])**2 + (df['y'].values[:, None] - df['y'].values[None, :])**2)\n\n# Find the indices of the maximum distance for each car\nmax_indices = np.argmax(distances, axis=1)\n\n# Find the indices of the maximum distance for each car in each frame\nmax_frame_indices = np.argmax(distances, axis=0)\n\n# Get the indices of the farthest car for each frame\nfarthest_car_indices = np.argmax(distances, axis=1)\n\n# Get the farthest car for each frame\nfarthest_car = df['car'].values[farthest_car_indices]\n\n# Calculate the average distance for each frame\navg_distances = np.mean(distances, axis=1)\n\n# Create a dataframe to store results\ndf_results = pd.DataFrame({\n    'time': df['time'],\n    'farthest_car': farthest_car,\n    'avg_distance': avg_distances\n})\n\n# Print the results\nprint(df_results)\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "def concatenate_rows(df):\n    # Get the column names\n    col_names = df.columns.tolist()\n    \n    # Initialize an empty list to store the concatenated rows\n    concatenated_rows = []\n    \n    # Iterate over each column\n    for col in col_names:\n        # Get the values in the current column\n        col_values = df[col].values\n        \n        # Concatenate the values\n        concatenated_values = np.concatenate((concatenated_rows, col_values))\n        \n        # Append the concatenated values to the concatenated rows list\n        concatenated_rows.append(concatenated_values)\n    \n    # Concatenate the concatenated rows into a single array\n    concatenated_array = np.concatenate(concatenated_rows)\n    \n    return concatenated_array\n\n# Test the function\ndf_concatenated = concatenate_rows(df)\nprint(df_concatenated)\nEND SOLUTION\n<code>\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "def concatenate_rows(df):\n    # Get the column names\n    col_names = df.columns.tolist()\n    \n    # Initialize an empty list to store the concatenated rows\n    concatenated_rows = []\n    \n    # Iterate over each column\n    for col in col_names:\n        # Get the values in the current column\n        col_values = df[col].values\n        \n        # Concatenate the values\n        concatenated_values = np.concatenate((concatenated_rows, col_values))\n        \n        # Append the concatenated values to the concatenated rows list\n        concatenated_rows.append(concatenated_values)\n    \n    # Concatenate the concatenated rows into a single array\n    concatenated_array = np.concatenate(concatenated_rows)\n    \n    return concatenated_array\n\n# Test the function\ndf_concatenated = concatenate_rows(df)\nprint(df_concatenated)\nEND SOLUTION\n<code>\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "def concatenate_keywords(cols):\n    keywords = []\n    for col in cols:\n        if not np.isnan(col):\n            keywords.append(col)\n    return \"-\".join(keywords)\n\ndf[\"keywords_all\"] = df[\"keywords_0\"].apply(lambda cols: concatenate_keywords([df.keywords_1, df.keywords_2, df.keywords_3]))\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: concatenate_keywords([df.keywords_2, df.keywords_3]))\ndf[\"keywords_all\"] = df[\"keywords_all\"].apply(lambda cols: concatenate_keywords([df.keywords_3]))\n\nprint(df)\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "def concatenate_keywords(cols):\n    keywords = []\n    for col in cols:\n        keywords.extend(col.dropna().tolist())\n    return \"-\".join(keywords)\n\n\ndf[\"keywords_all\"] = df[\"keywords_0\"].apply(lambda cols: concatenate_keywords(cols))\ndf[\"keywords_all\"] = df[\"keywords_1\"].apply(lambda cols: concatenate_keywords(cols))\ndf[\"keywords_all\"] = df[\"keywords_2\"].apply(lambda cols: concatenate_keywords(cols))\ndf[\"keywords_all\"] = df[\"keywords_3\"].apply(lambda cols: concatenate_keywords(cols))\n\n\ndf = df.drop(columns=['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3'])\n\n\nprint(df)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "import numpy as np\n\n# Select 20% of rows\nn = int(0.2 * len(df))\nindexes = np.random.choice(df.index, n, replace=False)\n\n# Set Quantity to zero\ndf.loc[indexes, 'Quantity'] = 0\n\n# Return the indexes of the altered rows\nreturn indexes\nEND SOLUTION\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "import numpy as np\n\n# Select 20% of rows\nn = int(0.2 * len(df))\nindexes = np.random.choice(len(df), n, replace=False)\n\n# Select rows\nselected_rows = df.loc[indexes]\n\n# Change value of ProductId to zero\nselected_rows['ProductId'] = 0\n\n# Keep indexes\nindexes = selected_rows.index\n\n# Print result\nprint(selected_rows)\n\n# Print indexes\nprint(indexes)\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "import numpy as np\n\n# Select 20% of rows for each user\nn_rows = int(0.2 * df.shape[0])\nindices = np.random.choice(df.index, size=n_rows, replace=False)\n\n# Select rows for each user\ndf_selected = df.loc[indices]\n\n# Set Quantity to zero\ndf.loc[indices, 'Quantity'] = 0\n\n# Get the indexes of the altered rows\nidx_changed = df.index[~np.isin(df.index, indices)]\n\n# Print the results\nprint(\"Selected rows:\")\nprint(df_selected)\nprint(\"\\nIndexes of altered rows:\")\nprint(idx_changed)\nprint(\"\\nQuantity values after alteration:\")\nprint(df.loc[idx_changed, 'Quantity'])\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal DataFrame:\")\nprint(df)\n\n# Print the final DataFrame\nprint(\"\\nFinal", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = df.index[duplicate_bool]\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "duplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = df[duplicate_bool].index\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "\n    df_duplicates = df.drop_duplicates(keep='first')\n    df_duplicates['index_original'] = df.index[df.duplicated(keep=False)]\n    return df_duplicates\n\nresult = f()\nprint(result)\n## END SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "duplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\nduplicate_index = duplicate.index\ndf['index_original'] = df.index\nduplicate_index_col = df['index_original'].loc[duplicate_index]\ndf['index_original'] = duplicate_index_col\nduplicate_index_col\nEND SOLUTION\n<code>\nprint(duplicate_index_col)\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "\n# Find duplicates\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\n\n# Get the index of the last duplicate\nindex_last_duplicate = df[duplicate_bool].index\n\n# Add a new column referring to the index of the last duplicate\ndf['index_original'] = index_last_duplicate\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "import numpy as np\n\n# Find max count for each group\nmax_count = np.max(df['count'])\n\n# Find indices of max count\nmax_count_indices = np.where(df['count'] == max_count)[0]\n\n# Select rows where count equals max\nresult = df[df['count'].equals(max_count)]\n\n# Select rows where count equals max for each group\nresult_group = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == max_count])\n\n# Print results\nprint(result)\nprint(result_group)\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "import numpy as np\n\n# group by ['Sp','Mt'] columns\ngroup = df.groupby(['Sp','Mt'])\n\n# find max count\nmax_count = group['count'].max()\n\n# find rows where count is max\nmax_count_rows = df[df['count'].equals(max_count)]\n\n# get the rows where count is max in each group\nresult = max_count_rows\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "import numpy as np\n\n# Find the minimum count for each group\nmin_count = np.min(df['count'])\n\n# Find the indices where count equals min_count\nidx = np.where(df['count'] == min_count)[0]\n\n# Select rows where count equals min_count\nresult = df.loc[idx]\n\nprint(result)\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "import numpy as np\n\n# Create a mask to select rows where count equals max\nmask = np.where(df['count'].groupby(df['Sp', 'Value']).max() == df['count'], True, False)\n\n# Select rows where count equals max\nresult = df[mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df[df['Category'].isin(filter_list)]\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "df_filter = df[df['Category'].apply(lambda x: x not in filter_list)]\n", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "import numpy as np\n\n\ndef column_melt(df, col1, col2):\n    # Get the column indices for the two levels\n    idx1 = np.where(df[col1] == 1)[0]\n    idx2 = np.where(df[col2] == 1)[0]\n\n    # Create a new column index for each level\n    new_col_idx = np.concatenate([[0], idx1 + 1, idx2 + 1, [len(df) + 1]])\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_values = df[new_col_names].values\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col2}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col2}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col2}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col2}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col2}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col2}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col2}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col2}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level\n    new_col_names = [f'{col1}_{i}' for i in range(1, len(new_col_idx) - 1)]\n\n    # Create a new column index for each level", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "import numpy as np\n\n\ndef column_melt(df, col1, col2, col3):\n    # Create a new column with the values from the input columns\n    new_col = np.column_stack((df[col1], df[col2], df[col3]))\n    \n    # Create a new column for the column levels\n    new_col = np.column_stack((new_col, np.repeat(np.arange(3), len(new_col))))\n    \n    # Create a new column for the values\n    new_col = np.column_stack((new_col, df.values))\n    \n    # Reshape the new column to have the correct shape\n    new_col = new_col.reshape(-1, 3)\n    \n    # Create a new column for the variable names\n    new_col = np.column_stack((new_col, np.repeat(['A', 'B', 'C'], len(new_col))))\n    \n    # Create a new column for the values\n    new_col = np.column_stack((new_col, np.arange(3)))\n    \n    # Reshape the new column to have the correct shape\n    new_col = new_col.reshape(-1, 4)\n    \n    return new_col\n\n\n# Define the columns to be melted\ncol1 = 'col1'\ncol2 = 'col2'\ncol3 = 'col3'\n\n\n# Call the function to melt the data frame\nresult = column_melt(df, col1, col2, col3)\n\n\n# Print the result\nprint(result)\n\n\n# Output\n# [[0 0 0 0]\n#  [1 1 1 1]\n#  [2 2 2 2]\n#  [3 3 3 3]\n#  [4 4 4 4]\n#  [5 5 5 5]\n#  [6 6 6 6]\n#  [7 7 7 7]\n#  [8 8 8 8]\n#  [9 9 9 9]\n#  [10 10 10 10]\n#  [11 11 11 11]\n#  [12 12 12 12]\n#  [13 13 13 13]\n#  [14 14 14 14]\n#  [15 15 15 15]\n#  [16 16 16 16]\n#  [17 17 17 17]\n#  [18 18 18 18]\n#  [19 19 19 19]\n#  [20 20 20 20]\n#  [21 21 21 21]\n#  [22 22 22 22]\n#  [23 23 23 23]\n#  [24 24 24 24]\n#  [25 25 25 25]\n#  [26 26 26 26]\n#  [27 27 27 27]\n#  [28 28 28 28]\n#  [29 29 29 29]\n#  [30 30 30 30]\n#  [31 31 31 31]\n#  [32 32 32 32]\n#  [33 33 33 33]\n#  [34 34 34 34]\n#  [35 35 35 35]\n#  [36 36 36 36]\n#  [37 37 37 37]\n#  [38 38 38 38]\n#  [39 39 39 39]\n#  [40 40 40 40]\n#  [41 41 41 41]\n#  [42", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id').cumsum(['val'])['val']\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "import numpy as np\n\ndf['cumsum'] = np.cumsum(df['val'])\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "import numpy as np\n\ndf['cumsum'] = np.cumsum(df['val'])\ndf['cumsum'] = df['cumsum'].apply(lambda x: int(x))\nprint(df)\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "import numpy as np\n\ndf['cummax'] = np.cummax(df['val'])\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "import numpy as np\n\ndf['cumsum'] = np.cumsum(df['val'])\ndf['cumsum'] = np.where(df['cumsum'] < 0, 0, df['cumsum'])\n", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "import numpy as np\n\n# Create a mask to identify NaN values in the 'v' column\nmask = np.isnan(df['v'])\n\n# Create a mask to identify rows where 'r' equals 'right'\nright_mask = df['r'] == 'right'\n\n# Apply the mask to the 'v' column and calculate the sum\nresult = np.where(mask & right_mask, np.nan, df['v'][~mask & right_mask])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "import numpy as np\n\n# Create a mask to identify NaN values in the 'v' column\nmask = np.isnan(df['v'])\n\n# Replace NaN values with NaN in the 'v' column\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Calculate the sum of 'v' column for each group\ngroup_sum = df.groupby('r')['v'].sum()\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Replace NaN values in the group sum with NaN in the original dataframe\ndf['v']", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "import numpy as np\n\n# Create a mask to identify NaN values in the 'v' column\nmask = np.isnan(df['v'])\n\n# Replace NaN values with NaN in the 'v' column\ndf['v'] = df['v'].where(~mask, df['v'])\n\n# Calculate the sum of 'v' column for each group\ndf['v'] = df['v'].groupby(df['l']).apply(np.sum)\n\n# Replace NaN values in the 'v' column with NaN in the original dataframe\ndf['v'] = df['v'].where(mask, df['v'])\n\n# Print the resulting dataframe\nprint(df)\n", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "import numpy as np\n\ndef relationship_type(column1, column2):\n    if np.all(column1 == column2):\n        return 'one-to-one'\n    elif np.all(column1 == np.unique(column2)):\n        return 'one-to-many'\n    elif np.all(column2 == np.unique(column1)):\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\n\ndef column_relationship(df):\n    columns = df.columns\n    result = []\n    for i in range(len(columns)):\n        for j in range(i + 1, len(columns)):\n            column1 = df[columns[i]]\n            column2 = df[columns[j]]\n            result.append(f'{columns[i]} {columns[j]} {relationship_type(column1, column2)}')\n    return result\n\nprint(column_relationship(df))\nEND SOLUTION\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "import numpy as np\n\ndef relationship_type(col1, col2):\n    if np.all(col1 == col2):\n        return 'one-2-one'\n    elif np.all(col1 == col2[:, None]):\n        return 'one-2-many'\n    elif np.all(col1[:, None] == col2):\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\n\ndef column_relationship(df):\n    columns = df.columns\n    result = []\n    for i in range(len(columns)):\n        for j in range(i + 1, len(columns)):\n            column1 = df[columns[i]]\n            column2 = df[columns[j]]\n            result.append(f'{columns[i]} {columns[j]} {relationship_type(column1, column2)}')\n    return result\n\n\nresult = column_relationship(df)\nprint(result)\nEND SOLUTION\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "import numpy as np\n\ndef relationship(col1, col2):\n    if np.all(col1 == col2):\n        return 'one-to-one'\n    elif np.all(col1 == np.unique(col2)):\n        return 'one-to-many'\n    elif np.all(col2 == col1):\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\n\ndef column_relationship(df):\n    columns = df.columns\n    result = np.zeros((len(columns), len(columns)))\n    for i in range(len(columns)):\n        for j in range(i + 1, len(columns)):\n            result[i, j] = relationship(df[columns[i]], df[columns[j]])\n            result[j, i] = result[i, j]\n    return result\n\n\nresult = column_relationship(df)\nprint(result)\nEND SOLUTION\n<code>\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "import numpy as np\n\ndef relationship(col1, col2):\n    if np.all(col1 == col2):\n        return 'one-2-many'\n    elif np.all(col1 == col2):\n        return 'many-2-many'\n    elif np.any(col1 != col2):\n        return 'many-2-one'\n    else:\n        return 'one-2-many'\n\n\ndef column_relationship(df):\n    result = np.zeros((df.shape[1], df.shape[1]))\n    for i in range(df.shape[1]):\n        for j in range(i + 1, df.shape[1]):\n            result[i, j] = relationship(df[:, i], df[:, j])\n            result[j, i] = result[i, j]\n    return result\n\n\nresult = column_relationship(df)\nprint(result)\nEND SOLUTION\n<code>\n", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "# get the index of unique values, based on firstname, lastname, email\n# convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\n# sort by bank\ndfiban_uniq = dfiban_uniq.sort_values('bank')\n# remove duplicates\ndfiban_uniq = dfiban_uniq.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')\n# reverse order\ndfiban_uniq = dfiban_uniq[::-1]\n# print\nprint(dfiban_uniq)\nEND SOLUTION\n<code>\n# output\n#  firstname lastname  email  bank\n# 0  Foo Bar  Foo bar  NaN\n# 2  Bar Bar    Bar  NaN\n# 1  Foo Bar  Foo bar  NaN\n# 3  Bar Bar    Bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  NaN\n# 1  Foo Bar    Foo bar  NaN\n# 3  Foo Bar    Foo bar  NaN\n# 2  Foo Bar    Foo bar  Na", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": " using numpy API.\n\nAnswer: import numpy as np\n\n# Create a 2D array with 3 rows and 4 columns\narr = np.zeros((3, 4))\n\n# Write \"Hello, world\" in the first column of each row\narr[:, 0] = \"Hello, world\"\n\n# Print the array\nprint(arr)", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "import numpy as np\n\n# Define the condition\ncondition = (df['SibSp'] > 0) | (df['Parch'] > 0)\n\n# Define the groups\ngroups = np.where(condition, 'Has Family', 'No Family')\n\n# Calculate the mean for each group\nmean_group = np.mean(df[groups == 'Has Family'].values, axis=0)\nmean_group_no_family = np.mean(df[groups == 'No Family'].values, axis=0)\n\n# Combine the mean values into a single array\nmean_values = np.concatenate((mean_group, mean_group_no_family))\n\n# Create a new column with the mean values\ndf['Mean'] = mean_values\n\n# Print the result\nprint(df['Mean'])\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "import numpy as np\n\n# Define a function to calculate mean\ndef calculate_mean(group):\n    return np.mean(group['SibSp'])\n\n# Apply function to group by 'Survived' and 'Parch'\nresult = df.groupby(['Survived', 'Parch']).apply(calculate_mean)\n\n# Reshape the result to get the desired output\nresult = result.unstack().reset_index()\n\n# Replace NaN values with 0\nresult = result.fillna(0)\n\n# Replace NaN values with mean of 'SibSp'\nresult['SibSp'] = result['SibSp'].fillna(result['SibSp'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'SibSp'\nresult['SibSp'] = result['SibSp'].fillna(result['SibSp'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace NaN values with mean of 'Parch'\nresult['Parch'] = result['Parch'].fillna(result['Parch'].mean())\n\n# Replace", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "# Define condition\ncondition = (df['SibSp'] == 1) & (df['Parch'] == 1)\nnew_group_name = 'Has Family'\n\n# Create new group\nnew_group = df[df['Survived'] == 0]\n\n# Calculate mean for new group\nmean_new_group = np.mean(new_group['Survived'])\n\n# Define condition for new group\ncondition_new_group = (df['SibSp'] == 0) & (df['Parch'] == 0)\n\n# Calculate mean for new group\nmean_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group_new_group_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group_new_group_new_group_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group_new_group_new_group_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group_new_group_new_group_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group_new_group_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0])\n\n# Calculate mean for new group\nmean_new_group = np.mean(df[df['Survived'] == 0", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "result = df.groupby('cokey').sort('A').reset_index(drop=True)\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult = result[result['cokey'].sort_values().index]\nresult = result.reset_index(drop=True)\nresult =", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "result = df.groupby('cokey').sort('A')\nresult.sort_index(axis=0, ascending=True)\n", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = [('Caps', 'Lower'), 'A', 'B']\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'Caps Lower'})\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.drop(columns='Caps')\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Lower'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps'})\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.drop(columns='Caps')\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps'})\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps', 'Caps': 'Lower'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps', 'Caps': 'Lower'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps', 'Caps': 'Lower'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps', 'Caps': 'Lower'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps', 'Caps': 'Lower'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps', 'Caps': 'Lower'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Lower': 'Caps', 'Caps': 'Lower'})\ndf = df.sort_values(by=('Lower', 'Caps'))\ndf = df.drop(columns='Lower')\ndf = df.sort_values(by=('Caps', 'Lower'))\ndf = df.reset_index", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = [('Caps', 'Lower', 'A'), 'a', 'b']\ndf = df.reset_index()\ndf = df.rename(columns={'index': 'Caps Lower'})\ndf = df.drop(columns=['Caps', 'Lower'])\ndf = df.sort_values(by=['Caps Lower', 'a'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps', 'a': 'Lower', 'b': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index(drop=True)\n", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df = df.set_index(['Caps', 'Lower'])\ndf = df.sort_index()\ndf = df.reset_index()\ndf = df.drop(columns=['Caps', 'Lower'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps Lower'])\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps Lower'])\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps Lower'])\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps Lower'])\ndf = df.rename(columns={'Caps Lower': 'Caps', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['Caps Lower', 'Lower'])\ndf = df.reset_index()\ndf = df.drop(columns=['Caps'])\ndf = df.rename(columns={'Caps': 'Caps Lower', 'Lower': 'Lower'})\ndf = df.sort_values(by=['", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "df = pd.DataFrame({'birdType': someTuple[0], 'birdCount': someTuple[1]})\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "import numpy as np\n\ndef stdMean(x):\n    return np.std(np.mean(x))\n\nresult = pd.Series(df.groupby('a').b.apply(stdMean))\nprint(result)\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "import numpy as np\n\ndef stdMean(x):\n    return np.std(np.mean(x))\n\nresult = pd.Series(df.groupby('b').a.apply(stdMean))\nprint(result)\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "import numpy as np\n\n# calculate softmax\nsoftmax = np.exp(df['b']) / np.sum(np.exp(df['b']), axis=1, keepdims=True)\n\n# normalize min-max\nmin_max = (df['b'] - df['b'].min()) / (df['b'].max() - df['b'].min())\n\n# add min-max normalization to data frame\ndf['min-max'] = min_max\n\n# add softmax to data frame\ndf['softmax'] = softmax\n\nprint(df)\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "import numpy as np\n\n# Create a mask to select rows and columns that only have zeros\nmask = np.all(df == 0, axis=1)\n\n# Select rows and columns that only have zeros\ndf_masked = df[mask, mask]\n\n# Replace zeros with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 1 with 1\ndf_masked = np.where(df_masked == 1, 1, df_masked)\n\n# Replace 0 with 0\ndf_masked = np.where(df_masked == 0, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where(df_masked == 1, 0, df_masked)\n\n# Replace 0 with 1\ndf_masked = np.where(df_masked == 0, 1, df_masked)\n\n# Replace 1 with 0\ndf_masked = np.where", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "import numpy as np\n\n# Create a mask to select rows and columns with sum equal to 0\nmask = np.ones((df.shape[0], df.shape[1]), dtype=bool)\nfor i in range(df.shape[0]):\n    for j in range(df.shape[1]):\n        if df.loc[i, j] == 0:\n            mask[i, j] = False\n\n# Select rows and columns with sum equal to 0\nmasked_df = df.loc[mask]\n\n# Replace zero values with NaN\ndf_masked = df.copy()\ndf_masked.replace(0, np.nan, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.fillna(0, inplace=True)\n\n# Replace zero values with NaN in the masked dataFrame\ndf_masked.replace(0, np.nan, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.fillna(0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=True)\n\n# Replace NaN values with zero values\ndf_masked.replace(np.nan, 0, inplace=", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "import numpy as np\n\n# Create a 2D array from the dataFrame\narr = df.values\n\n# Find the maximum value in each column\nmax_values = np.max(arr, axis=0)\n\n# Find the indices of the maximum values\nmax_indices = np.where(arr == max_values)\n\n# Remove rows and columns with maximum values\nmask = np.ones_like(arr, dtype=bool)\nfor i in range(max_indices[0].size):\n    mask[max_indices[0][i], max_indices[1][i]] = False\n\n# Remove rows and columns with maximum values\nresult = df[~mask]\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "import numpy as np\n\n# Create a 2D array from the dataframe\narr = df.values\n\n# Find the maximum value in each column\nmax_values = np.max(arr, axis=0)\n\n# Find the indices of the maximum values\nmax_indices = np.argmax(arr, axis=0)\n\n# Replace the maximum values in the original array with 0\nnp.put(arr, max_indices, 0)\n\n# Replace the original array with the modified array\ndf = df.assign(A=arr[:,0], B=arr[:,1], C=arr[:,2], D=arr[:,3])\n\n# Print the modified dataframe\nprint(df)\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "s = s.sort_values(ascending=True)\nresult = s.sort_index()\n", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.sort_values(ascending=True).reset_index(drop=True)\ndf = df.sort_values(by=0, ascending=True).reset_index(drop=True)\ndf = df.rename(columns={0:'index', 1:'value'})\ndf = df.rename(index={0:'146tf150p', 1:'645', 2:'anything', 3:'anytime', 4:'beerage', 5:'done', 6:'lei', 7:'nothing', 8:'nothing', 9:'nothing', 10:'nothing', 11:'nothing', 12:'nothing', 13:'nothing', 14:'nothing', 15:'nothing', 16:'nothing', 17:'nothing', 18:'nothing', 19:'nothing', 20:'nothing', 21:'nothing', 22:'nothing', 23:'nothing', 24:'nothing', 25:'nothing', 26:'nothing', 27:'nothing', 28:'nothing', 29:'nothing', 30:'nothing', 31:'nothing', 32:'nothing', 33:'nothing', 34:'nothing', 35:'nothing', 36:'nothing', 37:'nothing', 38:'nothing', 39:'nothing', 40:'nothing', 41:'nothing', 42:'nothing', 43:'nothing', 44:'nothing', 45:'nothing', 46:'nothing', 47:'nothing', 48:'nothing', 49:'nothing', 50:'nothing', 51:'nothing', 52:'nothing', 53:'nothing', 54:'nothing', 55:'nothing', 56:'nothing', 57:'nothing', 58:'nothing', 59:'nothing', 60:'nothing', 61:'nothing', 62:'nothing', 63:'nothing', 64:'nothing', 65:'nothing', 66:'nothing', 67:'nothing', 68:'nothing', 69:'nothing', 70:'nothing', 71:'nothing', 72:'nothing', 73:'nothing', 74:'nothing', 75:'nothing', 76:'nothing', 77:'nothing', 78:'nothing', 79:'nothing', 80:'nothing', 81:'nothing', 82:'nothing', 83:'nothing', 84:'nothing', 85:'nothing', 86:'nothing', 87:'nothing', 88:'nothing', 89:'nothing', 90:'nothing', 91:'nothing', 92:'nothing', 93:'nothing', 94:'nothing', 95:'nothing', 96:'nothing', 97:'nothing', 98:'nothing', 99:'nothing', 100:'nothing', 101:'nothing', 102:'nothing', 103:'nothing', 104:'nothing', 105:'nothing', 106:'nothing', 107:'nothing', 108:'nothing', 109:'nothing', 110:'nothing', 111:'nothing', 112:'nothing', 113:'nothing', 114:'nothing', 115:'nothing', 116:'nothing', 117:'nothing', 118:'nothing', 119:'nothing', 120:'nothing', 121:'nothing', 122:'nothing', 123:'nothing', 124:'nothing', 125:'nothing', 126:'nothing', 127:'nothing', 128:'nothing', 129:'nothing', 130:'nothing', 131:'nothing', 132:'nothing', 133:'nothing', 134:'nothing', 135:'nothing', 136:'nothing', 137:'nothing', 138:'nothing', 139:'nothing', 140:'nothing', 141:'nothing', 142:'nothing', 143:'nothing', 144:'nothing', 145:'nothing', 146:'nothing', 147:'nothing', 148:'nothing',", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "import numpy as np\n\n# Select records where A value are integer or numeric\nmask = np.isnumeric(df['A'])\nresult = df[mask]\n", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "import numpy as np\n\n# Select records where A value are string\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n\n# Convert 'A' column to integer\ndf['A'] = df['A'].apply(lambda x: np.where(np.isin(x, 's'), 2, x))\n\n# Select records where B value are 'blue'\nresult2 = df[df['B'] == 'blue']\n\n# Select records where B value are 'black'\nresult3 = df[df['B'] == 'black']\n\n# Select records where B value are 'red'\nresult4 = df[df['B'] == 'red']\n\n# Select records where B value are 'yellow'\nresult5 = df[df['B'] == 'yellow']\n\n# Select records where B value are 'green'\nresult6 = df[df['B'] == 'green']\n\n# Select records where B value are 'blue' or 'black'\nresult7 = df[(df['B'] == 'blue') | (df['B'] == 'black')]\n\n# Select records where B value are 'red' or 'yellow'\nresult8 = df[(df['B'] == 'red') | (df['B'] == 'yellow')]\n\n# Select records where B value are 'blue' or 'black' or 'red' or 'yellow'\nresult9 = df[(df['B'].apply(lambda x: x in ['blue', 'black', 'red', 'yellow']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green'\nresult10 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green']))]\n\n# Select records where B value are 'green' or 'yellow'\nresult11 = df[(df['B'].apply(lambda x: x in ['green', 'yellow']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green' or 'black'\nresult12 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green', 'black']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green' or 'black' or 'red'\nresult13 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green', 'black', 'red']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green' or 'black' or 'yellow'\nresult14 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green', 'black', 'yellow']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green' or 'black' or 'yellow' or 'red'\nresult15 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green', 'black', 'yellow', 'red']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green' or 'black' or 'yellow' or 'red' or 'blue'\nresult16 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green', 'black', 'yellow', 'red', 'blue']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green' or 'black' or 'yellow' or 'red' or 'blue' or 'yellow'\nresult17 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green', 'black', 'yellow', 'red', 'blue', 'yellow']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green' or 'black' or 'yellow' or 'red' or 'blue' or 'yellow' or 'green'\nresult18 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green', 'black', 'yellow', 'red', 'blue', 'yellow', 'green']))]\n\n# Select records where B value are 'blue' or 'yellow' or 'green' or 'black' or 'yellow' or 'red' or 'blue' or 'yellow' or 'green' or 'black'\nresult19 = df[(df['B'].apply(lambda x: x in ['blue', 'yellow', 'green', 'black', 'yellow',", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "import numpy as np\n\n# Find max count for each group\nmax_count = np.max(df['count'])\n\n# Find indices of max count\nmax_count_indices = np.where(df['count'] == max_count)[0]\n\n# Select rows where count equals max\nresult = df[df['count'].equals(max_count)]\n\n# Select rows where count equals max for each group\nresult_group = df.groupby(['Sp', 'Mt']).apply(lambda x: x[x['count'] == max_count])\n\n# Print results\nprint(result)\nprint(result_group)\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "import numpy as np\n\n# Create a mask to find the indices of rows with max count in each group\nmask = np.argmax(df['count'], axis=0)\n\n# Get the rows with max count\nresult = df.loc[mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "import numpy as np\n\n# Find the minimum count for each group\nmin_count = np.min(df['count'])\n\n# Find the indices where count equals min_count\nidx = np.where(df['count'] == min_count)[0]\n\n# Select rows where count equals min_count\nresult = df.loc[idx]\n\nprint(result)\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "import numpy as np\n\n# Create a mask to select rows where count equals max\nmask = np.where(df['count'].groupby(df['Sp', 'Value']).max() == df['count'], True, False)\n\n# Select rows where count equals max\nresult = df[mask]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df['Date'] = df['Member'].map(dict)\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].fillna('17/8/1926')\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    # Create a new column in the DataFrame to store the values from the dict\n    df['new_col'] = df['Member'].map(dict)\n    # Replace NaN values in the new column with the corresponding value from the dict\n    df['new_col'].fillna(df['Member'], inplace=True)\n    # Replace the Member column with the new column\n    df.drop('Member', axis=1, inplace=True)\n    df.rename(columns={'new_col': 'Member'}, inplace=True)\n    # Return the updated DataFrame\n    return df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = df['Date'].apply(lambda x: x.replace('/', '-'))\ndf['Date'] = df['Date'].apply(lambda x: np.datetime64(x))\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%d-%b-%Y'))\n", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Count_d'] = df['Date'].groupby(['Year', 'Month']).size()\ndf['Count_m'] = df['Date'].groupby(['Year', 'Month']).count()\ndf['Count_y'] = df['Date'].groupby(['Year']).count()\nprint(df)\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Year_Month'] = df['Year'].astype(str).str.cat(df['Month'].astype(str), sep='-')\ndf['Count_Year'] = df['Date'].groupby(df['Year']).size()\ndf['Count_Month'] = df['Date'].groupby(df['Year_Month']).size()\ndf['Count_Val'] = df['Val'].groupby(df['Year']).size()\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Weekday'] = df['Date'].dt.weekday\ndf['Val'] = df['Val'].astype(int)\ndf['Count_Year'] = df['Year'].value_counts()\ndf['Count_Month'] = df['Month'].value_counts()\ndf['Count_Weekday'] = df['Weekday'].value_counts()\ndf['Count_Val'] = df['Val'].value_counts()\n\n# Create a new column 'Count_Val' by counting the number of occurrences of each unique 'Val'\n# Create a new column 'Count_Weekday' by counting the number of occurrences of each unique 'Weekday'\n# Create a new column 'Count_Month' by counting the number of occurrences of each unique 'Month'\n# Create a new column 'Count_Year' by counting the number of occurrences of each unique 'Year'\n\n# Create a new column 'Count_Val' by counting the number of occurrences of each unique 'Val'\n# Create a new column 'Count_Weekday' by counting the number of occurrences of each unique 'Weekday'\n# Create a new column 'Count_Month' by counting the number of occurrences of each unique 'Month'\n# Create a new column 'Count_Year' by counting the number of occurrences of each unique 'Year'\n\n# Print the resulting dataframe\nprint(df)\n# END SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "result1 = df.groupby('Date').apply(lambda x: x.loc[x['B'] == 0, 'C']).reset_index(drop=True)\nresult2 = df.groupby('Date').apply(lambda x: x.loc[x['B'] != 0, 'C']).reset_index(drop=True)\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "import numpy as np\n\n\neven_result = np.where(df['B'] % 2 == 0, df['B'], 0) + np.where(df['C'] % 2 == 0, df['C'], 0)\nodd_result = np.where(df['B'] % 2 != 0, df['B'], 0) + np.where(df['C'] % 2 != 0, df['C'], 0)\n\n\nprint(\"Even values:\")\nprint(even_result)\nprint(\"\\n\")\nprint(\"Odd values:\")\nprint(odd_result)\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nprint(result)\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nprint(result)\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\nprint(result)\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\nprint(result)\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "result = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.max)\nprint(result)\nresult = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.min)\nprint(result)\n", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "import numpy as np\n\n\ndef split_strings(s):\n    return np.array([x for x in s.split(',')])\n\n\ndef split_dataframe(df):\n    df_new = df.copy()\n    df_new['var2'] = df['var2'].apply(split_strings)\n    return df_new\n\n\ndf_new = split_dataframe(df)\nprint(df_new)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "import numpy as np\n\n\ndef split_strings(s):\n    return np.array([x for x in s.split(',')])\n\n\ndef split_dataframe(df):\n    return df.apply(split_strings, axis=1)\n\n\nresult = split_dataframe(df)\nprint(result)\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "import numpy as np\n\n\ndef split_rows(df):\n    # Split the column 'var2' into multiple rows\n    df['var2'] = df['var2'].str.split('-')\n    \n    # Flatten the array of arrays into a single array\n    df['var2'] = df['var2'].apply(lambda x: [item for sublist in x for item in sublist])\n    \n    # Convert the array to a numpy array\n    df['var2'] = np.array(df['var2'])\n    \n    return df\n\n\n# Apply the function to the dataframe\ndf = split_rows(df)\n\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in string:\n        if(i.isalpha() == False):\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df['str'].apply(count_special_char)\nprint(df)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in string:\n        if(i.isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df['str'].apply(count_special_char)\nprint(df)\nEND SOLUTION\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "import numpy as np\n\n# Split the row cell into two columns\ndf['fips'] = np.array([x[:4] for x in df['row']])\ndf['row'] = np.array([x[4:] for x in df['row']])\n\n# Print the resulting dataframe\nprint(df)\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df['fips'] = df['row'].str[:2]\ndf['row'] = df['row'].str[2:]\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "import numpy as np\n\n# split the row cell into three columns\ndf['fips'] = df['row'].str[:4]\ndf['medi'] = df['row'].str[4:]\n\n# split the median value into two columns\ndf['medi'] = df['medi'].str.split(' ', 1).apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x[0], x[1]])\n\n# split the median value into two columns\ndf['medi'] = df['medi'].apply(lambda x: [x", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "import numpy as np\n\n# Calculate cumulative average\ncumulative_average = np.cumsum(df.values[:, 1:]) / np.arange(1, df.shape[1])\n\n# Replace the first column with the cumulative average\ndf = df.copy()\ndf.columns = ['Name'] + df.columns[1:]\ndf = df.assign(**{col: cumulative_average for col in df.columns[1:]})\n\n# Replace zero values with NaN\ndf = df.where(df != 0, np.nan)\n\n# Replace NaN with the previous value\ndf = df.fillna(df.loc[:, :'2001'].values[:, 0])\n\n# Replace NaN with the next value\ndf = df.fillna(df.loc[:, '2002':].values[:, 0])\n\n# Replace NaN with the average of the two previous values\ndf = df.fillna(df.loc[:, '2001':'2002'].values[:, 0] + df.loc[:, '2003':'2004'].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2004':'2005'].values[:, 0] + df.loc[:, '2006':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2005':].values[:, 0] + df.loc[:, '2006':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2006':].values[:, 0] + df.loc[:, '2007':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2006':].values[:, 0] + df.loc[:, '2007':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2007':].values[:, 0] + df.loc[:, '2008':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2008':].values[:, 0] + df.loc[:, '2009':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2009':].values[:, 0] + df.loc[:, '2010':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2010':].values[:, 0] + df.loc[:, '2011':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2011':].values[:, 0] + df.loc[:, '2012':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2012':].values[:, 0] + df.loc[:, '2013':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2013':].values[:, 0] + df.loc[:, '2014':].values[:, 0] / 2)\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "import numpy as np\n\n# Create a new column with the cumulative average\ndf['cumulative_average'] = np.cumsum(df.values[:, 1:], axis=1) / np.arange(1, df.shape[1])\n\n# Replace zero values with NaN\ndf = df.replace(0, np.nan)\n\n# Replace NaN with the previous value\ndf = df.fillna(df.loc[:, :'2001'].values[:, 0])\n\n# Replace NaN with the next value\ndf = df.fillna(df.loc[:, '2002':].values[:, -1])\n\n# Replace NaN with the average of the two previous values\ndf = df.fillna(df.loc[:, '2001':'2002'].mean())\n\n# Replace NaN with the average of the two next values\ndf = df.fillna(df.loc[:, '2003':].mean())\n\n# Replace NaN with the average of the three previous values\ndf = df.fillna(df.loc[:, '2002':'2003'].mean())\n\n# Replace NaN with the average of the three next values\ndf = df.fillna(df.loc[:, '2004':].mean())\n\n# Replace NaN with the average of the four previous values\ndf = df.fillna(df.loc[:, '2003':'2004'].mean())\n\n# Replace NaN with the average of the four next values\ndf = df.fillna(df.loc[:, '2005':].mean())\n\n# Replace NaN with the average of the five previous values\ndf = df.fillna(df.loc[:, '2004':'2005'].mean())\n\n# Replace NaN with the average of the five next values\ndf = df.fillna(df.loc[:, '2006':].mean())\n\n# Replace NaN with the average of the six previous values\ndf = df.fillna(df.loc[:, '2005':'2006'].mean())\n\n# Replace NaN with the average of the seven previous values\ndf = df.fillna(df.loc[:, '2006':].mean())\n\n# Replace NaN with the average of the seven next values\ndf = df.fillna(df.loc[:, '2007':].mean())\n\n# Replace NaN with the average of the eight previous values\ndf = df.fillna(df.loc[:, '2006':].mean())\n\n# Replace NaN with the average of the eight next values\ndf = df.fillna(df.loc[:, '2007':].mean())\n\n# Replace NaN with the average of the nine previous values\ndf = df.fillna(df.loc[:, '2007':].mean())\n\n# Replace NaN with the average of the nine next values\ndf = df.fillna(df.loc[:, '2008':].mean())\n\n# Replace NaN with the average of the ten previous values\ndf = df.fillna(df.loc[:, '2008':].mean())\n\n# Replace NaN with the average of the ten next values\ndf = df.fillna(df.loc[:, '2009':].mean())\n\n# Replace NaN with the average of the eleven previous values\ndf = df.fillna(df.loc[:, '2009':].mean())\n\n# Replace NaN with the average of the eleven next values\ndf = df.fillna(df.loc[:, '2010':].mean())\n\n# Replace NaN with the average of the eleven next values\ndf = df.fillna(df.loc[:, '2011':].mean())\n\n# Replace NaN with the average of the eleven next values\ndf = df.fillna(df.loc[:, '2012':].mean())\n\n# Replace NaN with the average of the eleven next values\ndf = df.fillna(df.loc[:, '2013':].mean())\n\n# Replace NaN with the average of the eleven next values\ndf = df.fillna(df.loc[:", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "\n    # Calculate the cumulative average for each row\n    cum_avg = (df.cumsum() / np.arange(1, df.shape[1] + 1)[:, None])\n    \n    # Replace zero values with NaN\n    NaN = np.where(df == 0, np.nan, df)\n    \n    # Replace zero values with NaN\n    NaN = np.where(cum_avg == 0, np.nan, cum_avg)\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, -1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, -1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, -1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, -1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, -1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, -1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, -1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, -1))\n    \n    # Replace NaN values with the previous value\n    cum_avg = np.where(np.isnan(cum_avg), np.nan, np.roll(cum_avg, 1))\n    \n    # Replace", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "import numpy as np\n\n# Create a new column with the cumulative average of each row\ndf['cumulative_average'] = np.cumsum(df.values[:, 1:], axis=1) / np.arange(1, df.shape[1])\n\n# Replace zero values with NaN\ndf = df.replace(0, np.nan)\n\n# Replace NaN with the previous value\ndf = df.fillna(df.loc[:, :'2001'].values[:, 0])\n\n# Replace NaN with the next value\ndf = df.fillna(df.loc[:, '2002':].values[:, -1])\n\n# Replace NaN with the average of the two adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':]) / 2)\n\n# Replace NaN with the average of the three adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':] + df.loc[:, '2003':]) / 3)\n\n# Replace NaN with the average of the four adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':] + df.loc[:, '2003':] + df.loc[:, '2004':]) / 4)\n\n# Replace NaN with the average of the five adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':] + df.loc[:, '2003':] + df.loc[:, '2004':] + df.loc[:, '2005':]) / 5)\n\n# Replace NaN with the average of the six adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':] + df.loc[:, '2003':] + df.loc[:, '2004':] + df.loc[:, '2005':] + df.loc[:, '2006':]) / 6)\n\n# Replace NaN with the average of the seven adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':] + df.loc[:, '2003':] + df.loc[:, '2004':] + df.loc[:, '2005':] + df.loc[:, '2006':] + df.loc[:, '2007':]) / 7)\n\n# Replace NaN with the average of the eight adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':] + df.loc[:, '2003':] + df.loc[:, '2004':] + df.loc[:, '2005':] + df.loc[:, '2006':] + df.loc[:, '2007':] + df.loc[:, '2008':]) / 8)\n\n# Replace NaN with the average of the nine adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':] + df.loc[:, '2003':] + df.loc[:, '2004':] + df.loc[:, '2005':] + df.loc[:, '2006':] + df.loc[:, '2007':] + df.loc[:, '2008':] + df.loc[:, '2009':]) / 9)\n\n# Replace NaN with the average of the ten adjacent values\ndf = df.fillna((df.loc[:, '2001':] + df.loc[:, '2002':] + df.loc[:, '2003':] + df.loc[:, '2004':] + df.loc[:, '20", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "\n# Create a new column 'label'\ndf['label'] = np.where(df['Close'].shift(1) > df['Close'], 1, 0)\n\n# Replace the first row with 1\ndf.loc[0, 'label'] = 1\n\n# Replace the last row with 0\ndf.loc[-1, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-2, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-3, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-4, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-5, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-6, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-7, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-8, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-9, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-10, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-11, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-12, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-13, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-14, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-15, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-16, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-17, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-18, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-19, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-20, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-21, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-22, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-23, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-24, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-25, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-26, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-27, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-28, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-29, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-30, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-31, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-32, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-33, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-34, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-35, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-36, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-37, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-38, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-39, 'label'] = 0\n\n# Replace the last row with 0\ndf.loc[-40, 'label'] = 0\n\n# Replace the last row with 0\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "import numpy as np\n\n# Create a new column 'label'\ndf['label'] = np.where(df['Close'].diff() > 0, 1, np.where(df['Close'].diff() == 0, 0, -1))\n\n# Replace the first row with 1\ndf.loc[0, 'label'] = 1\n\n# Replace the first row with 0\ndf.loc[0, 'label'] = 0\n\n# Replace the first row with -1\ndf.loc[0, 'label'] = -1\n\n# Print the final dataframe\nprint(df)\nEND SOLUTION\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "import numpy as np\n\n# Create a new column 'label'\ndf['label'] = np.where(df['Close'].diff() > 0, 1,\n                       np.where(df['Close'].diff() == 0, 0, -1))\n\n# Replace the first row with 1\ndf.loc[0, 'label'] = 1\n\n# Format DateTime\ndf['DateTime'] = df['DateTime'].dt.strftime(\"%d-%b-%Y\")\n\n# Print the result\nprint(df)\nEND SOLUTION\n<code>\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "import numpy as np\n\n# Convert strings to datetime64\ndf['arrival_time'] = np.datetime64(df['arrival_time'])\ndf['departure_time'] = np.datetime64(df['departure_time'])\n\n# Calculate time difference\ndf['Duration'] = df['departure_time'] - df['arrival_time']\n\n# Convert time difference to days and minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'D')) * np.timedelta64(1, 'D') + np.floor(df['Duration'] % np.timedelta64(1, 'D'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to days\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'D')) * np.timedelta64(1, 'D') + np.floor(df['Duration'] % np.timedelta64(1, 'D'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor(df['Duration'] % np.timedelta64(1, 'M'))\n\n# Convert time difference to minutes\ndf['Duration'] = np.floor(df['Duration'] / np.timedelta64(1, 'M')) * np.timedelta64(1, 'M') + np.floor", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "import numpy as np\n\n# Convert strings to datetime64\ndf['arrival_time'] = np.datetime64(df['arrival_time'])\ndf['departure_time'] = np.datetime64(df['departure_time'])\n\n# Calculate time difference in seconds\ndf['Duration'] = (df['departure_time'] - df['arrival_time']).astype(np.int64) / 10**9\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "import numpy as np\n\n# Convert strings to datetime64\ndf['arrival_time'] = np.datetime64(df['arrival_time'])\ndf['departure_time'] = np.datetime64(df['departure_time'])\n\n# Calculate time difference in seconds\ndf['Duration'] = np.diff(df['departure_time'].values)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df['Duration'].astype(np.int64)\n\n# Convert time difference to seconds\ndf['Duration'] = df", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "import numpy as np\n\n# Create a mask to select rows where key2 is 'one'\nmask = df['key2'] == 'one'\n\n# Count the number of rows where key2 is 'one'\ncount = np.sum(mask)\n\n# Create a new dataframe with the count\nresult = pd.DataFrame({'key1': df['key1'], 'count': [count]}, index=df['key1'])\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "import numpy as np\n\n# Create a mask to select rows where key2 is 'two'\nmask = df['key2'] == 'two'\n\n# Count the number of rows where key2 is 'two'\ncount = np.sum(mask)\n\n# Create a new dataframe with the count\nresult = pd.DataFrame({'key1': df['key1'], 'count': [count]}, index=df['key1'])\n\nprint(result)\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "import numpy as np\n\n# Create a mask to select rows where 'key2' ends with 'e'\nmask = np.where(df['key2'].str.endswith('e'), 1, 0)\n\n# Count the number of rows where 'key2' ends with 'e'\ncount = np.sum(mask)\n\n# Create a new dataframe with the count\nresult = pd.DataFrame({'key1': df['key1'], 'count': [count]}, index=[0])\n\nprint(result)\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.index[df['value'].argmax()]\nmin_result = df.index[df['value'].argmin()]\nprint(max_result)\nprint(min_result)\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "mode_result = df.mode(axis=0)\nmedian_result = df.median()\nprint(mode_result)\nprint(median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "df = df[(df['closing_price'].between(99, 101))]\n", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "df = df[(df['closing_price'] < 100) | (df['closing_price'] > 102)]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "import numpy as np\n\n\ndef drop_rows(df, column, threshold):\n    mask = df[column] >= threshold\n    return df[~mask]\n\n\n# Define the minimum value for each column\nmin_values = {\n    \"diff\": 1,\n    \"otherstuff\": 0\n}\n\n\n# Iterate over each column and drop rows where the value is less than the minimum\nfor column, threshold in min_values.items():\n    df = drop_rows(df, column, threshold)\n\n\nprint(df)\n", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[1]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": "\n    # Use numpy's vectorized operations to split the strings\n    # Then use numpy's boolean indexing to select the strings with '_'\n    # Finally use numpy's str.split() function to split the strings after '_'\n    # Here is the solution\n    return df['SOURCE_NAME'].str.split('_').str[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "# Create a mask to select the first half of the NaN values\nmask_first_half = np.arange(len(df)) < 0.5 * len(df)\n\n# Create a mask to select the second half of the NaN values\nmask_second_half = np.arange(len(df)) >= 0.5 * len(df)\n\n# Use the mask to select the first half of the NaN values\nfirst_half = df[mask_first_half]\n\n# Use the mask to select the second half of the NaN values\nsecond_half = df[mask_second_half]\n\n# Replace the first half with 0 and the second half with 1\ndf[mask_first_half] = 0\ndf[mask_second_half] = 1\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "# Create a mask to select the first 30% of NaN values\nmask_first = np.random.choice([True, False], size=len(df), p=[0.7, 0.3])\n\n# Create a mask to select the middle 30% of NaN values\nmask_middle = np.random.choice([True, False], size=len(df), p=[0.7, 0.7])\n\n# Create a mask to select the last 30% of NaN values\nmask_last = np.random.choice([True, False], size=len(df), p=[0.7, 0.7])\n\n# Apply the mask to fill NaN values\ndf['Column_x'] = np.where(mask_first, 0, df['Column_x'])\ndf['Column_x'] = np.where(mask_middle, 0.5, df['Column_x'])\ndf['Column_x'] = np.where(mask_last, 1, df['Column_x'])\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "# Create a mask to select NaN values\nmask = np.isnan(df['Column_x'])\n\n# Count the number of non-NaN values in each column\ncount_non_nan = np.sum(~mask, axis=0)\n\n# Calculate the number of '1' and '0' values in each column\ncount_1 = np.sum(df['Column_x'], axis=0)\ncount_0 = count_non_nan - count_1\n\n# Calculate the number of '1' and '0' values to fill in the NaN values\nnum_1 = np.sum(df['Column_x'])\nnum_0 = len(df['Column_x']) - num_1\n\n# Calculate the number of '0' and '1' values to fill in the NaN values\nnum_0_to_fill = num_0\nnum_1_to_fill = num_1\n\n# Create a mask to select the indices of '0' values\nmask_0 = df['Column_x'] == 0\n\n# Create a mask to select the indices of '1' values\nmask_1 = df['Column_x'] == 1\n\n# Fill in the NaN values with '0'\ndf['Column_x'][mask_0] = np.random.choice(['0', '1'], size=num_0_to_fill, replace=True)\n\n# Fill in the NaN values with '1'\ndf['Column_x'][mask_1] = np.random.choice(['0', '1'], size=num_1_to_fill, replace=True)\n\n# Print the result\nprint(df)\n", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "def create_tuples(a, b):\n    return np.array([(a.iloc[i, 0], b.iloc[i, 0]) for i in range(a.shape[0])]).reshape(-1, 2)\n\na_b = create_tuples(a, b)\nprint(a_b)\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "def create_tuples(a, b, c):\n    # Create a list of tuples\n    list_of_tuples = []\n    # Iterate over each row in a, b, and c\n    for i in range(len(a)):\n        # Append a tuple containing elements from a, b, and c\n        list_of_tuples.append((a.iloc[i, 0], b.iloc[i, 0], c.iloc[i, 0]))\n    return list_of_tuples\n\n# Create a list of dataframes\ndataframes = [a, b, c]\n\n# Use list comprehension to create a list of tuples\nresult = create_tuples(*dataframes)\n\n# Convert the list of tuples to a DataFrame\nresult_df = pd.DataFrame(result, columns=['one', 'two', 'three'])\n\nprint(result_df)\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "def create_tuples(a, b):\n    # Find the maximum length of a and b\n    max_len = max(len(a), len(b))\n    \n    # Create an empty list to store tuples\n    tuples = []\n    \n    # Iterate over the range of lengths from 0 to max_len\n    for i in range(max_len):\n        # Append a tuple of elements from a and b at current length\n        tuples.append((a.iat[i, 0], b.iat[i, 0]))\n    \n    # Convert the list to a NumPy array\n    array = np.array(tuples)\n    \n    # Reshape the array to match the shape of a and b\n    array = array.reshape(len(a), len(b))\n    \n    return array\n\n# Create tuples for a and b\na_b = create_tuples(a, b)\n\n# Print the result\nprint(a_b)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "# Create bins\nbins = [1, 10, 25, 50, 100]\n\n# Create bins for views\nviews_bins = pd.cut(df.views, bins=bins)\n\n# Create a new DataFrame with views bins and count of views\ndf_counts = df.groupby(views_bins).views.count().reset_index()\n\n# Rename columns\ndf_counts.columns = ['views_bins', 'count']\n\n# Sort by views_bins\ndf_counts = df_counts.sort_values('views_bins')\n\n# Reshape to get count of views for each bin\ncounts = df_counts['count'].values\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\nmask = np.where(np.diff(bins) > 0, 1, 0)\n\n# Count views in each bin\ncounts_in_bins = np.cumsum(mask)\n\n# Create a binary mask to identify bins with views\n", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "# Create bins\nbins = [1, 10, 25, 50, 100]\n\n# Create bins for username\nbins_username = [0, 1, 2]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_username = [0, 1, 2]\n\n# Create bins for views\nbins_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views_views_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views_views_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views_views_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 10, 25, 50, 100]\n\n# Create bins for views\nbins", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "# Create bins\nbins = [1, 10, 25, 50, 100]\n\n# Create bins for user\nbins_user = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views\nbins_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for user\nbins_views_user = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views\nbins_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views\nbins_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views\nbins_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views\nbins_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views for views for views for views for views for views\nbins_views_views_views_views_views_views_views = [0, 1, 10, 25, 50, 100]\n\n# Create bins for views", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "import numpy as np\n\ndef merge_rows(df):\n    text_list = df['text'].values.tolist()\n    text_list = [', '.join(row) for row in text_list]\n    new_text = np.array(text_list)\n    df_new = pd.DataFrame({'text': new_text}, index=df.index)\n    return df_new\n\n\ndf_new = merge_rows(df)\nprint(df_new)\n", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "import numpy as np\n\ndef merge_rows(df):\n    text = df['text'].values\n    text_strings = np.concatenate([np.full(len(text), x) for x in text])\n    text_strings = np.concatenate([text_strings, np.full(len(text), '-')])\n    text_strings = np.concatenate([text_strings, np.full(len(text), '')])\n    return np.concatenate([np.full(len(text), x) for x in text]).astype(str)\n\n\nresult = merge_rows(df)\nprint(result)\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "import numpy as np\n\ndef merge_rows(df):\n    texts = df['text'].values\n    texts = np.concatenate((texts, texts))\n    texts = texts[np.argsort(np.arange(len(texts)))]\n    texts = np.unique(texts)\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np.argsort(np.argsort(np.argsort(texts)))))]\n    texts = texts[np.argsort(np.argsort(np", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "import numpy as np\n\n# Create a new column with the concatenation of the text\ndf['text'] = df['text'].apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.values.flatten(), axis=1)\n\n# Convert the result to a single row\ndf = pd.Series(df).apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Convert the result to a single row\ndf = pd.Series(df).apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Convert the result to a single row\ndf = pd.Series(df).apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: x.split(', '))\n\n# Reshape the data to get a single row\ndf = df.apply(lambda x: ', '.join(x))", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "import numpy as np\n\n# Create a new column with the concatenation of the text columns\ndf['text'] = df['text'].apply(lambda x: '-'.join(x))\n\n# Reshape the data to get a single row\ndf = df.values.reshape(-1)\n\n# Convert the array to a string\nresult = np.char.concatenate((df, ''))\n\n# Convert the result to a string\nresult = result.astype(str)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "# Create a new column 'city' in df2\ndf2['city'] = np.nan\n# Concatenate df1 and df2 along the 0 axis\nresult = pd.concat([df1, df2], axis=0)\n# Replace NaN values in 'city' column with the corresponding value in 'district' column\nresult['city'] = result['district'].where(~np.isnan(result['city']), result['district'])\n# Replace NaN values in 'district' column with the corresponding value in 'city' column\nresult['district'] = result['city'].where(~np.isnan(result['district']), result['city'])\n# Replace NaN values in 'value' column with 0\nresult['value'] = np.where(np.isnan(result['value']), 0, result['value'])\n# Reshape the result dataframe to have one row for each unique combination of 'id', 'city', and 'district'\nresult = result.groupby(['id', 'city', 'district']).first()\n# Reshape the result dataframe back to have multiple rows for each unique combination of 'id', 'city', and 'district'\nresult = result.reset_index()\n# Replace the 'id' column with the sum of the values in the 'value' column for each unique combination of 'city', 'district'\nresult['id'] = result.groupby(['city', 'district']).value.transform(lambda x: np.sum(x))\n# Replace the 'value' column with the average of the values in the 'value' column for each unique combination of 'city', 'district'\nresult['value'] = result.groupby(['city', 'district']).value.transform(lambda x: np.mean(x))\n# Reshape the result dataframe back to have one row for each unique combination of 'id', 'city', and 'district'\nresult = result.groupby(['id', 'city', 'district']).first()\n# Reshape the result dataframe back to have multiple rows for each unique combination of 'id', 'city', and 'district'\nresult = result.reset_index()\n# Print the result\nprint(result)\n# End\n", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "\n\n# Create a new column 'city_district' by concatenating 'city' and 'district'\ndf1['city_district'] = df1['city'].astype(str).apply(lambda x: x + ' ' + x)\n\n\n# Create a new column 'date_district' by concatenating 'date' and 'district'\ndf1['date_district'] = df1['date'].astype(str).apply(lambda x: x + ' ' + x)\n\n\n# Concatenate df1 and df2 along the 0 axis\nresult = pd.concat([df1, df2], axis=0)\n\n\n# Replace the 'district' column with the first two characters of the 'city_district' column\nresult['district'] = result['city_district'].apply(lambda x: x[:2])\n\n\n# Replace the 'city_district' column with the first two characters of the 'date_district' column\nresult['city_district'] = result['date_district'].apply(lambda x: x[:2])\n\n\n# Replace the 'date' column with the first two characters of the 'date_district' column\nresult['date'] = result['date_district'].apply(lambda x: x[:2])\n\n\n# Drop the 'date_district' column\nresult = result.drop('date_district', axis=1)\n\n\n# Drop the 'city_district' column\nresult = result.drop('city_district', axis=1)\n\n\n# Reset the index\nresult = result.reset_index(drop=True)\n\n\n# Print the result\nprint(result)\n\n\n# End\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "\n\ndef cluster_rows(df1, df2):\n    # Join df1 and df2 on id\n    df = pd.merge(df1, df2, on='id', how='inner')\n\n    # Sort by id and date\n    df = df.sort_values(['id', 'date'])\n\n    # Group by id and fill NaN with the first value\n    df = df.groupby('id').apply(lambda x: x.fillna(x.iloc[0]))\n\n    # Reshape the data to get the desired output\n    df = df.reset_index(drop=True).reset_index(name='id')\n\n    # Reorder columns\n    df = df[['id', 'date', 'value', 'city', 'district']]\n\n    return df\n\n\n# Apply the function\nresult = cluster_rows(df1, df2)\n\n# Print the result\nprint(result)\n\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "C = C.set_index('A')\nD = D.set_index('A')\nresult = C.merge(D, left_index=True, right_index=True, how='outer', left_on='A', right_on='A')\nresult = result.reset_index()\nresult.columns = ['A', 'B_x', 'B_y']\nresult = result[['A', 'B_x', 'B_y']]\nresult = result.sort_values('A')\nresult = result.reset_index(drop=True)\nresult\n", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "C = C.set_index('A')\nD = D.set_index('A')\nresult = C.merge(D, left_index=True, right_index=True, how='outer')\nresult = result.reset_index()\nresult.columns = ['A', 'B_x', 'B_y']\nresult = result[['A', 'B_x', 'B_y']]\nresult = result.sort_values('A')\nresult = result.reset_index(drop=True)\nresult\n", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "# Create a mask to identify duplicates\nmask = C['A'].apply(lambda x: D['A'].tolist().count(x))\n# Create a new column 'dulplicated'\nC['dulplicated'] = mask == 2\n# Replace values in B column of C with values from D where the corresponding row is duplicated\nC['B'] = D['B'].where(C['dulplicated'])\n# Print the result\nprint(C)\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "import numpy as np\n\n\ndef sort_transactions(df):\n    # Sort by time and amount\n    sorted_df = df.sort_values(by=['time', 'amount'], ascending=[True, True])\n    \n    # Create a new column to store the sorted amounts\n    sorted_df['amount'] = sorted_df['amount'].astype(str)\n    \n    # Create a new column to store the sorted time\n    sorted_df['time'] = sorted_df['time'].astype(str)\n    \n    # Create a new column to store the sorted amounts and time\n    sorted_df['new_amount'] = sorted_df['amount'].astype(str) + ' ' + sorted_df['time'].astype(str)\n    \n    # Sort by new column\n    sorted_df = sorted_df.sort_values(by=['new_amount'], ascending=True)\n    \n    return sorted_df\n\n\n# Test the function\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\n\n# Print the result\nprint(sort_transactions(df))\nEND SOLUTION\n<code>\n# Output\n#     user  time  amount\n# 1  1  20.0  10.99\n# 0  1  10.0  4.99\n# 2  2  11.0  2.99\n# 3  2  18.0  1.99\n# 4  3  15.0  10.99\n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n# \n#", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "import numpy as np\n\n\ndef sort_amount_time(x):\n    # Sort by time and then amount\n    x = x.sort_values(by=['time', 'amount'])\n    # Convert to tuple\n    x = x.apply(lambda x: (x['time'], x['amount']), axis=1)\n    return x\n\n\n# Define the function to aggregate the data\ndef aggregate_data(df):\n    # Sort the data by user and time\n    df = df.sort_values(by=['user', 'time'])\n    # Group by user\n    grouped = df.groupby('user')\n    # Apply the function to each group\n    result = grouped.apply(sort_amount_time)\n    return result\n\n\n# Call the function\nresult = aggregate_data(df)\n\n\nprint(result)\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "import numpy as np\n\n\ndef sort_dataframe(df):\n    # Create a new column where each row is a tuple of the amount and time\n    df['amount-time-tuple'] = np.column_stack((df['amount'], df['time']))\n    \n    # Sort the new column in ascending order\n    df = df.sort_values(by='amount-time-tuple', ascending=True)\n    \n    # Reshape the column into a 2D array\n    df = df['amount-time-tuple'].values.reshape(-1, 2)\n    \n    return df\n\n\n# Test the function\ndf = sort_dataframe(df)\nprint(df)\nEND SOLUTION\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "import numpy as np\n\ndef expand_series(series):\n    # Get the index values\n    index_values = np.array(list(series.index))\n    \n    # Get the values\n    values = np.array([np.array(val) for val in series.values])\n    \n    # Concatenate the values\n    df = pd.DataFrame(np.column_stack((index_values, values)), columns=['index', 'value'])\n    \n    return df\n\n# Use the function\ndf = expand_series(series)\nprint(df)\n", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "import numpy as np\n\ndef expand_series(series):\n    # Get the shape of the array\n    shape = series.values[0].shape\n    \n    # Create a new dataframe with zeros\n    df = pd.DataFrame(np.zeros((len(series), len(shape))), index=series.index, columns=['x', 'y', 'z'])\n    \n    # Iterate over each element in the series\n    for i, arr in enumerate(series.values):\n        # Extract the values from the array\n        values = arr.flatten()\n        \n        # Assign the values to the dataframe\n        df.loc[i, 'x'] = values[0]\n        df.loc[i, 'y'] = values[1]\n        df.loc[i, 'z'] = values[2]\n    \n    return df\n\n# Use the function\ndf = expand_series(series)\nprint(df)\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "result = df[df.columns.apply(lambda x: s in x)].columns.tolist()\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "result = df[df.columns.apply(lambda x: s in x)]\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "def find_column_name(df, s):\n    result = []\n    for col in df.columns:\n        if s in col:\n            result.append(col)\n    return result\n\ndef get_column_names(df, result):\n    return [df[col] for col in result]\n\nresult = find_column_name(df, 'spike')\ncolumn_names = get_column_names(df, result)\nprint(column_names)\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "import numpy as np\n\ndef split_codes(codes):\n    if len(codes) == 0:\n        return np.array([])\n    else:\n        return np.array(codes)\n\n\ndef split_codes_list(codes):\n    return np.array([np.array(list(c)) for c in codes])\n\n\ndef split_codes_list_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list[0]), np.nan, axis=0)\n    return codes_list\n\n\ndef split_codes_list_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list[0]), np.nan, axis=0)\n    return pd.DataFrame(codes_list, columns=['code_0', 'code_1', 'code_2'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list[0]), np.nan, axis=0)\n    return pd.DataFrame(codes_list, columns=['code_0', 'code_1', 'code_2'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list[0]), np.nan, axis=0)\n    return pd.DataFrame(codes_list, columns=['code_0', 'code_1', 'code_2'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list[0]), np.nan, axis=0)\n    return pd.DataFrame(codes_list, columns=['code_0', 'code_1', 'code_2'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list[0]), np.nan, axis=0)\n    return pd.DataFrame(codes_list, columns=['code_0', 'code_1', 'code_2'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list[0]), np.nan, axis=0)\n    return pd.DataFrame(codes_list, columns=['code_0', 'code_1', 'code_2'])\n\n\ndef split_codes_list_to_dataframe", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "import numpy as np\n\n\ndef split_codes(codes):\n    if len(codes) == 0:\n        return np.array([])\n    else:\n        return np.array(codes)\n\n\ndef split_codes_list(codes):\n    return np.array([np.array(code) for code in codes])\n\n\ndef split_codes_list_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list), np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    return codes_list\n\n\ndef split_codes_list_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    return pd.DataFrame(codes_list, columns=['code_1', 'code_2', 'code_3'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    return pd.DataFrame(codes_list, columns=['code_1', 'code_2', 'code_3'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    return pd.DataFrame(codes_list, columns=['code_1', 'code_2', 'code_3'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    return pd.DataFrame(codes_list, columns=['code_1', 'code_2', 'code_3'])\n\n\ndef split_codes_list_to_dataframe_to_dataframe_to_dataframe(codes):\n    codes_list = split_codes_list(codes)\n    codes_list = np.insert(codes_list, 0, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    codes_list = np.insert(codes_list, len(codes_list) + 1, np.nan, axis=1)\n    return pd.Data", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "import numpy as np\n\n\ndef split_codes(codes):\n    if len(codes) == 0:\n        return np.nan, np.nan, np.nan\n    else:\n        codes = np.array(codes)\n        n = len(codes)\n        if n == 1:\n            return codes[0], np.nan, np.nan\n        elif n == 2:\n            return codes[0], codes[1], np.nan\n        else:\n            return codes[:-1], codes[-1], np.nan\n\n\ndef split_codes_list(codes):\n    if len(codes) == 0:\n        return []\n    else:\n        codes = np.array(codes)\n        n = len(codes)\n        if n == 1:\n            return [codes[0]]\n        elif n == 2:\n            return [codes[0], codes[1]]\n        else:\n            return [codes[i] for i in range(n - 1)]\n\n\ndef split_codes_list_list(codes):\n    if len(codes) == 0:\n        return []\n    else:\n        codes = np.array(codes)\n        n = len(codes)\n        if n == 1:\n            return [[codes[0]]]\n        else:\n            return [[codes[i], codes[i+1]] for i in range(n - 1)]\n\n\ndef split_codes_list_list_list(codes):\n    if len(codes) == 0:\n        return []\n    else:\n        codes = np.array(codes)\n        n = len(codes)\n        if n == 1:\n            return [[codes[0]]]\n        else:\n            return [[codes[i], codes[i+1], codes[i+2]] for i in range(n - 2)]\n\n\ndef split_codes_list_list_list_list(codes):\n    if len(codes) == 0:\n        return []\n    else:\n        codes = np.array(codes)\n        n = len(codes)\n        if n == 1:\n            return [[codes[0]]]\n        else:\n            return [[codes[i], codes[i+1], codes[i+2], codes[i+3]] for i in range(n - 3)]\n\n\ndef split_codes_list_list_list_list_list(codes):\n    if len(codes) == 0:\n        return []\n    else:\n        codes = np.array(codes)\n        n = len(codes)\n        if n == 1:\n            return [[codes[0]]]\n        else:\n            return [[codes[i], codes[i+1], codes[i+2], codes[i+3], codes[i+4]] for i in range(n - 4)]\n\n\ndef split_codes_list_list_list_list_list_list(codes):\n    if len(codes) == 0:\n        return []\n    else:\n        codes = np.array(codes)\n        n = len(codes)\n        if n == 1:\n            return [[codes[0]]]\n        else:\n            return [[codes[i], codes[i+1], codes[i+2], codes[i+3], codes[i+4], codes[i+5]] for i in range(n - 5)]\n\n\ndef split_codes_list_list_list_list_list_list_list(codes):\n    if len(codes) == 0:\n        return []\n    else:\n        codes = np.array(codes)\n        n = len(codes)\n        if n == 1:\n            return [[codes[0]]]\n        else:\n            return [[codes[i], codes[i+1], codes[i+2], codes[i+3], codes[i+4], codes[i+5], codes[i+6]] for i in range(n - 6)]\n\n\ndef split_codes_list_list_list_list_list_list_list(codes):\n    if len(codes) == 0:\n        return []\n    else:\n        codes = np.array(codes)\n        n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "import numpy as np\n\n\nresult = np.concatenate(df['col1'].values)\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "import numpy as np\n\ndef reverse_list(lst):\n    return lst[::-1]\n\ndef list_to_str(lst):\n    return ','.join(map(str, reverse_list(lst)))\n\ndef convert_column(df, column):\n    df[column] = df[column].apply(list_to_str)\n    return df\n\ndf = convert_column(df, 'col1')\nprint(df)\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "import numpy as np\n\ndef list_to_str(x):\n    return ','.join(map(str, x))\n\ndf['col1'] = df['col1'].apply(list_to_str)\n\nresult = df['col1'].values.tolist()\nprint(result)\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "# Sort the dataframe by time\ndf = df.sort_values('Time')\n# Create a new column that represents the bin index\ndf['bin'] = (df['Time'].astype(np.int64) // 60) % 60\n# Group by the bin index and calculate the mean value\ndf = df.groupby('bin').mean()\n# Reset the index\ndf = df.reset_index()\n# Reorder the columns\ndf = df[['bin', 'Time', 'Value']]\n# Convert the time column to a string\ndf['Time'] = df['Time'].apply(lambda x: x.strftime('%H:%M:%S'))\n# Convert the time column to integer\ndf['Time'] = df['Time'].apply(lambda x: int(x.strftime('%H')))\n# Reorder the columns\ndf = df[['Time', 'bin', 'Value']]\n# Print the result\nprint(df)\nEND SOLUTION\n", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "# Convert time to integer in seconds\ndf['Time'] = (df['Time'].astype(np.int64) // 10**9) % 10**9\n# Bin values into 3-minute intervals\ndf['bin'] = np.round(df['Time'] / 3).astype(int)\n# Count the number of occurrences in each bin\ncounts = np.bincount(df['bin'].astype(int))\n# Calculate the mean value for each bin\nmean_values = np.bincount(df['bin'].astype(int), weights=df['Value']) / counts\n# Create a new DataFrame with bin values and mean values\ndf_mean = pd.DataFrame({'bin': counts.nonzero()[0], 'mean': mean_values})\n# Sort the DataFrame by bin value\ndf_mean = df_mean.sort_values('bin')\n# Reshape the mean values to match the original DataFrame shape\ndf_mean = df_mean.pivot(index='bin', columns='mean', values='mean').values\n# Replace NaN values with zero\ndf_mean = np.nan_to_num(df_mean)\n# Reshape the mean values back to the original shape\ndf_mean = df_mean.reshape(df['bin'].shape)\n# Replace NaN values with zero\ndf_mean = np.nan_to_num(df_mean)\n# Print the mean values\nprint(df_mean)\n# Print the bin values\nprint(df['bin'])\n# Print the mean values for each bin\nprint(df_mean)\n# Print the bin values for each mean value\nprint(df['bin'])\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values for each bin\nprint(df_mean)\n# Print the mean values", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "import numpy as np\n\n\n# Convert time to integer\ndf['TIME'] = np.array([np.datetime64(t) for t in df['TIME']])\n\n\n# Rank by time for each id and group\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n\n\n# Convert back to datetime\ndf['TIME'] = df['TIME'].apply(lambda x: x.astype(np.datetime64))\n\n\nprint(df)\nEND SOLUTION\n<code>\n# Output\n#  ID  TIME\n# 01  2018-07-11 11:12:20\n# 01  2018-07-12 12:00:23\n# 01  2018-07-13 12:00:00\n# 02  2019-09-11 11:00:00\n# 02  2019-09-12 12:00:00\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "import numpy as np\n\n\ndf['TIME'] = pd.to_datetime(df['TIME'])\n\n\ndf['RANK'] = np.arange(1, len(df) + 1)\n\n\ndf['RANK'] = df['TIME'].values.astype(np.int64)\n\n\ndf['RANK'] = df['RANK'].cumsum()\n\n\ndf['RANK'] = df['RANK'] - np.arange(1, len(df) + 1)\n\n\ndf['RANK'] = df['RANK'] + 1\n\n\ndf['RANK'] = df['RANK'].astype(np.int64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RANK'] = df['RANK'].astype(np.uint64)\n\n\ndf['RAN", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "import numpy as np\n\n\ndf['TIME'] = df['TIME'].apply(lambda x: x.strftime('%d-%b-%y %A %H:%M:%S'))\n\n\ndf['RANK'] = df.groupby('ID')['TIME'].apply(lambda x: np.arange(1, len(x) + 1))\n\n\ndf = df.sort_values('RANK')\n\n\ndf = df.drop('RANK', axis=1)\n\n\nprint(df)\nEND SOLUTION\n<code>\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "result = df[df.index.get_level_values('a') != 2]\nresult = result[result.index.get_level_values('a').isin([1,3])]\n", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "df[filt]", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "import numpy as np\n\ndef check_diff(row):\n    return np.any(np.isnan(row))\n\nresult = np.where(np.apply_along_axis(check_diff, 1, df.values), np.arange(df.shape[1]), np.arange(df.shape[1]))\n", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "import numpy as np\n\ndef check_same_columns(row):\n    return np.isnan(row).any()\n\ndef find_same_columns(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]\n\ndef find_same_columns_in_row_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan_with_nan(row):\n    return np.where(np.isnan(row))[0]", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "import numpy as np\n\ndef find_diff_columns(df):\n    # Create a mask to identify columns with NaN values\n    mask = np.isnan(df).any(axis=0)\n    \n    # Find the column indices where the mask is True\n    column_indices = np.where(mask)[0]\n    \n    return column_indices\n\n# Test the function\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\"%d for d in range(10)])\nmask = np.isnan(df).any(axis=0)\ncolumn_indices = find_diff_columns(df)\nprint(column_indices)\n", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "import numpy as np\n\ndef find_diff_columns(row):\n    diff_columns = []\n    for col in row.index:\n        if np.isnan(row[col]) and np.isnan(row[col]):\n            continue\n        elif np.isnan(row[col]) and not np.isnan(row[col + 1]):\n            diff_columns.append((col, row[col + 1]))\n        elif np.isnan(row[col + 1]) and not np.isnan(row[col]):\n            diff_columns.append((row[col], col + 1))\n        elif np.isnan(row[col]) and np.isnan(row[col + 1]):\n            continue\n        elif np.isnan(row[col]) and np.isnan(row[col - 1]):\n            continue\n        elif np.isnan(row[col]) and not np.isnan(row[col - 1]):\n            if np.isnan(row[col - 2]):\n                diff_columns.append((col, row[col - 2]))\n            else:\n                diff_columns.append((col, row[col - 2]))\n        elif np.isnan(row[col]) and not np.isnan(row[col + 2]):\n            if np.isnan(row[col - 1]):\n                diff_columns.append((col, row[col - 1]))\n            else:\n                diff_columns.append((col, row[col - 1]))\n        elif np.isnan(row[col]) and not np.isnan(row[col - 2]):\n            if np.isnan(row[col + 1]):\n                diff_columns.append((col, row[col + 1]))\n            else:\n                diff_columns.append((col, row[col + 1]))\n        elif np.isnan(row[col]) and not np.isnan(row[col + 2]):\n            if np.isnan(row[col - 2]):\n                diff_columns.append((col, row[col - 2]))\n            else:\n                diff_columns.append((col, row[col - 2]))\n        elif np.isnan(row[col]) and not np.isnan(row[col - 3]):\n            if np.isnan(row[col - 2]):\n                diff_columns.append((col, row[col - 2]))\n            else:\n                diff_columns.append((col, row[col - 2]))\n        elif np.isnan(row[col]) and not np.isnan(row[col + 3]):\n            if np.isnan(row[col - 2]):\n                diff_columns.append((col, row[col - 2]))\n            else:\n                diff_columns.append((col, row[col - 2]))\n        elif np.isnan(row[col]) and not np.isnan(row[col - 4]):\n            if np.isnan(row[col - 3]):\n                diff_columns.append((col, row[col - 3]))\n            else:\n                diff_columns.append((col, row[col - 3]))\n        elif np.isnan(row[col]) and not np.isnan(row[col + 4]):\n            if np.isnan(row[col - 3]):\n                diff_columns.append((col, row[col - 3]))\n            else:\n                diff_columns.append((col, row[col - 3]))\n        elif np.isnan(row[col]) and not np.isnan(row[col - 5]):\n            if np.isnan(row[col - 4]):\n                diff_columns.append((col, row[col - 4]))\n            else:\n                diff_columns.append((col, row[col - 4]))\n        elif np.isnan(row[col]) and not np.isnan(row[col + 5]):\n            if np.isnan(row[col - 4]):\n                diff_columns.append((col, row[col - 4]))\n            else:\n                diff_columns.append((col, row[col - 4]))\n        elif np.isnan(row[col]) and not np.isnan(row[col -", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df_new = df.stack().reset_index(level=0, drop=True).reshape(-1, 5)\ndf_new = df_new.T\ndf_new = df_new.reshape(-1, 5)\ndf_new = df_new.T\ndf_new = df_new.reshape(-1, 5)\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf_new = df_new.T\ndf", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df = df.T\ndf.columns = ['A_'+str(i) for i in range(df.shape[0])]\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index'])\ndf = df.T\ndf = df.reset_index()\ndf.columns = ['index', 'A', 'B', 'C', 'D', 'E']\ndf = df.drop(columns=['index", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2))\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "import numpy as np\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# round columns\ndf['dogs'] = df['dogs'].round(2)\ndf['cats'] = df['cats'].round(2)\n\n# replace None with NaN\ndf = df.replace(None, np.nan)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np.nan, None)\n\n# replace NaN with None\ndf = df.replace(np", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "def avg(cols):\n    return np.mean(df[cols])\n\ndf['Avg'] = avg(list_of_my_columns)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "result = df.sort_index(axis=0, kind='mergesort')\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "result = df.sort_values(by=['VIM', 'time'], ascending=[True, True])\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "# Delete rows where 'Date' is in the list of days to be deleted\ndf = df[~df['Date'].isin(['2020-02-17', '2020-02-18'])]\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "# Convert the Date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create a new column 'Day' to store the day of the week\ndf['Day'] = df['Date'].dt.day_name()\n\n# Create a new column 'Weekday' to store the day of the week as a number (0=Sunday, 6=Saturday)\ndf['Weekday'] = df['Date'].dt.day_name().map({'Sunday': 0, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6})\n\n# Create a new column 'Month' to store the month as a number\ndf['Month'] = df['Date'].dt.month\n\n# Create a new column 'Year' to store the year\ndf['Year'] = df['Date'].dt.year\n\n# Create a new column 'Hour' to store the hour\ndf['Hour'] = df['Date'].dt.hour\n\n# Create a new column 'Minute' to store the minute\ndf['Minute'] = df['Date'].dt.minute\n\n# Create a new column 'Second' to store the second\ndf['Second'] = df['Date'].dt.second\n\n# Create a new column 'Millisecond' to store the millisecond\ndf['Millisecond'] = df['Date'].dt.microsecond / 1000\n\n# Create a new column 'Date' to store the date as a string\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n\n# Create a new column 'Time' to store the time as a string\ndf['Time'] = df['Date'].astype(str) + ' ' + df['Hour'].astype(str) + ':' + df['Minute'].astype(str)\n\n# Create a new column 'DateTime' to store the date and time as a string\ndf['DateTime'] = df['Date'].astype(str) + ' ' + df['Time'].astype(str)\n\n# Create a new column 'Year-Month' to store the year and month as a string\ndf['Year-Month'] = df['DateTime'].astype(str).str.split(' ').str[0] + '-' + df['DateTime'].astype(str).str.split(' ').str[1]\n\n# Create a new column 'Year-Month-Day' to store the year, month, and day as a string\ndf['Year-Month-Day'] = df['DateTime'].astype(str).str.split(' ').str[0] + '-' + df['DateTime'].astype(str).str.split(' ').str[1] + '-' + df['DateTime'].astype(str).str.split(' ').str[2]\n\n# Create a new column 'Year-Month-Day-Hour' to store the year, month, day, and hour as a string\ndf['Year-Month-Day-Hour'] = df['DateTime'].astype(str).str.split(' ').str[0] + '-' + df['DateTime'].astype(str).str.split(' ').str[1] + '-' + df['DateTime'].astype(str).str.split(' ').str[2] + '-' + df['DateTime'].astype(str).str.split(' ').str[3]\n\n# Create a new column 'Year-Month-Day-Hour-Minute' to store the year, month, day, hour, and minute as a string\ndf['Year-Month-Day-Hour-Minute'] = df['DateTime'].astype(str).str.split(' ').str[0] + '-' + df['DateTime'].astype(str).str.split(' ').str[1] + '-' + df['DateTime'].astype(str).str.split(' ').str[2] + '-' + df['DateTime'].astype(str).str.split(' ').str[3] + '-' + df['DateTime'].astype(str).str.split(' ').str[4]\n\n# Create a new column 'Year-Month-Day-Hour-Minute-Second' to store the year, month, day, hour, minute, and second as a string\ndf['Year-Month-Day-Hour-Minute-Second'] = df['DateTime'].astype(str).str.split(' ').str[0", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "corr_above_0_3 = corr.where(corr > 0.3, 0)\nresult = corr_above_0_3[corr_above_0_3 > 0.3]\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "corr_above_0_3 = corr.abs() > 0.3\nresult = corr_above_0_3.stack(index=corr_above_0_3.columns).where(corr_above_0_3, 0).stack(index=corr_above_0_3.columns).where(corr_above_0_3, 0)\nresult = result.reset_index()\nresult = result[result == 1]\nresult = result.drop(columns=['index', 'column'])\nresult = result.sort_values(by='value', ascending=False)\nresult = result.reset_index(drop=True)\nresult = result[['column', 'index', 'value']]\nresult = result.rename(columns={'column': 'Column', 'index': 'Row', 'value': 'Correlation'})\nresult = result.astype(float)\nresult = result.round(2)\nresult = result.sort_values(by='Correlation', ascending=False)\nresult = result.reset_index(drop=True)\nresult = result.astype(str)\nresult = result.replace('0.00', '')\nresult = result.replace('-0.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('1.00', '')\nresult = result.replace('-1.00', '')\nresult = result.replace('0.00", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "df.columns[-1] = 'Test'\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.columns[0] = 'Test'\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "import numpy as np\n\ndef find_frequent(row):\n    unique, counts = np.unique(row, return_counts=True)\n    return unique[np.argmax(counts)]\n\ndef find_freq_count(row):\n    unique, counts = np.unique(row, return_counts=True)\n    return counts[np.argmax(counts)]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "import numpy as np\n\ndef find_frequent(row):\n    unique, counts = np.unique(row, return_counts=True)\n    return unique[np.argmax(counts)]\n\ndef find_freq_count(row):\n    unique, counts = np.unique(row, return_counts=True)\n    return counts[np.argmax(counts)]\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "import numpy as np\n\ndef find_frequent(row):\n    values, counts = np.unique(row, return_counts=True)\n    frequent = values[counts == np.max(counts)]\n    return frequent\n\ndef find_freq_count(row):\n    frequent = find_frequent(row)\n    return len(frequent)\n\ndf['frequent'] = df.apply(find_frequent, axis=1)\ndf['freq_count'] = df.apply(find_freq_count, axis=1)\n\nprint(df)\nEND SOLUTION\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "import numpy as np\n\n\n# Create a new column 'bar' with values 2 and 3\ndf['bar'] = np.where(df['id2'] == 2, 2, 3)\n\n\n# Group by 'id1' and 'id2' and calculate mean of 'foo' and 'bar'\nres = df.groupby(['id1', 'id2']).mean()\n\n\n# Reset the 'bar' column to its original values\nres['bar'] = df['bar'].where(df['id2'] == 2, 3)\n\n\n# Get mean of 'foo' and 'bar' for each group\nres = res[['foo', 'bar']]\n\n\n# Reorder columns\nres = res.reset_index()\n\n\n# Reorder columns\nres = res[['id1', 'id2', 'foo', 'bar']]\n\n\n# Print result\nprint(res)\nEND SOLUTION\n<code>\n# Output\n#   id1  id2  foo  bar\n# 0   1    1  8.0  None\n# 1   1    1  5.0  None\n# 2   1    1  3.0  None\n# 3   1    2  7.0  None\n# 4   1    2  4.0  2.0\n# 5   1    2  3.0  3.0\n# 6   1    3  9.0  None\n# 7   1    3  7.0  None\n# 8   1    3  5.0  None\n# 9   1    3  7.0  None\n# 10  2    2  7.0  None\n# 11  2    2  7.0  None\n# 12  2    3  9.0  None\n# 13  2    3  9.0  None\n# 14  3    4  7.0  None\n# 15  3    4  7.0  None\n# 16  3    4  7.0  None\n# 17  3    4  7.0  None\n# 18  3    4  7.0  None\n# 19  4    2  7.0  None\n# 20  4    2  7.0  None\n# 21  4    3  9.0  None\n# 22  4    3  9.0  None\n# 23  5    2  7.0  None\n# 24  5    2  7.0  None\n# 25  5    3  9.0  None\n# 26  5    3  9.0  None\n# 27  6    2  7.0  None\n# 28  6    2  7.0  None\n# 29  6    3  9.0  None\n# 30  6    3  9.0  None\n# 31  7    2  7.0  None\n# 32  7    2  7.0  None\n# 33  7    3  9.0  None\n# 34  7    3  9.0  None\n# 35  8    2  7.0  None\n# 36  8    2  7.0  None\n# 37  8    3  9.0  None\n# 38  8    3  9.0  None\n# 39  9    2  7.0  None\n# 40  9    2  7.0  None\n# 41  9    3  9.0  None\n# 42  9    3  9.0  None\n# 43  1    3  9.0  None\n# 44  1    3  9.0  None\n# 45  1    4  7.0  None\n# 46  1    4  7.0  None\n# 47  2    2  7.0  None\n# 48  2    2  7.0  None\n# 49  2    3  9.0  None\n# 50  2    3  9.0  None\n# 51  3    4  7.0  None\n# 52  3    4  7.0", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "import numpy as np\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n# replace 'NULL' with NaN\ndf['bar'] = df['bar'].replace('NULL', np.nan)\n\n\n#", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "result = pd.merge(df_a, df_b, left_on='EntityNum', right_on='EntityNum', how='left')\nresult = result[['EntityNum','foo','a_col']]\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "result = pd.merge(df_a, df_b, on='EntityNum', left_index=True, right_index=True)\nresult = result[['EntityNum','foo','b_col']]\n", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "result = a.shape\n", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = np.delete(x, np.where(np.isnan(x)))\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "import numpy as np\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nnp.nan_to_num(x, nan=np.inf)\n", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "result = x.tolist()\nresult = [row for row in result if not np.isnan(row).any()]\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.zeros((len(a), 4))\nb[:, 0] = a\nb[:, 1:] = np.eye(4)\nb[a, :] = 0\nb[a, a] = 1\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.zeros((a.shape[0], max(a) + 1))\nb[:, a] = 1\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "b = np.zeros((len(a), max(a) + 1))\nb[:, a] = 1\nb[:, 0] = 1\nb[:, -1] = 0\n", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "b = np.zeros((a.shape[0], a.shape[0]))\nfor i in range(a.shape[0]):\n    for j in range(a.shape[0]):\n        if i < j:\n            b[i, j] = 1\n        elif i == j:\n            b[i, j] = 0\n        else:\n            b[i, j] = 0\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "b = np.zeros((a.max()+1, a.max()+1))\nb[a[:,0], a[:,1]] = 1\nb[a[:,1], a[:,0]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,1], a[:,2]] = 1\nb[a[:,2], a[:,1]] = 1\nb[a[:,0], a[:,0]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,1], a[:,1]] = 1\nb[a[:,0], a[:,1]] = 1\nb[a[:,1], a[:,0]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,1], a[:,1]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,1], a[:,2]] = 1\nb[a[:,0], a[:,1]] = 1\nb[a[:,2], a[:,1]] = 1\nb[a[:,1], a[:,2]] = 1\nb[a[:,2], a[:,1]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,1], a[:,0]] = 1\nb[a[:,0], a[:,1]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,0], a[:,0]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,1], a[:,2]] = 1\nb[a[:,2], a[:,1]] = 1\nb[a[:,1], a[:,0]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,0], a[:,1]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,0], a[:,0]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,1], a[:,2]] = 1\nb[a[:,2], a[:,1]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,0], a[:,1]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,0], a[:,0]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,1], a[:,2]] = 1\nb[a[:,2], a[:,1]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,0], a[:,1]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,0], a[:,0]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,1], a[:,2]] = 1\nb[a[:,2], a[:,1]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]] = 1\nb[a[:,0], a[:,1]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,0], a[:,0]] = 1\nb[a[:,2], a[:,2]] = 1\nb[a[:,1], a[:,2]] = 1\nb[a[:,2], a[:,1]] = 1\nb[a[:,0], a[:,2]] = 1\nb[a[:,2], a[:,0]]", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "result = np.percentile(a, p)\nprint(result)\n", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\nB = vec2matrix(A, ncol)\nprint(B)\nEND SOLUTION\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "def vec2matrix(A, nrow):\n    return np.reshape(A, (nrow, -1))\nB = vec2matrix(A, nrow)\nprint(B)\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "def vec2matrix(A, ncol):\n    return np.reshape(A, (-1, ncol))\nB = vec2matrix(A, ncol)\nprint(B)\nEND SOLUTION\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "def vec2matrix(A, ncol):\n    nrows = len(A) // ncol\n    if len(A) % ncol != 0:\n        nrows += 1\n    B = np.zeros((nrows, ncol))\n    for i in range(nrows):\n        for j in range(ncol):\n            B[i, j] = A[i*ncol + j]\n    return B\nB = vec2matrix(A, ncol)\nprint(B)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "def shift_array(arr, shift):\n    if shift < 0:\n        return np.concatenate((arr[-shift:], arr[:-shift]))\n    else:\n        return np.concatenate((arr[:shift], arr))\n\nresult = shift_array(a, 3)\nprint(result)\n# Output:\n# [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\nresult = shift_array(a, -3)\nprint(result)\n# Output:\n# [  3.  4.  5.  6.  7.  8.  9.  0.  1.  2.]\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "def shift(x, n):\n    if n == 0:\n        return x\n    else:\n        return np.concatenate((x[n:], x[:n]))\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "def shift(x, shift):\n    return np.roll(x, shift, axis=0)\n", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "max_value_index = np.unravel_index(np.argmax(a, axis=None), a.shape)\nprint(max_value_index)\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "result = np.unravel_index(np.argmin(a), a.shape)\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "max_idx = np.unravel_index(np.argmax(a, axis=None), a.shape)\nprint(max_idx)\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "max_idx = np.unravel_index(np.argmax(a, axis=None), a.shape)\nprint(max_idx)\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": "\n    max_value = np.max(a)\n    max_indices = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    return max_indices\n    ### END SOLUTION\n\nf()\n## END\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "max_value = np.max(a)\nmax_indices = np.unravel_index(np.argmax(a), a.shape)\nprint(max_indices)\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "import numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t      [1., 2., 3., 9]])\nz = np.any(np.isnan(a), axis=0)\na = a[:, ~z]\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "import numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n\t      [1., 2., 3., 9]])\nmask = np.isnan(a).any(axis=1)\na = a[~mask]\nprint(a)\n", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "result = np.array(a)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "def permutation_matrix(permutation):\n    n = len(permutation)\n    P = np.eye(n)\n    for i in range(n):\n        P[i, permutation[i]] = 0\n    return P\na = np.array([[10, 20, 30, 40, 50],\n       [ 6,  7,  8,  9, 10]])\npermutation_matrix_P = permutation_matrix(permutation)\na = np.dot(permutation_matrix_P, a)\nprint(a)\n", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "result = np.transpose(a, permutation)\nprint(result)\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_idx = np.unravel_index(np.argmin(a), a.shape)\nprint(min_idx)\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_idx = np.unravel_index(np.argmax(a, axis=None), a.shape)\nprint(max_idx)\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "result = np.unravel_index(np.argmin(a, axis=None), a.shape)\nprint(result)\n</code\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "np.sin(np.deg2rad(degree))\n", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "cos_degree = np.cos(np.radians(degree))\nprint(cos_degree)\n", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "def is_degree(number):\n    return np.sin(np.radians(number)) > 0.99999\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "import numpy as np\nvalue = 1.0\ndegree = np.arctan(value) * 180 / np.pi\nresult = np.degrees(degree)\nprint(result)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.size), mode='constant', constant_values=0)\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    return np.pad(A, (0, length - A.size), mode='constant', constant_values=0)\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.power(a, power)\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": "\n    return np.power(a, power)\n    ### END SOLUTION\n\n# Test the function\nprint(f())\nprint(f(power=3))\nprint(f(power=2))\nprint(f(power=1))\nprint(f(power=0))\nprint(f(power=-1))\nprint(f(power=-2))\nprint(f(power=-3))\nprint(f(power=-4))\nprint(f(power=-5))\nprint(f(power=-6))\nprint(f(power=-7))\nprint(f(power=-8))\nprint(f(power=-9))\nprint(f(power=-10))\nprint(f(power=-11))\nprint(f(power=-12))\nprint(f(power=-13))\nprint(f(power=-14))\nprint(f(power=-15))\nprint(f(power=-16))\nprint(f(power=-17))\nprint(f(power=-18))\nprint(f(power=-19))\nprint(f(power=-20))\nprint(f(power=-21))\nprint(f(power=-22))\nprint(f(power=-23))\nprint(f(power=-24))\nprint(f(power=-25))\nprint(f(power=-26))\nprint(f(power=-27))\nprint(f(power=-28))\nprint(f(power=-29))\nprint(f(power=-30))\nprint(f(power=-31))\nprint(f(power=-32))\nprint(f(power=-33))\nprint(f(power=-34))\nprint(f(power=-35))\nprint(f(power=-36))\nprint(f(power=-37))\nprint(f(power=-38))\nprint(f(power=-39))\nprint(f(power=-40))\nprint(f(power=-41))\nprint(f(power=-42))\nprint(f(power=-43))\nprint(f(power=-44))\nprint(f(power=-45))\nprint(f(power=-46))\nprint(f(power=-47))\nprint(f(power=-48))\nprint(f(power=-49))\nprint(f(power=-50))\nprint(f(power=-51))\nprint(f(power=-52))\nprint(f(power=-53))\nprint(f(power=-54))\nprint(f(power=-55))\nprint(f(power=-56))\nprint(f(power=-57))\nprint(f(power=-58))\nprint(f(power=-59))\nprint(f(power=-60))\nprint(f(power=-61))\nprint(f(power=-62))\nprint(f(power=-63))\nprint(f(power=-64))\nprint(f(power=-65))\nprint(f(power=-66))\nprint(f(power=-67))\nprint(f(power=-68))\nprint(f(power=-69))\nprint(f(power=-70))\nprint(f(power=-71))\nprint(f(power=-72))\nprint(f(power=-73))\nprint(f(power=-74))\nprint(f(power=-75))\nprint(f(power=-76))\nprint(f(power=-77))\nprint(f(power=-78))\nprint(f(power=-79))\nprint(f(power=-80))\nprint(f(power=-81))\nprint(f(power=-82))\nprint(f(power=-83))\nprint(f(power=-84))\nprint(f(power=-85))\nprint(f(power=-86))\nprint(f(power=-87))\nprint(f(power=-8", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\nprint(result)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": "\n    result = np.divide(numerator, denominator)\n    return (int(result.numerator), int(result.denominator))\n    ### END SOLUTION\n    # return result\n    # return np.divide(numerator, denominator)\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(denominator)])\n    # return np.array([numerator, denominator])\n    # return np.array([np.int64(numerator), np.int64(", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "result = np.divide(numerator, denominator)\nresult = np.where(denominator == 0, np.nan, result)\nprint(result)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = np.where(b == 20, a, np.where(c == 40, b, (a + b) / 2))\n", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.where(a == b, a, np.where(a == c, a, b))\n", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "def diagonal_from_top_right(arr):\n    n = arr.shape[0]\n    return arr[np.arange(n), n - np.arange(n)[::-1]]\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "def diagonal_from_top_right(a):\n    return a[np.triu_indices(a.shape[0], 1, 1)]\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "def get_diagonal(a):\n    n = a.shape[0]\n    diagonal = np.zeros(n)\n    for i in range(n):\n        diagonal[i] = a[i, i]\n    return diagonal\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\nresult = get_diagonal(a)\nprint(result)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "def diagonal_bottom_left(a):\n    n = a.shape[0]\n    return np.diag(a, k=-n)\n\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n\nresult = diagonal_bottom_left(a)\nprint(result)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = np.empty(X.size, dtype=np.object)\nfor i in range(X.size):\n    result[i] = X[i // X.shape[1], i % X.shape[1]]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "result = np.sort(X.flatten())\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": "\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i, j])\n    return result\n    ### END SOLUTION\n\nf()\n## END\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "result = np.empty(X.size, dtype=np.object)\nfor i in range(X.size):\n    result[i] = X[i // X.shape[1], i % X.shape[1]]\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "result = np.array([int(i) for i in mystr])\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "result = a[:, col] * multiply_number\ncumulative_sum = np.cumsum(result)\nprint(cumulative_sum)\n", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "result = multiply_number * a[row, :] + np.cumsum(a[row, :])\n", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "a[row, :] /= divide_number\nresult = np.prod(a[row, :])\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "import numpy as np\n\ndef get_max_independent_vectors(a):\n    # Get the number of columns in a\n    n = a.shape[1]\n    \n    # Initialize an array to store the indices of independent vectors\n    indices = np.arange(n)\n    \n    # Initialize an array to store the maximum number of independent vectors\n    max_independent_vectors = 0\n    \n    # Iterate over all possible combinations of indices\n    while True:\n        # Get the current set of linearly independent vectors\n        vectors = a[:, indices]\n        \n        # Check if the current set of linearly independent vectors is maximal\n        if np.all(np.linalg.matrix_rank(vectors) == n, axis=0):\n            # Update the maximum number of independent vectors\n            max_independent_vectors = max(max_independent_vectors, np.sum(np.all(vectors == 0, axis=1)))\n            \n            # Find the next index to be added to the set of independent vectors\n            next_index = np.where(np.all(vectors == 0, axis=1))[0][0]\n            \n            # Update the indices array\n            indices = np.delete(indices, next_index)\n            \n            # If no more indices can be added, break the loop\n            if len(indices) == 0:\n                break\n            \n            # Reset the indices array\n            indices = np.arange(n)\n            \n        else:\n            # If the current set of linearly independent vectors is not maximal,\n            # find the next index to be added to the set of independent vectors\n            next_index = np.where(np.all(vectors == 0, axis=1))[0][0]\n            \n            # Update the indices array\n            indices = np.delete(indices, next_index)\n            \n            # If no more indices can be added, break the loop\n            if len(indices) == 0:\n                break\n            \n            # Reset the indices array\n            indices = np.arange(n)\n    \n    return max_independent_vectors\n\n# Test the function\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\nprint(get_max_independent_vectors(a))\n```\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "row_size = a.shape[1]\n", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "np.random.seed(0)\na = np.random.randn(40)\nb = np.random.randn(50)\nn1 = len(a)\nn2 = len(b)\nmean_a = np.mean(a)\nmean_b = np.mean(b)\nstd_dev_a = np.std(a)\nstd_dev_b = np.std(b)\n# calculate the t-statistic\nt = (mean_a - mean_b) / np.sqrt((std_dev_a**2 / n1) + (std_dev_b**2 / n2))\n# calculate the p-value\np_value = np.abs(scipy.stats.t.pdf(t, n1-1))\nprint(p_value)\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "n = len(a)\na_mean = np.mean(a)\nb_mean = np.mean(b)\na_std = np.std(a)\nb_std = np.std(b)\nn_a = len(a)\nn_b = len(b)\na_mean_n = n_a\nb_mean_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n = n_b\na_std_n = n_a\nb_std_n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "n1 = anobs\nn2 = bnobs\nmean1 = amean\nmean2 = bmean\nvar1 = avar\nvar2 = bvar\nn = n1 + n2\nt_stat = (mean1 - mean2) / np.sqrt((var1 / n1) + (var2 / n2))\np_value = (1 - scipy.stats.t.cdf(np.abs(t_stat), n)) * 2\nprint(p_value)\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "# Remove elements from A that are in B\noutput = np.setdiff1d(A, B)\n", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "# Get elements from A that are not in B\nnot_in_B = np.setdiff1d(A, B)\n# Get elements from B that are not in A\nnot_in_A = np.setdiff1d(B, A)\n# Combine the two results\nresult = np.concatenate((not_in_B, not_in_A))\nprint(result)\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "sort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "# Calculate the sum of each matrix\nsums = np.sum(b, axis=(1, 2))\n# Get the indices of the sorted sums\nindices = np.argsort(sums)\n# Sort the matrices based on the sorted indices\nsorted_b = b[indices]\nprint(sorted_b)\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "import numpy as np\na = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nprint(a)\n# Output:\n# [[ 1  2  3  4]\n#  [ 5  6  7  8]\n#  [ 9 10 11 12]]\na = np.delete(a, 2, axis=1)\nprint(a)\n# Output:\n# [[ 1  2  3]\n#  [ 5  6  7]\n#  [ 9 10 11]]\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "import numpy as np\na = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\na = np.delete(a, 2, axis=0)\nprint(a)\n", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "import numpy as np\na = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\na = np.delete(a, [0, 2], axis=1)\nprint(a)\n", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "def_--col = np.array([1, 2, 4, 5])\ndel_col = np.delete(del_col, np.where(del_col < 0))\nresult = np.delete(a, del_col, axis=1)\nprint(result)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a = np.insert(a, pos, element)\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": "\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION\n    return a\nprint(f())\n[1 2 66 3 4]\nprint(f(pos=3, element=7))\n[1 2 7 3 4]\nprint(f(element=8))\n[1 2 8 3 4]\nprint(f(pos=2, element=66))\n[1 66 2 3 4]\nprint(f(element=9))\n[1 66 9 3 4]\nprint(f(pos=4, element=10))\n[1 66 9 10 4]\nprint(f(element=11))\n[1 66 11 10 4]\nprint(f(pos=5, element=12))\n[1 66 11 12 4]\nprint(f(element=13))\n[1 66 13 12 4]\nprint(f(pos=6, element=14))\n[1 66 13 14 4]\nprint(f(element=15))\n[1 66 13 15 4]\nprint(f(pos=7, element=16))\n[1 66 13 16 4]\nprint(f(element=17))\n[1 66 13 17 4]\nprint(f(pos=8, element=18))\n[1 66 13 18 4]\nprint(f(element=19))\n[1 66 13 19 4]\nprint(f(pos=9, element=20))\n[1 66 13 20 4]\nprint(f(element=21))\n[1 66 13 21 4]\nprint(f(pos=10, element=22))\n[1 66 13 22 4]\nprint(f(element=23))\n[1 66 13 23 4]\nprint(f(pos=11, element=24))\n[1 66 13 24 4]\nprint(f(element=25))\n[1 66 13 25 4]\nprint(f(pos=12, element=26))\n[1 66 13 26 4]\nprint(f(element=27))\n[1 66 13 27 4]\nprint(f(pos=13, element=28))\n[1 66 13 28 4]\nprint(f(element=29))\n[1 66 13 29 4]\nprint(f(pos=14, element=30))\n[1 66 13 30 4]\nprint(f(element=31))\n[1 66 13 31 4]\nprint(f(pos=15, element=32))\n[1 66 13 32 4]\nprint(f(element=33))\n[1 66 13 33 4]\nprint(f(pos=16, element=34))\n[1 66 13 34 4]\nprint(f(element=35))\n[1 66 13 35 4]\nprint(f(pos=17, element=36))\n[1 66 13 36 4]\nprint(f(element=37))\n[1 66 13 37 4]\nprint(f(pos=18, element=38))\n[1 66 13 38 4]\nprint(f(element=39))\n[1 66 13 39 4]\nprint(f(pos=19, element=40))\n[1 66 13 40 4]\nprint(f(element=41))\n[1 66 13 41 4]\nprint(f(pos=", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\nprint(a)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "import numpy as np\narray_of_arrays = np.array([np.arange(a*b).reshape(a,b) for (a, b) in pairs])\ndeep_copy_of_arrays = np.copy(array_of_arrays)\nfor i in range(len(array_of_arrays)):\n    for j in range(len(array_of_arrays[i])):\n        for k in range(len(array_of_arrays[i][j])):\n            deep_copy_of_arrays[i][j][k] = np.copy(array_of_arrays[i][j][k])\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "np.all(a[:, 1:] == a[:, :-1])\n</code\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "np.all(a[:, 1:] == a[:, :-1])\n</code\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": "\n    return np.all(np.all(a == a[0], axis = 1))\n    ### END SOLUTION\n\n# Test the function\nprint(f())  # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])))  # Output: False\nprint(f(np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])))  # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])))  # Output: False\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.all(np.all(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]] == np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0), axis = 1))) ) # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.all(np.all(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]] == np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0), axis = 1))) ) # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.all(np.all(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]] == np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0), axis = 1))) ) # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.all(np.all(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]] == np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0), axis = 1))) ) # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.all(np.all(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]] == np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0), axis = 1))) ) # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.all(np.all(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]] == np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0), axis = 1))) ) # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.all(np.all(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]] == np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0), axis = 1))) ) # Output: True\nprint(f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nx_values = np.linspace(0, 1, 10)\ny_values = np.linspace(0, 1, 20)\nx_values, y_values = np.meshgrid(x_values, y_values)\nresult = np.sum(np.cos(x)**4 + np.sin(y)**2)\nprint(result)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": "\n    return (cos(x)**4) + (sin(y)**2)\n    ### END SOLUTION\n\nx = np.linspace(0, 1, 20)\ny = np.linspace(0, 1, 30)\nx, y = np.meshgrid(x, y)\n\nf_values = f(x, y)\n\n# Use Simpson's rule to integrate the function\ndef simps_rule(f, x, y):\n    hx = np.diff(x)\n    hy = np.diff(y)\n    return (hx * hy).sum() / 3.0\n\n# Use Simpson's rule to integrate the function\nresult = simps_rule(f, x, y)\n\nprint(result)\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(x):\n    x_sorted = np.sort(x)\n    n = len(x_sorted)\n    x_mean = np.mean(x_sorted)\n    return np.arange(n) / n, x_sorted - x_mean\n\nx = np.array([93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61])\n\nx_mean = np.mean(x)\nx_sorted = np.sort(x)\n\nx_mean_diff = np.mean(x_sorted - x_mean)\n\nx_sorted_diff = np.sort(x_sorted - x_mean)\n\nx_sorted_diff_squared = np.square(x_sorted_diff)\n\nx_sorted_diff_squared_mean = np.mean(x_sorted_diff_squared)\n\nx_sorted_diff_squared_sum = np.sum(x_sorted_diff_squared)\n\nx_sorted_diff_squared_sum_mean = np.mean(x_sorted_diff_squared_sum)\n\nx_sorted_diff_squared_sum_mean_squared = np.square(x_sorted_diff_squared_sum_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean = np.mean(x_sorted_diff_squared_sum_mean_squared)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean_mean = np.mean(x_sorted_diff_squared_sum_mean_squared_mean_mean_mean_mean)\n\nx_sorted_diff_squared_sum_mean_squared_mean_mean_", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(x):\n    x = np.sort(x)\n    n = len(x)\n    y = np.arange(1, n + 1) / n\n    return y\nresult = ecdf(grades)\nprint(result)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def ecdf(x):\n    # normalize X to sum to 1\n    x = x / np.sum(x)\n    return np.cumsum(x)\n\ndef find_longest_interval(x, threshold):\n    # compute ECDF\n    ecdf_x = ecdf(x)\n    # find the index of the first element where ECDF(x) is greater than threshold\n    idx = np.where(ecdf_x < threshold)[0]\n    # find the longest interval\n    if idx.size == 0:\n        return None, None\n    low = x[idx[0]]\n    high = x[idx[-1]]\n    return low, high\n\n# test the function\nlow, high = find_longest_interval(grades, threshold)\nprint(f\"Longest interval: [{low}, {high}]\")\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "np.random.seed(0)\nrandom_array = np.random.randint(2, size=size)\none_ratio_index = int(size * one_ratio)\none_index = np.random.choice(size, size=one_ratio_index, replace=False)\nzero_index = np.random.choice(size, size=size - one_ratio_index, replace=False)\nrandom_array[one_index] = 1\nrandom_array[zero_index] = 0\nprint(random_array)\n", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "np_a = a.numpy()\nprint(np_a)\n", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = np.ascontiguousarray(a)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\na_pt = np.ascontiguousarray(a_pt)\na_pt = np.transpose(a_pt, (1, 0))\na_pt = np.reshape(a_pt, (1, 5))\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = np.array(a)\n", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = np.asarray(a)\na_tf = np.expand_dims(a_tf, axis=0)\na_tf = np.expand_dims(a_tf, axis=3)\na_tf = np.expand_dims(a_tf, axis=4)\na_tf = np.asarray(a_tf)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "result = np.argsort(a)[::-1]\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "result = np.argsort(a) + 1\nresult = result[::-1]\n</code\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "# Get the indices of the N biggest elements in decreasing order\nindices = np.argsort(a)[::-1][:N]\n# Get the corresponding values\nvalues = a[indices]\n# Get the indices of the smallest elements\nsmallest_indices = np.argsort(values)\n# Get the corresponding values\nsmallest_values = values[smallest_indices]\n# Get the indices of the smallest elements\nresult = smallest_indices\nprint(result)\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "result = np.power(A, n)\n", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "def extract_patches(a, patch_size):\n    h, w = a.shape[:2]\n    patches = []\n    for i in range(0, h - patch_size + 1):\n        for j in range(0, w - patch_size + 1):\n            patch = a[i:i+patch_size, j:j+patch_size]\n            patches.append(patch)\n    return np.array(patches)\n\npatch_size = 2\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "def extract_patches(a, patch_size):\n    h, w = a.shape[:2]\n    patches = []\n    for i in range(0, h - patch_size + 1):\n        for j in range(0, w - patch_size + 1):\n            patch = a[i:i+patch_size, j:j+patch_size]\n            patches.append(patch)\n    return np.array(patches)\n\npatch_size = 2\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\npatches = extract_patches(a, patch_size)\nprint(patches.shape)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "def extract_patches(a, patch_size):\n    h, w = a.shape[:2]\n    patches = []\n    for i in range(0, h - patch_size + 1):\n        for j in range(0, w - patch_size + 1):\n            patch = a[i:i+patch_size, j:j+patch_size]\n            patches.append(patch)\n    return np.array(patches)\n\npatch_size = 2\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "def extract_patches(a, patch_size):\n    rows, cols = a.shape\n    patch_rows = (rows - patch_size + 1) // patch_size\n    patch_cols = (cols - patch_size + 1) // patch_size\n    patches = np.zeros((patch_rows, patch_cols, patch_size, patch_size))\n    for i in range(patch_rows):\n        for j in range(patch_cols):\n            patches[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n    return patches\n\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "result = np.reshape(a, (h, w, -1))\nresult = np.transpose(result, (0, 2, 1))\nresult = np.reshape(result, (h * w, -1))\nresult = np.split(result, h)\nresult = np.array([np.split(row, w) for row in result])\nresult = np.reshape(result, (h * w, w))\nresult = np.reshape(result, (h, w))\nresult = np.transpose(result, (1, 0))\nresult = np.reshape(result, (w, h))\nresult = np.reshape(result, (w,))\nresult = np.tile(result, (w, 1))\nresult = np.reshape(result, (w, w))\nresult = np.reshape(result, (1, w, w))\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result, ())\nresult = np.reshape(result,", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "def extract_patches(a, patch_size):\n    rows, cols = a.shape\n    patch_rows = (rows - patch_size + 1) // patch_size\n    patch_cols = (cols - patch_size + 1) // patch_size\n    patches = np.zeros((patch_rows, patch_cols, patch_size, patch_size))\n    for i in range(patch_rows):\n        for j in range(patch_cols):\n            patches[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n    return patches\n\nresult = extract_patches(a, patch_size)\nprint(result)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high+1]\n", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high+1]\n", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "result = np.take(a, np.arange(low, high + 1), axis=1)\n", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "import numpy as np\ndef from_string(s):\n    return np.array([[float(x) for x in row.split()] for row in s.strip('[]').split('],[')])\na = from_string(\"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\")\nprint(a)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "min_log = np.log(min)\nmax_log = np.log(max)\nn_log = np.log(n)\n\nlog_dist = np.random.uniform(min_log, max_log, n_log)\n\nresult = np.exp(log_dist)\n\nprint(result)\n", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "def loguni(min, max, n):\n    return np.exp(np.random.uniform(low=np.log(min), high=np.log(max), size=n))\n\nresult = loguni(min, max, n)\nprint(result)\n", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": "\n    x = np.random.uniform(min, max, n)\n    llog = np.log(x)\n    llog_min = llog.min()\n    llog_max = llog.max()\n    llog_range = llog_max - llog_min\n    llog_scale = llog_range / (np.log(max) - np.log(min))\n    llog_shift = llog_min\n    llog_result = llog * llog_scale + llog_shift\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result = llog_result.astype(np.float64)\n    llog_result = llog_result.astype(np.int64)\n    llog_result", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "def calculate_B(A, a, b):\n    B = np.zeros(len(A))\n    B[0] = a * A[0]\n    for i in range(1, len(A)):\n        B[i] = a * A[i] + b * B[i-1]\n    return B\na = 2\nb = 3\nB = calculate_B(A, a, b)\nprint(B)\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "def calculate_B(A, a, b, c):\n    B = np.zeros(len(A))\n    B[0] = a * A[0]\n    B[1] = a * A[1] + b * B[0]\n    for i in range(2, len(A)):\n        B[i] = a * A[i] + b * B[i-1] + c * B[i-2]\n    return B\n\na = 2\nb = 3\nc = 4\nB = calculate_B(A, a, b, c)\nprint(B)\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "result = np.empty((0,), dtype=int)\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "result = np.zeros((3,0))\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "def sub2ind(shape, indices):\n    return np.ravel_multi_index(indices, shape)\n\n# Test the function\ndims = (3, 4, 2)\nindex = (1, 0, 1)\nprint(sub2ind(dims, index))\n# Output: 16\n# END SOLUTION\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n#", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "def sub2ind(dims, index):\n    return index[0] * np.prod(dims[:-1]) + index[1] * dims[1] + index[2]\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "values = np.zeros((2,3), dtype='int32,float32')\ndf = pd.DataFrame(data=values, index=index, columns=columns)\nprint(df)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "def accum_np(accmap, a):\n    accum = np.zeros(accmap.size)\n    for i in range(accmap.size):\n        accum[i] = np.sum(a[accmap == i])\n    return accum\nresult = accum_np(accmap, a)\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = np.max(a[index])\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "result = np.cumsum(a[accmap])\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = np.minimum.reduceat(a, index)\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "def elementwise_function(element_1, element_2):\n    return element_1 + element_2\nx = np.array([[2, 2, 2],\n              [2, 2, 2],\n              [2, 2, 2]])\ny = np.array([[3, 3, 3],\n              [3, 3, 3],\n              [3, 3, 1]])\nz = np.zeros_like(x)\nfor i in range(x.shape[0]):\n    for j in range(x.shape[1]):\n        z[i, j] = elementwise_function(x[i, j], y[i, j])\nprint(z)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "np.random.choice(lista_elegir, samples, probabilit, replace=False)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "result = np.pad(a, ((low_index, high_index), (low_index, high_index), (0, 0)), mode='constant')\nresult = result[low_index:high_index, low_index:high_index]\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "import numpy as np\nx = np.array([-2, -1.4, -1.1, 0, 1.2, 2.2, 3.1, 4.4, 8.3, 9.9, 10, 14, 16.2])\nresult = x[x > 0]\nprint(result)\n", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "import numpy as np\nx = np.array([-2+1j, -1.4, -1.1, 0, 1.2, 2.2+2j, 3.1, 4.4, 8.3, 9.9, 10+0j, 14, 16.2])\nresult = x[(x.imag != 0) & (x.real != 0)]\nprint(result)\n", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = np.array_split(data, len(data) // bin_size + 1)\nmean_data = [np.mean(bin) for bin in bin_data]\nprint(mean_data)\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data = np.array_split(data, len(data) // bin_size + 1)\nbin_data_max = np.max(bin_data, axis=1)\nprint(bin_data_max)\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "bin_data = np.array_split(data, bin_size)\nmean_data = np.array([np.mean(bin) for bin in bin_data])\nprint(mean_data)\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "bin_data = np.split(data, np.arange(bin_size, len(data), bin_size))\nbin_data_mean = np.array([np.mean(bin_data[i]) for i in range(len(bin_data))])\nprint(bin_data_mean)\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "def bin_data(data, bin_size):\n    return np.split(data, np.arange(0, len(data), bin_size))\n\ndef calculate_mean(data):\n    return np.mean(data, axis=1)\n\ndata_bin = bin_data(data, bin_size)\nmean_data = calculate_mean(data_bin)\nprint(mean_data)\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "def bin_data(data, bin_size):\n    bin_data = np.split(data, np.arange(0, len(data), bin_size))\n    return bin_data\n\ndef calculate_mean(data):\n    return np.mean(data, axis=1)\n\ndef align_data(data, bin_size):\n    data = np.array(data)\n    bin_data = bin_data(data, bin_size)\n    mean_data = calculate_mean(bin_data)\n    return mean_data\n\nbin_data_mean = align_data(data, bin_size)\nprint(bin_data_mean)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, min_val, max_val):\n    return np.where(\n        x < min_val,\n        min_val,\n        np.where(\n            x > max_val,\n            max_val,\n            x\n        )\n    )\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = (x - x_min) / (x_max - x_min)\n    t = np.clip(t, 0, 1)\n    t = t * t * (3 - 2 * t)\n    return x_min + t * (x_max - x_min)\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "import numpy as np\n\ndef circular_correlation(a, b):\n    # Calculate the circular correlation\n    correlation = np.correlate(a, b, mode='full')\n    # Calculate the indices of the maximum correlation value\n    max_idx = np.argmax(correlation)\n    # Calculate the maximum correlation value\n    max_corr = correlation[max_idx]\n    # Calculate the length of the arrays a and b\n    n = len(a)\n    # Calculate the length of the array b\n    m = len(b)\n    # Calculate the length of the array a\n    p = len(a)\n    # Calculate the length of the array b\n    n_b = m\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the array b\n    m_b = n\n    # Calculate the length of the array a\n    p_a = n\n    # Calculate the length of the", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "result = df.values\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = np.array([[np.random.rand(5) for _ in range(4)] for _ in range(15)])\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "def binary_array(num):\n    m = np.bitwise_and(num, 2**m - 1)\n    binary_array = np.zeros((m,), dtype=np.uint8)\n    for i in range(m):\n        binary_array[i] = np.bitwise_and(num, 2**(i+1))\n        num = np.bitwise_right_shift(num, 1)\n    return binary_array\nresult = np.array([binary_array(num) for num in a])\nprint(result)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "def binary_array(num):\n    binary = np.binary_repr(num, width=m)\n    return np.array([int(b) for b in binary], dtype=np.uint8)\n\nresult = np.array([binary_array(i) for i in a])\nprint(result)\n", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "def binary_to_xor(num):\n    binary = np.binary_repr(num, width=m)\n    return np.xor.reduce(np.array([int(b) for b in binary]), axis=0)\n\nresult = np.array([binary_to_xor(i) for i in range(2**m)])\nprint(result)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "import numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\nmean = np.mean(a)\nstd = np.std(a)\nstart = mean - 3 * std\nend = mean + 3 * std\nresult = (start, end)\nprint(result)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "import numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\nmean = np.mean(a)\nstd = np.std(a)\nstart = mean - 2 * std\nend = mean + 2 * std\nresult = (start, end)\nprint(result)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": "\n    mean = np.mean(a)\n    std = np.std(a)\n    sigma = std\n    start = mean - 3 * sigma\n    end = mean + 3 * sigma\n    return (start, end)\n    ### END SOLUTION\n\nprint(f())\n## END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "import numpy as np\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\nstd_dev = np.std(a)\nmean = np.mean(a)\nsigma = std_dev\nx_min = mean - 2 * sigma\nx_max = mean + 2 * sigma\nresult = (a < x_min) | (a > x_max)\nprint(result)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "import numpy as np\ndata = np.array([[1, 2, 3], [4, 5, 6]])\nmasked_data = np.ma.masked_where(data < 0, data)\nprob = np.percentile(masked_data, percentile)\nprint(prob)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "import numpy as np\n\ndef zero_out_row_col(a, row, col):\n    a[row, :] = 0\n    a[:, col] = 0\n    return a\n\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nprint(zero_out_row_col(a, 0, 0))\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "import numpy as np\n\ndef zero_out_rows_cols(a, zero_rows, zero_cols):\n    # Create a mask to select rows and columns to zero\n    mask_rows = np.ones(a.shape[0], dtype=bool)\n    mask_cols = np.ones(a.shape[1], dtype=bool)\n    for i in zero_rows:\n        mask_rows[i] = False\n    for j in zero_cols:\n        mask_cols[j] = False\n\n    # Zero out rows and columns\n    a[mask_rows, :] = 0\n    a[:, mask_cols] = 0\n    return a\n\n# Test the function\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = [1, 3]\nzero_cols = [1, 2]\nprint(zero_out_rows_cols(a, zero_rows, zero_cols))\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "import numpy as np\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\na[1, 0] = 0\na[:, 0] = 0\nprint(a)\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "mask = np.where(np.argmax(a, axis=1) == 0, True, False)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.where(a == np.min(a, axis=1), True, False)\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "import numpy as np\ndef calculate_correlation(post, distance):\n    # Calculate the mean of each list\n    mean_post = np.mean(post)\n    mean_distance = np.mean(distance)\n    \n    # Calculate the variance of each list\n    var_post = np.var(post)\n    var_distance = np.var(distance)\n    \n    # Calculate the covariance\n    covariance = np.sum((post - mean_post) * (distance - mean_distance)) / (distance.size - 1)\n    \n    # Calculate the correlation coefficient\n    correlation = covariance / (var_post * var_distance) ** 0.5\n    \n    return correlation\n\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\nresult = calculate_correlation(post, distance)\nprint(result)\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ij,ij->ij', X, X)\n", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.zeros((Y.shape[1], Y.shape[2]))\nfor i in range(Y.shape[1]):\n    for j in range(Y.shape[2]):\n        for k in range(Y.shape[2]):\n            X[i, j] += Y[:, i, j, k]\n", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "def contains(a, number):\n    return number in a\nis_contained = contains(a, number)\nprint(is_contained)\n", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A = np.delete(A, np.where(A==B))\nC = A\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "A_unique = np.unique(A)\nC = A_unique[np.in1d(A, B)]\n", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "C = np.where((A[:,None] == B) | (A[None,:] == B), A, 0)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete(C, np.where(C==0)[0], axis=1)\nC = np.delete(C, np.where(C==0)[0], axis=0)\nC = np.delete", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "ranked_values = rankdata(a)\nresult = np.sort(ranked_values)[::-1]\n", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "ranked_a = rankdata(a)\nresult = np.sort(ranked_a)[::-1]\n", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": "\n    # Use rankdata function to get the rank of each value\n    ranks = rankdata(a)\n    # Then use the inverse of the rank data to get the rank of each value in descending order\n    desc_ranks = len(a) - ranks\n    # Return the result\n    return desc_ranks\nprint(f())\n## END SOLUTION\n## END\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "import numpy as np\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, 1, -2],\n                 [ -1,  0, 1],\n                 [ -2,  1,  0]])\n\ndist_tuples = np.stack((x_dists, y_dists), axis=2)\ndist_tuples = dist_tuples.reshape((x_dists.shape[0], x_dists.shape[1], 2))\ndist_tuples = dist_tuples.transpose((2, 0, 1))\n\nprint(dist_tuples)\n", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "dist_tuples = np.stack((x_dists, y_dists), axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 2))\ndist_tuples = np.transpose(dist_tuples, (0, 1, 2))\ndist_tuples = np.stack((dist_tuples, np.ones((x_dists.shape[0], x_dists.shape[1], 1))), axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 3))\ndist_tuples = np.delete(dist_tuples, 2, axis=2)\ndist_tuples = np.transpose(dist_tuples, (0, 1, 2))\ndist_tuples = np.stack((dist_tuples, np.ones((x_dists.shape[0], x_dists.shape[1], 1))), axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 4))\ndist_tuples = np.delete(dist_tuples, 3, axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 2))\ndist_tuples = np.stack((dist_tuples, np.ones((x_dists.shape[0], x_dists.shape[1], 1))), axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 3))\ndist_tuples = np.delete(dist_tuples, 2, axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 1))\ndist_tuples = np.concatenate((dist_tuples, np.ones((x_dists.shape[0], x_dists.shape[1], 1))), axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 2))\ndist_tuples = np.stack((dist_tuples, np.ones((x_dists.shape[0], x_dists.shape[1], 1))), axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 3))\ndist_tuples = np.delete(dist_tuples, 2, axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 1))\ndist_tuples = np.concatenate((dist_tuples, np.ones((x_dists.shape[0], x_dists.shape[1], 1))), axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 2))\ndist_tuples = np.stack((dist_tuples, np.ones((x_dists.shape[0], x_dists.shape[1], 1))), axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 3))\ndist_tuples = np.delete(dist_tuples, 2, axis=2)\ndist_tuples = np.reshape(dist_tuples, (x_dists.shape[0], x_dists.shape[1], 1))\ndist_tuples = np.concatenate((", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "result = np.take(a, np.array([second, third]).T, axis=0)\n", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20,10,10,2))\n", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "x = np.array([LA.norm(v,ord=1) for v in X])\nx = x / x.sum()\nprint(x)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "x = np.linalg.norm(X, axis=1)\nX_normalized = X / x[:, np.newaxis]\nprint(X_normalized)\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=np.inf) for v in X])\nnormalized_X = X / x[:, np.newaxis]\nprint(normalized_X)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = [a[\"properties_path\"].str.contains(target)\n              ]\nresult = np.select(conditions, choices, default=np.nan)\nprint(result)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "def distance(x, y):\n    return np.sqrt(np.sum((x - y) ** 2))\n\ndef pdist(X):\n    n = X.shape[0]\n    dist = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            dist[i, j] = dist[j, i] = distance(X[i], X[j])\n    return dist\n\nX = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\n\ndist = pdist(X)\nprint(dist)\n", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "def pdist(a, b):\n    return np.sum((a[:, np.newaxis] - b) ** 2, axis=2)\n\na = np.random.rand(5, 4)\ndistances = pdist(a, a)\nprint(distances)\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "def pdist(a, b):\n    return np.sum((a[:, np.newaxis] - b) ** 2, axis=2)\n\ndistances = pdist(a, a)\nprint(distances)\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA, axis=0)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA, axis=0)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "import numpy as np\n\ndef remove_duplicate_zero_and_zero(arr):\n    # Remove adjacent duplicates\n    arr = np.delete(arr, np.where(np.diff(arr) == 0))\n    # Remove zero values\n    arr = np.delete(arr, np.where(arr == 0))\n    return arr\n\n# Test the function\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\nprint(remove_duplicate_zero_and_zero(a))\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "import numpy as np\n\ndef remove_duplicate_zero_and_zero(arr):\n    arr = np.sort(arr)\n    mask = np.where(arr[:-1] != arr[1:])[0]\n    return arr[mask + 1]\n\narr = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3]).reshape(-1, 1)\nprint(remove_duplicate_zero_and_zero(arr))\n", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame(np.column_stack((lat, lon, val)), columns=['lat', 'lon', 'val'])\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "\n    # Create a dataframe\n    df = pd.DataFrame({'lat': lat[0], 'lon': lon[0], 'val': val[0]})\n    # Iterate over the rest of the arrays\n    for i in range(1, len(lat)):\n        # Append the next row to the dataframe\n        df = df.append({'lat': lat[i], 'lon': lon[i], 'val': val[i]}, ignore_index=True)\n    return df\n    ### END SOLUTION\n\n# Test the function\nprint(f())\n## END\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame({'lat': lat, 'lon': lon, 'val': val})\ndf['maximum'] = df.apply(lambda row: np.max(row['lat']), axis=1)\nprint(df)\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    return np.roll(np.pad(a, ((size[0]//2, size[0]//2), (size[1]//2, size[1]//2)), 'constant'), (-size[0]//2, size[0]//2), axis=0),\n                   (-size[1]//2, size[1]//2), axis=1)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    return np.roll(np.pad(a, ((size[0]//2, size[0]//2), (size[1]//2, size[1]//2)), 'constant'), (-size[0]//2, size[0]//2), axis=0),\n                   (-size[1]//2, size[1]//2), axis=1)\n\nresult = rolling_window(a, size)\nprint(result)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": "\n    return np.mean(a)\n    ### END SOLUTION\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of the array\n    return np.mean(a)\n    # return the mean of", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "def slice_array(array, index):\n    return array[:, :, index]\n\nresult = slice_array(Z, -1)\nprint(result)\n", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "import numpy as np\na = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\nresult = a[-1:, :]\nprint(result)\n", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "def is_in_contours(c, CNTS):\n    for i in CNTS:\n        if np.array_equal(i, c):\n            return True\n    return False\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "def is_in_CNTS(c, CNTS):\n    for i in CNTS:\n        if np.array_equal(i, c):\n            return True\n    return False\n\ndef remove_from_CNTS(c, CNTS):\n    CNTS.remove(c)\n    return CNTS\n\n# test\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ np.nan, 749]],\n              [[ 75, 749]]])\nCNTS = [np.array([[[  np.nan, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ np.nan, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ np.nan, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ np.nan, 749]],\n                  [[ 75, 749]]])]\n\nprint(is_in_CNTS(c, CNTS))  # output: True\n\nCNTS = remove_from_CNTS(c, CNTS)\nprint(is_in_CNTS(c, CNTS))  # output: False\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = intp.interp2d(x_new, y_new, a, kind='linear')\nresult = f(np.linspace(0, 2, 10), np.linspace(0, 2, 10))\nprint(result)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.diag(i)\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a[np.triu_indices(a.shape[0], k=1)] = 0\n", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "import numpy as np\nimport pandas as pd\nstart = \"23-FEB-2015 23:09:19.445506\"\nend = \"24-FEB-2015 01:09:22.404973\"\nn = 50\nstart_datetime = pd.to_datetime(start)\nend_datetime = pd.to_datetime(end)\nstep = (end_datetime - start_datetime).seconds / n\ndatetime_array = np.arange(start_datetime, end_datetime, step=step)\ndatetime_array = np.array([datetime_array[i].strftime(\"%d-%b-%Y %H:%M:%S.%f\") for i in range(len(datetime_array))])\nresult = datetime_array\nprint(result)\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "import numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\nindex = np.where((x == a) & (y == b))[0]\nresult = index[0] if index.size else -1\nprint(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "import numpy as np\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\nidx = np.where((x == a) & (y == b))[0]\nresult = idx\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "a, b, c = np.polyfit(x, y, 2)\nresult = np.array([a, b, c])\nprint(result)\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "def f(x, a, b, c):\n    return a + b * x + c * x ** 2\n\ndef minimize_squared_error(x, y, degree):\n    a = np.min(y)\n    b = np.max(y)\n    c = np.mean(y)\n    return np.array([c, b, a])\n\nx = np.array([-1, 2, 5, 100])\ny = np.array([123, 456, 789, 1255])\n\nresult = minimize_squared_error(x, y, degree)\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "temp_arr = np.arange(4)\ndef subtract_arr(row):\n    return np.subtract(row, temp_arr)\ndf = df.apply(subtract_arr, axis=1)\nprint(df)\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)\n", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "scaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\na_normalized = scaler.fit_transform(a)\nprint(a_normalized)\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "import numpy as np\n\narr = np.random.rand(100, 50)-0.5\nmask = arr < -10\narr[mask] = 0\nmask2 = arr >= 15\narr[mask2] = 30\nmask3 = mask ^ mask2\narr[mask3] += 5\n\nprint(arr)\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "arr = np.where(arr < n1, 0, np.where(arr >= n2, 30, arr + 5))\n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "result = np.sum(s1 != s2)\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "result = np.sum(np.abs(s1 - s2) != 0)\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0],a[1]) and np.array_equal(result,a[2])\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "def check_nan(arr):\n    return np.isnan(arr).all()\n\nresult = np.all(np.array([check_nan(arr) for arr in a]))\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "def zero_pad(arr, shape):\n    pad_width = [(0, shape[0] - arr.shape[0])]\n    pad_width += [(0, shape[1] - arr.shape[1])]\n    return np.pad(arr, pad_width)\n\na = np.ones((41, 13))\nresult = zero_pad(a, shape)\nprint(result.shape)\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "def zero_pad(arr, shape):\n    pad_width = [(0, shape[0] - arr.shape[0])]\n    pad_width += [(0, shape[1] - arr.shape[1])]\n    return np.pad(arr, pad_width)\n\nresult = zero_pad(a, shape)\nprint(result.shape)\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "def pad_array(arr, shape, element):\n    diff = np.zeros(shape)\n    diff[:arr.shape[0], :arr.shape[1]] = arr\n    return np.pad(diff, ((0, shape[0] - arr.shape[0]), (0, shape[1] - arr.shape[1])), mode='constant', constant_values=element)\n\na = np.ones((41, 12))\nresult = pad_array(a, shape, element)\nprint(result.shape)\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": "\n    max_rows, max_cols = shape\n    rows, cols = arr.shape\n    if rows > max_rows:\n        pad_rows = rows - max_rows\n        arr = np.pad(arr, ((0, pad_rows), (0, 0)), mode='constant')\n    if cols > max_cols:\n        pad_cols = cols - max_cols\n        arr = np.pad(arr, ((0, 0), (0, pad_cols)), mode='constant')\n    return arr\n    ### END SOLUTION\n\n# Test the function\narr = np.ones((41, 13))\nprint(f(arr))\n\n# Test with a larger array\narr = np.random.rand(100, 13)\nprint(f(arr))\n\n# Test with shape (93,13)\narr = np.ones((41, 13))\nprint(f(arr, shape=(93,13)))\n\n# Test with thousands of rows\narr = np.random.rand(1000, 13)\nprint(f(arr))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(100, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(1000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(10000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(100000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(1000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(10000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(100000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(1000000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(10000000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(100000000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(1000000000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(10000000000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.rand(100000000000000, 13)\nprint(f(arr, shape=(93,13)))\n\n# Test with shape (93,13) and a larger array\narr = np.random.", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "def zero_pad(arr, shape):\n    max_rows, max_cols = shape\n    rows, cols = arr.shape\n    if rows > max_rows:\n        pad_rows = rows - max_rows\n        pad_cols = max_cols\n    elif cols > max_cols:\n        pad_cols = cols - max_cols\n        pad_rows = 0\n    else:\n        pad_rows = 0\n        pad_cols = 0\n    return np.pad(arr, ((0, pad_rows), (0, pad_cols)), mode='constant')\n\na = np.ones((41, 12))\nprint(zero_pad(a, shape))\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = np.array([i for i in range(0, 12)]).reshape(a.shape[0]/3,3)\n", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "result = np.take_along_axis(a, b[:, np.newaxis], axis=2)\n", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "result = np.take_along_axis(a, b[:, :, np.newaxis], axis=2)\n", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "result = np.take(a, b[:, None], axis=2)\n", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "result = np.sum(a[:, :, b[:, 0]], axis=2)\n", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "result = np.sum(a[np.arange(a.shape[0]), b[:, 0], :], axis=2)\n", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "result = df[(df['a'] > 1) & (df['a'] <= 4)]['b'].values\nresult = np.where(result, df['b'], np.nan)\nprint(result)\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "def remove_peripheral_zeros(im):\n    rows, cols = im.shape\n    new_im = np.zeros((rows, cols))\n    for i in range(rows):\n        for j in range(cols):\n            if i == 0 or i == rows - 1 or j == 0 or j == cols - 1:\n                new_im[i, j] = im[i, j]\n            else:\n                if (im[i-1, j] == 0 and im[i+1, j] == 0) or (im[i, j-1] == 0 and im[i, j+1] == 0):\n                    new_im[i, j] = 0\n                else:\n                    new_im[i, j] = im[i, j]\n    return new_im\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "import numpy as np\ndef truncate_array(A):\n    # Find the indices of non-zero values\n    non_zero_indices = np.where(A != 0)\n    \n    # Find the minimum and maximum indices\n    min_row, max_row = np.min(non_zero_indices[0]), np.max(non_zero_indices[0])\n    min_col, max_col = np.min(non_zero_indices[1]), np.max(non_zero_indices[1])\n    \n    # Truncate the array\n    truncated_A = A[min_row:max_row+1, min_col:max_col+1]\n    \n    return truncated_A\n\n# Test the function\nA = np.array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 1, 1, 0, 0, 0],\n           [0, 0, 0, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]])\n\nresult = truncate_array(A)\nprint(result)\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "def remove_peripheral_zeros(arr):\n    rows, cols = arr.shape\n    for i in range(rows):\n        if np.all(arr[i, :] == 0):\n            arr[i, :] = 0\n        elif np.all(arr[i, :] == 1):\n            arr[i, :] = 1\n    for j in range(cols):\n        if np.all(arr[:, j] == 0):\n            arr[:, j] = 0\n        elif np.all(arr[:, j] == 1):\n            arr[:, j] = 1\n    return arr\nresult = remove_peripheral_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "def remove_peripheral_zeros(im):\n    rows, cols = im.shape\n    for i in range(rows):\n        if np.all(im[i, :] == 0):\n            im[i, :] = 0\n    for j in range(cols):\n        if np.all(im[:, j] == 0):\n            for i in range(rows):\n                im[i, j] = 0\n    return im\nresult = remove_peripheral_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "plt.scatter(x, y, label=\"x-y\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.minor_axis.set_ticks(np.arange(0, 1, 0.1))\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.minorticks_on()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.minor_axis.set_ticks(np.arange(0, 1, 0.1))\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "line_styles = ['-', '--', '-.', ':']\nfor i, style in enumerate(line_styles):\n    y = np.random.rand(10)\n    plt.plot(x, y, style)\n    plt.title('Line Style')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "line_styles = ['-', '--', '-.', ':']\nfor i, style in enumerate(line_styles):\n    y = np.random.rand(10)\n    plt.plot(x, y, style)\n    plt.title('Line Style')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.scatter(x, y, marker='d', color='b')\nplt.scatter(x, y, marker='o', color='r')\nplt.scatter(x, y, marker='s', color='g')\nplt.scatter(x, y, marker='p', color='c')\nplt.scatter(x, y, marker='*', color='y')\nplt.scatter(x, y, marker='+', color='k')\nplt.scatter(x, y, marker='x', color='b')\nplt.scatter(x, y, marker='v', color='r')\nplt.scatter(x, y, marker='s', color='g')\nplt.scatter(x, y, marker='p', color='c')\nplt.scatter(x, y, marker='*', color='y')\nplt.scatter(x, y, marker='+', color='k')\nplt.scatter(x, y, marker='x', color='b')\nplt.scatter(x, y, marker='v', color='r')\nplt.scatter(x, y, marker='s', color='g')\nplt.scatter(x, y, marker='p', color='c')\nplt.scatter(x, y, marker='*', color='y')\nplt.scatter(x, y, marker='+', color='k')\nplt.scatter(x, y, marker='x', color='b')\nplt.scatter(x, y, marker='v', color='r')\nplt.scatter(x, y, marker='s', color='g')\nplt.scatter(x, y, marker='p', color='c')\nplt.scatter(x, y, marker='*', color='y')\nplt.scatter(x, y, marker='+', color='k')\nplt.scatter(x, y, marker='x', color='b')\nplt.scatter(x, y, marker='v', color='r')\nplt.scatter(x, y, marker='s', color='g')\nplt.scatter(x, y, marker='p', color='c')\nplt.scatter(x, y, marker='*', color='y')\nplt.scatter(x, y, marker='+', color='k')\nplt.scatter(x, y, marker='x', color='b')\nplt.scatter(x, y, marker='v', color='r')\nplt.scatter(x, y, marker='s', color='g')\nplt.scatter(x, y, marker='p', color='c')\nplt.scatter(x, y, marker='*', color='y')\nplt.scatter(x, y, marker='+', color='k')\nplt.scatter(x, y, marker='x', color='b')\nplt.scatter(x, y, marker='v', color='r')\nplt.scatter(x, y, marker='s', color='g')\nplt.scatter(x, y, marker='p', color='c')\nplt.scatter(x, y, marker='*', color='y')\nplt.scatter(x, y, marker='+', color='k')\nplt.scatter(x, y, marker='x', color='b')\nplt.scatter(x, y, marker='v', color='r')\nplt.scatter(x, y, marker='s', color='g')\nplt.scatter(x, y, marker='p', color='c')\nplt.scatter(x, y, marker='*', color='y')\nplt.scatter(x, y, marker='+', color='k')\nplt.scatter(x, y, marker='x', color", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.scatter(x, y, marker='d', color='b', s=100)\nplt.scatter(x, y, marker='o', color='r', s=50)\nplt.scatter(x, y, marker='s', color='g', s=75)\nplt.scatter(x, y, marker='*', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='+', color='g', s=75)\nplt.scatter(x, y, marker='v', color='b', s=100)\nplt.scatter(x, y, marker='p', color='r', s=50)\nplt.scatter(x, y, marker='h', color='g', s=75)\nplt.scatter(x, y, marker='s', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=75)\nplt.scatter(x, y, marker='x', color='b', s=100)\nplt.scatter(x, y, marker='x', color='r', s=50)\nplt.scatter(x, y, marker='x', color='g', s=7", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "x_range = np.arange(2, 5)\nplt.plot(x_range, np.full(4, 'k'), 'k-')\nplt.plot(x_range, np.full(4, 'k'), 'k:')\nplt.plot(x_range, np.full(4, 'k'), 'k-', linewidth=1)\nplt.plot(x_range, np.full(4, 'k'), 'k:', linewidth=1)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "x = np.linspace(0, 1, 100)\ny = np.linspace(0, 2, 100)\nX, Y = np.meshgrid(x, y)\nplt.plot(X, Y)\nplt.show()\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([2, 0])\nplt.plot(x, y, 'r-')\nplt.show()\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "plt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\")\nplt.xlabel(\"Height (cm)\")\nplt.ylabel(\"Weight (kg)\")\nplt.title(\"Weight and Height by Gender\")\nplt.show()\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set(style='whitegrid')\nplt.scatter(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter plot of Y with X as a variable')\nplt.show()\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "df = pd.DataFrame({'x': x, 'y': y})\nsns.lineplot(data=df, x='x', y='y')\nplt.show()\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, '+', thickness=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\nplt.title(\"Cosine Function\", fontsize=20)\nplt.xlabel(\"X\", fontsize=20)\nplt.ylabel(\"Y\", fontsize=20)\nplt.show()\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.title('Cosine Function', fonsize=20)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend(['Cosine'])\nplt.show()\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.scatter(range(10), x, alpha=0.2, color='b')\nplt.scatter(range(10), y, alpha=0.2, color='r')\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.gca().set_axisbelow(True)\nplt.gca().tick_params(axis='both', which='both', length=0)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().xaxis.set_ticks([])\nplt.gca().yaxis.set_ticks([])\nplt.gca().set_ylim(-10, 10)\nplt.gca().set_xlim(-10, 10)\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.gca().set_title('')\nplt.gca().set_xlabel('')\nplt.gca().set_ylabel('')\nplt.g", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.colorbar(plt.ColorBar(plot_range=(0, 1)), label=\"Value\")\nplt.colorbar(plt.ColorBar(plot_range=(0, 1)), label=\"Value\")\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "x_rot = np.cos(x + np.pi / 4)\ny_rot = np.sin(x + np.pi / 4)\nplt.plot(x_rot, y_rot, label=\"cos\")\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "x_rot = np.cos(x + np.pi / 4)\ny_rot = np.sin(x + np.pi / 4)\nplt.plot(x_rot, y_rot, label=\"cos\")\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "x_ticks = np.arange(0, 2 * np.pi, 0.5)\nplt.xticks(x_ticks)\nplt.xlabel(\"Angle (Radians)\")\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='upper right')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "np.random.seed(0)\nnp.random.shuffle(H)\n\n# plot the 2d array H\nplt.imshow(H, cmap='hot')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.imshow(H, cmap='gray')\nplt.show()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel('X')\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "plt.xticks(rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "x_text = np.array([[0, 100]])\ny_text = np.array([[10]])\nplt.text(x_text, y_text, myTitle, size=10, horizontalalignment='center')\n", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y = -y\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "x_ticks = np.arange(0, 2, 0.1)\nplt.xticks(x_ticks)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter plot of X and Y')\nplt.show()\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "y_ticks = np.arange(-1, 2, 0.5)\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "x_y_z = np.column_stack((x, y, z))\nx_y_z_sorted = x_y_z[np.argsort(x_y_z[:, 0])]\n\nplt.scatter(x_y_z_sorted[:, 0], x_y_z_sorted[:, 1])\nplt.scatter(x_y_z_sorted[:, 1], x_y_z_sorted[:, 2])\nplt.scatter(x_y_z_sorted[:, 2], x_y_z_sorted[:, 0])\n\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "x_points = np.array([x, x])\ny_points = np.array([y, y])\n\nplt.scatter(x, y, color='blue', label='Points')\nplt.scatter(x, y, color='black', label='Points')\nplt.scatter(x, y, color='black', label='Points')\n\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "x_ticks = np.arange(0, 10, 1)\ny_ticks = np.arange(0, 20, 2)\nplt.xticks(x_ticks)\nplt.yticks(y_ticks)\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().get_xaxis().set_major_formatter(lambda x, c: f\"{x:0.2f}\")\nplt.gca().get_yaxis().set_major_formatter(lambda x, c: f\"{x:0.0f}\")\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax.plot(x, y, 'k-', alpha=0.5, linewidth=1.5, dashed=True)\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y1, label='Sin(x)')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Sin(x) and Cos(x)')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y2, label='Cos(x)')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Cos(x) and Sin(x)')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y1, label='Sin(x)')\nplt.title('Sin(x)')\nplt.xlabel('x')\nplt.ylabel('Sin(x)')\nplt.legend()\nplt.grid(True)\nplt.subplot(1, 2, 2)\nplt.plot(x, y2, label='Cos(x)')\nplt.title('Cos(x)')\nplt.xlabel('x')\nplt.ylabel('Cos(x)')\nplt.legend()\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xlabel('')\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "x_ticks = np.arange(0, 10, 1)\nplt.xticks(x_ticks)\nplt.grid(axis='y', linewidth=0.5,ALSE='both')\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "y_ticks = np.arange(0, 10, 1)\nplt.yticks(y_ticks)\nplt.grid(axis='y', linestyle='-', linewidth=0.5, color='gray')\nplt.grid(axis='x', linestyle='-', linewidth=0.5, color='gray')\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.grid(True, axis='x', linestyle='-', color='0.75')\nplt.grid(True, axis='y', linestyle='-', color='0.75')\nplt.grid(True, axis='x', linewidth=1, color='0.75')\nplt.grid(True, axis='y', linewidth=1, color='0.75')\nplt.grid(True, axis='x', linewidth=2, color='0.75')\nplt.grid(True, axis='y', linewidth=2, color='0.75')\nplt.grid(True, axis='x', linewidth=3, color='0.75')\nplt.grid(True, axis='y', linewidth=3, color='0.75')\nplt.grid(True, axis='x', linewidth=4, color='0.75')\nplt.grid(True, axis='y', linewidth=4, color='0.75')\nplt.grid(True, axis='x', linewidth=5, color='0.75')\nplt.grid(True, axis='y', linewidth=5, color='0.75')\nplt.grid(True, axis='x', linewidth=6, color='0.75')\nplt.grid(True, axis='y', linewidth=6, color='0.75')\nplt.grid(True, axis='x', linewidth=7, color='0.75')\nplt.grid(True, axis='y', linewidth=7, color='0.75')\nplt.grid(True, axis='x', linewidth=8, color='0.75')\nplt.grid(True, axis='y', linewidth=8, color='0.75')\nplt.grid(True, axis='x', linewidth=9, color='0.75')\nplt.grid(True, axis='y', linewidth=9, color='0.75')\nplt.grid(True, axis='x', linewidth=10, color='0.75')\nplt.grid(True, axis='y', linewidth=10, color='0.75')\nplt.grid(True, axis='x', linewidth=11, color='0.75')\nplt.grid(True, axis='y', linewidth=11, color='0.75')\nplt.grid(True, axis='x', linewidth=12, color='0.75')\nplt.grid(True, axis='y', linewidth=12, color='0.75')\nplt.grid(True, axis='x', linewidth=13, color='0.75')\nplt.grid(True, axis='y', linewidth=13, color='0.75')\nplt.grid(True, axis='x', linewidth=14, color='0.75')\nplt.grid(True, axis='y', linewidth=14, color='0.75')\nplt.grid(True, axis='x', linewidth=15, color='0.75')\nplt.grid(True, axis='y', linewidth=15, color='0.75')\nplt.grid(True, axis='x', linewidth=16, color='0.75')\nplt.grid(True, axis='y', linewidth=16, color='0.75')\nplt.grid(True, axis='x', linewidth=17, color='0.75')\nplt.grid(True, axis='y', linewidth=17, color='0.75')\nplt.grid(True, axis='x', linewidth=18, color='0.75')\nplt.grid(True, axis='y', linewidth=18, color='0.75')\nplt.grid(True, axis='x', linewidth=1", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.tight_layout()\nplt.show()\nplt.clf()  # To clear the plot to display the new subplot padding", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.set_xlabel('Columns')\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.axis('space-between')\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.title('Plot of Y over X')\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.xlabel('X')\nplt.ylabel('Y')\nplt.axis('left')\nplt.axis('right')\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', color='green')\nsns.scatterplot(data=tips, x='total_bill', y='tip', color='blue')\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(data=tips, x='total_bill', y='tip', color='green')\nsns.histplot(data=tips, x='total_bill', color='blue')\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg')\nplt.show()\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "x = df[\"celltype\"]\ny1 = df[\"s1\"]\ny2 = df[\"s2\"]\n\nplt.bar(x, y1, label=\"s1\")\nplt.bar(x, y2, bottom=y1, label=\"s2\")\n\nplt.xlabel(\"Cell Type\")\nplt.ylabel(\"S1 and S2\")\nplt.title(\"S1 and S2 for Different Cell Types\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "plt.axis('equal')\nplt.xlabel('Cell Type', rotation=45)\nplt.bar(df['celltype'], df['s1'], color='b')\nplt.bar(df['celltype'], df['s2'], color='r')\nplt.show()\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.xlabel(\"X\", color='red')\nplt.ylabel(\"Y\", color='red')\nplt.xlabel('X', color='red')\nplt.ylabel('Y', color='red')\nplt.plot(x, y, 'r')\nplt.show()\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, 'r')\nplt.xlabel(\"X\")\nplt.title(\"Plot of y over X\")\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.tick_params(axis='both', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "x = np.array([0.22058956, 0.33088437, 2.20589566])\ny = np.full(3, 0)\nplt.vlines(x, 0, 1, colors='b')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Vertical Lines at X=[0.22058956, 0.33088437, 2.20589566]')\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "xlabels = list(\"ABCD\")\nylabels = list(\"CDEF\")\nrand_mat = numpy.random.rand(4, 4)\n\n# Plot the heatmap\nplt.imshow(rand_mat, cmap='hot', interpolation='nearest')\n\n# Set x-axis labels\nplt.xlabel('X-Axis Labels')\nplt.ylabel('Y-Axis Labels')\n\n# Invert y-axis labels\nplt.yticks(range(4), ylabels[::-1])\n\n# Display the plot\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "# copy the code of the above plot and edit it to have legend for all three cruves in the two subplots\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y, 'b')\nplt.title('Y')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y, 'r')\nplt.title('Y')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", size=\"flipper_length_mm\", size_scale=10, markersize=30)\nsns.desplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", size=\"flipper_length_mm\", size_scale=10, markersize=30)\nsns.plt.show()\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "x = np.arange(len(a))\nplt.scatter(x, a, c=c)\nplt.xlabel('Index')\nplt.ylabel('A')\nplt.title('A over B')\nfor i in range(len(a)):\n    plt.text(x[i], a[i], c[i], horizontalalignment='center')\nplt.show()\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Line Chart\")\nplt.title(\"Line Chart\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.title(\"y over x\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, bins=5, alpha=0.7, color='g', edgecolor='black', linewidth=1.2)\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(2, 3, figsize=(10, 10))\nfor ax in axs.flat:\n    ax.set_width(0.3)\n    ax.set_height(0.3)\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins=bins, alpha=0.5, label='x')\nplt.hist(y, bins=bins, alpha=0.5, label='y')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.hist([x, y], bins=5, alpha=0.7, label=['X', 'Y'])\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "x = np.linspace(0, 5, 100)\ny = np.linspace(0, 5, 100)\nX, Y = np.meshgrid(x, y)\n\n# calculate the line coordinates\nX_line = np.linspace(X.min(), X.max(), 100)\nY_line = np.interp(X_line, X, Y)\n\n# plot the line\nplt.plot(X_line, Y_line)\n\n# set the x and y limits\nplt.xlim(X.min(), X.max())\nplt.ylim(Y.min(), Y.max())\n\n# plot the points\nplt.scatter(X, Y, c='r')\n\n# plot the line and points\nplt.show()\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "x_colormap = np.random.random((10, 10))\ny_colormap = np.random.random((10, 10))\n\nplt.figure(figsize=(10, 10))\n\n# create two subplots\nax1 = plt.subplot(121)\nax2 = plt.subplot(122)\n\n# plot the two colormaps\nax1.imshow(x_colormap, cmap='viridis')\nax2.imshow(y_colormap, cmap='viridis')\n\n# create a colorbar for each subplot\ncbar1 = plt.colorbar(cmap='viridis')\ncbar2 = plt.colorbar()\n\n# set the labels for the colorbars\ncbar1.set_label('X Colormap')\ncbar2.set_label('Y Colormap')\n\n# set the titles for the subplots\nax1.set_title('X Colormap')\nax2.set_title('Y Colormap')\n\n# set the labels for the x and y axes\nax1.set_xlabel('X Index')\nax1.set_ylabel('Y Index')\nax2.set_xlabel('X Index')\nax2.set_ylabel('Y Index')\n\n# set the title for the main plot\nplt.title('X and Y Colormaps')\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.scatter(x[:, 0], x[:, 1])\nplt.xlabel('Column 1')\nplt.ylabel('Column 2')\nplt.title('Scatter plot of x')\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "plt.figure()\nplt.subplot(1, 2, 1)\nplt.plot(x, y, 'r')\nplt.title(\"Y over X\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(a, z, 'b')\nplt.title(\"Z over A\")\nplt.xlabel(\"A\")\nplt.ylabel(\"Z\")\nplt.grid(True)\n\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "np.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points)\nnp.log10(points", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.title('Plot of y over x', fontsize=20)\nplt.xlabel('X-axis', fontsize=18)\nplt.ylabel('Y-axis', fontsize=16)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_title('Title')\nax.set_xticks(x)\nax.set_yticks(y)\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "for i in range(len(lines)):\n    for line in lines[i]:\n        x1, y1 = line[0]\n        x2, y2 = line[1]\n        plt.plot([x1, x2], [y1, y2], color=c[i])\n\n# Plot the points specified in c\nfor i in range(len(c)):\n    x, y = c[i][:2]\n    plt.scatter(x, y, color=c[i])\n\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "x_log = np.log10(x)\ny_log = np.log10(y)\n\nplt.loglog(x_log, y_log)\nplt.xlabel('log(x)')\nplt.ylabel('log(y)')\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "# Create a line plot\nplt.figure(figsize=(10, 6))\nplt.plot(df.index, df[\"A\"], label=\"A\")\nplt.plot(df.index, df[\"B\"], label=\"B\")\nplt.plot(df.index, df[\"C\"], label=\"C\")\nplt.plot(df.index, df[\"D\"], label=\"D\")\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "x = np.arange(len(data))\ny = data\n\n# Normalize the data\ny = y / np.sum(data)\n\n# Format y tick labels\ny_tick_labels = [f'{p}%' for p in range(10, 100, 10)]\n\n# Plot the histogram\nplt.hist(data, bins=10, alpha=0.7, color='b', edgecolor='k')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.title('Histogram of Data')\nplt.xticks(x, y_tick_labels)\nplt.ylabel('Frequency')\nplt.yscale('log')\nplt.xscale('log')\nplt.show()\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, 'o-', alpha=0.5)\nplt.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(x, y, label=\"y\")\naxs[1].plot(a, z, label=\"z\")\naxs[0].legend()\naxs[1].legend()\nfig.legend()\nplt.show()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "plt.figure(figsize=(10, 4))\n\n# Plot a seaborn regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\nax1 = plt.subplot(121)\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", ax=ax1)\n\n# Plot a seaborn regression plot of \"flipper_length_mm\" over \"bill_length_mm\"\nax2 = plt.subplot(122)\nsns.regplot(data=df, x=\"bill_length_mm\", y=\"flipper_length_mm\", ax=ax2)\n\n# Set titles and labels\nplt.title(\"Regression Plots\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Bill Depth (mm)\")\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xlabel(\"Second x axis label\")\nax.set_ylabel(\"Second y axis label\")\nax.set_title(\"Second plot\")\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label=r'$\\lambda$')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "x_ticks = np.arange(0, 10, 2)\nplt.xticks(x_ticks, ['2', '4', '8'])\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60, alignment='left')\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.xticks(rotation=-60)\nplt.tick_params(axis='y', rotation=0)\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.gca().set_xticks(x, ['2019', '2020'])\nplt.gca().set_xticklabels(['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\nplt.gca().set_xticks(np.arange(2010, 2020), ['2019', '2020'], rotation=90, fontsize=10)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(0.0)\nplt.plot(x, y)\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(0.0)\nplt.show()\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y)\nplt.title(\"Figure\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n\nplt.subplot(1, 2, 2)\nplt.plot(x, y)\nplt.title(\"Figure\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Figure\")\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "plt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Line Chart\")\nplt.plot(df.index, df[\"Type A\"], label=\"Type A\")\nplt.plot(df.index, df[\"Type B\"], label=\"Type B\")\nplt.legend()\nplt.show()  ", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, marker='|', hatch='+', color='k')\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, marker='|', linewidths=0)\nplt.plot([0, 0], [0, 10], 'k-')\nplt.show()\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*')\nplt.show()\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "star_hatch = np.array([[1, 0], [0, 1]])\nvertical_line_hatch = np.array([[0, 1], [1, 0]])\n\nstar_hatch_size = 100\nvertical_line_hatch_size = 10\n\nstar_hatch_points = np.dot(star_hatch, np.array([[10], [10]]))\nvertical_line_hatch_points = np.dot(vertical_line_hatch, np.array([[10], [10]]))\n\nplt.scatter(star_hatch_points[0], star_hatch_points[1], s=star_hatch_size, c='r')\nplt.scatter(vertical_line_hatch_points[0], vertical_line_hatch_points[1], s=vertical_line_hatch_size, c='b')\n\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "x_min, x_max = 1, 5\ny_min, y_max = 1, 4\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.imshow(data, cmap='hot', interpolation='nearest', extent=[x_min, x_max, y_min, y_max])\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(x, y, orientation='horizontal')\nplt.show()\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "x = list(d.keys())\ny = [d[key] for key in x]\ncolors = [c[key] for key in x]\nplt.bar(x, y, color=colors)\nplt.xlabel('Keys')\nplt.ylabel('Values')\nplt.title('Bar plot of `d`')\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "x_line = np.array([3])\ny_line = np.ones(1)\nplt.plot(x_line, y_line, label=\"cutoff\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(6, 4))\nplt.pie(height, labels=labels, startangle=90, counterclock=False, shadow=True)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedge_width=0.4, startangle=90, counterclock=False)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b-')\nplt.grid(True, color='0.75')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.minor_ticks(0)\nplt.grid(axis='y', linestyle='-', linewidth=0.5, color='0.85')\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, startangle=140, shadow=True,wedge_width=0.2)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, startangle=140, shadow=True,wedge_width=0.2)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'o-', color='blue', alpha=0.5)\nplt.show()\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.vlines(55, 0, 100, color=\"green\")\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "# Define the height of the blue and orange bars\nblue_height, orange_height, _ = blue_bar\n\n# Calculate the width of the bars\nbar_width = 0.3\n\n# Calculate the x coordinates for the blue and orange bars\nx = np.arange(3)\n\n# Plot the blue and orange bars\nplt.bar(x - bar_width/2, [blue_height], width=bar_width, color='b')\nplt.bar(x + bar_width/2, [orange_height], width=bar_width, color='orange')\n\n# Add labels and title\nplt.xlabel('Classes')\nplt.ylabel('Values')\nplt.title('Bar plot of blue and orange bars')\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y, label='y')\nplt.plot(a, z, label='z')\nplt.legend()\nplt.title('Line Chart')\nplt.xlabel('x')\nplt.ylabel('y and z')\nplt.grid(True)\n\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\nplt.colorbar()\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "np.set_printoptions(precision=2)\nnp.set_printoptions(threshold=np.inf)\nprint(x)\nprint(y)\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "plt.figure(figsize=(10, 4))\ng = sns.catplot(data=df, x=\"bill_length_mm\", y=\"species\", hue=\"sex\", col=\"species\", height=4, aspect=1.5, kind=\"bar\")\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(\"Sex\")\ng.set_xlabels(\"Bill Length (mm)\")\ng.set_ylabels(\"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Species 2\", \"Species 3\"], labels=[\"Penguin 1\", \"Penguin 2\", \"Penguin 3\"])\ng.set_axis_labels(\"Bill Length (mm)\", \"Species\")\ng.set_titles(titles=[\"Species 1\", \"Spec", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "x = np.linspace(0, 2 * np.pi, 100)\ny = np.sin(x)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.title(r'$\\phi$', fonsize=16)\nplt.plot(x, y, 'b-')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.show()\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt.legend([], [], frameon=False)\nplt", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.figure(figsize=(8, 6))\nplt.plot(x, y, label=\"Line\")\nplt.legend(loc='upper right', bbox_to_anchor=(0.5, 1), ncol=1, loc='lower')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend()\nplt.plot(x, y, marker=\"*\", label=\"Line\")\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "plt.imshow(data, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.title('Figure 1', fonsize=16)\nplt.xlabel('X Label')\nplt.ylabel('Y Label')\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(x, y)\nplt.invert_xaxis()\nplt.show()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.scatter(x, y, s=100, color='red')\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.axis('equal')\nplt.show()\n", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, c='red', edgecolors='black')\nplt.show()\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "plt.figure(figsize=(15, 15))\nfor i in range(4):\n    plt.subplot(2, 2, i + 1)\n    plt.plot(x, y)\n    plt.title('Plot for subplot {}'.format(i + 1))\nplt SOLUTION END\n\nplt.show()\n# END", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black', alpha=0.7)\nplt.xlabel('X Value')\nplt.ylabel('Frequency')\nplt.title('Histogram of X')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.errorbar(x, y, yerr=error, fmt='o', label='Data')\nplt.errorbar(x, y, yerr=error, fmt='', ecolor='red', label='Error')\nplt.legend()\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.axhline(0, color='white', linewidth=1)\nplt.axvline(0, color='white', linewidth=1)\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "for i in range(len(box_position)):\n    ax.errorbar(box_position[i], box_errors[i], yerr=box_errors[i], fmt='o', color=c[i])\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(x, y, label='Y')\nplt.title('Y')\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(x, z, label='Z')\nplt.title('Z')\nplt.legend()\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig = plt.figure(figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        plt.subplot(4, 4, i * 4 + j + 1)\n        plt.plot(x, y)\n        plt.tick_params(axis='both', labelsize=8)\n        plt.title(f'x={i}, y={j}')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "plt.figure(figsize=(8, 8))\nplt.matshow(d, cmap='binary')\nplt.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "plt.table(\n    celltext=df.values,\n    collabels=df.columns,\n    rowlabels=df.index,\n    cellcolours=[\"white\", \"white\", \"white\", \"white\"],\n    rowcolours=[\"white\", \"white\", \"white\", \"white\"],\n    colcolours=[\"white\", \"white\", \"white\", \"white\"],\n    cellfont={\"size\": 12},\n    rowheight=10,\n    colwidth=10,\n    rowspacing=0,\n    colspacing=0,\n    edgecolor=\"white\",\n    linewidth=1,\n    bbox=[0, 0, 1, 1],\n)\nplt.axis(\"off\")\nplt.show()  ", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "plt.plot(x, y)\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Line Chart')\nplt.xticks(x)\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "plt.plot(x, y)\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.title('Line Chart')\nplt.xticks(x)\nplt.yticks(np.arange(0, 10, 1))\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "plt.figure(figsize=(10, 4))\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", legend=False)\nplt.title(\"Group: Fat\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pulse\")\nplt.legend(title=\"Diet\")\nplt.show()\n\nplt.figure(figsize=(10, 4))\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", legend=False)\nplt.title(\"Group: No Fat\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pulse\")\nplt.legend(title=\"Diet\")\nplt.show()\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "x_label = \"Exercise Time\"\ny_label = \"Pulse\"\nhue_label = \"Diet\"\ncol_label = \"Diet\"\n\nplt.figure(figsize=(10, 6))\nplt.scatter(df[\"time\"], df[\"pulse\"], hue=df[\"diet\"], col=df[\"diet\"], alpha=0.7)\nplt.xlabel(x_label)\nplt.ylabel(y_label)\nplt.legend(title=hue_label, loc='upper left')\nplt.colormap('viridis')\nplt.title(\"Pulse versus Exercise Time by Kind and Diet\")\nplt.show()\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.title(\"Pulse over Time by Kind\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pulse\")\nplt.legend(title=\"Diet\")\nplt.subplot(1, 2, 2)\nsns.scatterplot(data=df, x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\")\nplt.title(\"Pulse over Time by Kind\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pulse\")\nplt.legend(title=\"Diet\")\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\nplt.show()\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label=\"y\")\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "plt.figure(figsize=(10, 4))\nplt.subplot(1, 3, 1)\nplt.plot(t, a, label='Sin(t)')\nplt.plot(t, b, label='Cos(t)')\nplt.title('Sin and Cos')\nplt.xlabel('T')\nplt.ylabel('Y')\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.plot(t, c, label='Sin(t) + Cos(t)')\nplt.title('Sin and Cos')\nplt.xlabel('T')\nplt.ylabel('Y')\nplt.legend()\n\nplt.subplot(1, 3, 3)\nplt.plot(t, a, label='Sin(t)')\nplt.title('Sin and Cos')\nplt.xlabel('T')\nplt.ylabel('Y')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "sns.stripplot(data=df, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", legend=False)\nplt.show()\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "# Create a new column \"c_group\" to group \"c\" by 2 intervals\ndf[\"c_group\"] = np.round(df[\"c\"] / 2) * 2\n# Use seaborn FaceGrid to plot pointplots of \"c\" over \"a\"\nsns.FaceGrid(df, col_wrap=3, height=4, width=6).plot(kind=\"scatter\", x=\"a\", y=\"c\", hue=\"c_group\")\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "plt.figure(figsize=(10, 5))\nax = plt.subplot(1, 2, 1, projection='3d')\nax.scatter(x, y, z)\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\nax.view_axis = 'y'\nax.set_axis_range(xmin=-0.1, xmax=1.1, ymin=-0.1, ymax=1.1)\nax.set_axis_label('Y', rotation=90)\n\nax = plt.subplot(1, 2, 2)\nplt.scatter(x, y, c=z)\nplt.colorbar()\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "gs = gridspec.GridSpec(nrow, ncol)\nfor i in range(nrow * ncol):\n    ax = fig.add_subplot(gs[i])\n    ax.imshow(x[i], cmap='gray')\n    ax.axis('off')\n    ax.set_title('')\n    ax.set_axishidden(True)\n    ax.set_ticklabels('')\n    ax.set_yticks([])\n    ax.set_xticks([])\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "x_value = x.numpy()\nx_value = int(x_value)\nprint(x_value)\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "x = tf.Variable(114514)\nprint(x.numpy())\n", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "# Create a 2D tensor with zeros\nnum_classes = 10\nnum_samples = len(labels)\nresult = np.zeros((num_samples, num_classes), dtype=np.int32)\n\n# Iterate over the labels and set the corresponding row to 1\nfor i, label in enumerate(labels):\n    result[i, label] = 1\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\n# Define the number of classes\nnum_classes = 10\n\n# Create a tensor of ones with shape (len(labels), num_classes)\nones_tensor = np.ones((len(labels), num_classes), dtype=np.int32)\n\n# Set the labels to 0\nones_tensor[np.arange(len(labels)), labels] = 0\n\n# Select the rows where the value in the first column is 1\nselected_rows = ones_tensor[ones_tensor[:, 0] == 1]\n\n# Select the first column of the selected rows\nresult = selected_rows[:, 0]\n\nEND SOLUTION\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "# Create a 2D tensor with zeros\nresult = np.zeros((len(labels), 10), dtype=np.int32)\n\n# Iterate over labels\nfor i, label in enumerate(labels):\n    # Set the corresponding row to 1\n    result[i, label] = 1\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": "\n    # Create a 2D tensor with shape (len(labels), 10)\n    result = np.zeros((len(labels), 10), dtype=np.int32)\n    # Iterate over the labels\n    for i, label in enumerate(labels):\n        # Set the corresponding row in the tensor to 1\n        result[i, label] = 1\n    # Return the result\n    return result\n\n# Test the function\nprint(f())\n## END SOLUTION\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "# Define the target labels\ntarget_labels = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n                          [1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n                          [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n                          [1, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n                          [1, 1, 1, 1, 1, 1, 0, 1, 1, 1]])\n\n# Convert the labels to a tensor\nlabels_tensor = np.array(labels)\n\n# Reshape the labels tensor to match the target labels\nlabels_tensor = labels_tensor.reshape(-1, 1)\n\n# Repeat the labels tensor to match the target labels\nlabels_tensor = np.tile(labels_tensor, (1, 10))\n\n# Calculate the accuracy\naccuracy = np.sum(labels_tensor == target_labels) / (10 * len(labels))\n\n# Print the accuracy\nprint(accuracy)\n\n# Convert the accuracy to a tensor\naccuracy_tensor = np.full((1, 10), accuracy)\n\n# Print the accuracy tensor\nprint(accuracy_tensor)\n\n# Print the target labels\nprint(target_labels)\n\n# Print the labels\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list\nprint(labels)\n\n# Print the labels list", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return [[i, i+1, i+2]]\n\n\ndef map_func(i):\n  return [i]\n\n\ndef map_func_list(i):\n  return [i]\n\n\ndef map_func_list_list(i):\n  return [[i]]\n\n\ndef map_func_list_list_list(i):\n  return [[i]] * 3\n\n\ndef map_func_list_list_list_list(i):\n  return [[i]] * 3 * 3\n\n\ndef map_func_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 * 3 * 3 * 3 * 3 * 3 * 3\n\n\ndef map_func_list_list_list_list_list_list_list_list_list_list(i):\n  return [[i]] * 3 *", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": "\n    return [i for i in range(input, input + 3)]\n    ### END SOLUTION\n\nresult = f()\nprint(result)\n## END\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n\n\nEND\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\n# Create a mask of 1s and 0s\nmask = np.zeros((len(lengths), 8), dtype=np.int32)\n\n# Create a mask of 1s for the first element\nmask[0, :lengths[0]] = 1\n\n# Create a mask of 0s for the remaining elements\nfor i in range(1, len(lengths)):\n    mask[i, :lengths[i]] = 1\n\n# Pad the mask to the desired length\nmask = np.pad(mask, ((0, 0), (8 - len(lengths) * 8)), mode='constant')\n\n# Reshape the mask to the desired shape\nmask = mask.reshape((len(lengths), 8))\n\n# Print the mask\nprint(mask)\n\nEND SOLUTION\n<code>\n\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\n# Create a mask of 1s and 0s\nmask = np.zeros((len(lengths), 8), dtype=np.uint8)\n\n# Create a mask of 1s for the first element\nmask[0, :lengths[0]] = 1\n\n# Create a mask of 1s for the remaining elements\nfor i in range(1, len(lengths)):\n    mask[i, lengths[i-1]:] = 1\n\n# Pad the mask to 8 columns\nmask = np.pad(mask, (0, 8 - len(mask[0])), mode='constant')\n\n# Reshape the mask to the desired shape\nmask = mask.reshape((len(lengths), 8))\n\nreturn mask\n", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a mask of 1s and 0s\nmask = np.zeros((len(lengths), 8), dtype=np.int32)\n\n# Create a mask of 1s with zeros at the beginning\nfor i in range(len(lengths)):\n    mask[i, :lengths[i]] = 1\n    mask[i, lengths[i]:] = 0\n\n# Pad zeros to the total length of 8\nmask = np.pad(mask, ((0, 8 - len(lengths)), (0, 0)), mode='constant')\n\n# Convert to tensor\nmask = np.array(mask, dtype=np.int32)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask = np.tensor(mask)\n\n# Convert to tensor\nmask", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": "\n    mask = np.zeros((len(lengths), 8), dtype=np.int32)\n    for i in range(len(lengths)):\n        mask[i, :lengths[i]] = 1\n    return mask\n    ### END SOLUTION\n\nresult = f()\nprint(result)\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "# Create a mask of 1s and 0s\nmask = np.zeros((len(lengths), 8), dtype=np.int32)\n\n# Create a mask of 1s for the first element\nmask[0, :lengths[0]] = 1\n\n# Create a mask of 0s for the remaining elements\nfor i in range(1, len(lengths)):\n    mask[i, :lengths[i]] = 1\n\n# Pad the mask to the total length\nmask = np.pad(mask, ((0, 0), (0, 8 - len(lengths) * 8)), mode='constant')\n\n# Reshape the mask to the desired shape\nmask = mask.reshape((len(lengths), 8))\n\n# Print the mask\nprint(mask)\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "result = tf.stack([a, b], axis=1)\nprint(result)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "\n    return tf.stack([example_a, example_b], axis=1)\n    ### END SOLUTION\n\n# Test the function\nprint(f())\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "# Reshape the tensor\nresult = a.reshape(a.shape[:3])\n# Drop the third dimension\nresult = a[:, :, :, 0]\n", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "a = np.expand_dims(a, axis=-1)\nprint(a.shape)\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "a = np.expand_dims(a, axis=0)\na = np.expand_dims(a, axis=3)\na = np.expand_dims(a, axis=4)\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "result = np.sum(A, axis=1)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "result = np.prod(A, axis=1)\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = -1 / A\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "# calculate the square of the difference between elements\ndiff = tf.square(a - b)\n# calculate the sum of each row\nrow_sums = tf.reduce_sum(diff, axis=1)\n# calculate the sum of all elements\nsum_of_elements = tf.reduce_sum(diff)\n# calculate the distance\ndistance = tf.sqrt(sum_of_elements)\nprint(distance)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "# calculate the square difference\ndiff = tf.square(tf.sub(a, b))\n# calculate the sum of each row\nrow_sums = tf.reduce_sum(diff, axis=1)\n# calculate the square root of each row\ndistance = tf.sqrt(row_sums)\nprint(distance)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": "\n    # Calculate the square of the difference between each element of A and B\n    diff = tf.square(tf.sub(A, B))\n    # Calculate the sum of each row\n    row_sums = tf.reduce_sum(diff, axis=1)\n    # Return the result\n    return row_sums\n\nresult = f()\nprint(result)\n## END SOLUTION\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n## END\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "m = x[y,z]\nprint(m)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "result = x[row, col]\nprint(result)\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "\n    return x[y,z]\n    ### END SOLUTION\n\nresult = f()\nprint(result)\n# Output:\n# [[2 6]]\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "import numpy as np\n\ndef dot_product(A, B):\n    # Get the shape of the input tensors\n    batch_size_A, num_vectors_A, vector_size_A = A.shape\n    batch_size_B, num_vectors_B, vector_size_B = B.shape\n\n    # Initialize an empty array to store the result\n    result = np.zeros((batch_size_A, batch_size_B, num_vectors_A, num_vectors_B))\n\n    # Iterate over each pair of vectors\n    for i in range(batch_size_A):\n        for j in range(batch_size_B):\n            # Calculate the dot product between each element in the batch from A and each element in the batch from B\n            result[i, j, :, :] = np.dot(A[i, :, :], B[j, :, :])\n\n    return result\n\n# Test the function\nA = np.random.randint(low=0, high=5, size=(10, 20, 30))\nB = np.random.randint(low=0, high=5, size=(10, 20, 30))\nresult = dot_product(A, B)\nprint(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "import numpy as np\n\ndef dot_product(A, B):\n    # Get the shape of the input tensors\n    batch_size, num_vectors_A, vector_size = A.shape\n    batch_size, num_vectors_B, _ = B.shape\n\n    # Initialize an empty array to store the result\n    result = np.zeros((batch_size, num_vectors_A, num_vectors_B))\n\n    # Iterate over each pair of vectors\n    for i in range(batch_size):\n        for j in range(num_vectors_A):\n            for k in range(num_vectors_B):\n                # Calculate the dot product\n                result[i, j, k] = np.dot(A[i, j, :], B[i, k, :])\n\n    return result\n\n# Test the function\nA = np.random.randint(low=0, high=5, size=(10, 20, 30))\nB = np.random.randint(low=0, high=5, size=(10, 20, 30))\nresult = dot_product(A, B)\nprint(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\nx = np.array(x)\n\n# Convert the byte array to string\nresult = np.char.encode(x)\n\n# Convert the string array to string\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string array\nresult = np.char.decode(result)\n\n# Convert the string array to string", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": "\n    x = np.array(x)\n    x = x.astype(np.uint8)\n    x = x.view(np.uint8)\n    x = x.tobytes()\n    x = np.frombuffer(x, dtype=np.uint8)\n    x = x.view(np.uint8)\n    x = x.astype(np.str_)\n    x = x.view(np.str_)\n    x = x.astype(str)\n    return x\n    ### END SOLUTION\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "# Get the shape of the input tensor\nshape = x.shape\n\n# Find the last dimension of the shape\nlast_dim = shape[-1]\n\n# Find the indices of non-zero entries\nnon_zero_indices = np.where(x != 0)\n\n# Count the number of non-zero entries in each dimension\ncounts = np.bincount(np.ravel(non_zero_indices), minlength=last_dim)\n\n# Calculate the average of non-zero entries in each dimension\naverage = counts / last_dim\n\n# Replace the non-zero entries with their averages\nx[non_zero_indices] = average[:, None]\n\n# Return the result\nreturn x\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "# Get the shape of X\nshape = x.shape\n# Get the indices of non-zero entries\nnon_zero_indices = np.where(x != 0)\n# Calculate the mean of non-zero entries\nmean = np.mean(x[non_zero_indices])\n# Calculate the variance of non-zero entries\nvariance = np.var(x[non_zero_indices])\n# Create a mask to select non-zero entries\nmask = np.ones(shape, dtype=bool)\nmask[non_zero_indices] = False\n# Select non-zero entries and calculate their mean and variance\nnon_zero_mean = np.mean(x[mask])\nnon_zero_variance = np.var(x[mask])\n# Calculate the variance of the mean\nmean_variance = np.var(mean)\n# Calculate the variance of the variance\nvariance_variance = np.var(variance)\n# Calculate the variance of the mean and variance\nvariance_mean_variance = np.var(mean) + np.var(variance)\n# Return the variance of the mean and variance\nreturn mean_variance, variance_variance\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    # Calculate the sum of non-zero entries in each dimension\n    sum_non_zero = tf.reduce_sum(x, axis=-1, keepdims=True)\n    \n    # Calculate the number of non-zero entries in each dimension\n    count_non_zero = tf.reduce_sum(x != 0, axis=-1, keepdims=True)\n    \n    # Calculate the average of non-zero entries\n    average = tf.where(count_non_zero != 0, sum_non_zero / count_non_zero, 0)\n    \n    return average\n\nresult = f()\nprint(result)\n## END SOLUTION\n# Output:\n# [[[1.5 2.5 3.5]\n#   [2.  2.  4. ]\n#   [1.  2.  3. ]\n#   [1.  2.  3. ]]\n#  [[0.5 1.5 1.5]\n#   [1.5 2.5 3.5]\n#   [1.  2.  3. ]\n#   [1.  2.  3. ]]]\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "import numpy as np\n\nA = np.random.rand(100, 100)\nB = np.random.rand(100, 100)\n\nresult = np.dot(A, B)\n\nprint(result)\nEND SOLUTION\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "import numpy as np\n\n# Get the indices of the maximum values in each row\nmax_indices = np.argmax(a, axis=1)\n\n# Convert the indices to a tensor\nmax_indices_tensor = np.tensor(max_indices)\n\n# Print the result\nprint(max_indices_tensor)\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "import numpy as np\n\n# Get the indices of the maximum values in each column\nmax_indices = np.argmax(a, axis=0)\n\n# Convert the indices to a tensor\nmax_indices_tensor = np.tensor(max_indices)\n\n# Print the result\nprint(max_indices_tensor)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": "\n    # Get the maximum values and their indices\n    max_values = np.max(a, axis=1)\n    max_indices = np.argmax(a, axis=1)\n    \n    # Return the indices\n    return max_indices\n\n# Test the function\nprint(f())\n## END SOLUTION\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "import numpy as np\n\ndef find_min_index(a):\n    return np.argmin(a, axis=0)\n\nresult = find_min_index(a)\nprint(result)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\n# Save the model in \"export/1\"\nmodel.save('export/1')\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "import numpy as np\n\n# set seed\nnp.random.seed(seed_x)\n\n# generate random integers\nresult = np.random.randint(1, 5, size=10)\n\nprint(result)\nEND SOLUTION\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "import numpy as np\n\n# Define the uniform distribution\nuniform_dist = tf.distributions.Uniform(low=2, high=5)\n\n# Generate 114 random integers\nresult = np.random.default_rng(seed=seed_x).integers(2, 5, size=114, dtype=np.int32)\n\nprint(result)\nEND SOLUTION\n</code", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    random_integer = tf.random.uniform(shape=(10,), minval=1, maxval=4, dtype=tf.int32)\n    return random_integer\n    ### END SOLUTION\n\nresult = f(seed_x=10)\nprint(result)\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "import tensorflow as tf\n\nprint(tf.__version__)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "import numpy as np\nimport scipy\n\n# Define the polynomial order\norders = np.array([1, 2, 3])\n\n# Initialize arrays to store results\nresults = np.zeros((len(orders), 2))\n\n# Iterate over orders\nfor i, order in enumerate(orders):\n    # Fit polynomial\n    p = np.polyfit(x, y, order)\n    results[i] = p\n\n# Print results\nprint(results)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "import numpy as np\nimport scipy.polyfit\n\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\n\n# Fit polynomial of order 2\np1, c1 = np.polyfit(x, y, 2)\n\n# Fit polynomial of order 3\np2, c2 = np.polyfit(x, y, 3)\n\n# Fit polynomial of order 4\np3, c3 = np.polyfit(x, y, 4)\n\n# Fit polynomial of order 5\np4, c4 = np.polyfit(x, y, 5)\n\n# Combine results\nresult = np.array([p1, p2, p3, p4])\n\nprint(result)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\np_optimal, _ = scipy.optimize.curve_fit(func, x, y, p0=p0)\nprint(p_optimal)\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "test_stat, p_value = stats.ks_test(x, y)\nprint(f\"p-value: {p_value}\")\n", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "test_stat, p_value = stats.ks_test(x, y)\nresult = p_value < alpha\nprint(result)\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(c):\n    a, b, c = c\n    return sqrt((sin(pi/2) + sin(0) + sin(c) - 2)**2 + (cos(pi/2) + cos(0) + cos(c) - 1)**2)\n\nresult = optimize.minimize(f, initial_guess, method='Newton-CG')\nprint(result.x)\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = np.array([scipy.stats.norm.cdf(-z) for z in z_scores])\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "def z_to_p(z_scores, mu, sigma):\n    return 1 - scipy.stats.norm.cdf(z_scores, loc=mu, scale=sigma)\n\np_values = z_to_p(z_scores, mu, sigma)\nprint(p_values)\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = np.abs(scipy.stats.norm.inv_cdf(1 - p_values))\n", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu], loc=stddev)\nresult = dist.cdf(x)\nprint(result)\n", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm(stddev, loc=mu)\nexpected_value = dist.mean()\nmedian = dist.median()\nprint(f\"Expected value: {expected_value}, Median: {median}\")\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "result = sa * sb\n", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": "\n    result = sA.dot(sB)\n    return result\n    ### END SOLUTION\n    return result\nprint(f())\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "# Create a 3D grid of points\nx = points[:, 0]\ny = points[:, 1]\nz = points[:, 2]\n\n# Create a 3D grid of values\nV_grid = np.zeros((len(x), len(y), len(z)))\n\n# Fill the 3D grid with values\nfor i in range(len(x)):\n    for j in range(len(y)):\n        for k in range(len(z)):\n            V_grid[i, j, k] = np.interp(np.array([[x[i], y[j], z[k]]]), points[:, :3], V)\n\n# Interpolate the value at the point (25, 20, -30)\nresult = np.interp(np.array([[25, 20, -30]]), points[:, :3], V_grid[np.where(points[:, 0] == points[0, 0])[0][0], np.where(points[:, 1] == points[0, 1])[0][0], np.where(points[:, 2] == points[0, 2])[0][0]])\n\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "# Create a 3D grid of points\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nz = np.linspace(-10, 10, 100)\nX, Y, Z = np.meshgrid(x, y, z)\n\n# Create a 3D grid of values\nV_grid = np.zeros((len(X), len(Y), len(Z)))\n\n# Iterate over each point in the grid\nfor i in range(len(X)):\n    for j in range(len(Y)):\n        for k in range(len(Z)):\n            # Calculate the distance from the point to the center of the grid\n            distance = np.sqrt((X[i, j, k] - 0.5) ** 2 + (Y[i, j, k] - 0.5) ** 2 + (Z[i, j, k] - 0.5) ** 2)\n            # Assign the corresponding value from the original array\n            V_grid[i, j, k] = V[np.argmin(distance)]\n\n# Interpolate the values at the points in the grid\ninterpolated_values = np.zeros(len(request))\nfor i in range(len(request)):\n    x, y, z = request[i]\n    # Find the indices of the grid points closest to the point\n    idx = np.argmin(np.sqrt((X - x) ** 2 + (Y - y) ** 2 + (Z - z) ** 2))\n    # Interpolate the value at the closest point\n    interpolated_values[i] = V_grid[idx]\n\nprint(interpolated_values)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "from scipy import misc\nfrom scipy.ndimage import rotate\nimport numpy as np\ndata_orig = misc.face()\nx0,y0 = 580,300 # left eye; (xrot,yrot) should point there\nangle = np.random.randint(1, 360)\ndata_rot = rotate(data_orig, angle, reshape=False)\nxrot, yrot = np.array([x0, y0])\nxrot, yrot = np.round(xrot), np.round(yrot)\nprint(xrot, yrot)\n# output:\n# 580.0 300.0\n# END SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "result = M.diagonal()\n", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "def test_uniformity(times):\n    # Calculate the K-S test for uniformity\n    d = np.abs(times - np.mean(times))\n    return stats.ks_test(d, 'uniform')\nresult = test_uniformity(times)\nprint(result)\n", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": "\n    # Define the function to test the uniformity of the time array\n    def test_uniformity(times):\n        # Calculate the mean and standard deviation of the time array\n        mean = np.mean(times)\n        std = np.std(times)\n        # Calculate the K-S test for uniformity\n        d = np.abs(times - mean) / std\n        # Return the K-S test result\n        return stats.ks_test(d, 'uniform')\n    # End of solution\n    return test_uniformity(times)\n# Test the function\nresult = f()\nprint(result)  # Output the result of the K-S test\n## END SOLUTION\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\nEND.\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "result = np.allclose(times, np.linspace(0, T, 100), rtol=0, atol=1)\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "Feature = np.concatenate((c1, c2), axis=1)\n", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "Feature = c1.concatenate('horizontal', c2)\n", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "Feature = np.vstack([c1, c2])\n", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "def distance(point1, point2):\n    return np.sqrt(np.sum((point1-point2)**2))\n\ndef minimize_distance(points1, points2):\n    distances = np.zeros((points1.shape[0], points2.shape[0]))\n    for i in range(points1.shape[0]):\n        for j in range(points2.shape[0]):\n            distances[i, j] = distance(points1[i], points2[j])\n    return np.min(distances)\n\nresult = minimize_distance(points1, points2)\nprint(result)\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "def distance(point1, point2):\n    return np.sum(np.abs(point1-point2))\n\ndef minimize_distance(points1, points2):\n    distances = np.zeros((points1.shape[0], points2.shape[0]))\n    for i in range(points1.shape[0]):\n        for j in range(points2.shape[0]):\n            distances[i, j] = distance(points1[i], points2[j])\n    return np.min(distances)\n\nresult = minimize_distance(points1, points2)\nprint(result)\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\nc = b.toarray()\nc = sparse.csr_matrix(c)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "# Count the number of regions of cells which value exceeds a given threshold\ndef count_regions(img, threshold):\n    # Find the indices of cells which value exceeds the threshold\n    mask = img > threshold\n    # Use ndimage.label function to find regions\n    labels = ndimage.label(mask)\n    return labels[0].max() + 1\n\n# Count the number of regions\nnum_regions = count_regions(img, threshold)\nprint(num_regions)\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "region_count = np.zeros((512, 512), dtype=np.uint8)\nfor i in range(512):\n    for j in range(512):\n        if img[i, j] > threshold:\n            region_count[i, j] = 1\n            for k in range(i-1, i+2):\n                for l in range(j-1, j+2):\n                    if k >= 0 and k <512 and l >= 0 and l <512:\n                        if region_count[k, l] == 0 and img[k, l] > threshold:\n                            region_count[k, l] = 2\n                        elif region_count[k, l] == 0 and img[k, l] <= threshold:\n                            region_count[k, l] = 1\nregion_count = region_count.astype(np.uint8)\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :]\nregion_count = region_count[:, ::-1]\nregion_count = region_count[::-1, ::-1]\nregion_count = region_count[::-1, :", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": "\n    # Calculate the number of regions\n    regions = ndimage.label(img > threshold)\n    # Return the number of regions\n    return regions[1].max()\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "import numpy as np\n\ndef find_regions(img, threshold):\n    regions = []\n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            if img[i, j] > threshold:\n                region = []\n                stack = [(i, j)]\n                while stack:\n                    x, y = stack.pop()\n                    if (x, y) not in region:\n                        region.append((x, y))\n                        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]:\n                            nx, ny = x + dx, y + dy\n                            if 0 <= nx < img.shape[0] and 0 <= ny < img.shape[1] and img[nx, ny] > threshold:\n                                stack.append((nx, ny))\n                regions.append(region)\n    return regions\n\ndef calculate_distance(region):\n    center_x, center_y = np.mean([x for x, y in region], axis=0), np.mean([y for x, y in region], axis=0)\n    return np.sqrt((center_x - 0)**2 + (center_y - 0)**2)\n\nregions = find_regions(img, threshold)\ndistances = [calculate_distance(region) for region in regions]\n\nprint(distances)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "def make_symmetric(M):\n    # Create a copy of M\n    M_copy = M.copy()\n    # Replace the diagonal elements with 1\n    np.fill_diagonal(M_copy, 1)\n    # Subtract the diagonal elements from 1\n    M_copy -= M_copy\n    return M_copy\n\nM = make_symmetric(M)\nprint(M)\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": "\n    for i in range(sA.shape[0]):\n        for j in range(i+1, sA.shape[1]):\n            if sA[i, j] != sA[j, i]:\n                sA[i, j] = sA[j, i]\n    return sA\n    ### END SOLUTION\nf(example_sA)\nprint(example_sA)\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np.allclose(example_sA, example_sA.T))\nprint(np", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "import numpy as np\nimport scipy.ndimage\nsquare = np.zeros((32, 32))\nsquare[10:-10, 10:-10] = 1\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = 1\n# erode and dilate\neroded = scipy.ndimage.binary_erosion(square, size=3)\ndilated = scipy.ndimage.binary_dilation(eroded, size=3)\n# remove single cells\nsingle_cells = np.ones((32, 32))\nsingle_cells[10:-10, 10:-10] = 0\nresult = np.logical_and(single_cells, dilated)\nprint(result)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "import numpy as np\nimport scipy.ndimage\n\ndef erode_dilate(image, erode_size, dilate_size):\n    # Erode the image\n    eroded_image = scipy.ndimage.binary_erosion(image, size=erode_size)\n    \n    # Dilate the image\n    dilated_image = scipy.ndimage.binary_dilation(eroded_image, size=dilate_size)\n    \n    return dilated_image\n\n# Define parameters\nerode_size = 3\ndilate_size = 5\n\n# Apply erode and dilate operations\nresult = erode_dilate(square, erode_size, dilate_size)\n\nprint(result)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col)\nstd_dev = np.std(col)\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_val = np.max(col)\nmin_val = np.min(col)\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "import numpy as np\n\nmedian_value = np.median(col)\nmode_value = np.bincount(col.flatten()).argmax()\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, a0, a1, ..., aN):\n    return a0 + a1 * np.cos(x * np.pi / tau) + ... + aN * np.cos(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_cosine(x, a0, a1, ..., aN):\n    return a0 + a1 * np.cos(x * np.pi / tau) + ... + aN * np.cos(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_sin(x, a0, a1, ..., aN):\n    return a0 + a1 * np.sin(x * np.pi / tau) + ... + aN * np.sin(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_cosine_sin(x, a0, a1, ..., aN):\n    return a0 + a1 * np.cos(x * np.pi / tau) + ... + aN * np.sin(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_sin_cos(x, a0, a1, ..., aN):\n    return a0 + a1 * np.sin(x * np.pi / tau) + ... + aN * np.cos(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_sin_sin(x, a0, a1, ..., aN):\n    return a0 + a1 * np.sin(x * np.pi / tau) + ... + aN * np.sin(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_cosine_cos(x, a0, a1, ..., aN):\n    return a0 + a1 * np.cos(x * np.pi / tau) + ... + aN * np.cos(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_sin_cos(x, a0, a1, ..., aN):\n    return a0 + a1 * np.sin(x * np.pi / tau) + ... + aN * np.cos(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_cosine_sin(x, a0, a1, ..., aN):\n    return a0 + a1 * np.cos(x * np.pi / tau) + ... + aN * np.sin(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_sin_sin(x, a0, a1, ..., aN):\n    return a0 + a1 * np.sin(x * np.pi / tau) + ... + aN * np.sin(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_cosine_cos(x, a0, a1, ..., aN):\n    return a0 + a1 * np.cos(x * np.pi / tau) + ... + aN * np.cos(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_cosine_sin(x, a0, a1, ..., aN):\n    return a0 + a1 * np.cos(x * np.pi / tau) + ... + aN * np.sin(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_sin_cos(x, a0, a1, ..., aN):\n    return a0 + a1 * np.sin(x * np.pi / tau) + ... + aN * np.cos(x * np.pi / tau * np.arange(1, N + 1))\n\ndef fourier_cosine_cos(x, a0, a1, ..., aN):\n    return a0 + a1 * np.cos(x * np.pi / tau) + ... + aN * np", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "def calculate_min_distance(array):\n    n = array.shape[0]\n    distances = np.full((n, n), np.inf)\n    for i in range(n):\n        for j in range(i + 1, n):\n            distance = np.linalg.norm(array[i] - array[j])\n            distances[i, j] = distance\n            distances[j, i] = distance\n    return distances\n\ndef find_min_distance(array):\n    n = array.shape[0]\n    distances = calculate_min_distance(array)\n    min_distances = np.full(n, np.inf)\n    for i in range(n):\n        min_distances[i] = np.min(distances[i])\n    return min_distances\n\nresult = find_min_distance(example_array)\nprint(result)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "def calculate_min_distance(array):\n    n = array.shape[0]\n    distances = np.full((n, n), np.inf)\n    for i in range(n):\n        for j in range(i + 1, n):\n            distance = np.sum(np.abs(array[i] - array[j]))\n            distances[i, j] = distance\n            distances[j, i] = distance\n    return distances\n\ndef find_min_distance(array):\n    distances = calculate_min_distance(array)\n    min_distance = np.min(distances)\n    return min_distance\n\n# Test the function\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])\n\nprint(find_min_distance(example_array))\n```\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": "\n    # Calculate the distance matrix\n    distance_matrix = np.zeros((example_array.shape[0], example_array.shape[0]))\n    for i in range(example_array.shape[0]):\n        for j in range(i + 1, example_array.shape[1]):\n            distance_matrix[i, j] = np.linalg.norm(example_array[i] - example_array[j])\n            distance_matrix[j, i] = distance_matrix[i, j]\n    # Find the minimum distance between each pair of regions\n    min_distances = np.min(distance_matrix, axis=1)\n    return min_distances\n    ### END SOLUTION\n\n# Test the function\nprint(f())\n## END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "def extrapolate(x, y):\n    x_val = np.linspace(-1, 1, 100)\n    y_val = np.interp(x_val, x, y)\n    return y_val\n\nresult = extrapolate(x, y)\nprint(result)\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1=np.array([38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0])\nx2=np.array([39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8])\nx3=np.array([34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0])\nx4=np.array([34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8])\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\nprint(statistic)\nprint(critical_values)\nprint(significance_level)\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1=np.array(x1)\nx2=np.array(x2)\nresult = ss.anderson_ksamp(x1)>0.05\nprint(result)\n", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "def tau1(x):\n    y = np.array(df['A'])\n    tau, p_value = stats.kendalltau(x, y)\n    return tau\n\ndf['AB'] = np.roll(df['B'], 1)  # roll the column 'B' to the left\ndf['AC'] = np.roll(df['C'], 1)  # roll the column 'C' to the left\ndf['BC'] = np.roll(df['B'], -1)  # roll the column 'B' to the right\n\n# calculate AB and AC\ndf['AB'] = df['B'].rolling(3).apply(lambda x: tau1(x))\ndf['AC'] = df['C'].rolling(3).apply(lambda x: tau1(x))\n\n# calculate BC\ndf['BC'] = df['B'].rolling(3).apply(lambda x: tau1(x))\n\n# remove NaN values\ndf = df.dropna()\n\nprint(df)\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return len(my_csr_matrix.nonzero()[0]) == 0\n\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nprint(is_csr_matrix_only_zeroes(sa))\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_lil_matrix_only_zeroes(my_lil_matrix):\n    return (len(my_lil_matrix.nonzero()[0]) == 0)\n\nsa = sparse.random(10, 10, density = 0.01, format = 'lil')\nprint(is_lil_matrix_only_zeroes(sa))\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "result = block_diag(a[0], a[1], a[2])\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "result = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\nprint(p_value)\n", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": "\n    result = stats.ranksums(pre_course_scores,during_course_scores)\n    return result.pvalue\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "import numpy as np\n\ndef kurtosis(x):\n    n = len(x)\n    mean = np.mean(x)\n    variance = np.var(x)\n    std_dev = np.sqrt(variance)\n    kurt = np.mean((x - mean)**4) / std_dev**4\n    return kurt\n\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\nkurtosis_result = kurtosis(a)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "import numpy as np\nimport scipy.stats\n\na = np.array([   1. ,    2. ,    2.5,  400. ,    6. ,    0. ])\n\n# calculate mean\nmean_a = np.mean(a)\n\n# calculate variance\nvariance_a = np.var(a)\n\n# calculate standard deviation\nstd_a = np.sqrt(variance_a)\n\n# calculate kurtosis\nkurtosis_result = np.mean((a - mean_a)**4) / (std_a**4)\n\nprint(kurtosis_result)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interp = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interp(x, y)\n", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": "\n    # Create a 2D grid of values\n    z_grid = np.meshgrid(x, y)\n    # Calculate the interpolated value\n    z_interp = np.interp2d(x, y, z, method='cubic')\n    # Get the interpolated values for the given values of s and t\n    result = z_interp(s, t)\n    return result\nresult = f()\nprint(result)\n## END SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "import numpy as np\n\ndef count_points_in_cells(vor, extraPoints):\n    # Get the regions of the voronoi tessellation\n    regions = vor.regions\n    # Initialize an array to store the count of points in each cell\n    counts = np.zeros(len(vor.regions))\n    # Iterate over each region\n    for i, region in enumerate(vor.regions):\n        # If the region is not empty\n        if region != -1:\n            # Count the number of points in this region\n            counts[i] = len(vor.vertices[region])\n    # Count the number of points in extra points\n    for point in extraPoints:\n        # Find the region that the point belongs to\n        point_region = -1\n        for i, region in enumerate(vor.regions):\n            if np.all(vor.vertices[region] == point):\n                point_region = i\n                break\n        # If the point belongs to a region, increment the count\n        if point_region != -1:\n            counts[point_region] += 1\n    return counts\n\n# Test the function\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\nresult = count_points_in_cells(vor, extraPoints)\nprint(result)\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "import numpy as np\n\ndef count_points_in_cells(vor, extraPoints):\n    # Get the regions of the voronoi tessellation\n    regions = vor.regions\n    # Initialize an array to store the count of points in each cell\n    counts = np.zeros(len(vor.regions))\n    # Iterate over each region\n    for i, region in enumerate(vor.regions):\n        # If the region is not empty\n        if region != -1:\n            # Count the number of points in this region\n            counts[i] = len(vor.vertices[region])\n    # Count the number of points in the extra points\n    for point in extraPoints:\n        # Find the nearest point to the point\n        distances = np.linalg.norm(vor.points - point, axis=1)\n        # Find the index of the nearest point\n        index = np.argmin(distances)\n        # Increment the count of the nearest point in the cell\n        counts[index] += 1\n    return counts\n\n# Test the function\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\nvor = scipy.spatial.Voronoi([[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]])\nresult = count_points_in_cells(vor, extraPoints)\nprint(result)\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "max_size = max(len(v) for v in vectors)\npadded_vectors = [v + np.zeros(max_size - len(v)) for v in vectors]\nsparse_matrix = sparse.csc_matrix(padded_vectors)\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "import numpy as np\nimport scipy.ndimage\n\na = np.zeros((5, 5))\na[1:4, 1:4] = np.arange(3*3).reshape((3, 3))\n\n# Define the kernel\nkernel = np.array([[0, 1, 0],\n                   [1, 0, 1],\n                   [0, 1, 0]])\n\n# Apply the kernel\nb = scipy.ndimage.median_filter(a, kernel=kernel, mode='constant')\n\nprint(b)\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "result = M[row, column]\n", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "result = np.array([M[row[i], column[i]] for i in range(len(row))])\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "new_array = np.zeros((100, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        f = scipy.interpolate.interp1d(x, array[i, j, :])\n        new_array[:, i, j] = f(x_new)\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u,o2,x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.integrate(NDfx,-dev,dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\nx = 2.5\nu = 1\no2 = 3\nprob = NormalDistro(u,o2,x)\nprint(prob)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": "\n    P_inner = scipy.integrate.integrate(NDfx, -dev, dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION\n\n# Test the function\nprint(f())\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n# END\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "def dctn(N):\n    dct_matrix = np.zeros((N, N))\n    for k in range(N):\n        dct_matrix[k, :] = np.cos(np.arange(N) * k * 2 * np.pi / N)\n    return dct_matrix\n\nresult = dctn(N)\nprint(result)\n", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "diags(matrix, [-1,0,1], (5, 5)).toarray()\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "def choose(n, k):\n    return scipy.stats.combination(n, k, exact=True)\n\nM = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(i+1):\n        M[i, j] = choose(i, j) * p**j * (1-p)**(i-j)\n\nprint(M)\nEND SOLUTION\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "import numpy as np\n\n# calculate row-zscore\nz_scores = np.zeros((df.shape[0], df.shape[1]))\nfor i in range(df.shape[0]):\n    for j in range(df.shape[1]):\n        if df.columns[j] != 'probegenes':\n            z_scores[i, j] = (df.loc[probegenes, j] - df.loc[probegenes, 'probegenes'].mean()) / df.loc[probegenes, 'probegens'].std()\n\n# calculate row-mean\nmean = df.loc['probegens'].mean()\n\n# calculate row-std\nstd = df.loc['probegens'].std()\n\n# calculate row-zscore\nresult = np.zeros((df.shape[0], df.shape[1]))\nfor i in range(df.shape[0]):\n    for j in range(df.shape[1]):\n        if df.columns[j] != 'probegens':\n            result[i, j] = (df.loc[probegens, j] - mean) / std\n\n# add row-mean and row-std to result\nresult = np.hstack((result, np.zeros((df.shape[0], 1))))\nresult[:, -1] = np.hstack((mean, std))\n\nreturn result\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "import numpy as np\n\n# calculate z-score for each column\nz_scores = np.apply_along_axis(lambda x: stats.zscore, 0, df.values)\n\n# add z-scores to the data frame\ndf_z_scores = pd.DataFrame(z_scores, index=df.index, columns=df.columns)\n\n# add z-scores to the data frame\ndf_z_scores = df.add(df_z_scores, fill_value=0)\n\n# print the result\nprint(df_z_scores)\n", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "import numpy as np\n\n# calculate zscore for each row\nzscore_df = df.apply(lambda x: stats.zscore(x), axis=1)\n\n# create a dataframe to store data and zscore\nresult = pd.DataFrame(data=np.column_stack((df.values, zscore_df.values)),\n                      columns=df.columns + ['zscore'])\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "# calculate zscore\ndf_zscore = df.copy()\nfor col in df.columns:\n    df_zscore[col] = np.round(stats.zscore(df[col]),3)\n\n# create dataframe for output\ndf_output = pd.DataFrame(index=df.index)\nfor col in df.columns:\n    df_output[col] = df[col]\n    df_output['zscore'] = df_zscore[col]\n\n# print output\nprint(df_output)\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "def test_grad(x):\n    return [2*x[0],2*x[1]]\n\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\n\nstarting_point = [1.8, 1.7]\ndirection = [-1, -1]\n\nresult = sp.optimize.line_search(test_func,test_grad,starting_point,direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "def get_distance_2(y, x):\n    mid = np.zeros((len(y), len(x)))\n    for i in range(len(y)):\n        for j in range(len(x)):\n            mid[i, j] = np.linalg.norm(np.array([y[i], x[j]]))\n    return mid\n# test the function\ny = np.arange(0, 6)\nx = np.arange(0, 6)\nresult = get_distance_2(y, x)\nprint(result)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "def get_distance_2(y, x):\n    mid = np.zeros((shape[0], shape[1], 2))\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n            mid[i, j, 0] = i - shape[0] // 2\n            mid[i, j, 1] = j - shape[1] // 2\n    return distance.cdist(mid, mid, 'euclidean')\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": "\n    x = np.arange(shape[1])\n    y = np.arange(shape[0])\n    x, y = np.meshgrid(x, y)\n    center = np.array([[x.shape[1] // 2, y.shape[0] // 2]]).T\n    return distance.cdist(center, np.vstack((x.flatten(), y.flatten())).T, metric='euclidean')\n    ### END SOLUTION\n\n# Test the function\nprint(f())\n## END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "import numpy as np\nimport scipy.ndimage\n\ndef resample_array(x, shape):\n    x_shape = x.shape\n    x = np.reshape(x, x_shape)\n    resampled_shape = (shape[0] // x_shape[0], shape[1] // x_shape[1])\n    resampled_x = np.zeros(shape)\n    for i in range(x_shape[0]):\n        for j in range(x_shape[1]):\n            x_sub_resampled = scipy.ndimage.zoom(x[:, :, j], resampled_shape, order=1)\n            resampled_x[:, :, j] = x_sub_resampled\n    return resampled_x\n\nx = np.arange(9).reshape(3, 3)\nshape = (6, 8)\nresult = resample_array(x, shape)\nprint(result.shape)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x, a):\n    return np.dot(a, x ** 2)\n\ndef residual(pars, a, y):\n    x = pars['x']\n    return (y - func(x, a)) ** 2\n\nx0 = np.array([2, 3, 1, 4, 20])\na = np.random.rand(3, 5)\ny = a.dot(x_true ** 2)\n\nout = scipy.optimize.minimize(residual, x0, args=(a, y))\nprint(out.x)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def residual(x):\n    return np.sum((a.dot(x ** 2) - y) ** 2)\n\nx_lower_bounds = np.array([0, 0, 0, 0, 0])\nx_upper_bounds = np.inf * np.ones(5)\n\nout = scipy.optimize.minimize(residual, x0, bounds=(x_lower_bounds, x_upper_bounds), method='LBFGS')\nprint(out.x)\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\nimport numpy as np\nimport scipy.integrate\n\ndef dN1_dt(t, N1):\n    return -100 * N1 + np.sin(t)\n\nN0 = 10\ntime_span = [-0.1, 0.1]\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt(t, N1):\n    return -100 * N1 if 0 < t < 2 * np.pi else 2 * np.pi\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\nimport numpy as np\nimport scipy.integrate\n\ndef dN1_dt(t, N1):\n    return -100 * N1 - np.cos(t)\n\nN0 = 10\ntime_span = [-0.1, 0.1]\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt, t_span=time_span, y0=[N0,])\nresult = sol.y\nprint(result)\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\nprint(x)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "result = sparse.vstack([sa, sb])\nprint(result)\n", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "a = sa.toarray()\nb = sb.toarray()\nresult = np.hstack((a, b))\nprint(result)\n", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "import numpy as np\ndef func(x, c):\n    return 2 * c * x\nresult = np.zeros(len(c))\nfor i in range(len(c)):\n    result[i] = np.integrate.quad(lambda x: func(x, c[i]), low, high)[0]\nprint(result)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": "\n    def eqn(x):\n        return 2*x*c\n    result, error = scipy.integrate.quad(eqn, low, high)\n    return result\n    ### END SOLUTION\n# test the function\nfor c in range(1, 10):\n    print(f(c))\n# output:\n# 1.0\n# 2.0\n# 3.0\n# 4.0\n# 5.0\n# 6.0\n# 7.0\n# 8.0\n# 9.0\n# 10.0\n# END\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "x = np.full(V.shape[0], x)\nV_dok = V.toarray().flatten()\nx_dok = x.flatten()\nx_dok = np.where(x_dok == 0, x_dok, x_dok + V_dok)\nV_dok = np.array(x_dok)\nV = sparse.dok_matrix(V_dok)\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "x = np.full(V.shape[0], x)\nV_add = V + x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "x = np.random.rand(1, 1)\ny = np.random.rand(1, 1)\nA = V + x\nB = A + y\nprint(A.shape)\nprint(B.shape)\nprint(A.data.sum())\nprint(B.data.sum())\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "def normalize_column(self, Column):\n    Len = math.sqrt(np.sum(Column ** 2))\n    return Column / Len\n\ndef update_matrix(self):\n    for Col in xrange(self.__WeightMatrix__.shape[1]):\n        Column = self.__WeightMatrix__[:,Col].data\n        self.__WeightMatrix__[:,Col] = self.normalize_column(Column)\n\nself.__WeightMatrix__ = self.__WeightMatrix__.tocsc()\nself.update_matrix()\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "def normalize_column(self, Column):\n    Len = math.sqrt(np.sum(Column ** 2))\n    return Column / Len\n\ndef update_matrix(self):\n    for Col in xrange(self.__WeightMatrix__.shape[1]):\n        Column = self.__WeightMatrix__[:,Col].data\n        self.__WeightMatrix__[:,Col] = self.normalize_column(Column)\n\nself.__WeightMatrix__ = self.__WeightMatrix__.tocsr()\nself.update_matrix()\nprint(self.__WeightMatrix__)\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "import numpy as np\ndef binary_matrix(a):\n    return np.where(a > 0, 1, 0)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "import numpy as np\ndef binary_matrix(a):\n    n = a.shape[0]\n    binary_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j and a[i, j] == 0:\n                binary_matrix[i, j] = 1\n    return binary_matrix\n\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\nprint(binary_matrix(a))\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "def find_closest(centroids, data):\n    distances = np.zeros((data.shape[0], centroids.shape[0]))\n    for i in range(data.shape[0]):\n        for j in range(centroids.shape[0]):\n            distances[i, j] = np.linalg.norm(data[i] - centroids[j])\n    return np.argmin(distances, axis=1)\nresult = find_closest(centroids, data)\nprint(result)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "def closest_point(centroid, data):\n    distance_matrix = scipy.spatial.distance.cdist(data, centroid)\n    closest_point_index = np.argmin(distance_matrix, axis=1)\n    return closest_point_index\n\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\n\nclosest_point_index = closest_point(centroids, data)\n\nresult = data[closest_point_index]\nprint(result)\n", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "dist_matrix = scipy.spatial.distance.cdist(data, centroids)\nclosest_index = np.argmin(dist_matrix, axis=1)\nresult = np.zeros((data.shape[0], 1))\nfor i in range(data.shape[0]):\n    result[i] = closest_index[i]\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "def eqn_a(x, b):\n    return x + 2*b - b**2\n\nresult = []\nfor x in xdata:\n    for b in bdata:\n        x_result, _ = fsolve(eqn_a, x)\n        result.append((x, b, x_result))\n\nresult = np.array(result)\nprint(result)\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "def eqn(x, a, b):\n    return x + 2*a - b**2\n\ndef solve(xdata,adata):\n    result = []\n    for i in range(len(adata)):\n        a = adata[i]\n        for x in xdata:\n            b = a*x\n            result.append([x, a, b])\n    return np.array(result)\n\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\n\nresult = solve(xdata,adata)\nprint(result)\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "def test_fit(x, a, m, d):\n    p = bekkers(x, a, m, d)\n    f = lambda x: p(x)\n    f_integral, _ = integrate.integrate.integrate(f, 1, 10)\n    f_statistic, pvalue = stats.kstest(x, f)\n    return f_statistic, pvalue\nresult = test_fit(sample_data, estimated_a, estimated_m, estimated_d)\nprint(result)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "def test_fit(x, a, m, d):\n    p = bekkers(x, a, m, d)\n    return (np.sum((x - p)**2))\nx = np.linspace(range_start, range_end, 100)\nresult = True\nif integrate.integrate.integrate(test_fit, range_start, range_end) < 1e-10:\n    result = False\nelse:\n    result = False\nprint(result)\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "def integrate_column(df, column_name):\n    # Convert column to time index\n    df['time'] = pd.to_datetime(df['Time'])\n    # Integrate column over time\n    integral = integrate.trapz(df[column_name], df['time'])\n    return integral\n\n# Define column to integrate\ncolumn_name = 'A'\n\n# Calculate integral\nintegral_df = integrate_column(df, column_name)\n\nprint(integral_df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "x = np.array([point[0] for point in x])\ny = np.array([point[1] for point in y])\nx_eval = np.array([point[0] for point in eval])\ny_eval = np.array([point[1] for point in eval])\ninterp = scipy.interpolate.LinearInterpolation(x, y)\nresult = interp(x_eval, y_eval)\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "def multinomial_probability(x, weights):\n    return np.exp(np.sum(np.log(x) * weights))\n\ndef multinomial_likelihood(x, weights):\n    return multinomial_probability(x, weights)\n\ndef multinomial_optimize(x, weights):\n    return -np.sum(np.log(x) * weights)\n\nx = np.array([0, 1, 2, 3, 4])\nweights = np.array([0.001, 0.1, 0.2, 0.12, 0.2])\n\noptimized_weights = sciopt.minimize(multinomial_likelihood, weights, method='Newton-Raphson', args=(weights,), bounds=((0, None),))\n\nprint(optimized_weights.x)\n", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "popt = sciopt.fmin(e, pmin, args=(x,y))\nprint(popt)\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "def find_relative_extrema(arr, n):\n    # Find the indices of elements less than or equal to the neighbouring elements\n    less_than_neighbours = np.where((arr[:-2] < arr[2:]) | (arr[:-2] < arr[1:-1]))[0]\n    # Find the indices of elements greater than or equal to the neighbouring elements\n    greater_than_neighbours = np.where((arr[:-2] > arr[2:]) | (arr[:-2] > arr[1:-1]))[0]\n    # Combine the results\n    indices = np.concatenate((less_than_neighbours, greater_than_neighbours))\n    return indices\n\n# Test the function\narr = np.array([-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,])\nn = 2\nprint(find_relative_extrema(arr, n))\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "def find_relative_extrema(arr, n):\n    # Get the number of rows and columns in the array\n    rows, cols = arr.shape\n    # Initialize an empty list to store the indices of relative extrema\n    extrema = []\n    # Iterate over each row\n    for i in range(rows):\n        # Get the current row\n        row = arr[i]\n        # Find the relative extrema in the current row\n        extrema.extend(find_extrema(row, n))\n    return extrema\n\ndef find_extrema(row, n):\n    # Initialize an empty list to store the indices of relative extrema\n    extrema = []\n    # Iterate over each element in the row\n    for i in range(len(row)):\n        # Get the current element\n        element = row[i]\n        # Check if the current element is less than the elements to the left\n        if i > 0:\n            left = row[i - 1]\n            if element <= left:\n                # Check if the current element is less than the elements to the right\n                if i < len(row) - 1:\n                    right = row[i + 1]\n                    if element <= right:\n                        # Add the current element to the list of relative extrema\n                        extrema.append(i)\n    return extrema\n\n# Test the function\narr = np.array([[-624.59309896, -624.59309896, -624.59309896,\n                      -625., -625., -625.,], [3, 0, 0, 1, 2, 4]])\nn = 2\nprint(find_relative_extrema(arr, n))\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "import numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": "\n    data1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                         columns=data['feature_names'] + ['target'])\n    return data1\n    ### END SOLUTION\n\ndata1 = solve(data)\nprint(data1)\n## END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "import numpy as np\ndef one_hot_encode(df, column_name):\n    unique_elements = np.unique(df[column_name])\n    for element in unique_elements:\n        df[element] = np.where(df[column_name] == element, 1, 0)\n    return df\ndf_out = one_hot_encode(df, 'Col3')\nprint(df_out)\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "import numpy as np\ndef one_hot_encode(df, column_names):\n    # Get unique values\n    unique_values = np.unique(df[column_names])\n    \n    # Initialize a 2D NumPy array to store one-hot encoded values\n    one_hot_encoded_df = np.zeros((df.shape[0], len(column_names)), dtype=int)\n    \n    # Iterate over each unique value\n    for i, value in enumerate(unique_values):\n        # Get indices of values in column_names\n        column_indices = np.where(column_names == column_names)[0]\n        \n        # Set one-hot encoded values\n        one_hot_encoded_df[np.arange(df.shape[0]), column_indices] = 1\n    \n    return one_hot_encoded_df\n\n# Example usage\ndf_out = one_hot_encode(df, ['Col1', 'Col2'])\nprint(df_out)\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "import numpy as np\ndef one_hot_encode(data):\n    unique_elements = np.unique(data)\n    one_hot_encoded_data = np.zeros((len(data), len(unique_elements)))\n    for i, element in enumerate(unique_elements):\n        one_hot_encoded_data[:, i] = (data == element).astype(int)\n    return one_hot_encoded_data\n\ndf_out = one_hot_encode(df['Col4'])\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "import numpy as np\ndef one_hot_encode(df, column):\n    unique_elements = np.unique(df[column])\n    for element in unique_elements:\n        df[element] = np.where(df[column] == element, 1, 0)\n    return df\n\ndef one_hot_encode_df(df):\n    for col in df.columns:\n        df = one_hot_encode(df, col)\n    return df\n\ndf_out = one_hot_encode_df(df)\nprint(df_out)\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "import numpy as np\ndef one_hot_encode(df, column):\n    unique_elements = np.unique(df[column])\n    for element in unique_elements:\n        df[element] = np.where(df[column] == element, 1, 0)\n    return df\n\ndef one_hot_encode_df(df):\n    for col in df.columns:\n        df = one_hot_encode(df, col)\n    return df\n\ndf_out = one_hot_encode_df(df)\nprint(df_out)\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "predicted_test= svmmodel.predict(x_test)\npredicted_test_scores= svmmodel.decision_function(x_test)\nprob = np.exp(predicted_test_scores) / (1 + np.exp(predicted_test_scores))\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "from sklearn.calibration import CalibratedClassifierCV\nmodel_cal = svm.LinearSVC()\nmodel_cal.fit(X, y)\nprob_estimates = model_cal.predict_proba(x_predict)\nprint(prob_estimates)\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef load_data():\n    # Assume that the output of sklearn.preprocessing.OneHotEncoder is a csr_matrix\n    # and we want to convert it back to a dataframe\n    # Assume that the output is a csr_matrix\n    # and we want to convert it back to a dataframe\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr_matrix\n    # Assume that the output is a csr", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "# Assume transform_output is a csr_matrix\ndf_merge = pd.DataFrame(transform_output.toarray())\ndf_merge.columns = ['value']\ndf_merge = pd.concat([df_origin, df_merge], axis=1)\nprint(df_merge)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": "\n    # Convert the output from scipy.sparse.csr.csr_matrix to a numpy array\n    transform_output_array = transform_output.toarray()\n    # Convert the output from scipy.sparse.csr.csr_matrix to a pandas DataFrame\n    transform_output_df = pd.DataFrame(transform_output_array, columns=transform_output.feature_names)\n    # Concatenate the original dataframe and the transformed output dataframe\n    result = pd.concat([df, transform_output_df], axis=1)\n    return result\n    ### END SOLUTION\n\n# Test the function\ndf_origin = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\ntransform_output = csr_matrix(\n    [[1, 0, 0],\n     [0, 1, 0],\n     [0, 0, 1]],\n    shape=(3, 3)\n)\nresult = solve(df_origin, transform_output)\nprint(result)\n# Output:\n#   A  B\n# 0  1  4\n# 1  2  5\n# 2  3  6\n# END\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\n# Delete a step\nsteps = clf.named_steps\ndel steps['poly']\nprint(clf)\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\n# Delete a step\nsteps = clf.named_steps\nsteps.pop('dim_svm')\nclf = Pipeline(estimators)\nprint(clf)\n# Insert a step\nsteps = clf.named_steps\nsteps['dim_svm'] = PCA()\nclf = Pipeline(estimators)\nprint(clf)\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\n# Delete the 2nd step\nestimators = [('reduce_dim', PCA()), ('pOly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a step\nclf.steps.append('new_step', PolynomialFeatures())\nprint(clf)\n# Delete a step\ndel clf.steps['poly']\nprint(clf)\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\n# Insert a step\nclf.steps.append('new_step', SVC())\nprint(clf)\n# Delete a step\ndel clf.steps[0]\nprint(clf)\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\n# Insert ('t1919810', PCA()) right before 'svdm'\nestimators = [('reduce_dIm', PCA()), ('pOly', PolynomialFeatures()), ('t1919810', PCA()), ('svdm', SVC())]\nclf = Pipeline(estimators)\nprint(clf)\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\ndef gridsearch(trainX, trainY, testX, testY):\n    model = xgb.XGBRegressor()\n    paramGrid = {\n        'learning_rate': [0.01, 0.1, 1],\n        'max_depth': [2, 5, 10],\n        'n_estimators': [10, 20, 50],\n        'gamma': [0, 0.5, 1]\n    }\n    gridsearch = GridSearchCV(model, paramGrid, verbose=0, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1, iid=True)\n    gridsearch.fit(trainX, trainY)\n    b = gridsearch.best_score_\n    c = gridsearch.predict(testX)\n    return b, c\n\ntestX = np.random.rand(100, 5)\ntestY = np.random.rand(100)\ntrainX = np.random.rand(500, 5)\ntrainY = np.random.rand(500)\n\nb, c = gridsearch(trainX, trainY, testX, testY)\nprint(b, c)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nimport numpy as np\nimport xgboost.sklearn as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# Define parameters\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 1],\n    'max_depth': [2, 5, 8],\n    'n_estimators': [100, 200, 300],\n    'gamma': [0, 0.5, 1],\n    'alpha': [0.01, 0.1, 1],\n    'lambda': [0, 1, 2]\n}\n\n# Define model\nmodel = xgb.XGBRegressor()\n\n# Define grid search parameters\nfit_params = {\n    'early_stopping_rounds': 42,\n    'verbose': 0,\n    'cv': TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]),\n    'n_jobs': -1,\n    'iid': False,\n    'eval_metric': 'mae',\n    'eval_set': [[testX, testY]]\n}\n\n# Perform grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=0, fit_params=fit_params)\ngridsearch.fit(trainX, trainY)\n\n# Get score and prediction\nb = gridsearch.best_score_\nc = gridsearch.predict(testX)\n\nprint(f\"Score: {b}\")\nprint(f\"Prediction: {c}\")\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.concatenate(proba)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = np.linalg.inv(scaler.scale_)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": "\n    t = np.linspace(0, 10, 100)\n    t_scaled = scaler.transform(t)\n    t_inverse = np.invert(scaler.inverse_transform(t_scaled))\n    return t_inverse\n    ### END SOLUTION\n\n# test the function\nscaled = solve(data, scaler, scaled)\nprint(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)\n# print(scaled)", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.mean_score}')\n", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "model_name = model.getName()\nprint(model_name)\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "model_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {model.score(X, y)}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data)\nprint(pipe.named_steps[\"tf_idf\"].get_params())\nprint(pipe.named_steps[\"nmf\"].get_params())\n", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data)\n", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.steps[1][1].feature_selection_output_\n", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\n# Define the parameters for GridSearch\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\n\n# Use GridSearchCV to find the best parameters\ngrid_search = GridSearchCV(bc, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\n\n# Use the best parameters to create a new BaggingClassifier\ndt = DecisionTreeClassifier(max_depth=best_params['base_estimator__max_depth'])\nbc = BaggingClassifier(dt, n_estimators=20, max_samples=best_params['max_samples'])\n\n# Fit the BaggingClassifier\nbc.fit(X_train, y_train)\n\n# Use the BaggingClassifier to predict probabilities\nproba = bc.predict_proba(X_test)\n\nprint(proba)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nregressor.fit(X, y)\npredict = regressor.predict(X_test)\nprint(predict)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=1.0, random_state=42)\nregressor.fit(X, y)\npredict = regressor.predict(X_test)\nprint(predict)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\n# Define the preprocessor\npreprocessor = preprocess\n\n# Define the vectorizer\nvectorizer = TfidfVectorizer(preprocessor=preprocessor)\n\n# Define a sample dataset\ndata = ['This is a sample text.', 'Another sample text.', 'Yet another sample text.']\n\n# Fit and transform the dataset\nX = vectorizer.fit_transform(data)\n\n# Print the vectorized dataset\nprint(X)\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n\n# Example data\ndata = [\"This is a test.\", \"Python is a programming language.\", \"Numpy is a library for the Python programming language.\"]\n\n# Fit and transform the data\nX = tfidf.fit_transform(data)\n\n# Print the transformed data\nprint(X)\nEND SOLUTION\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "# Create a copy of the data\ndf_copy = data.copy()\n# Apply preprocessing.scale to the data\ndf_copy = preprocessing.scale(df_copy)\n# Convert the data back to a pandas DataFrame\ndf_out = pd.DataFrame(df_copy, columns=data.columns)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "df_out = preprocessing.scale(data)\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "# Fit the pipeline\npipe.fit(X, y)\n# Get the coefficients\ncoef = pipe.named_steps['model'].coef_\nprint(coef)\n", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "# Fit the pipeline\npipe.fit(X, y)\n# Get the coefficients\ncoef = pipe.named_steps['model'].coef_\nprint(coef)\n", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "import numpy as np\n\n# get feature importance\nfeature_importances = clf.feature_importances_\n\n# get column names\ncolumn_names = np.array(X.columns)\n\n# create a dataframe to store feature names and their importance\nfeature_df = pd.DataFrame({\n    'Feature': column_names,\n    'Importance': feature_importances\n})\n\n# sort feature importance in descending order\nfeature_df = feature_df.sort_values(by='Importance', ascending=False)\n\n# print feature names and their importance\nprint(feature_df)\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = np.array(X.columns)[np.where(X_new != 0)]\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "import numpy as np\n\n# get feature names\nfeature_names = np.array([f'feature_{i}' for i in range(X.shape[1])])\n\n# get feature importance\nfeature_importances = clf.feature_importances_\n\n# get indices of top 5 features\ntop_5_indices = np.argsort(feature_importances)[-5:]\n\n# get top 5 feature names\ntop_5_names = feature_names[top_5_indices]\n\n# save feature names\ncolumn_names = top_5_names\n\nprint(column_names)\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "import numpy as np\n\n# get feature importance\nfeature_importances = clf.feature_importances_\n\n# get column names\ncolumn_names = np.array(X.columns)\n\n# create a list of column names and their corresponding feature importances\nfeature_list = list(zip(column_names, feature_importances))\n\n# sort feature list by feature importance in descending order\nfeature_list.sort(key=lambda x: x[1], reverse=True)\n\n# print feature list\nprint(feature_list)\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\nclosest_50_samples = X[km.labels_.argsort()[:50]]\n", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "km.fit(X)\nclosest_50_samples = np.sort(km.cluster_centers[km.labels_ == km.predict(X)])[:50]\n", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\nclosest_samples = km.cluster_centers[km.labels == p]\nclosest_samples = closest_samples.reshape(-1, X.shape[1])\nclosest_samples = np.sort(closest_samples, axis=0)\nclosest_samples = closest_samples[-100:, :]\nclosest_samples = closest_samples[::-1]\nclosest_samples = closest_samples[:100, :]\nclosest_samples = np.vstack((closest_samples, X))\nclosest_samples = np.sort(closest_samples, axis=0)\nclosest_samples = closest_samples[:100, :]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, :2]\nclosest_samples = np.vstack((closest_samples, X))\nclosest_samples = np.sort(closest_samples, axis=0)\nclosest_samples = closest_samples[:100, :]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, :2]\nclosest_samples = np.vstack((closest_samples, X))\nclosest_samples = np.sort(closest_samples, axis=0)\nclosest_samples = closest_samples[:100, :]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, :2]\nclosest_samples = np.vstack((closest_samples, X))\nclosest_samples = np.sort(closest_samples, axis=0)\nclosest_samples = closest_samples[:100, :]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_samples = closest_samples[:, ::-1]\nclosest_s", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": "\n    km.fit(X)\n    centers = km.cluster_centers_\n    indices = np.argsort(np.linalg.norm(centers - p, axis=1))\n    return centers[indices[:p]]\n    ### END SOLUTION\n\n# Example usage:\np = np.array([[1, 2], [3, 4], [5, 6]])\nX = np.array([[1, 2], [3, 4], [5, 6]])\nkm = KMeans()\nprint(get_samples(p, X, km))\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "# get dummies\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# convert categorical variable to matrix\nX_train = pd.get_dummies(X_train)\n\n# merge back with original training data\nX_train = X_train.reset_index(drop=True)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert data type to float\nX_train = X_train.astype(float)\n\n# convert data type to int\nX_train = X_train.astype(int)\n\n# convert", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\nX = np.random.rand(100, 10)\ny = np.random.rand(100)\n# fit the model\nsvm_model = SVR(kernel='rbf')\nsvm_model.fit(X, y)\n# predict\ny_pred = svm_model.predict(X)\nprint(y_pred)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn.svm import SVR\nfrom sklearn.kernel import GaussianKernel\n\n# Define the SVR model\nsvm_model = SVR(kernel='rbf')\n\n# Fit the model\nsvm_model.fit(X, y)\n\n# Predict\ny_pred = svm_model.predict(X)\n\nprint(y_pred)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([1, 2, 3])\nsvm_model = SVR(kernel='poly', degree=2)\nsvm_model.fit(X, y)\npredict = svm_model.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([1, 2, 3])\nsvm_model = SVR(kernel='poly', degree=2)\nsvm_model.fit(X, y)\ny_pred = svm_model.predict(X)\nprint(y_pred)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarity(query_tfidf, documents_tfidf):\n    # calculate cosine similarity\n    cosine_similarity = np.dot(query_tfidf, documents_tfidf.T) / (np.linalg.norm(query_tfidf, axis=1)[:, np.newaxis] * np.linalg.norm(documents_tfidf, axis=1))\n    return cosine_similarity\n\ndef get_cosine_similarity_matrix(queries, documents):\n    # get tf-idf matrix of documents\n    documents_tfidf = TfidfVectorizer().fit_transform(documents)\n    # get tf-idf matrix of queries\n    queries_tfidf = TfidfVectorizer().fit_transform(queries)\n    # calculate cosine similarity matrix\n    cosine_similarities = np.zeros((len(queries), len(documents)))\n    for i in range(len(queries)):\n        cosine_similarities[i] = get_cosine_similarity(queries_tfidf[i], documents_tfidf)\n    return cosine_similarities\n\ncosine_similarities_of_queries = get_cosine_similarity_matrix(queries, documents)\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarity(query_tfidf, documents_tfidf):\n    # calculate cosine similarity\n    cosine_similarity = np.dot(query_tfidf, documents_tfidf.T) / (np.linalg.norm(query_tfidf, axis=1)[:, None] * np.linalg.norm(documents_tfidf, axis=1))\n    return cosine_similarity\n\ndef get_cosine_similarity_matrix(queries, documents):\n    # get tf-idf matrix of documents\n    documents_tfidf = TfidfVectorizer().fit_transform(documents)\n    \n    # get tf-idf matrix of queries\n    queries_tfidf = TfidfVectorizer().fit_transform(queries)\n    \n    # calculate cosine similarity matrix\n    cosine_similarities = np.zeros((len(queries), len(documents)))\n    for i in range(len(queries)):\n        for j in range(len(documents)):\n            cosine_similarities[i, j] = get_cosine_similarity(queries_tfidf[i], documents_tfidf[j])\n    \n    return cosine_similarities\n\ncosine_similarities_of_queries = get_cosine_similarity_matrix(queries, documents)\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": "\n    tfidf_query_matrix = tfidf.transform(queries)\n    cosine_similarities = np.zeros((len(queries), len(documents)))\n    for i, query in enumerate(queries):\n        query_tfidf = tfidf.transform([query])\n        cosine_similarities[i] = np.dot(query_tfidf, tfidf_query_matrix.T) / (np.linalg.norm(query_tfidf, axis=1)[:, None] * np.linalg.norm(tfidf_query_matrix, axis=1))\n    return cosine_similarities\n\ndef load_data():\n    documents = [\"Document 1\", \"Document 2\", \"Document 3\", \"Document 4\", \"Document 5\"]\n    queries = [\"Query 1\", \"Query 2\", \"Query 3\"]\n    return documents, queries\n\n# Test the function\ndocuments, queries = load_data()\ncosine_similarities = solve(queries, documents)\nprint(cosine_similarities)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "features = np.array(features)\nfeatures = np.unique(features, axis=0)\nfeatures = features[:, np.argsort(features[:, 0])]\nfeatures = features[:, 1:]\nfeatures = features.astype(np.str)\nfeatures = np.array([list(map(str, row)) for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x == '1'] for row in features])\nfeatures = np.array([[x for x in row if x == '0'] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[x for x in row if x != ''] for row in features])\nfeatures = np.array([[", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "import numpy as np\n\ndef load_data():\n    f = np.array([\n        ['t1'],\n        ['t2', 't5', 't7'],\n        ['t1', 't2', 't3', 't4', 't5'],\n        ['t4', 't5', 't6']\n    ])\n    return f\n\ndef convert_features(f):\n    # get unique features\n    unique_features = np.unique(f)\n    # get the number of unique features\n    n_unique_features = len(unique_features)\n    # create a 2D array with zeros\n    new_f = np.zeros((len(f), n_unique_features))\n    # fill the 2D array with ones\n    for i, feature in enumerate(f):\n        for j, value in enumerate(feature):\n            new_f[i, unique_features.tolist().index(value)] = 1\n    return new_f\n\nnew_f = convert_features(f)\nprint(new_f)\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "features = np.array(features)\nfeatures = np.unique(features, axis=0)\nfeatures = features[:, np.argsort(features[:, 0])]\nfeatures = features[:, 1:]\nfeatures = features.astype(str)\nfeatures = np.array([list(map(int, row)) for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = features.astype(str)\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x) for x in row] for row in features])\nfeatures = np.array([[int(x)", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": "\n    features = np.array(features)\n    features = features.reshape(-1, len(features[0]))\n    features = features[:, features[:, 0] == 1]\n    return features\n    ### END SOLUTION\n\nfeatures = [\n    ['f1', 'f2', 'f3'],\n    ['f2', 'f4', 'f5', 'f6'],\n    ['f1', 'f2']\n]\n\nprint(solve(features))\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "import numpy as np\n\ndef convert_features(features):\n    # Convert features to a 2D array\n    features_array = np.zeros((len(features), len(features[0])))\n    for i, feature in enumerate(features):\n        for j, feature_name in enumerate(feature):\n            features_array[i, j] = int(feature_name)\n    return features_array\n\nfeatures = [\n    ['t1'],\n    ['t2', 't5', 't7'],\n    ['t1', 't2', 't3', 't4', 't5'],\n    ['t4', 't5', 't6']\n]\n\nnew_features = convert_features(features)\nprint(new_features)\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Define the distance matrix\ndistance_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n\n# Create an instance of AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=2)\n\n# Fit and predict the cluster labels\ncluster_labels = clustering.fit_predict(distance_matrix)\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\ncluster_labels = AgglomerativeClustering(n_clusters=2, distance_metric='euclidean').fit_predict(data_matrix)\nprint(cluster_labels)\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\nsimM = np.array([[0, 0.6, 0.8], [0.6, 0, 0.111], [0.8, 0.111, 0]])\n\ncluster = AgglomerativeClustering(n_clusters=2, distance_metric='euclidean')\ncluster_labels = cluster.fit_predict(simM)\n\nprint(cluster_labels)\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "import numpy as np\nimport scipy.cluster\n\ndef load_data():\n    data_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n    return data_matrix\n\ndef hierarchical_clustering(data_matrix):\n    # Calculate the distance matrix\n    distance_matrix = np.zeros((len(data_matrix), len(data_matrix)))\n    for i in range(len(data_matrix)):\n        for j in range(len(data_matrix)):\n            distance_matrix[i, j] = np.sqrt(np.sum((data_matrix[i] - data_matrix[j]) ** 2))\n\n    # Perform hierarchical clustering\n    cluster_labels = scipy.cluster.hierarchy.cluster_centers(\n        scipy.cluster.hierarchy.distance.matrix(distance_matrix),\n        method='centroid',\n        distance_metric='euclidean'\n    )\n    return cluster_labels\n\n# Test the function\ndata_matrix = load_data()\ncluster_labels = hierarchical_clustering(data_matrix)\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "import numpy as np\nimport scipy.cluster\n\ndef load_data():\n    data_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\n    return data_matrix\n\ndef hierarchical_clustering(data_matrix):\n    # Calculate the distance matrix\n    distance_matrix = np.zeros((data_matrix.shape[0], data_matrix.shape[0]))\n    for i in range(data_matrix.shape[0]):\n        for j in range(i + 1, data_matrix.shape[0]):\n            distance_matrix[i, j] = distance_matrix[j, i] = np.sqrt(np.sum((data_matrix[i] - data_matrix[j]) ** 2))\n\n    # Perform hierarchical clustering\n    cluster_labels = scipy.cluster.hierarchy.cluster_centers(\n        scipy.cluster.hierarchy.distance.matrix(distance_matrix),\n        method='centroid',\n        distance_metric='euclidean'\n    )\n    return cluster_labels\n\n# Test the function\ndata_matrix = load_data()\ncluster_labels = hierarchical_clustering(data_matrix)\nprint(cluster_labels)\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "import numpy as np\nimport scipy.cluster\n\ndef load_data():\n    # Define the distance matrix\n    simM = np.array([[0, 0.6, 0.8],\n                     [0.6, 0, 0.111],\n                     [0.8, 0.111, 0]])\n    return simM\n\ndef hierarchical_clustering(simM):\n    # Perform hierarchical clustering\n    cluster_labels = scipy.cluster.hierarchy.cluster_labels(simM)\n    return cluster_labels\n\n# Load the distance matrix\nsimM = load_data()\n\n# Perform hierarchical clustering\ncluster_labels = hierarchical_clustering(simM)\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "import numpy as np\nimport sklearn\n\ndef center_scale(data):\n    mean = np.mean(data)\n    std = np.std(data)\n    return (data - mean) / std, std\n\ndata = np.random.rand(100, 10)\ncentered_scaled_data, std = center_scale(data)\nprint(centered_scaled_data)\nprint(std)\n", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import scale, CenterScaler\ndata_scaled = scale(data)\nprint(data_scaled)\n", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from scipy.stats import boxcox\ndata_box_cox, lambda_box_cox = boxcox(data)\nprint(lambda_box_cox)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from scipy.stats import boxcox\ndata_box_cox, lambda_box_cox = boxcox(data)\nprint(lambda_box_cox)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import YeoJohnson\n\ndata = np.random.rand(100, 2)\n\nyj = YeoJohnson()\nyj.fit(data)\n\ntransformed_data = yj.transform(data)\n\nprint(transformed_data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import YeoJohnson\ndata = np.array([[1, 2, 3], [4, 5, 6]])\nyj = YeoJohnson()\nyj.fit(data)\ndata_transformed = yj.transform(data)\nprint(data_transformed)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "vectorizer = CountVectorizer(punct_markers=['!',\"'\", \"\\\"\", \"\\\"'\"] )\ntransformed_text = vectorizer.fit_transform(text)\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "import numpy as np\nimport pandas as pd\n\n# Load data\ndataset = pd.read_csv('example.csv', header=None, sep=',')\n\n# Split data into training and testing\ntrain_size = int(0.8 * len(dataset))\ntrain_data, test_data = dataset[:train_size], dataset[train_size:]\n\n# Split x and y\nx_train, y_train = train_data.drop(columns=-1), train_data.columns\nx_test, y_test = test_data.drop(columns=-1), test_data.columns\n\n# Check the shape of x_train and y_train\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\nx = data.drop(columns='target')\ny = data['target']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "import numpy as np\nimport pandas as pd\n\n# Load data\ndataset = pd.read_csv('example.csv', header=None, sep=',')\n\n# Split data into training and testing\ntrain_size = int(0.8 * len(dataset))\ntrain_data, test_data = dataset[:train_size], dataset[train_size:]\n\n# Split x and y\nx_train, y_train = train_data.drop(columns=[-1]), train_data[-1]\nx_test, y_test = test_data.drop(columns=[-1]), test_data[-1]\n\n# Check the shape of x_train and y_train\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    np.random.seed(42)\n    indices = np.arange(data.shape[0])\n    np.random.shuffle(indices)\n    train_size = int(0.8 * data.shape[0])\n    train_indices = indices[:train_size]\n    test_indices = indices[train_size:]\n    x_train = data.loc[train_indices, :-1]\n    y_train = data.loc[train_indices, -1]\n    x_test = data.loc[test_indices, :-1]\n    y_test = data.loc[test_indices, -1]\n    return x_train, y_train, x_test, y_test\n    ### END SOLUTION\n\ndata = pd.DataFrame({\n    'A': np.arange(10),\n    'B': np.arange(10, 20),\n    'C': np.arange(20, 30),\n    'D': np.arange(30, 40),\n    'E': np.arange(40, 50),\n    'F': np.arange(50, 60),\n    'G': np.arange(60, 70),\n    'H': np.arange(70, 80),\n    'I': np.arange(80, 90),\n    'J': np.arange(90, 100)\n})\n\nx_train, y_train, x_test, y_test = solve(data)\nprint(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\n\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "X = df['mse'].values.reshape(-1, 1)\nkmeans = KMeans(n_clusters=2).fit(X)\nlabels = kmeans.predict(X)\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "from numpy import array\n\ndef load_data():\n    # Load data\n    data = np.array([\n        ['2009-06-04', 3.11],\n        ['2009-06-08', 3.33],\n        ['2009-06-12', 7.52],\n    ])\n    # Reshape data\n    data = data[:, 1]\n    return data\n\ndef main():\n    # Create a 2D array with shape (n_samples, 1)\n    X = array([data[0] for data in data]).reshape(-1, 1)\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2).fit(X)\n    labels = kmeans.predict(X)\n    return labels\n\nif __name__ == \"__main__\":\n    data = load_data()\n    labels = main()\n    print(labels)\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "svm = LinearSVC(penalty='l1', random_state=0, max_iter=1000)\nsvm.fit(X, y)\nselected_features = np.asarray(vectorizer.get_feature_names())[svm.get_support()]\nprint(selected_features)\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "# Select features\nselector = LinearSVC(penalty='l1', random_state=0, tol=1e-5, max_iter=10000)\nselector.fit(X, y)\nselected_features = np.asarray(vectorizer.get_feature_names())[selector.get_support()]\nprint(selected_features)\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": "\n    # Create a linear SVM model\n    svm_model = LinearSVC(penalty='l1', loss='squared_loss', random_state=42)\n    # Fit the model\n    svm_model.fit(X, y)\n    # Get the indices of selected features\n    selected_feature_indices = np.where(svm_model.coef_ != 0)[0]\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names()\n    selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n    return selected_feature_names\n    ### END SOLUTION\n\n# Test the function\ncorpus = ['This is a test sentence.', 'Another test sentence.', 'Yet another test sentence.']\ny = [1, 0, 1]\nX = vectorizer.transform(corpus)\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n## END\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript', 'Python', 'CSS', 'SQL', 'NodeJS', 'Angular', 'Mongo', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UX', 'UI Design', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'PHP', 'Python', 'CSS', 'SQL', 'NodeJS', 'Angular', 'Mongo', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UX', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python',", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = X.toarray()\nprint(feature_names)\nprint(X)\nEND SOLUTION\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UX'})\nX = vectorizer.fit_transform(corpus)\nvocabulary = vectorizer.get_feature_names()\nX = X.toarray()\nX = np.transpose(X)\nX = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.append(vocabulary, np.array(['']))\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary, np.where(vocabulary == ''))\nvocabulary = np.sort(vocabulary)\nvocabulary = np.concatenate((np.array(['']), vocabulary), axis=0)\nvocabulary = np.unique(vocabulary)\nvocabulary = np.delete(vocabulary,", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', \"Java\", 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UI design', 'Web', 'Integration', 'Database design', 'UX'})\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names()\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "series = np.array([])\nfor col in df1.columns:\n    df2 = df1[~np.isnan(df1[col])]\n    X, Y = np.matrix(df2[['Time', col]].values), np.matrix(df2['Time'].values).T\n    model = LinearRegression().fit(X, Y)\n    slope = model.coef_[0]\n    series = np.concatenate((series, slope), axis=0)\nreturn series\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "def get_slope(df, col):\n    X = df[col].values.reshape(-1, 1)\n    Y = df['A1'].values.reshape(-1, 1)\n    model = LinearRegression().fit(X, Y)\n    return model.coef_[0][0]\n\nslopes = np.array([])\nfor col in df1.columns:\n    if col != 'A1':\n        slope = get_slope(df1, col)\n        slopes = np.concatenate((slopes, slope), axis=0)\n    else:\n        slopes = np.concatenate((slopes, 0), axis=0)\n\nreturn slopes\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "# Create a LabelEncoder object\nle = LabelEncoder()\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "\n    df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n    ### END SOLUTION\n    return df\n\ndef load_data():\n    # Load data\n    df = pd.read_csv('titanic.csv')\n    return df\n\ndf = load_data()\ndf = Transform(df)\nprint(df)  # print the data frame to verify the transformation", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "# ElasticNet Regression\nfrom sklearn import linear_model\n\nElasticNet = linear_model.ElasticNet()\nElasticNet.fit(X_train, y_train)\n\n# print(lasso.coef_)\n# print (lasso.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\")\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\")\nprint (ElasticNet.score(X_test, y_test))\n\n# print(ElasticNet.coef_)\n# print(ElasticNet.intercept_)\nEND SOLUTION\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "scaler = MinMaxScaler()\nnp_array_normalized = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "scaler = MinMaxScaler()\nnp_array_normalized = scaler.fit_transform(np_array)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": "\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION\n\n# Test the function\nnp_array = np.array([[1, 2], [3, 4]])\nprint(Transform(np_array))\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "# Predict the close price\nclose_price = clf.predict(np.concatenate([close, ma50, ma100, ma200], axis=1))\nprint(close_price)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\n# Convert string data to float\nnew_X = np.array([[1, 0], [1, 1]])\n# Train the model\nclf.fit(new_X, ['1', '1'])\n# Predict\nprint(clf.predict([[1, 0]]))\n# Predict\nprint(clf.predict([[1, 1]]))\n# Predict\nprint(clf.predict([[0, 0]]))\n# Predict\nprint(clf.predict([[0, 1]]))\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Convert string to float\ndef convert_to_float(x):\n    return np.array([float(i) for i in x])\n\n# Convert input data to numpy array\nnew_X = np.array([convert_to_float(i) for i in X])\n\n# Fit the classifier\nclf = DecisionTreeClassifier()\nclf.fit(new_X, ['2', '3'])\n\n# Test the classifier\nprint(clf.predict([['1', '2'], ['3', '4']]))\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\n# Convert string data to float\nnew_X = np.array([[float(x) for x in row] for row in X])\n# Train the model\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# Data set Preprocess data\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n\n# Fit logistic regression model\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n\n# Predict\nX_test = np.array([[1, 2, 3, 4, 5]])\ny_pred = logReg.predict(X_test)\nprint(y_pred)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Convert the data to numpy array\nX = dataframe.astype(float)\n# Define the target variable\ny = dataframe.iloc[:, -1].values\n# Fit the logistic regression model\nlogReg = LogisticRegression()\nlogReg.fit(X, y)\n# Predict the target variable\npredict = logReg.predict(X)\nprint(predict)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "train_dataframe = features_dataframe[features_dataframe['date'] < test_dataframe['date']]\ntest_dataframe = features_dataframe[features_dataframe['date'] >= test_dataframe['date']]\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "train_dataframe = features_dataframe.sort_values(by=\"date\")\ntest_dataframe = features_dataframe.sort_values(by=\"date\")\ntrain_size = 0.8\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=1 - train_size, random_state=42)\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    train_size = 0.2\n    train_dataframe, test_dataframe = train_test_split(features_dataframe, train_size=train_size)\n    train_dataframe = train_dataframe.sort(\"date\")\n    test_dataframe = test_dataframe.sort(\"date\")\n    return train_dataframe, test_dataframe\n    ### END SOLUTION\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "cols = df.columns[2:4]\ndf[cols + '_scale'] = df.groupby('Month')[cols].apply(lambda x: x.apply(scaler.fit_transform))\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "# Select columns A2 and A3\ncols = myData.columns[2:4]\n# Apply minmax scaler to columns A2 and A3\nmyData['new_' + cols] = myData.groupby('Month')[cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n# Reshape data to get columns new_A2 and new_A3 for each month\nmyData = myData.groupby('Month').apply(lambda x: x.reshape(-1, 2)).values\n# Reshape data back to original shape\nmyData = myData.reshape(-1, 4)\n# Convert data type to float\nmyData = myData.astype(float)\n# Print data\nprint(myData)\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "count = CountVectorizer(lowercase = False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "count = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nprint(full_results)\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results.sort_values(by='mean_fit_time', inplace=True)\nprint(full_results)\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\ndef save_model(model, filename):\n    np.save(filename, model.weights_)\n    np.save(filename, model.means_)\n    np.save(filename, model.covs_)\n\ndef load_data():\n    # Load the model from the file named \"sklearn_model\"\n    weights_ = np.load('sklearn_model.npy')\n    means_ = np.load('sklearn_model.npy')\n    covs_ = np.load('sklearn_model.npy')\n    return IsolationForest(weights_, means_, covs_)\n\n# Save the model\nfitted_model = IsolationForest()\nsave_model(fitted_model, \"sklearn_model\")\n\n# Load the model\nloaded_model = load_data()\n\n# Save the weights, means, and covs\nnp.save('sklearn_model.npy', loaded_model.weights_)\nnp.save('sklearn_model.npy', loaded_model.means_)\nnp.save('sklearn_model.npy', loaded_model.covs_)\n\n# Load the weights, means, and covs\nweights_ = np.load('sklearn_model.npy')\nmeans_ = np.load('sklearn_model.npy')\ncovs_ = np.load('sklearn_model.npy')\n\n# Load the model from the saved data\nloaded_model = IsolationForest(weights_, means_, covs_)\n\n# Save the weights, means, and covs\nnp.save('sklearn_model.npy', loaded_model.weights_)\nnp.save('sklearn_model.npy', loaded_model.means_)\nnp.save('sklearn_model.npy', loaded_model.covs_)\n\n# Load the weights, means, and covs\nweights_ = np.load('sklearn_model.npy')\nmeans_ = np.load('sklearn_model.npy')\ncovs_ = np.load('sklearn_model.npy')\n\n# Save the weights, means, and covs\nnp.save('sklearn_model.npy', weights_)\nnp.save('sklearn_model.npy', means_)\nnp.save('sklearn_model.npy', covs_)\n\n# Load the weights, means, and covs\nweights_ = np.load('sklearn_model.npy')\nmeans_ = np.load('sklearn_model.npy')\ncovs_ = np.load('sklearn_model.npy')\n\n# Save the weights, means, and covs\nnp.save('sklearn_model.npy', weights_)\nnp.save('sklearn_model.npy', means_)\nnp.save('sklearn_model.npy', covs_)\n\n# Load the weights, means, and covs\nweights_ = np.load('sklearn_model.npy')\nmeans_ = np.load('sklearn_model.npy')\ncovs_ = np.load('sklearn_model.npy')\n\n# Save the weights, means, and covs\nnp.save('sklearn_model.npy', weights_)\nnp.save('sklearn_model.npy', means_)\nnp.save('sklearn_model.npy', covs_)\n\n# Load the weights, means, and covs\nweights_ = np.load('sklearn_model.npy')\nmeans_ = np.load('sklearn_model.npy')\ncovs_ = np.load('sklearn_model.npy')\n\n# Save the weights, means, and covs\nnp.save('sklearn_model.npy', weights_)\nnp.save('sklearn_model.npy', means_)\nnp.save('sklearn_model.npy', covs_)\n\n# Load the weights, means, and covs\nweights_ = np.load('sklearn_model.npy')\nmeans_ = np.load('sklearn_model.npy')\ncovs_ = np.load('sklearn_model.npy')\n\n# Save the weights, means, and covs\nnp.save('sklearn_model.npy', weights_)\nnp.save('sklearn_model", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "# tokenizing each description\ndescription_tokens = df['description'].apply(lambda x: x.split())\n# transform the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(description_tokens)\n# calculate cosine similarity between each description text\ncosine_similarity_matrix = np.zeros((df.shape[0], df.shape[0]))\nfor i in range(df.shape[0]):\n    for j in range(i+1, df.shape[0]):\n        cosine_similarity_matrix[i, j] = 1 - np.dot(tfidf_matrix[i], tfidf_matrix[j]) / (np.linalg.norm(tfidf_matrix[i]) * np.linalg.norm(tfidf_matrix[j]))\n        cosine_similarity_matrix[j, i] = cosine_similarity_matrix[i, j]\nprint(cosine_similarity_matrix)\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptim.param_groups[0]['lr'] = 0.001\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\ndef update_learning_rate(optimizer, loss_increase):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = 0.001\n    return optimizer\n\noptim = update_learning_rate(optim, 0.01)\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptim.param_groups[0]['lr'] = 0.0005\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\ndef update_learning_rate(optimizer, loss_increase):\n    lr = optimizer.param_groups[0]['lr']\n    if loss_increase:\n        lr *= 0.1\n    else:\n        lr /= 10\n    optimizer.param_groups[0]['lr'] = lr\n\n# Example usage:\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005)\nloss_increase = True\nupdate_learning_rate(optimizer, loss_increase)\nprint(optimizer.param_groups[0]['lr'])  # Output: 0.05\n\nloss_increase = False\nupdate_learning_rate(optimizer, loss_increase)\nprint(optimizer.param_groups[0]['lr'])  # Output: 0.0005\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "embedding_size = 100\nembedding_weights = np.zeros((len(input_Tensor), embedding_size))\nfor i, word in enumerate(input_Tensor):\n    if word in word2vec.wv.index_to_word:\n        embedding_weights[i] = word2vec.wv[word]\n    else:\n        embedding_weights[i] = np.random.rand(embedding_size)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_weights = np.array(embedding_weights)\nembedding_weights = np.stack([embedding_weights for _ in range(embedding_size)], axis=1)\nembedding_we", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": "\n    embedding_weights = np.zeros((len(input_Tensor), 100))\n    for i, word in enumerate(input_Tensor):\n        if word in word2vec.wv.index_to_word:\n            embedding_weights[i] = word2vec.wv[word]\n    return embedding_weights\n    ### END SOLUTION\n\nembedding_weights = get_embedded_input(input_Tensor)\nprint(embedding_weights.shape)\nprint(embedding_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "def pd_to_np(x):\n    return np.array(x)\n\ndef pd_to_tensor(x):\n    return x.astype(np.float32)\n\ndef pd_to_pandas(x):\n    return pd.Series(x)\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_tensor_list(x):\n    return [pd_to_tensor(x) for x in x]\n\ndef pd_to_pandas_list(x):\n    return [pd_to_pandas(x) for x in x]\n\ndef pd_to_dataframe_list(x):\n    return [pd_to_dataframe(x) for x in x]\n\ndef pd_to_tensor_list_list(x):\n    return [pd_to_tensor_list(x) for x in x]\n\ndef pd_to_pandas_list_list(x):\n    return [pd_to_pandas_list(x) for x in x]\n\ndef pd_to_dataframe_list_list(x):\n    return [pd_to_dataframe_list(x) for x in x]\n\ndef pd_to_tensor_list_list_list(x):\n    return [pd_to_tensor_list_list(x) for x in x]\n\ndef pd_to_pandas_list_list_list(x):\n    return [pd_to_pandas_list_list(x) for x in x]\n\ndef pd_to_dataframe_list_list_list(x):\n    return [pd_to_dataframe_list_list(x) for x in x]\n\ndef pd_to_tensor_list_list_list_list(x):\n    return [pd_to_tensor_list_list_list(x) for x in x]\n\ndef pd_to_pandas_list_list_list_list(x):\n    return [pd_to_pandas_list_list_list(x) for x in x]\n\ndef pd_to_dataframe_list_list_list_list(x):\n    return [pd_to_dataframe_list_list(x) for x in x]\n\ndef pd_to_tensor_list_list_list_list(x):\n    return [pd_to_tensor_list_list(x) for x in x]\n\ndef pd_to_pandas_list_list_list(x):\n    return [pd_to_pandas_list(x) for x in x]\n\ndef pd_to_dataframe_list_list(x):\n    return [pd_to_dataframe(x) for x in x]\n\ndef pd_to_tensor_list(x):\n    return [pd_to_tensor(x) for x in x]\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_tensor(x):\n    return x.astype(np.float32)\n\ndef pd_to_np(x):\n    return np.array(x)\n\ndef pd_to_dataframe_list(x):\n    return [pd_to_dataframe(x) for x in x]\n\ndef pd_to_dataframe_list(x):\n    return [pd_to_dataframe(x) for x in x]\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_dataframe(x):\n    return pd.DataFrame(x)\n\ndef pd_to_dataframe(x", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "import numpy as np\nimport pandas as pd\nimport numpy as np\nimport torch\nimport pandas as pd\n\nx = np.random.rand(4,4)\npx = pd.DataFrame(x)\nprint(px)\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "def load_data():\n    x = np.random.rand(6,6)\n    x = np.array(x)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dims(x, axis=0)\n    x = np.expand_dim", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "import numpy as np\nimport pandas as pd\nimport torch\n\nA_log = np.array([1, 0, 1])\nB = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Convert NumPy arrays to PyTorch tensors\nA_log = torch.ByteTensor(A_log)\nB = torch.LongTensor(B)\n\n# Use logical indexing to slice the tensor\nC = B[:, A_log]\n\nprint(C)\n", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "import numpy as np\nimport pandas as pd\nimport torch\n\ndef load_data():\n    A_logical = np.array([1, 0, 1])\n    B = np.array([[1, 2, 3], [4, 5, 6]])\n    return A_logical, B\n\nA_logical, B = load_data()\nC = np.where(A_logical, B, 0)\nprint(C)\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "import numpy as np\nimport pandas as pd\nimport torch\n\nA_log = np.array([1, 1, 0])\nB = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch = np.array([True, True, False])\nB_torch = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor = np.array([True, True, False])\nB_torch_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor_tensor_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor_tensor_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor_tensor_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor_tensor_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor_tensor_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor_tensor_tensor = np.array([True, True, False])\nB_torch_tensor_tensor_tensor_tensor_tensor = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n\nA_log_torch_tensor_tensor_tensor_tensor_tensor = np.array([True", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "import numpy as np\nimport pandas as pd\nimport torch\n\nA_log = np.array([0, 1, 0])\nB = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Convert NumPy arrays to PyTorch tensors\nA_log = torch.ByteTensor(A_log)\nB = torch.ByteTensor(B)\n\n# Use logical indexing to slice the tensor\nC = B[:, A_log]\n\n# Convert back to NumPy array\nC = C.numpy()\n\nprint(C)\n", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": "\n    C = B[:, A_log]\n    return C\n    ### END SOLUTION\n\n# Test the function\nA_log = np.array([1, 0, 1])\nB = np.array([[1, 2, 3], [4, 5, 6]])\nC = solve(A_log, B)\nprint(C)\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "import numpy as np\nimport torch\n\ndef load_data():\n    A_log = np.array([0, 0, 1])\n    B = np.array([[999, 777, 114514], [9999, 7777, 1919810]])\n    return A_log, B\n\ndef logical_indexing(A_log, B):\n    return B[:, A_log]\n\nA_log, B = load_data()\nC = logical_indexing(A_log, B)\nprint(C)\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "import numpy as np\nimport pandas as pd\nimport torch\n\ndef index_select(B, idx):\n    # Get the shape of B\n    shape = B.shape\n    # Create a mask to select columns\n    mask = np.ones(shape[1], dtype=bool)\n    # Select columns based on the index\n    mask[idx] = False\n    # Select the columns\n    C = B[:, mask]\n    return C\n\nC = index_select(B, idx)\nprint(C)\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "import numpy as np\nimport torch\n\ndef convert_array_to_tensor(array):\n    # Convert the numpy array to a list of lists\n    array_list = [list(x) for x in array]\n    # Convert the list of lists to a numpy array\n    array_tensor = np.array(array_list)\n    # Convert the numpy array to a tensor\n    tensor = torch.tensor(array_tensor)\n    return tensor\n\n# Example usage\narray_array = np.array([\n    [0.5, 1.0, 2.0],\n    [4.0, 6.0, 8.0]\n])\n\ntensor = convert_array_to_tensor(array_array)\nprint(tensor)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "x_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(i) for i in x_array])\nx_tensor = np.array([np.array(", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": "\n    return np.array(a)\n    ### END SOLUTION\n\n# Test the function\na = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=float16),\n   np.array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nprint(Convert(a))\n## END\n\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "import numpy as np\nimport torch\n\ndef batch_lengths_to_mask(lens):\n    max_len = np.max(lens)\n    mask = np.zeros((len(lens), max_len), dtype=np.int32)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask\n\nlens = np.array([3, 5, 4])\nmask = batch_lengths_to_mask(lens)\nprint(mask)\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "import numpy as np\nimport numpy.ma as ma\n\ndef batch_lengths_to_mask(lens):\n    max_len = np.max(lens)\n    mask = np.zeros((len(lens), max_len), dtype=np.int32)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask\n\nlens = np.array([1, 9, 3, 5])\nmask = batch_lengths_to_mask(lens)\nprint(mask)\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "import numpy as np\nimport torch\n\ndef batch_lengths_to_mask(lens):\n    max_len = np.max(lens)\n    mask = np.zeros((len(lens), max_len), dtype=np.int32)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask\n\nlens = np.array([3, 5, 4])\nmask = batch_lengths_to_mask(lens)\nprint(mask)\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": "\n    mask = np.zeros((len(lens), max(lens)), dtype=np.int32)\n    for i in range(len(lens)):\n        mask[i, :lens[i]] = 1\n    return mask\n    ### END SOLUTION\n    return mask\n\nmask = get_mask(lens)\nprint(mask)\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "import numpy as np\nimport torch\n\ndef load_data():\n    # Assume you have a 2D tensor\n    tensor_2d = np.array([[1, 2], [3, 4]])\n    return tensor_2d\n\ndef diagonal_matrix(tensor_2d):\n    # Get diagonal elements\n    diagonal_elements = np.diag(tensor_2d)\n    # Create diagonal matrix\n    diagonal_matrix = np.diag(diagonal_elements)\n    return diagonal_matrix\n\n# Example usage\ntensor_2d = load_data()\ndiagonal_matrix_2d = diagonal_matrix(tensor_2d)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=0)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=1)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=2)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=3)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=4)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=5)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=6)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=7)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=8)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=9)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=10)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=11)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=12)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=13)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=14)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=15)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=16)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=17)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=18)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=19)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=20)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=21)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=22)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=23)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=24)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=25)\n\n# Convert to tensor\ntensor_3d = np.expand_dims(diagonal_matrix_2d, axis=26)\n\n# Convert to tensor\ntensor_3d = np.expand_dim", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": "\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    diagonal_size = t.shape[0]\n    diagonal_matrix = np.eye(diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size, diagonal_size, diagonal_size)\n    result = np.tile(diagonal_matrix, (batch_size, 1, 1))\n    # Create a 3D tensor with shape (batch_size,", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "a = np.array([[1, 2, 3]])\nb = np.array([[4, 5, 6]])\nab = np.stack((a, b), axis=0)\nprint(ab)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nab = np.stack((a, b), axis=0)\nprint(ab)\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": "\n    # Stack the tensors a and b vertically\n    ab = np.vstack((a, b))\n    ### END SOLUTION\n    return ab\n\n# Test the function\na = np.random.rand(1, 11)\nb = np.random.rand(1, 11)\nprint(solve(a, b))\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "a[ : , lengths : , : ] = 0\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "a[:, lengths, :] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "a[:, :lengths, :] = 0\n", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "a[:, :lengths, :] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "tensor_of_tensors = torch.tensor(list_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "import numpy as np\nimport torch\n\ndef load_data():\n    return [torch.tensor([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9])]\n\nlist = load_data()\nnew_tensors = np.array(list)\nprint(new_tensors)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": "\n    return np.array(lt)\n    ### END SOLUTION\n\n# Test the function\nlist_of_tensors = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])]\nprint(Convert(list_of_tensors))\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "tensor_of_tensors = np.array(list_of_tensors)\ntensor_of_tensors = torch.tensor(tensor_of_tensors)\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "import numpy as np\nimport torch\n\ndef index_tensor(tensor, idx):\n    return np.take(tensor, idx)\n\nt = np.array([[-0.2,  0.3],\n    [-0.5,  0.1],\n    [-0.4,  0.2]])\nidx = np.array([1, 0, 1])\n\nresult = index_tensor(t, idx)\nprint(result)\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "import numpy as np\nimport torch\n\ndef load_data():\n    t = np.array([[-22.2,  33.3],\n                  [-55.5,  11.1],\n                  [-44.4,  22.2]])\n    idx = np.array([1, 1, 0])\n    return t, idx\n\ndef index_tensor(tensor, idx):\n    return tensor[idx]\n\nt, idx = load_data()\nresult = index_tensor(t, idx)\nprint(result)\n", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "import numpy as np\nimport torch\n\ndef load_data():\n    t = np.array([[-0.2,  0.3],\n                  [-0.5,  0.1],\n                  [-0.4,  0.2]])\n    idx = np.array([1, 0, 1])\n    return t, idx\n\ndef index_tensor(tensor, idx):\n    return np.take(tensor, idx)\n\nt, idx = load_data()\nresult = index_tensor(t, idx)\nprint(result)\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\n# Get the indices of the maximum scores\nids = np.argmax(x, axis=1)\n\n# Select the scores corresponding to the selected indices\nresult = x[np.arange(x.shape[0]), ids]\n\n# Reshape the result to match the shape of x\nresult = result.reshape(x.shape[0], 2)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\n# Get the indices of the maximum scores\nmax_scores_idx = np.argmax(x, axis=1)\n\n# Select the scores corresponding to the maximum indices\nmax_scores = x[np.arange(x.shape[0]), max_scores_idx, :]\n\n# Reshape the scores to match the shape of x\nresult = max_scores.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1, x.shape[2])\n\n# Reshape the result to match the shape of x\nresult = result.reshape(-1", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "# Get the indices ofbolds marked dimension of x which should be selected\nselected_ids = np.argmax(x, axis=2)\n# Select the scores for each element\nscores = x[np.arange(x.shape[0]), selected_ids, :]\n# Select the scores with the highest score\nresult = scores[np.argmax(scores, axis=1)]\n", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "import numpy as np\nimport torch\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\ndef load_data():\n    return np.array([[0.2, 0.1, 0.7],\n                     [0.6, 0.2, 0.2],\n                     [0.1, 0.8, 0.1]])\n\ndef get_max_index(x):\n    return np.argmax(x)\n\ndef get_max_probability(x):\n    return np.max(x)\n\ndef get_classes(softmax_output):\n    max_index = get_max_index(softmax_output)\n    return np.argmax(softmax_output, axis=1)\n\nsoftmax_output = load_data()\ny = get_classes(softmax_output)\nprint(y)\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "import numpy as np\nimport torch\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\ndef load_data():\n    return np.array([[0.7, 0.2, 0.1],\n                     [0.2, 0.6, 0.2],\n                     [0.1, 0.1, 0.8]])\n\ndef get_classes(softmax_output):\n    return np.argmax(softmax_output, axis=1)\n\nsoftmax_output = load_data()\ny = get_classes(softmax_output)\nprint(y)\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "import numpy as np\nimport torch\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\ndef get_classes(softmax_output):\n    return np.argmax(softmax_output, axis=1)\n\nsoftmax_output = np.array([[0.2, 0.1, 0.7],\n                           [0.6, 0.3, 0.1],\n                           [0.15, 0.8, 0.05]])\n\nsoftmax_output = np.array([[0.2, 0.1, 0.7],\n                           [0.6, 0.3, 0.1],\n                           [0.15, 0.8, 0.05]])\n\nsoftmax_output = softmax(softmax_output)\n\nclasses = get_classes(softmax_output)\n\nprint(classes)\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": "\n    max_prob = np.argmax(softmax_output, axis=1)\n    return max_prob\n    ### END SOLUTION\n\n# Test the function\nsoftmax_output = np.array([[0.2, 0.1, 0.7],\n                           [0.6, 0.2, 0.2],\n                           [0.1, 0.8, 0.1]])\nprint(solve(softmax_output))\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "import numpy as np\nimport torch\n\n# Define the input tensor\ninput_tensor = np.array([[0.2, 0.1, 0.7],\n                         [0.6, 0.3, 0.1],\n                         [0.15, 0.8, 0.05]])\n\n# Convert the input tensor to a tensor\ntensor = torch.tensor(input_tensor)\n\n# Apply softmax to the tensor\nsoftmax_output = torch.softmax(tensor, dim=1)\n\n# Find the index of the minimum value in each row\nmin_indices = np.argmin(softmax_output.numpy(), axis=1)\n\n# Create a tensor to store the result\nresult = np.zeros(softmax_output.shape[0], dtype=int)\n\n# Assign the index of the minimum value to the result tensor\nresult = min_indices\n\n# Convert the result to a tensor\nresult_tensor = np.array(result)\n\n# Print the result\nprint(result_tensor)\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "def cross_entropy2d(input, target):\n    n, c, h, w = input.shape\n    target = target.reshape(-1)\n    input = input.reshape(-1, c)\n    target = target.astype(np.int64)\n    input = input.astype(np.float32)\n    target = target.astype(np.float32)\n    target = target.reshape(-1)\n    input = input.reshape(-1, c)\n    target = target.astype(np.int64)\n    target = target.reshape(-1)\n    input = input.astype(np.float32)\n    target = target.astype(np.float32)\n    target = target.reshape(-1)\n    input = input.astype(np.float32)\n    target = target.astype(np.int64)\n    target = target.reshape(-1)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target = target.astype(np.int64)\n    target", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "def check_equal(A, B):\n    return np.sum(A == B)\n\ncnt_equal = check_equal(A, B)\nprint(cnt_equal)\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "def count_equal(A, B):\n    return np.sum(A == B)\n\nA = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\nB = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\nprint(count_equal(A, B))\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "def check_not_equal(A, B):\n    return np.sum(A != B)\n\nA = np.random.rand(1000, 1)\nB = np.random.rand(1000, 1)\n\ncnt_not_equal = check_not_equal(A, B)\nprint(cnt_not_equal)\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": "\n    return np.sum(A == B)\n    ### END SOLUTION\n\n# Test the function\nA = np.random.rand(1000)\nB = np.random.rand(1000)\nprint(Count(A, B))\n## END\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND\nEND", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "def check_last_elements(A, B):\n    return np.sum(A[-A.shape[0]:] == B[-A.shape[0]:])\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[1, 2], [3, 5]])\nprint(check_last_elements(A, B))\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "def check_last_elements(A, B):\n    return np.sum(A[-A.shape[0]:] != B[-A.shape[0]:])\n\nA, B = load_data()\ncnt_not_equal = check_last_elements(A, B)\nprint(cnt_not_equal)\n", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensor_i = a[:, :, i*10:(i+1)*10, :, :]\n    tensors_31.append(tensor_i)\ntensors_31 = np.array(tensors_31)\nprint(tensors_31.shape)\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\ntensors_31 = []\nfor i in range(chunk_dim):\n    tensor_i = a[:, :, i, :, :]\n    tensors_31.append(tensor_i)\ntensors_31 = np.array(tensors_31)\nprint(tensors_31.shape)\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "output = np.where(mask, clean_input_spectrogram, clean_input_spectrogram)\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "output = np.where(mask == 0, clean_input_spectrogram, output)\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "sign_x = np.sign(x)\nsign_y = np.sign(y)\nmin_abs = np.minimum(np.abs(x), np.abs(y))\nsign_min = np.where(np.abs(x) < np.abs(y), sign_x, sign_y)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) < np.abs(x), sign_y, sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(y) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min, -sign_min)\nsign_min = np.where(np.abs(x) == min_abs, sign_min", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "import numpy as np\nimport torch\n\ndef sign_max(x, y):\n    sign_x = np.sign(x)\n    sign_y = np.sign(y)\n    max_abs = np.abs(x) if np.abs(x) > np.abs(y) else np.abs(y)\n    return sign_x * max_abs, sign_y * max_abs\n\nsign_x, sign_y = sign_max(x, y)\nmax_abs = np.abs(x) if np.abs(x) > np.abs(y) else np.abs(y)\n\nsign_max_abs = np.sign(max_abs)\n\nsign_max_abs_x, sign_max_abs_y = sign_max(x, y)\n\nprint(sign_max_abs)\nprint(sign_max_abs_x)\nprint(sign_max_abs_y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": "\n    sign_x = np.sign(x)\n    sign_y = np.sign(y)\n    min = np.minimum(np.abs(x), np.abs(y))\n    sign_min = np.where(np.abs(x) < np.abs(y), sign_x, sign_y)\n    return sign_min * min\n    ### END SOLUTION\n\n# Test the function\nx = np.array([[1, 2], [3, 4]])\ny = np.array([[5, 6], [7, 8]])\nprint(solve(x, y))\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = np.max(output.reshape(1, 3), axis=1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\noutput = predict_allCharacters(np.random.rand(4))\nprint(output)\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "def merge_tensors(a, b):\n    # Get the shape of the tensors\n    shape_a = a.shape\n    shape_b = b.shape\n    \n    # Calculate the overlap\n    overlap_rows = min(shape_a[0], shape_b[0])\n    overlap_cols = min(shape_a[1], shape_b[1])\n    \n    # Create a new tensor to store the result\n    result = np.zeros((overlap_rows, overlap_cols + 2))\n    \n    # Iterate over the overlap rows\n    for i in range(overlap_rows):\n        # Calculate the overlap columns\n        overlap_cols_a = min(shape_a[1], overlap_cols)\n        overlap_cols_b = min(shape_b[1], overlap_cols)\n        \n        # Calculate the average\n        average = (a[i, overlap_cols_a] + b[i, 0]) / 2\n        \n        # Copy the overlap columns from a and b\n        result[i, :overlap_cols] = a[i, :overlap_cols]\n        result[i, overlap_cols] = average\n        result[i, overlap_cols + 1] = b[i, overlap_cols + 1]\n    \n    return result\n\n# Test the function\na = np.array([[1, 2, 3], [1, 2, 3]])\nb = np.array([[5, 6, 7], [5, 6, 7]])\nprint(merge_tensors(a, b))\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": "\n    # Calculate the average of the last column of 'a' and the first column of 'b'\n    average = (a[:, -1] + b[:, 0]) / 2\n    # Create a new column with the average\n    new_col = np.column_stack((a[:, :-1], average))\n    # Return the new column\n    return new_col\n    ### END SOLUTION\n\n# Test the function\na = np.array([[1, 2, 3], [1, 2, 3]])\nb = np.array([[5, 6, 7], [5, 6, 7]])\nprint(solve(a, b))\n## END\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n\nEND\n", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "import numpy as np\nimport pandas as pd\nimport torch\n\n# Define the tensor\nt = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n\n# Convert to tensor\ntensor = np.array(t)\ntensor = np.reshape(tensor, (1, 4, 2))\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)\ntensor = np.stack((tensor, np.zeros((1, 1))), axis=2)", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "import numpy as np\nimport pandas as pd\nimport torch\n\ndef load_data():\n    t = np.array([[1, 2], [3, 4]])\n    return t\n\ndef stack_tensor(t, new):\n    return np.stack((t, new), axis=0)\n\nt = load_data()\nnew = np.array([[0., 0., 0., 0.]])\nprint(stack_tensor(t, new))\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "import numpy as np\nimport pandas as pd\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy as np\nimport numpy", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = np.dot(data.numpy(), W.flatten())\nresult = result.reshape(data.shape[0], data.shape[1], hid_dim)\nresult = result.view(data.shape[0], data.shape[1], hid_dim)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
