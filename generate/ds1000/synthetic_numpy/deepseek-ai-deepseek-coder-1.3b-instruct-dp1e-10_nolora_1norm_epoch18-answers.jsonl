{"id": 0, "code": "\ndf['Type'] = df.apply(lambda row: List[row['Type']], axis=1)\ndf = df.reindex(df['Type'])\ndf = df.drop(columns=['Type'])\n\nresult = df\n", "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": "\ndf['Type_Order'] = df.groupby('Type')['Type'].cumcount()\ndf_shuffled = df.reindex(List)\ndf_shuffled = df_shuffled.merge(df[df.Type_Order==0].groupby('Type').first_valid(), on='Type')\ndf_shuffled = df_shuffled.drop(columns=['Type_Order'])\n\nresult = df_shuffled\n", "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": "\n# Create a function to replace values\ndef replace_values(df):\n    for col in df.columns:\n        df[col] = df[col].replace(df[col].value_counts().index[df[col].value_counts() < 2], 'other')\n    return df\n\n# Apply the function to the dataframe\ndf = replace_values(df)\n\n# Show the result\nprint(df)\n", "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": "\n# Create a function to check the value counts\ndef check_value_counts(df):\n    value_counts = df.value_counts()\n    for i in value_counts.index:\n        if value_counts[i] < 3:\n            df[df == i] = 'other'\n    return df\n\n# Apply the function to the columns\ndf['Qu1'] = check_value_counts(df['Qu1'])\ndf['Qu2'] = check_value_counts(df['Qu2'])\ndf['Qu3'] = check_value_counts(df['Qu3'])\n\n# Show the result\ndf\n", "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": "\n    for col in df.columns:\n        df[col] = df[col].apply(lambda x: 'other' if x in df[col].value_counts()[df[col].value_counts()>=2].index else x)\n    return df\n\ndf = f(example_df)\ndf\n", "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": "\n# Create a dictionary to store the new values\nnew_values_dict = {}\n\n# Iterate over the columns\nfor col in df.columns:\n    # Get the value counts\n    value_counts = df[col].value_counts()\n    \n    # If the count is greater than or equal to 3\n    if value_counts[0] >= 3:\n        # If the value is not in the dictionary, add it\n        if col not in new_values_dict:\n            new_values_dict[col] = value_counts[1]\n\n# Create a new series with the new values\nnew_series = pd.Series(new_values_dict)\n\n# Apply the new series to the original dataframe\ndf[df.columns.drop('Qu1')] = new_series.index.get(df.columns.drop('Qu1')[0])\n\n# Create the final dataframe\nresult = df\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": "\n# Create a dictionary to store the replacements\nreplacements = {}\n\n# Iterate over the columns\nfor col in df.columns:\n    # Get the values in the column\n    values = df[col]\n    # Get the value_counts\n    counts = values.value_counts()\n    # If the count is greater than or equal to 3\n    if counts[counts>=3] != None:\n        # Replace the values with 'other'\n        df[col] = df[col].replace(counts[counts>=3].index, 'other')\n        # Store the replacement in the dictionary\n        replacements[col] = counts[counts>=3].index\n\n# Apply the replacements to the dataframe\ndf[list(replacements.keys())] = df[list(replacements.keys())].replace(replacements)\n\n# Get the final result\nresult = df\n", "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": "\ndf['dup_url'] = df.duplicated(subset='url', keep=False)\ndf['keep_if_dup'] = df['keep_if_dup'].map({'Yes': True, 'No': False})\n\nresult = df[~df['dup_url'] & df['keep_if_dup']]\n\ndf.drop(columns=['dup_url', 'keep_if_dup'])\n", "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": "\ndf.drop_duplicates(subset='url', keep='first', inplace=True)\ndf.drop(df[df.duplicated(subset='url')].index, inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": "\ndf['dup_col'] = df.duplicated(subset='url', keep='last')\ndf = df[df['dup_col'] == True]\ndf = df.drop(columns=['dup_col'])\n\nresult = df\n", "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": "\ndef df_to_dict(df):\n    result_dict = {}\n    for i in df['name'].unique():\n        temp_dict = {}\n        temp_df = df[df['name'] == i]\n        for j in temp_df['v1'].unique():\n            temp_dict[j] = {}\n            temp_df_sub = temp_df[temp_df['v1'] == j]\n            for k in temp_df_sub['v2'].unique():\n                temp_dict[j][k] = temp_df_sub[tempumerate(temp_df_sub['v2'] == k)['v3'][0]]\n        result_dict[i] = temp_dict\n    return result_dict\n\ndf_to_dict(df)\n", "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": "\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    return df\n\nexample_df = f(example_df)\nexample_df\n", "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": "df['datetime'] = df['datetime'].dt.tz_convert(None)\n", "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": "\ndf['message'] = df['message'].str.replace('[','').str.replace(']','').str.split(', ')\ndf['job'] = df['message'][0]\ndf['money'] = df['message'][1]\ndf['wife'] = df['message'][2]\ndf['group'] = df['message'][3]\ndf['kids'] = df['message'][4]\n\ndf = df[['name', 'status', 'number', 'job', 'money', 'wife', 'group', 'kids']]\n\nresult = df\n\n", "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": "df['score'] = df.apply(lambda row: row['score']*10 if row['product'] in products else row['score'], axis=1)\n", "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": "df['score'] = df['score'] * 10 if df['product'].isin(products) else df['score']\n", "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": "\ndf['score'] = df['product'].apply(lambda x: products[0][0] if x in products[0] else products[0][1])\ndf\n", "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": "df['score'] = df.loc[df['product'].isin(products), 'score'] / df.loc[df['product'].isin(products), 'score'].max()\ndf.loc[df['product'].isin(products), 'score'] *= df.loc[df['product'].isin(products), 'score'].min()\ndf['score'] = df.loc[df['product'].isin(products), 'score']\n", "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": "df['category'] = df.iloc[:, ::-1].sum(axis=1)\n", "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": "df['category'] = df[['A', 'B', 'C', 'D']].apply(lambda x: 'A' if x.sum() == 4 else 'B' if x.sum() == 3 else 'C' if x.sum() == 2 else 'D', axis=1)\n", "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": "df['category'] = df.apply(lambda row: [col for col in df.columns if row[col]==1], axis=1)\n", "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": "df['Date'] = df['Date'].dt.strftime('%b-%y')\n", "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": "df['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf", "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": "df['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf = df.iloc[:-1, :]\n", "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": "df['#1'] = df['#1'].shift(-1)\ndf['#2'] = df['#2'].shift(1)\ndf = df.iloc[:-1, :]\n", "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": "df['#1'] = df['#1'].shift(1, fill_value=0)\ndf['#2'] = df['#2'].shift(1, fill_value=0)\ndf", "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": "df['#1'] = df['#1'].shift(1)\ndf['#2'] = df['#2'].shift(1)\ndf = df.iloc[1:]\ndf.columns = [df.columns[0], df.columns[1]+'_shifted']\ndf\n", "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": "df.columns = [col + 'X' for col in df.columns]\n", "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": "df.columns = ['X' + col for col in df.columns]\n", "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": "df.columns = ['X' + col if col.endswith('X') else 'Header' + col for col in df.columns]\n", "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": "\ndef get_mean_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df_mean = df.groupby('group')[numeric_cols].mean()\n    return df_mean\n\nresult = get_mean_columns(df)\n", "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": "result = df.groupby('group')[df.columns[df.columns.get_dtype(include='object')]].sum()\nresult\n", "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": "\ndef get_agg_func(df):\n    agg_dict = {}\n    for col in df.columns:\n        if df[col].str.endswith('2') and col != 'group':\n            agg_dict[col] = 'mean'\n        else:\n            agg_dict[col] = 'sum'\n    return agg_dict\n\n\nresult = df.groupby('group').agg(get_agg_func(df))\n\nprint(result)\n", "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": "\ndef meanAdvance(df, row_list, column_list):\n    return df.loc[row_list, column_list].mean()\n\nresult = meanAdvance(df, row_list, column_list)\nprint(result)\n", "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": "\nresult = df.loc[row_list, column_list].sum()\n", "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": "\ndef sum_columns(df, row_list, column_list):\n    df_subset = df.loc[row_list, column_list]\n    return df_subset.sum()\n\nresult = sum_columns(df, row_list, column_list)\ndf = df.drop(df[df['q']==max(df['q'])].index)\n", "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": "\n# First, get the unique values of the 'temp' and 'name' columns\nunique_temp = df['temp'].unique()\nunique_name = df['name'].unique()\n\n# Then, get the counts of each unique value in the 'temp' and 'name' columns\ncounts_temp = df['temp'].value_counts()\ncounts_name = df['name'].value_counts()\n\n# Combine the counts into a single Series\nresult = pd.Series(index=pd.concat([unique_temp, unique_name]), data=pd.concat([counts_temp, counts_name]))\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": "\n# First, calculate the counts of 'null' in each column\ncounts = df.apply(lambda x: x.value_counts() if x.name == 'null' else None)\n\n# Then, create a new DataFrame with only the columns with 'null'\nresult = df[counts.index.tolist()]\n\n# Finally, drop the 'null' column from the original DataFrame\ndf.drop('null', axis=1, inplace=True)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": "\ndef get_value_counts(df):\n    result = \"\"\n    for col in df.columns:\n        if df[col].nunique() == 1:\n            result += f\"---- {col} ---\\n{df[col].value_counts()}\\n\"\n    return result\n\nresult = get_value_counts(df)\nprint(result)\n", "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": "df = df.iloc[2:4, :]\ndf.columns = df.loc[0, :]\ndf = df.iloc[:, 0:2]\ndf = df.append(df.iloc[0])\ndf = df.iloc[1:4, :]\ndf", "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": "df = df.iloc[1:3, :]\ndf.columns = df.loc[0, :]\ndf = df.iloc[:1, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :]\ndf.columns = df.loc[0, :]\ndf = df.reset_index(drop=True)\ndf = df.iloc[0:1, :", "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": "df.loc[df.notnull().any(axis=1), :] = df.loc[df.notnull().any(axis=1), :] + 1\ndf.loc[df.isnull().any(axis=1), :] = df.loc[df.isnull().any(axis=1), :] - 1\nresult = df\n", "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": "df = df.apply(lambda x: x.fillna(x.value_counts().index[0] if x.isnull().any() else np.nan),axis=0)\nresult = df\n", "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": "\ndf['0'] = df['0'].apply(lambda x: x if pd.notnull(x) else np.nan)\ndf\n", "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": "\nmask = df['value'] < thresh\ndf.loc['X'] = df.loc[mask, 'value'].sum()\ndf = df.loc[~mask]\nresult = df\n", "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": "\ndf_small = df[df['value'] < thresh]\ndf_large = df[df['value'] >= thresh]\n\ndf_large_avg = df_large['value'].mean()\n\ndf_large_avg_row = pd.DataFrame({'value': df_large_avg}, index=[thresh])\n\nresult = pdips.concat([df_small, df_large_avg_row])\n\nresult.reset_index(drop=True, inplace=True)\n\nEND SOLUTION\n", "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": "\n# create a mask for the rows to be replaced\nmask = (df['value'] >= section_left) & (df['value'] <= section_right)\n\n# apply the mask to the dataframe\ndf.loc[mask, 'value'] = df.loc[mask, 'value'].mean()\n\n# reset the index\ndf = df.reset_index()\n\n# set the index back to 'lab'\ndf.set_index('lab', inplace=True)\n\n# get the result\nresult = df\n", "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": "\ndf_inv = df.copy()\nfor col in df.columns:\n    df_inv[f'inv_{col}'] = 1/df[col]\n\nresult = df_inv\n\nprint(result)\n", "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": "\ndef exp_column(df):\n    for col in df.columns:\n        df['exp_'+col] = df[col]**2\n    return df\n\ndf = exp_column(df)\n\nprint(df)\n", "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": "\ndf['inv_A'] = df['A'].apply(lambda x: 1/x if x != 0 else 0)\ndf['inv_B'] = df['B'].apply(lambda x: 1/x if x != 0 else 0)\n\nresult = df\n", "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": "\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\ndf_sigmoid = df.apply(sigmoid)\ndf_sigmoid.columns = [f'sigmoid_{col}' for col in df.columns]\n\nresult = pd.concat([df, df_sigmoid], axis=1)\n", "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": "\nmax_mask = df.idxmax()\nmax_mask_last_min_mask = max_mask[max_mask != df.idxmin()[max_mask]]\n\nresult = df.index[max_mask_last_min_mask]\n", "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": "\nmax_locations = df.idxmax()\nmin_locations = df.idxmin()\n\n# get the first occurrence of the column-wise maximum\n# down to the location of the minimum\nresult = max_locations[max_locations < min_locations]\n\n# if the maximum is the first occurrence of the minimum\n# then the minimum is the first occurrence of the maximum\n# so the result is the minimum\nif len(result) == 0:\n    result = min_locations\n\nprint(result)\n", "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": "df['val'] = df.groupby('user')['val'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('max')\n\ndf = df.pivot_table(index='dt', columns='user', values='val')\ndf = df.fillna(0)\ndf", "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": "df['val'] = df.groupby('user')['val'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('max')\ndf['val'] = df.groupby('user')['val'].fillna(0)\ndf\n", "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": "df['val'] = df.groupby('user')['val'].transform('first')\ndf['val'] = df.groupby('user')['val'].transform('last')\ndf = df.groupby(['user','dt'])['val'].apply(list).reset_index()\ndf['val'] = df['val'].apply(lambda x: x[0] if len(x)>0 else '233')\ndf = df.pivot_table(index='dt', columns='user', values='val')\ndf = df.fillna(method='ffill').fillna(method='bfill')\ndf = df.groupby(level=0).transform('first')\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['dt'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['dt'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['dt'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['val'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['val'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['val'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['val'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['val'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['val'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['val'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['val'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['val'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['val'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['val'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['val'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['val'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['val'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['val'].max().reset_index(drop=True), on='user')\ndf = df.groupby('user')['val'].value_count()\ndf = df.reset_index()\ndf = df.merge(df.groupby('user')['val'].min().reset_index(drop=True), on='user')\ndf = df.merge(df.groupby('user')['", "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": "df['min_max'] = df.groupby('user')['dt'].transform('min').astype(str) + ' ' + df.groupby('user')['dt'].transform('max').astype(str)\ndf = df.set_index(['user','dt']).unstack().stack()\ndf = df.fillna(method='ffill').fillna(method='bfill')\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmin()))\ndf = df.groupby(level=0).apply(lambda x: x.fillna(x.value_count().idxmax()))\ndf =", "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": "df['dt'] = df.groupby('user')['dt'].transform('min')\ndf['val'] = df.groupby('user')['val'].transform('max')\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.groupby('user')['val'].apply(lambda x: x).reset_index(drop=True)\ndf = df.groupby('user')['dt'].apply(lambda x: x.dt.strftime('%d-%b-%Y')).reset_index(drop=True)\ndf = df.", "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": "df['name'] = df.groupby('name')['name'].transform('first')\ndf.drop(df.groupby('name')['name'].transform('first')-1, inplace=True)\ndf.sort_index(inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.reset_index", "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": "df['a'] = df.groupby('name')['a'].transform('first')\nresult = df\n", "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": "\n    df.loc[:, 'name'] = range(1, len(df)+1)\n    df.set_index('name', inplace=True)\n    return df\n    ### END SOLUTION\n\ndf = f()\nprint(df)\n", "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": "df['ID'] = df.groupby('name')['a'].ngroup()\ndf = df.drop(columns=['a'])\ndf = df.groupby('ID')[['b', 'c']].apply(lambda x: x.reset_index(drop=True))\ndf\n", "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": "df = df.pivot_table(index='user', columns='date', values='value', aggfunc='first')\ndf.columns = ['date', 'value']\ndf\n", "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": "df['others'] = df.groupby('user')['02/12/15'].shift()\ndf['value'] = df.groupby('user')['02/12/15']\ndf = df.drop(columns=['02/12/15'])\ndf", "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": "df = df.pivot_table(index='user', columns='date', values='value')\ndf.columns = ['date', 'value']\ndf = df.reset_index()\ndf\n", "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": "df_sub = df[df.c > 0.5][columns]\nresult = df_sub.values", "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": "df_sub = df[df.c > 0.45][columns]\nresult = df_sub.values", "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": "\n    df_sub = df.loc[df['c'] > 0.5][columns]\n    result = df_sub.values\n    ### END SOLUTION\n    return result\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ndf['c'] = df['c'] * 10\ndf['e'] = df['e'] * 10\ndf['f'] = df['e'] * 10\ndf['g'] = df['e'] * 10\ndf['h'] = df['e'] * 10\ndf['i'] = df['e'] * 10\ndf['j'] = df['e'] * 10\ndf['k'] = df['e'] * 10\ndf['l'] = df['e'] * 10\ndf['m'] = df['e'] * 10\ndf['n'] = df['e'] * 10\ndf['o'] = df['e'] * 10\ndf['p'] = df['e'] * 10\ndf['q'] = df['e'] * 10\ndf['r'] = df['e'] * 10\ndf['s'] = df['e'] * 10\ndf['t'] = df['e'] * 10\ndf['u'] = df['e'] * 10\ndf['v'] = df['e'] * 10\ndf['w'] = df['e'] * 10\ndf['x'] = df['e'] * 10\ndf['y'] = df['e'] * 10\ndf['z'] = df['e'] * 10\ndf['aa'] = df['e'] * 10\ndf['ab'] = df['e'] * 10\ndf['ac'] = df['e'] * 10\ndf['ad'] = df['e'] * 10\ndf['ae'] = df['e'] * 10\ndf['af'] = df['e'] * 10\ndf['ag'] = df['e'] * 10\ndf['ah'] = df['e'] * 10\ndf['ai'] = df['e'] * 10\ndf['aj'] = df['e'] * 10\ndf['ak'] = df['e'] * 10\ndf['al'] = df['e'] * 10\ndf['am'] = df['e'] * 10\ndf['an'] = df['e'] * 10\ndf['ao'] = df['e'] * 10\ndf['ap'] = df['e'] * 10\ndf['aq'] = df['e'] * 10\ndf['ar'] = df['e'] * 10\ndf['as'] = df['e'] * 10\ndf['at'] = df['e'] * 10\ndf['au'] = df['e'] * 10\ndf['av'] = df['e'] * 10\ndf['aw'] = df['e'] * 10\ndf['ax'] = df['e'] * 10\ndf['ay'] = df['e'] * 10\ndf['az'] = df['e'] * 10\ndf['aaa'] = df['e'] * 10\ndf['aab'] = df['e'] * 10\ndf['aac'] = df['e'] * 10\ndf['aad'] = df['e'] * 10\ndf['aae'] = df['e'] * 10\ndf['aaf'] = df['e'] * 10\ndf['aag'] = df['e'] * 10\ndf['aah'] = df['e'] * 10\ndf['aai'] = df['e'] * 10\ndf['aaj'] = df['e'] * 10\ndf['aak'] = df['e'] * 10\ndf['aal'] = df['e'] * 10\ndf['aam'] = df['e'] * 10\ndf['aan'] = df['e'] * 10\ndf['aao'] = df['e'] * 10\ndf['aap'] = df['e'] * 10\ndf['aq'] = df['e'] * 10\ndf['ar'] = df['e'] * ", "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": "\n    df_sub = df.loc[df['c'] > 0.5][columns]\n    df_sub['sum'] = df_sub[columns[0]] + df_sub[columns[1]]\n    return df_sub\n", "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": "\n    return df.loc[df['c'] > 0.5][columns]\n    ### END SOLUTION\n\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\ndf['c'] = df['c'] * 10\ndf['e'] = df['e'] * 10\n\ndf.loc[df['c'] > 0.5, 'e'] = df.loc[df['c'] > 0.5, 'e'] * 2\n\nprint(f(df))\n", "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": "\ndf['date'] = pd.to_datetime(df['date'])\ndf.sort_values('date', inplace=True)\n\ndf['check'] = df.groupby('ID')['date'].rank()\ndf_result = df[df['check'] <= X]\n\nresult = df_result[['ID', 'date', 'close']].reset_index(drop=True)\n\n", "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": "\ndef get_date_range(df, x):\n    dates = df['date'].apply(lambda date: pd.to_datetime(date) - pd.to_timedelta(x, unit='w'))\n    return dates\n\ndef remove_overlapping_dates(df, x):\n    dates = get_date_range(df, x)\n    df = df[~df['date'].isin(dates)]\n    return df\n\nresult = remove_overlapping_dates(df, X)\n", "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": "\ndef get_date_range(start_date, weeks):\n    return [start_date + timedelta(weeks=i) for i in range(weeks)]\n\ndf['date'] = pd.to_datetime(df['date'])\ndf.sort_values('date', inplace=True)\n\ndate_range = get_date_range(df['date'].iloc[0], X)\n\ndf = df[df['date'].isin(date_range)]\n\nresult = df\n", "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": "\ndf['col1'] = df.groupby(df.index // 3)['col1'].transform('mean')\nresult = df\n", "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": "\ndf['col1'] = df.groupby(df.index // 3)['col1'].transform('sum')\nresult = df\n", "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": "\ndf['col1'] = df.groupby(df.index // 4)['col1'].transform('sum')\nresult = df\n", "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": "\ndf['rolling_mean'] = df['col1'].rolling(window=3).mean()\ndf = df.drop(df.index[2])\nresult = df\n", "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": "\n# First, we need to create a group by object\ngroup_obj = df.groupby(df.index // 3)\n\n# Then, we can apply the functions to each group\nresult = group_obj.agg({'sum': 'sum', 'avg': 'mean'})\n\n# Finally, we need to reset the index\nresult.reset_index(drop=True, inplace=True)\n\n# Now, the result is a DataFrame with two columns: 'sum' and 'avg'\n", "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": "\ndf['rolling_sum'] = df['col1'].rolling(window=3).sum()\ndf['rolling_avg'] = df['col1'].rolling(window=2).mean()\n\nresult = df\n", "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": "df.loc[df.A == 0, 'A'] = df.loc[df.A != 0, 'A'].shift(-1)\ndf.loc[df.A == 0, 'A'] = df.loc[df.A != 0, 'A'].fillna(method='ffill')\n", "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": "df.loc[df.A==0, 'A'] = df.A[df.A != 0].values[0]\n", "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": "\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].groupby((df['A'] > 0).rolling(window=2).apply(lambda x: x.max())\ndf['A'] = df['A'].fillna(method='ffill').fillna(method='bfill')\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill').fillna(method='bfill')\n", "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": "df['time_days'] = df['duration'].str.extract('(\\d+)', expand=False).astype(int)\ndf['time'] = df['duration'].str.extract('(\\w+)', expand=False)\ndf\n", "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": "df['time'] = df['duration'].str.split().str[0]\ndf['time_day'] = df['duration'].str.split().str[1]\ndf['time_day'] = df['time_day'].apply(lambda x: int(x) if x.isdigit() else x)\ndf['time'] = df['time'].astype('category')\ndf['time_day'] = df['time_day'].astype('category')\ndf['numer'] = df['time'].replace(r'\\d.*', r'\\d', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'\\d.*', r'\\d', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'\\.w.+', r'\\w.+', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'\\s+', r' ', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True, inplace=True)\ndf['time_day'] = df['time_day'].replace(r'^\\s*$', r'0', regex=True,", "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": "\n    df['time'] = df['duration'].str.split(' ').str[0]\n    df['time_days'] = df['duration'].str.split(' ').str[1]\n    df['time_days'] = df['time_days'].apply(lambda x: int(x.replace('day', '').replace('week', '').replace('month', '')))\n    df['time'] = df['time'].map({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n    return df\n\ndf = f(example_df)\nprint(df)\n", "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": "df['time'] = df['duration'].str.split(' ').str[0]\ndf['number'] = df['duration'].str.split(' ').str[1]\ndf['time_day'] = df['time'].replace(r'(year|month|week|day)', r'(365|30|7|1)', regex=True, inplace=True)\ndf['time_day'] *= df['number'].astype(int)\ndf\n", "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": "\nresult = [(df1[column] == df2[column]).all() for column in columns_check_list]\n", "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": "\ndef check_columns(df1, df2, columns_check_list):\n    return [(df1[column] == df2[column]).all() for column in columns_check_list]\n\nresult = check_columns(df1, df2, columns_check_list)\nprint(result)\n", "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level='date')\n", "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": "df.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n", "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": "\n    df['date'] = pdipsy.to_datetime(df['date'])\n    df.set_index('date')\n    return df\n", "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": "\n    df.index = pdips.to_datetime(df.index)\n    df.columns.names = ['date','y']\n    df = df.swaplevel(0,1)\n    return df\n", "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": "df_melt = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf_pivot = df_melt.pivot_table(index=['Country', 'year'], columns='Variable', values='value')\ndf_pivot.columns = df_pivot.columns.get_level_values(1)[::-1]\ndf_pivot.reset_index(inplace=True)\ndf_pivot.columns = ['Country', 'year', 'var1', 'var2']\ndf_pivot\n", "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": "df_melt = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf_pivot = df_melt.pivot_table(index=['Country', 'year'], columns='Variable', values='value')\ndf_pivot.columns.name = None\ndf_pivot.reset_index()\n", "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": "df = df.abs().gt(1).all(axis=1)\n", "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": "df = df.abs().gt(1).all(axis=1)\n", "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": "df = df.abs().gt(1).any(axis=1)\ndf = df.loc[df.abs().gt(1).any(axis=1)]\n", "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": "df.replace('&AMP;', '&', regex=True)\n", "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": "df.replace(to_replace='&LT;', \n            replacement='<', \n            regex=True, \n            inplace=True)\n", "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": "\n    df.replace(to_replace='&AMP;', \n                replacement='&', \n                regex=True, \n                inplace=True)\n    return df\n\nf(example_df)\n", "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": "df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True, inplace=True)\n", "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": "df.replace({'&AMP;': ' & '}, regex=True, inplace=True)\n", "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": "\ndef validate_single_space_name(name: str) -> str:\n    pattern = re.compile(r'^.*( ){1}.*$')\n    match_obj = re.match(pattern, name)\n    if match_obj:\n        return name\n    else:\n        return None\n\ndf['first_name'], df['last_name'] = zip(*df['name'].apply(validate_single_space_name))\ndf = df.drop(columns='name')\ndf", "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": "df['1_name'] = df['name'].apply(lambda x: x.split(' ')[0])\ndf['2_name'] = df['name'].apply(lambda x: x.split(' ')[1])\ndf\n", "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": "\ndef split_name(name):\n    if ' ' in name:\n        first_name, last_name = name.split(' ', 1)\n        return first_name, last_name, None\n    else:\n        return None\n\ndf['first_name'], df['middle_name'], df['last_name'] = zip(*df['name'].apply(split_name))\ndf = df.drop(columns='name')\ndf.columns = ['first_name', 'middle_name', 'last_name']\ndf\n", "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": "df1.set_index('Timestamp')\ndf2.set_index('Timestamp')\n\ndf2['data'] = df1.loc[df2['Timestamp']]\n\ndf2.drop('Timestamp', axis=1, inplace=True)\n\ndf2.reset_index(drop=True)\n", "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": "df1.merge(df2, on='Timestamp')\n", "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": "df['state'] = np.where(df['col2'] <= df['col3'], df['col1'], df['col3'].max())\n", "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": "df['state'] = np.where(df['col2'] > df['col3'], df['col1'], df['col1'] + df['col2'] + df['col3'])\n", "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": "\ndf['Field1'] = df['Field1'].astype(str)\ndf['Field1'] = df['Field1'].apply(lambda x: x if pd.to_numeric(x, errors='coerce') is not None else 'and')\ndf\n", "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": "\ndf['Field1'] = df['Field1'].astype(str)\ndf['Field1'] = df['Field1'].apply(lambda x: [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, list) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.split(',') if i.isdigit()])\ndf['Field1'] = df['Field1'].apply(lambda x: x if isinstance(x, str) else [int(i) for i in x.", "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": "\n    return df[df['Field1'].apply(lambda x: isinstance(x, int))]\n    ### END SOLUTION\n\ndf_with_errors = f()\nprint(df_with_errors)\n", "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": "df['val1_percent'] = df['val1'] / df.groupby('cat')['val1'].transform('sum') * 100\ndf['val2_percent'] = df['val2'] / df.groupby('cat')['val2'].transform('sum') * 100\ndf['val3_percent'] = df['val3'] / df.groupby('cat')['val3'].transform('sum') * 100\ndf['val4_percent'] = df['val4'] / df.groupby('cat')['val4'].transform('sum') * 100\n\ndf\n", "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": "df['val1_percent'] = df['val1'] / df['val1'].sum() * 100\ndf['val2_percent'] = df['val2'] / df['val2'].sum() * 100\ndf['val3_percent'] = df['val3'] / df['val3'].sum() * 100\ndf['val4_percent'] = df['val4'] / df['val4'].sum() * 100\n\ndf = df.drop(columns=['val1', 'val2', 'val3', 'val4'])\n\ndf = df.rename(columns={'val1_percent': 'val1', 'val2_percent': 'val2', 'val3_percent': 'val3', 'val4_percent': 'val4'})\n\ndf", "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": "result = df.loc[test]\n", "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": "result = df.loc[test]\n", "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": "df = df.loc[~df.index.isin(test)]", "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": "\n    return df.loc[test]\n    ### END SOLUTION\n\ndf = pdips.DataFrame(...)\ntest = ['TP3','TP12','TP18', 'TP3']\n\ndf.loc[test]\n", "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": "\ndf['euclidean_distance'] = df.groupby('time')['car'].apply(lambda x: x.apply(lambda y: ((x.x-y.x)**2 + (x.y-y.y)**2)**0.5))\ndf['nearest_neighbour'] = df.groupby('time')['euclidean_distance'].transform('min')\ndf\n", "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": "\n# First, let's calculate the Euclidean distance for each car\ndf['euclidean_distance'] = ((df['x'] - df['x'].mean())**2 + (df['y'] - df['y'].mean())**2)**0.5\n\n# Then, group by 'time' and 'car', and apply 'min' function to get the least distance for each car\ndf['farmost_neighbour'] = df.groupby('time')['euclidean_distance'].transform('min')\n\n# Finally, group by 'time' and 'car', and apply 'mean' function to get the average of least distance for each car\ndf['average_distance'] = df.groupby('time')['euclidean_distance'].transform('mean')\n\ndf\n", "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)\ndf\n", "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().astype(str)), axis=1)\ndf\n", "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().astype(str)), axis=1)\ndf\n", "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": "df['keywords_all'] = df[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].apply(lambda x: '-'.join(x.dropna().astype(str)), axis=1)\ndf\n", "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": "\ndf['Quantity'] = df.sample(frac=0.2, random_state=0)['Quantity'].apply(lambda x: 0 if x==0 else x)\ndf\n", "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": "\ndf['ProductId'] = df.groupby('UserId')['ProductId'].transform('first')\ndf['ProductId'].mask(df['UserId'].duplicated(), 0)\ndf = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\n", "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": "\ndf['Quantity'] = 0\ndf = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2, random_state=0))\ndf['Quantity'] = 0\n", "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False) * df.index\ndf\n", "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False) * df.index\ndf\n", "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": "\n    df_dup = df.duplicated(subset=['col1','col2'], keep=False)\n    df_dup_bool = df_dup.duplicated(keep=False)\n    df_dup_bool_inv = df_dup_bool.map(lambda x: not x)\n    df_dup_bool_inv_inv = df_dup_bool_inv.map(lambda x: not x)\n    df_dup_bool_inv_inv_inv = dfumerate(df_dup_bool_inv_inv.index)\n    df_dup_bool_inv_inv_inv_inv = df_dup_bool_inv_inv_inv.map(lambda x: x+1)\n    df_dup_bool_inv_inv_inv_inv_inv = df_dup_bool_inv_inv_inv_inv.map(lambda x: x+1)\n    df_dup_bool_inv_inv_inv_inv_inv_inv = df_dupumerate(df_dup_bool_inv_inv_inv_inv.index)\n    df_dup_bool_inv_inv_inv_inv_inv_inv_inv = df_dup_bool_inv_inv_inv_inv_inv_inv.map(lambda x: x+1)\n    df_dup_bool_inv_inv_inv_inv_inv_inv_inv_inv = df_dup_bool_inv_inv_inv_inv_inv_inv_inv.map(lambda x: x+1)\n    df_dup_bool_inv_inv_inv_inv_inv_inv_inv_inv_inv = df_dup_bool_inv_inv_inv_inv_inv_inv_inv_inv.map(lambda x: x+1)\n    df_dup_bool_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv = df_dup_bool_inv_inv_inv_inv_inv_inv_inv_inv_inv.map(lambda x: x+1)\n    df_dup_bool_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv = df_dup_bool_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv.map(lambda x: x+1)\n    df_dup_bool_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv_inv", "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": "df['index_original'] = df.duplicated(subset=['col1','col2', '3col'], keep=False)\ndf\n", "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": "df['index_original'] = df.duplicated(subset=['col1','col2'], keep=False) * df.index\ndf\n", "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": "\nmax_count_df = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nmax_count_df = max_count_df[max_count_df['count'] == df['count'].max()]\nresult = max_count_df\n", "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": "\nmax_count_df = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nresult = df.loc[df['count'] == max_count_df['count'].max()]\n\nEND SOLUTION\n\nThis solution first groups the dataframe by 'Sp' and 'Mt' and then finds the maximum count in each group. It then uses this maximum count to filter the original dataframe to get the rows with the maximum count in each group.\n\nThe 'reset_index' function is used to reset the index of the grouped dataframe to get the maximum count in each group.\n\nThe 'loc' function is used to get the rows from the original dataframe that have the maximum count in each group.\n\nThe 'max_count_df' variable contains the maximum count in each group. The 'result' variable contains the rows from the original dataframe that have the maximum count in each group.\n", "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": "\ngrouped_df = df.groupby(['Sp','Mt'])\nmin_count_df = grouped_df['count'].transform('min')\nresult = df[df['count'] == min_count_df]\n\nresult\n", "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": "\nmax_count_df = df.groupby(['Sp','Value'])['count'].max().reset_index()\nmax_count_df = df[df['count'] == max_count_df['count'].max()]\nresult = max_count_df\n", "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": "result = df.query(\"Category in @filter_list\")\n", "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": "result = df.query(\"Category not in @filter_list\")\n", "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": "\n# Create a list of tuples where each tuple contains the first column level, the second column level and the third column level\nvalue_vars_list = [('A', 'B', 'E'),\n                   ('A', 'B', 'F'),\n                   ('A', 'C', 'G'),\n                   ('A', 'C', 'H'),\n                   ('A', 'D', 'I'),\n                   ('A', 'D', 'J')]\n\n# Apply the pd.melt function with the list of tuples\nresult = pd.melt(df, value_vars=value_vars_list)\n", "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": "\n# Create a list of tuples where each tuple contains the first column level, the second column level, and the third column level\nvalue_vars = [('col1', 'col2', 'col3'), ('col4', 'col5', 'col6')]\n\n# Use the pivot function to melt the DataFrame\nresult = df.pivot_table(index=value_vars[0][0], columns=value_vars[0][1], values=value_vars[0][2])\n\n# Iterate over the list of tuples\nfor i in range(1, len(value_vars)):\n    # Add the pivot_table for the current tuple to the result\n    result = result.join(df.pivot_table(index=value_vars[i][0], columns=value_vars[i][1], values=value_vars[i][2]))\n\n# Reset the index and drop the old columns\nresult.reset_index(drop=True)\n", "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf\n", "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf\n", "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf\n", "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": "df['cummax'] = df.groupby('id')['val'].cumsum().values\ndf\n", "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": "df['cumsum'] = df.groupby('id')['val'].cumsum()\ndf['cumsum'] = df['cumsum'].where(df['val']>=0, 0)\ndf", "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": "\ndf['v'] = df.groupby('l')['v'].transform('sum')\ndf['v'].fillna(np.nan, inplace=True)\ndf['v'].groupby(level=0).sum()\n\nresult = df\n", "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": "\ndf.groupby('r')['v'].apply(lambda x: x.sum() if np.isnan(x.sum()) else x.sum())\n", "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": "\ndf['v'] = df.groupby('l')['v'].transform(lambda x: x.sum() if np.isnan(x.sum()) else x.sum())\ndf\n", "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": "\ndef get_relations(df):\n    result = []\n    for c1 in df.columns:\n        for c2 in df.columns:\n            if c1 < c2:\n                if df[c1].nunique() < df[c2].nunique():\n                    result.append(f'{c1} {c2} one-to-many')\n                elif df[c1].nunique() > df[c2].nunique():\n                    result.append(f'{c1} {c2} many-to-one')\n                else:\n                    result.append(f'{c1} {c2} many-to-many')\n    return result\n\nresult = get_relations(df)\nprint(result)\n", "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": "\ndef get_relations(df):\n    result = []\n    for c1 in df.columns:\n        for c2 in df.columns:\n            if c1 < c2:\n                if df[c1].nunique() == 1:\n                    result.append(f'{c1} {c2} one-2-one')\n                elif df[c1].nunique() > 1 and df[c2].nunique() == 1:\n                    result.append(f'{c1} {c2} one-2-many')\n                else:\n                    result.append(f'{c1} {c2} many-2-many')\n    return result\n\nresult = get_relations(df)\nprint(result)\n", "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": "\ndef relationship(df):\n    result = pd.DataFrame(index=df.columns, columns=df.columns)\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if df[col1].nunique() > 1 and df[col2].nunique() > 1:\n                if df[col1].mode().values[0] in df[col2].unique():\n                    result.loc[col1, col2] = 'one-to-many'\n                elif df[col1].mode().values[0] in df[col1].unique():\n                    result.loc[col1, col2] = 'many-to-one'\n                else:\n                    result.loc[col1, col2] = 'many-to-many'\n    return result\n\nresult = relationship(df)\nresult\n", "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": "\ndef relationship(df):\n    df_rel = pd.DataFrame(index=df.columns, columns=df.columns)\n    for c1 in df.columns:\n        for c2 in df.columns:\n            df_rel.loc[c1, c2] = 'one-2-many' if c1 == c2 else 'one-2-one' if c1 == df[c2].value_counts().idxmax() else 'many-2-many' if c2 == df[c1].value_counts().idxmax() else 'many-2-one'\n    return df_rel\n\nresult = relationship(df)\n", "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": "\n# get the index of unique values, based on firstname, lastname, email\nuniq_indx = df.drop_duplicates(subset=['firstname', 'lastname', 'email']).index\n\n# save unique records\nresult = df.loc[uniq_indx]\n\n# check the result\nprint(result)\n", "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": "\n\n\nprint(\"Hello World\")\n\n# This is a simple python program that prints \"Hello World\"\n# It is a command-line based program that runs in the terminal\n# It is a very basic program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text \"Hello World\"\n# It is a very simple program that does not do anything more than print the text", "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": "df['Family'] = (df['SibSp'] > 0) | (df['Parch'] > 0)\ndf.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": "df['Family'] = (df['Survived'] > 0) | (df['Parch'] > 0)\ndf.groupby('Family')['SibSp'].mean()\n", "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": "\ndf['Family'] = (df['SibSp'] == 1) & (df['Parch'] == 1)\ndf.groupby('Family')['Survived'].mean()\n", "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": "\ndf.sort_values(by=['cokey','A'], inplace=True)\ndf.groupby('cokey').apply(lambda x: x.sort_values('A'))\n\nresult = df\n", "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": "\ndf['cokey'] = df.groupby('cokey')['A'].transform(lambda x: x.sort_values().index[0])\ndf = df.groupby('cokey').apply(lambda x: x.sort_values('A'))\ndf = df.reset_index(drop=True)\nresult = df\n", "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.tolist())\n", "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.tolist())\n", "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": "df.columns = pd.MultiIndex.from_tuples(df.columns.tolist())\ndf", "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": "df = pd.DataFrame(list(someTuple), columns=['birdType', 'birdCount'])\ndf.set_index('birdType', inplace=True)\nresult = df\n", "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": "\ndef std_dev(x):\n    return x.std()\n\ndef mean(x):\n    return x.mean()\n\ndf['mean'] = df.groupby('a')['b'].transform(mean)\ndf['std'] = df.groupby('a')['b'].transform(std_dev)\n\nresult = df\n", "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": "\ndef std_mean(x):\n    return x.std() / x.mean()\n\nresult = df.groupby('b')['a'].apply(std_mean)\n", "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": "\ndf['softmax'] = 1/(df['b'].values.astype(float))\ndf['min_max'] = (df['b'].values - df['b'].values.min()) / (df['b'].values.max() - df['b'].values.min())\n", "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": "\nmask = df.eq(0)\ndf = df.loc[~mask]\n\nresult = df\n", "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": "\nmask = df.sum(axis=0) > 0\ndf = df.loc[mask]\n\nresult = df\n", "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": "\nmax_value = df.max().max()\ndf = df[df.max() == max_value]\n\nresult = df\n", "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": "\nmax_value = df.max().max()\nmask = df == max_value\ndf.mask(mask, 0)\nresult = df\n", "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": "\n# First, we need to sort the series by its values in ascending order\nsorted_s = s.sort_values()\n\n# Then, we need to sort the series by its index in alphabetical order\nsorted_s_index = s.reindex(sorted_s.index)\n\n# Finally, we return the sorted series\nresult = sorted_s_index\n", "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": "df = s.reset_index().sort_values(by=['index'])\ndf.columns = ['index','1']\ndf\n", "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": "\ndf_result = df[df['A'].astype(str).astype('int') >= 1]\n\nresult = df_result\n", "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": "result = df[df['A'].astype(str) == 's']\n", "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": "\nmax_count_df = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nmax_count_df = max_count_df[max_count_df['count'] == df['count'].max()]\nresult = max_count_df\n", "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": "\nmax_count_df = df.groupby(['Sp','Mt'])['count'].max().reset_index()\nresult = df.merge(max_count_df, how='inner', on=['Sp','Mt'])\n", "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": "\ngrouped_df = df.groupby(['Sp','Mt'])\nmin_count_df = grouped_df['count'].transform('min')\nresult = df[df['count'] == min_count_df]\n\nresult\n", "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": "\nmax_count_df = df.groupby(['Sp','Value'])['count'].max().reset_index()\nmax_count_df = df[df['count'] == max_count_df['count'].max()]\nresult = max_count_df\n", "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": "df.loc[df['Member'].isin(dict.keys()), 'Date'] = df['Member'].map(dict)\ndf\n", "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'].fillna(pd.to_datetime('17/8/1926'), inplace=True)\n", "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": "\n    df.loc[df['Member'].isin(example_dict.keys()), 'Date'] = df['Member'].map(example_dict)\n    return df\n\ndf = f()\nprint(df)\n", "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": "df['Date'] = df['Member'].map(dict)\ndf['Date'] = pd.to_datetime(df['Date'])\ndf", "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year])['Val'].transform('count')\ndf\n", "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Count_d'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Val'].transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year])['Val'].transform('count')\ndf['Count_Val'] = df.groupby(['Date'])['Val'].transform('count')\ndf\n", "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['Count_d'] = df.groupby([df.Date.dt.year, df.Date.dt.month])['Val'].transform('count')\ndf['Count_m'] = df.groupby([df.Date.dt.year, df.Date.dt.month])['Val'].transform('count')\ndf['Count_y'] = df.groupby([df.Date.dt.year])['Val'].transform('count')\ndf['Count_w'] = df.groupby([df.Date.dt.weekofyear])['Val'].transform('count')\ndf['Count_Val'] = df.groupby(['Val'])['Val'].transform('count')\ndf\n", "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": "\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.sort_values('Date', inplace=True)\n\ndf['B_cumsum'] = df.groupby('Date')['B'].cumsum()\ndf['C_cumsum'] = df.groupby('Date')['C'].cumsum()\n\ndf['B_cumsum'] = df['B_cumsum'] - df['B_cumsum'].shift(1)\ndf['C_cumsum'] = df['C_cumsum'] - df['C_cumsum'].shift(1)\n\ndf_zero = df[df['B'] == 0]\ndf_non_zero = df[df['B'] != 0]\n\nresult1 = df_zero[['Date', 'B', 'C']]\nresult2 = df_non_zero[['Date', 'B', 'C']]\n", "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": "df['Date'] = pd.to_datetime(df['Date'])\ndf['B_even'] = df['B'] % 2\ndf['C_even'] = df['C'] % 2\n\ndf_even = df.groupby('Date')['B_even'].sum()\ndf_odd = df.groupby('Date')['C_even'].sum()\n\ndf_even.to_csv('even_B.csv')\ndf_odd.to_csv('odd_C.csv')\n", "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc=np.sum)\n\n# OR\n\nresult = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc=np.mean)\n", "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.sum)\nresult = pdseemed to be a solution to the problem.\n\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.mean)\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.mean)\n", "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc=np.sum)\nresult.columns = ['Sum']\n\nresult_mean = pd.pivot_table(df, values=['E','D'], index=['B'], aggfunc=np.mean)\nresult_mean.columns = ['Mean']\n\nresult = pd.concat([result, result_mean], axis=1)\n", "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": "\nresult = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc=np.max)\n\nresult = pd.pivot_table(df, values=['D','E'], index=['A','B'], aggfunc=np.min)\n", "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": "\ndef split_to_rows(df):\n    df_out = pd.DataFrame()\n    for i in df.index:\n        df_temp = df.loc[i]\n        df_temp_list = df_temp.str.split(',').tolist()\n        df_temp_list = [item.strip() for sublist in df_temp_list for item in sublist]\n        df_temp_df = pd.DataFrame(df_temp_list, columns=['var1', 'var2'])\n        df_out = pd.concat([df_out, df_temp_df])\n    return df_out\n\nresult = split_to_rows(df)\n", "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": "\ndef split_to_rows(df):\n    df_out = df.copy()\n    df_out['var2'] = df_out['var2'].str.split(',')\n    df_out = df_out.explode('var2')\n    return df_out\n\nresult = split_to_rows(df)\n", "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": "\ndf_split = df.set_index('var1').groupby(level=0).apply(lambda x: x.apply(lambda y: y.split('-')))\ndf_split = df_split.reset_index()\ndf_split.columns = ['var1', 'var2']\n\ndf_split\n", "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\n", "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": "def count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf['new'] = df['str'].apply(count_special_char)\n", "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": "df['fips'] = df['row'].str.split(', ', expand=True)\ndf = df[['fips', 'row']]\n", "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": "df[['fips', 'row']] = df['row'].str.split(' ', 1)\ndf\n", "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": "df[['fips', 'medi']] = df['row'].str.split(', ',expand=True)\ndf = df[['fips', 'medi', 'row']]", "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": "\ndf['2001'] = df['2001'] * 0\ndf['2002'] = df['2002'] * 0\ndf['2003'] = df['2003'] * 0\ndf['2004'] = df['2004'] * 0\ndf['2005'] = df['2005'] * 0\ndf['2006'] = df['2006'] * 0\n\ndf['2001'] = df.groupby('Name')['2001'].cumsum() / df.groupby('Name')['2001'].count()\ndf['2002'] = df.groupby('Name')['2002'].cumsum() / df.groupby('Name')['2002'].count()\ndf['2003'] = df.groupby('Name')['2003'].cumsum() / df.groupby('Name')['2003'].count()\ndf['2004'] = df.groupby('Name')['2004'].cumsum() / df.groupby('Name')['2004'].count()\ndf['2005'] = df.groupby('Name')['2005'].cumsum() / df.groupby('Name')['2005'].count()\ndf['2006'] = df.groupby('Name')['2006'].cumsum() / df.groupby('Name')['2006'].count()\n\ndf\n", "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": "\ndf['cumulative_avg'] = df.iloc[:, ::-1].cumsum(axis=1).div(df.iloc[:, ::-1]) * 100\ndf = df.iloc[:, ::-1]\ndf\n", "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": "\n    df_avg = df.set_index('Name').groupby(level=0).apply(lambda x: x.fillna(x.mean()).rolling(window=5).mean())\n    return df_avg\n\ndf_avg = f()\nprint(df_avg)\n", "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": "\ndf['cumulative_avg'] = df.iloc[:, 1:].cumsum() / df.iloc[:, 1:].gt(0).sum()\ndf\n", "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": "df['label'] = (df['Close'] > df['Close'].shift(1)).astype(int)\ndf\n", "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": "\ndf['label'] = df['Close'].diff().apply(lambda x: +1 if x > 0 else -1 if x < 0 else 0)\ndf.loc[0, 'label'] = 1\ndf\n", "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": "df['label'] = df['Close'].diff().apply(lambda x: +1 if x > 0 else -1 if x < 0 else 0)\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n", "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": "df['Duration'] = df.groupby('id')['departure_time'].apply(lambda x: x.iloc[1] - x.iloc[0])\ndf\n", "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": "df['Duration'] = df.departure_time.apply(lambda x: (df.arrival_time[id.index(x)+1] - df.arrival_time[id.index(x)]).total_seconds())\ndf\n", "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": "df['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['arrival_time'] = pdipsy.to_datetime(df['arrival_time'])\ndf['Duration'] = df['departure_time'] - df['arrival_time']\ndf['Duration'] = df['Duration'].astype('timedelta64[s]')\ndf", "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": "\nresult = df[df['key2'] == 'one'].groupby('key1')['key2'].count()\n", "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": "\nresult = df[df['key2'] == 'two'].groupby('key1').count()\n", "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": "\nresult = df[df['key2'].str.endswith('e')]\nresult = result.groupby('key1').count()\n", "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": "max_result = df.max()\nmin_result = df.min()\n", "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": "\nmode_result = df.mode(axis=0)\nmedian_result = df.median()\n\nprint(mode_result)\nprint(median_result)\n", "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": "df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n", "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": "df = df[~(df['closing_price'].between(99, 101))]\n", "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": "\ndf1 = df.groupby(\"item\")[\"diff\"].min()\ndf1 = df.groupby(\"item\")[\"otherstuff\"].min()\ndf1 = df.groupby(\"item\")[\"diff\", \"otherstuff\"].min()\n\nresult = df[df.groupby(\"item\")[\"diff\"].min().index.intersection(df.groupby(\"item\")[\"otherstuff\"].min().index)]\n", "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": "df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n", "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": "\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].str.split('_').str[0]\n    return df\n\nf(example_df)\n", "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": "df['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0])\n\n# Count the number of NaN values\nnans = df['Column_x'].isnull().sum()\n\n# Count the number of values that are '0'\nzeroes = df['Column_x'].value_counts()[0]\n\n# Count the number of values that are '1'\nones = df['Column_x'].value_counts()[1]\n\n# Calculate the number of values to be filled\nto_fill = nans // 2\n\n# Fill the first 50% with '0'\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = 0\n\n# Fill the last 50% with '1'\ndf.loc[~df['Column_x'].isnull(), 'Column_x'] = 1\n", "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": "\n# Calculate the number of NaN values\nnans = df['Column_x'].isnull().sum()\n\n# Calculate the number of values to replace\nreplace_nans = nans // 3\n\n# Loop through the DataFrame\nfor i in range(replace_nans):\n    df.loc[i, 'Column_x'] = 0.0\n    df.loc[i+replace_nans, 'Column_x'] = 0.5\n    df.loc[i+2*replace_nans, 'Column_x'] = 1.0\n\ndf\n", "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": "df['Column_x'] = df['Column_x'].fillna(df['Column_x'].mode()[0], inplace=True)\ndf['Column_x'] = df['Column_x'].astype(int)\ndf['Column_x'] = df['Column_x'].apply(lambda x: 1 if x==0 else 0)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].apply(lambda x: x if x=='1' else 0)\ndf['Column_x'] = df['Column_x'].apply(lambda x: x if x=='0' else 1)\ndf['Column_x'] = df['Column_x'].astype(int)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['Column_x'].astype(str)\ndf['Column_x'] = df['", "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": "\ndef create_tuples(df_list):\n    return [list(zip(df['one'], df['two'])) for df in df_list]\n\nresult = pd.DataFrame(create_tuples([a, b]), columns=['one', 'two'])\n", "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": "\ndef create_tuples(df_list):\n    result = []\n    for df1 in df_list:\n        for df2 in df_list:\n            if df1.columns[0] == df2.columns[0]:\n                result.append((df1.iloc[0,0], df2.iloc[0,0]))\n    return pd.DataFrame(result, columns=['one', 'two'])\n\nresult = create_tuples([a, b, c])\n", "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": "\ndef create_tuples(a, b):\n    result = []\n    for i in range(len(a)):\n        result.append((a.iloc[i]['one'], b.iloc[i]['two']))\n    return result\n\na_b = pd.DataFrame(create_tuples(a, b), columns=['one', 'two'])\n\nprint(a_b)\n", "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": "\ndf['binned_views'] = pd.cut(df.views, bins)\nresult = df.groupby('username')['binned_views'].value_counts()\n", "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": "\ndf['binned'] = pd.cut(df.views, bins)\nresult = df.groupby('username')['binned'].value_counts()\nresult\n", "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": "\ndf['binned_views'] = pd.cut(df.views, bins)\nresult = df.groupby('username')['binned_views'].value_counts()\n", "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": "\ndf['text'] = df['text'].str.cat(df['text'], ', ')\nresult = df\n", "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": "\ndf['text'] = df['text'].astype(str).str.join('-')\nresult = df\n", "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": "\ndf['text'] = df['text'].str.split(',').str[0]\nresult = df.groupby('text').count()\n", "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": "df['text'] = df['text'].str.cat(', ', Num_Concat=1)\nresult = df\n", "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": "df['text'] = df['text'].astype(str).str.cat(df['text'].shift(), '-')\nresult = df['text'].str.strip('-').str.split('-').str.join('-')\n", "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": "\ndf1 = df1.merge(df2, on='id', how='left')\nresult = df1\n", "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": "df1['date'] = pd.to_datetime(df1['date'])\ndf1 = df1.sort_values(by=['id', 'date'])\n\ndf2 = df2.sort_values(by='date')\n\ndf1 = df1.merge(df2, on='id', how='left')\n\ndf1['date'] = df1['date'].dt.strftime('%d-%b-%Y')\n\ndf1", "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": "df1['date'] = pd.to_datetime(df1['date'])\ndf2['date'] = pd.to_datetime(df2['date'])\n\ndf1 = df1.merge(df2, on='id', how='left')\ndf1['date'] = df1['date'].dt.strftime('%Y/%m/%d')\ndf1 = df1.sort_values(by=['id', 'date'])\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']>df1['value']], how='left')\ndf1 = df1.merge(df2[df2['value']<=df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']==df1['value']], how='left')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']>df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']<=df1['value']], how='left')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']==df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']>df1['value']], how='left')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']<=df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']==df1['value']], how='left')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']>df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']<=df1['value']], how='left')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']==df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']>df1['value']], how='left')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']<=df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']==df1['value']], how='left')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']>df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']<=df1['value']], how='left')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[df2['value']==df1['value']], how='right')\ndf1 = df1.groupby('id')['value'].first().reset_index()\ndf1 = df1.merge(df2[", "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": "\nmerged_df = pd.merge(C, D, on='A', how='outer')\n\n# if you want to keep the original order of rows\nmerged_df = merged_df.sort_values(by='A')\n\n# if you want to keep the original order of columns\nmerged_df = merged_df.reindex(columns=C.columns.tolist() + D.columns.tolist(), axis='columns')\n\nresult = merged_df\n", "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": "\nmerged_df = pd.merge(C, D, on='A', how='outer')\n\n# if you want to keep the original order of the rows\nmerged_df = merged_df.sort_values(by='A')\n\n# if you want to keep the original order of the columns\nmerged_df = merged_df.reindex(columns=C.columns.tolist() + D.columns.tolist(), axis='columns')\n\nresult = merged_df\n", "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": "\ndef duplicate_column(C, D):\n    merged = pd.merge(C, D, on='A')\n    merged['dulplicated'] = merged['A'].duplicated()\n    return merged\n\nresult = duplicate_column(C, D)\n", "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": "\ndef list_of_tuples(df):\n    return df.groupby('user')[['time', 'amount']].apply(list)\n\nresult = list_of_tuples(df)\n", "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": "\ndef tuple_to_df(user_df):\n    return user_df.groupby('user')[['time', 'amount']].apply(list).reset_index(drop=True)\n\nresult = tuple_to_df(df)\n", "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": "\ndef tuple_to_df(user_df):\n    return user_df.groupby('user')[['time', 'amount']].apply(list).reset_index(drop=True)\n\nresult = tuple_to_df(df)\n", "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": "\ndf_concatenated = pd.DataFrame(columns=['file1', 'file2', 'file3'])\n\nfor i in series.index:\n    df_concatenated[i] = series[i]\n\ndf_concatenated.index = series.index\n\ndf_concatenated", "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": "\ndf = pd.DataFrame(columns=['name', '0', '1', '2', '3'])\n\nfor i in series.index:\n    df = df.append(pd.Series([i, *series[i]], index=['name', '0', '1', '2', '3']), ignore_index=True)\n\ndf\n", "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": "\ndef find_column_with_no_match(df, s):\n    for col in df.columns:\n        if s in col and 'spike' not in col:\n            return col\n    return None\n\nresult = find_column_with_no_match(df, s)\nprint(result)\n", "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": "\ndef find_column_with_no_match(df, s):\n    for col in df.columns:\n        if s in col and 'spike' not in col:\n            return col\n    return None\n\nresult = find_column_with_no_match(df, s)\nprint(result)\n", "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": "\ndef find_spike(df):\n    for col in df.columns:\n        if s in col:\n            return col\n\nresult = find_spike(df)\ndf = df.rename(columns={result: 'spike1'})\ndf\n", "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": "\n# Split the list into a new DataFrame\ndf_split = df['codes'].apply(pd.Series)\n\n# Combine the original DataFrame with the new DataFrame\ndf = pd.concat([df.drop('codes', axis=1), df_split], axis=1)\n\n# Fill NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf.reset_index(drop=True, inplace=True)\n\n# Rename the columns\ndf.columns = ['code_' + str(col) for col in df.columns]\n\nresult = df\n", "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": "\n# Split the list into a new DataFrame\ndf_split = df['codes'].apply(pd.Series)\n\n# Combine the original DataFrame with the new DataFrame\ndf = pd.concat([df.drop('codes', axis=1), df_split], axis=1)\n\n# Fill NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Reset the index\ndf.reset_index(drop=True, inplace=True)\n\n# Create new columns\ndf.columns = ['code_' + str(col) for col in df.columns]\n\nresult = df\n", "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": "\n# Split the list into two columns\ndf['codes'] = df['codes'].apply(pd.Series)\n\n# Fill NaN with 0\ndf['codes'] = df['codes'].fillna(0)\n\n# Drop the list column\ndf = df.drop(columns='codes')\n\n# Rename the columns\ndf.columns = ['code_'+str(x) for x in df.columns]\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\nresult = df\n", "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": "\n# Convert the list of lists into a flat list\ndf['col1'] = df['col1'].apply(lambda x: [item for sublist in x for item in sublist])\n\n# Flatten the list\nresult = df['col1'].flatten().tolist()\n", "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": "\ndef reverse_list(lst):\n    return [i for i in lst[::-1]]\n\ndf['col1'] = df['col1'].apply(reverse_list)\n\nresult = ','.join(df['col1'].values.flatten())\n", "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": "\ndf['col1'] = df['col1'].apply(lambda x: ','.join(map(str, x)))\nresult = df['col1'].str.cat(sep=',')\n", "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": "df.set_index('Time', inplace=True)\ndf = df.resample('2min').mean()\ndf.reset_index(inplace=True)\ndf", "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": "df.set_index('Time', inplace=True)\ndf = df.resample('3min').sum()\ndf.reset_index(inplace=True)\ndf", "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\ndf\n", "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\ndf\n", "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": "df['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\ndf\n", "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": "\n\nresult = df[df.index.get_level_values('a') != 2]\nresult = result[result.index.get_level_values('b') != 2]\nresult = result[filt]\n", "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": "\nresult = df[filt]\n", "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": "\nmask = df.iloc[0] != df.iloc[8]\nresult = mask[mask].index.tolist()\n", "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": "mask = df.iloc[0] == df.iloc[8]\nresult = mask[mask].index.tolist()\n", "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": "\nmask = df.iloc[0] != df.iloc[8]\nresult = mask[mask].index.tolist()\n", "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": "\nmask = df.iloc[0] != df.iloc[8]\nresult = [(x, y) for x, y in zip(mask[mask==True].index, mask[mask==True].values) if x != y]\n", "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": "ts = pd.Series(df['Value'], index=df['Date'])\n", "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": "df_single_row = df.iloc[0].rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignoreipsy=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns=lambda x: 'A_'+str(x))\ndf_single_row = df_single_row.append(df, ignore_index=True)\ndf_single_row = df_single_row.rename(columns", "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": "df_single_row = df.iloc[0].values.reshape(1, -1).T.iloc[:,::-1].T.reset_index(drop=True)\ndf_single_row.columns = [f'A_{i}' if i==0 else f'A_{i}' if i==1 else f'A_{i}' if i==2 else f'A_{i}' if i==3 else f'A_{i}' if i==4 else f'A_{i}' for i in range(df_single_row.shape[0])]\ndf_single_row.columns = df_single_row.columns.str.strip()\ndf_single_row.index = df_single_row.index.str.strip()\ndf_single_row = df_single_row.reindex(columns=df.columns)\ndf_single_row = df_single_row.reset_index(drop=True)\ndf_single_row.loc[0] = df.loc[0]\ndf = df.loc[1:]\ndf_single_row = df_single_row.append(df.loc[0])\ndf = df_single_row\n", "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\n", "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": "df['dogs'] = df['dogs'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\n", "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": "df['Sum'] = df[list_of_my_columns].sum(axis=1)\n", "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\n", "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": "df['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n", "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": "df.sort_values(by=['treatment','time'])\n", "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": "df.sort_values(by=['treatment','time'])\n", "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": "\ndays_to_delete = ['2020-02-17', '2020-02-18']\ndf = df[~df['Date'].isin(days_to_delete)]\n", "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": "\ndf = df[~(df.index.weekday.isin([5, 6]))]\n\nresult = df\n", "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": "\n# Filter the correlation matrix to get only the values above 0.3\nresult = corr.stack()[abs(result) > 0.3]\n\n# Reset the index to get the desired DataFrame\nresult = result.reset_index()\n\n# Rename the columns\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": "\n# Get the correlation matrix where the absolute value of the correlation is greater than 0.3\nmask = np.abs(corr) > 0.3\n\n# Get the positions of the True values in the mask\npositions = np.where(mask == True)\n\n# Get the values in the positions\nresult = df.iloc[positions[0], positions[1]]\n", "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": "\nlast_column_name = df.columns[-1]\ndf.rename(columns={last_column_name: 'Test'}, inplace=True)\n\nresult = df\n", "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": "df.rename(columns={df.columns[0]: 'Test'}, inplace=True)\nresult = df\n", "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": "\ndf['frequent'] = df.apply(lambda row: max(set(row.values.tolist()), key=row.values.tolist().count), axis=1)\ndf['freq_count'] = df.apply(lambda row: list(row.values.tolist()).count(df['frequent'].max()), axis=1)\ndf\n", "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": "\ndf['frequent'] = df.apply(lambda row: max(set(row.values.tolist()), key=row.values.tolist().count), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\ndf\n", "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": "\ndf['frequent'] = df.apply(lambda row: list(set(row.value_counts().index)), axis=1)\ndf['freq_count'] = df.apply(lambda row: row.value_counts()[0], axis=1)\ndf\n", "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": "\ndf['bar'] = df.groupby('id1')['bar'].transform('mean')\ndf['foo'] = df.groupby('id1')['foo'].transform('mean')\n\ndf = df.groupby(['id1','id2'])['foo','bar'].mean()\n\nresult = df\n", "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": "\ndf['bar'] = df.groupby(['id1','id2'])['bar'].transform('count')\ndf['foo'] = df.groupby(['id1','id2'])['foo'].transform('mean')\ndf['bar'] = df.groupby(['id1','id2'])['bar'].transform('mean')\n\ndf = df.groupby(['id1','id2'])['foo'].apply(lambda x: x.fillna(0)).reset_index()\n\ndf\n", "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": "\ndf_c = pd.merge(df_a, df_b, on='EntityNum')\nresult = df_c\n", "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": "\ndf_c = pd.merge(df_a, df_b, on='EntityNum')\nresult = df_c\n", "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": "dimensions = a.shape\n", "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": "x = x[np.logical_not(np.isnan(x))]\n", "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": "x = np.where(np.isnan(x), np.inf, x)\n", "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": "\n# First, you need to identify the rows where the nan value is present\nmask = np.isnan(x)\n\n# Then, apply this mask to the array to get the rows where the nan value is not present\nresult = x[~mask]\n\n# Convert the result to a list of lists\nresult = result.tolist()\n\n# Now the result is a list of lists with the nan values removed\nprint(result)\n", "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": "b = np.eye(max(a))\nb[a-1] = 1\n", "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": "b = np.eye(max(a)+1)[a]\n", "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": "\ndef one_hot_encode(arr):\n    return np.eye(max(arr))[arr]\n\nb = one_hot_encode(a)\n", "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": "\ndef encode_one_hot(arr):\n    return np.eye(max(arr))[arr]\n\nb = encode_one_hot(a)\n", "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": "\ndef encode(a):\n    a_min = a.min()\n    a_max = a.max()\n    b = np.zeros((a.shape[0], a.shape[1]*a_max - a_min + 1))\n    b[:, a_min:a.shape[1]+a_min] = a\n    return b\n\nb = encode(a)\n", "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": "\n# calculate the percentile\nresult = np.percentile(a, p)\n", "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": "B = np.array(A).reshape(-1, ncol)\n", "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": "B = np.array(A).reshape(nrow, -1)\n", "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": "B = np.array(A[::ncol]).reshape(-1, ncol)\n", "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": "B = np.array(A).reshape(-1, ncol)\n", "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": "\ndef shift(a, shift):\n    return np.roll(a, shift)[:len(a)]\n\nresult = shift(a, shift)\nprint(result)\n", "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": "result = np.roll(a, shift, axis=1)\n", "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": "\ndef shift_2D(arr, shift):\n    rows, cols = arr.shape\n    rows, cols = arr.shape\n    rows_shift, cols_shift = [i - j for i, j in zip(shift, [0]*len(shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]\n    rows_shift = [i - j for i, j in zip(rows_shift, [0]*len(rows_shift))]", "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": "r_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n", "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": "max_value = a.max()\nmax_position = np.where(a == max_value)\nresult = max_position\n", "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": "min_value = a.min()\nmin_position = np.unravel_index(np.argmin(a, axis=None), a.shape)\nresult = min_position\n", "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": "max_value = a.max()\nmax_indices = np.unravel_index(np.argmax(a), a.shape)\nresult = max_indices\n", "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": "max_value = a.max()\nmax_indices = np.unravel_index(np.argmax(a), a.shape)\nresult = max_indices\n", "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": "\n    max_value = np.amax(a)\n    max_position = npumerate(max_value)\n    return max_position\n    ### END SOLUTION\n\nprint(f(example_a))\n", "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": "\n# Flatten the array\na_flat = a.flatten()\n\n# Get the second largest value\nsecond_largest = np.partition(a_flat, -2)[-2]\n\n# Get the position of the second largest value\nposition = npumerate(a_flat, start=0)[second_largest]\n\n# Get the unraveled index of the position\nresult = position\n\nEND SOLUTION\n<code>\n\nThis code will return the position of the second largest value in the flattened array.\n\nNote: The `enumerate()` function in Python returns the index and the value of each element in the array. The `start` parameter specifies the starting index to use.\n\nThe `np.partition()` function in Python returns a rearranged version of the input array such that the element at the specified position is the smallest element in the array. The `-2` in the function call `np.partition(a_flat, -2)[-2]` means that the second largest element in the array is the second smallest element in the array.\n\nThe `position` variable will hold the position of the second largest value in the flattened array.\n\nThe `result` variable will hold the position of the second largest value in the flattened array.\n", "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": "\na = np.delete(a, np.where(np.isnan(a))[1], axis=1)\n", "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": "a = a[~np.isnan(a).any(axis=1)]", "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": "\n# First, we need to flatten the list into a 1D list\nflat_list = [item for sublist in a for item in sublist]\n\n# Then, we can convert the flat list into a numpy array\nresult = np.array(flat_list)\n\nprint(result)\n", "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": "a = a[:, permutation]\n", "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": "\nresult = np.array([a[permutation[0]] , a[permutation[1]] , a[permutation[2]]])\n", "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": "min_val = a.min()\nmin_index = np.unravel_index(np.argmin(a), a.shape)\nresult = min_index\n", "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": "max_value = np.amax(a)\nmax_index = np.unravel_index(np.argmax(a), a.shape)\nresult = max_index\n", "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": "min_val_indices = np.unravel_index(a.argmin(), a.shape)\nresult = min_val_indices\n", "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": "\n# Convert the degree to radian\nradian = np.deg2rad(degree)\n\n# Calculate the sine value\nresult = np.sin(radian)\n\n# Convert the result back to degree\nresult = np.rad2deg(result)\n", "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": "\n# Convert the degree to radian\nradian = np.deg2rad(degree)\n\n# Compute cosine value\nresult = np.cos(radian)\n\n# Convert the result back to degree\nresult = np.rad2deg(result)\n", "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": "if np.sin(np.radians(number)) > np.sin(number):\n    result = 0\nelse:\n    result = 1\n", "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": "\ndef find_angle(sine_value):\n    # Convert the sine value to degrees\n    degree = np.degrees(sine_value)\n    return degree\n\nresult = find_angle(value)\nprint(result)\n", "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": "\ndef pad(A, length):\n    padded_A = np.pad(A, (0, length - A.shape[0] % length), 'constant')\n    return padded_A\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n", "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": "def pad(A, length):\n    A_padded = np.pad(A, (0, length - A.shape[0] % length), 'constant')\n    return A_padded\n\nA_padded = pad(A, length)\n", "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": "a = np.linalg.matrix_power(a, power)\n", "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": " ###\n    return np.linalg.matrix_power(a, power)\n    ### END SOLUTION ###\n\nprint(f(example_a, 2))\nprint(f(example_a, 3))\nprint(f(exampleips_a, 4))\n", "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": "result = np.divide(numerator, denominator)\n", "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": " ###\n    result = np.divide(numerator, denominator)\n    return (result.numerator, result.denominator)\n", "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": "\ndef reduce_fraction(n, d):\n    if d == 0:\n        return (np.nan, np.nan)\n    else:\n        return (n/d, n%d)\n\nresult = reduce_fraction(numerator, denominator)\nprint(result)\n", "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": "result = (a + b + c) / 3\n", "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": "result = np.where(a > b, a, b)\nresult = np.where(c > result, c, result)\n", "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": "result = np.diag(a, 1)\n", "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": "result = np.fliplr(a).diagonal()\n", "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": "\ndef get_diagonal(arr):\n    n = len(arr)\n    diagonal = [arr[i][i] for i in range(n)]\n    return diagonal\n\nresult = get_diagonal(a)\n", "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": "def get_diagonal(arr):\n    n = arr.shape[0]\n    result = np.zeros(n)\n    for i in range(n):\n        result[i] = arr[i, i]\n    return result\n\nresult = get_diagonal(a)\n", "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": "result = [item for sublist in X for item in sublist]\n", "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": "\ndef arrange_elements(X):\n    result = []\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            result.append(X[i][j])\n    return result\n\nresult = arrange_elements(X)\n", "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": "\n    result = [i for i in X.flatten()]\n    ### END SOLUTION\n    return result\n\nprint(f(example_X))\n", "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": "\ndef to_fortran_order(arr):\n    return arr.view(np.ctypeslib.ndarray)\n\ndef iterate_array(arr):\n    arr = to_fortran_order(arr)\n    result = []\n    for i in range(arr.shape[0]):\n        for j in range(arr.shape[1]):\n            result.append(arr[i, j])\n    return result\n\nresult = iterate_array(X)\n", "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": "\nresult = np.array(list(mystr))\nresult = [int(i) for i in result]\n\nprint(result)\n", "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": "\n# Step 1: Multiply the col-th column by the given number\na[:, col] = a[:, col] * multiply_number\n\n# Step 2: Calculate the cumulative sum of the numbers in that column\nresult = np.cumsum(a[:, col])\n", "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": "\n# Select the specific row\nrow_data = a[row, :]\n\n# Multiply the row_data by the given number\nrow_data = row_data * multiply_number\n\n# Calculate the cumulative sum of the row_data\ncumulative_sum = np.cumsum(row_data)\n\n# Store the result in the variable 'result'\nresult = cumulativeipsum\n", "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": "\n# Divide the row by the given number\na[row, :] = a[row, :] / divide_number\n\n# Calculate the multiplication of the numbers in that row\nresult = np.prod(a[row, :])\n", "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": "\ndef get_max_independent_vectors(a):\n    rows = a.shape[0]\n    cols = a.shape[1]\n    independent_vectors = []\n    for i in range(2**cols):\n        bits = format(i, f'0{cols}b').replace('0', '').replace('1', '')\n        vector = [a[int(b)] for b in bits]\n        if len(set(vector)) == cols:\n            independent_vectors.append(vector)\n    return independent_vectors\n\nresult = get_max_independent_vectors(a)\n", "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": "\nresult = a.shape[0]\n", "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b, equal=True)\n", "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": "t_statistic, p_value = scipy.stats.ttest_ind(a, b, equal=True)\n", "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": "# calculate the degrees of freedom\ndf_a = anobs - 1\ndf_b = bnobs - 1\n# calculate the t-statistic\nt_stat = (bmean - amean) / np.sqrt(avar/df_a + bvar/df_b)\n# calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_stat), df_a, df_b))\n", "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": "\n# get the rows of A that are not in B\nmask = np.in1d(A[:,0], B[:,0])\n\n# get the rows of A that are in B\noutput = A[mask]\n", "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": "\ndef get_diff(A, B):\n    A_set = set(A.flatten())\n    B_set = set(B.flatten())\n    return list(A_set - B_set) + list(B_set - A_set)\n\noutput = get_diff(A, B)\n", "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": "c = np.array([a[i] for i in np.argsort(a, axis=0)]).reshape(b.shape)\n", "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": "c = np.array([a[i] for i in np.argsort(a, axis=0)]).reshape(b.shape)\n", "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": "c = np.array([[a[i] for i in np.argsort(a, axis=0)] for _ in range(a.shape[0])])\n", "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": "result = np.argsort(a.sum(axis=2))\nresult = result.reshape((3, 3, 3))\nresult = b[result]\n", "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": "a = np.delete(a, 2, axis=1)\n", "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": "a = np.delete(a, 2, axis=0)\n", "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": "a = np.delete(a, [0, 2], axis=1)\n", "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": "\ndef delete_columns(a, del_col):\n    result = a[:, [i for i in del_col if i <= a.shape[1]]]\n    return result\n\nresult = delete_columns(a, del_col)\nprint(result)\n", "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": "a[pos] = element\n", "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": "\n    a = np.insert(a, pos, element)\n    return a\n    ### END SOLUTION\n\nexample_a = f(example_a, 2, 66)\nprint(example_a)\n", "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": "a = np.insert(a, pos, element, axis=0)\n", "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": "result = array_of_arrays.copy()\n", "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": "result = np.all(a == a[0], axis=-1)\n", "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": "result = np.all(a == a[0], axis=0)\n", "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": "\n    return np.all(a == a[0], axis=1)\n    ### END SOLUTION\n\n# testing the function\nprint(f(example_a))\n", "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": "x, y = np.meshgrid(x, y)\nz = (np.cos(x))**4 + (np.sin(y))**2\nresult = np.sum(z)\n", "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": "\n    return (np.cos(x)**4) + (np.sin(y)**2)\n    ### END SOLUTION\nexample_z = f(example_x, example_y)\n", "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": "def ecdf(data):\n    x = np.sort(data)\n    y = npips(x, range(len(x)))\n    return x, y\n\nresult = ecdf(grades)\n", "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": "def ecdf(data):\n    x = np.sort(data)\n    y = npips(x, range(len(x)))\n    return x, y\n\nresult = ecdf(grades)\nresult_eval = ecdf(eval)\n", "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": "def longest_interval(grades, threshold):\n    grades = np.array(grades)\n    ecdf = np.sort(grades)\n    cumulative_sum = np.cumsum(ecdf)\n    longest_interval_end = np.max(np.where(cumulative_sum < threshold)[0] + 1)\n    longest_interval_start = npips\n    return longest_interval_start, longest_interval_end\n\nlow, high = longest_interval(grades, threshold)\n", "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": "nums = np.random.randint(2, size=size)\nnums[:int(one_ratio*size)] = 1\n", "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": "a_pt = torch.from_numpy(a)\n", "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": "a_np = a.numpy()\n", "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": "a_tf = tf.convert_to_tensor(a)\n", "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": "\n# get the indexes of the elements in decreasing order\nindexes = np.argsort(a)[::-1]\n\n# get the elements in decreasing order\nresult = a[indexes]\n\nprint(result)\n", "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": "\n# Get the indices of the elements in the original array\nindices = np.argsort(a)\n\n# Get the indices of the elements in the original array in increasing order\nresult = [a[i] for i in indices]\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": "\n# Get the N biggest elements in decreasing order\nbiggest_elements = a[np.argsort(-a)]\n\n# Get the indexes of the biggest elements\nresult = biggest_elements[::-1]\n\n# Get the indexes of the biggest elements in decreasing order\nresult = [i for i in range(len(result)) if result[i] == biggest_elements[0]]\n\nEND SOLUTION\n", "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": "\nresult = np.linalg.matrix_power(A, n)\n", "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": "\ndef extract_patches(a, patch_size):\n    patches = []\n    for i in range(0, a.shape[0] - patch_size[0] + 1):\n        for j in range(0, a.shape[1] - patch_size[1] + 1):\n            patch = a[i:i+patch_size[0], j:j+patch_size[1]]\n            patches.append(patch)\n    return np.array(patches)\n\nresult = extract_patches(a, (2, 2))\n", "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": "\ndef extract_patches(a, patch_size):\n    patches = []\n    for i in range(0, a.shape[0] - patch_size[0] + 1):\n        for j in range(0, a.shape[1] - patch_size[1] + 1):\n            patch = a[i:i+patch_size[0], j:j+patch_size[1]]\n            patches.append(patch)\n    return np.array(patches)\n\nresult = extract_patches(a, (2, 2))\nprint(result)\n", "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": "\ndef extract_patches(a, patch_size):\n    patches = []\n    for i in range(0, a.shape[0] - patch_size[0] + 1):\n        for j in range(0, a.shape[1] - patch_size[1] + 1):\n            patch = a[i:i+patch_size[0], j:j+patch_size[1]]\n            patches.append(patch)\n    return np.array(patches)\n\nresult = extract_patches(a, (2, 2))\n", "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": "\ndef extract_patches(array, patch_size):\n    array_height, array_width = array.shape\n    patch_height, patch_width = patch_size\n    assert array_height % patch_height == 0\n    assert array_width % patch_width == 0\n    patches = []\n    for row_start in range(0, array_height, patch_height):\n        for col_start in range(0, array_width, patch_width):\n            patch = array[row_start:row_start+patch_height, col_start:col_start+patch_width]\n            patches.append(patch)\n    return np.array(patches)\n\nresult = extract_patches(a, patch_size)\n", "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": "\n# Solution:\n\n# First, we need to reshape the original array into a 2D array with the shape of (h*w, w)\nreshaped_array = a.reshape(h*w, w)\n\n# Then, we can apply any operation on the reshaped array as per requirement\n# Here, let's take the sum of each row\nresult = np.sum(reshaped_array, axis=1)\n\n# Now, the result is a 1D array with the sum of each row\n", "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": "\ndef extract_patches(array, patch_size):\n    array_height, array_width = array.shape\n    patch_height, patch_width = patch_size\n    assert array_height % patch_height == 0\n    assert array_width % patch_width == 0\n    patches = []\n    for row_start in range(0, array_height, patch_height):\n        for col_start in range(0, array_width, patch_width):\n            patch = array[row_start:row_start+patch_height, col_start:col_start+patch_width]\n            patches.append(patch)\n    return np.array(patches)\n\nresult = extract_patches(a, patch_size)\n", "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": "result = a[:, low:high+1]\n", "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": "result = a[low:high]\n", "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": "\n# get the number of columns in the array\nn_cols = a.shape[1]\n\n# check if the low index is less than 0 or greater than the number of columns\nif low < 0 or high > n_cols:\n    print(\"Invalid range\")\nelse:\n    # get the desired columns\n    result = a[:, low:high+1]\n", "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": "a = np.array(string)\n", "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": "\n# Generate samples from a uniform distribution\nsamples = np.random.uniform(min, max, n)\n\n# Apply the log transformation to the samples\nresult = npips.loguni(samples)\n", "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": "\n# Generate samples from a uniform distribution in the range [0, 1]\nsamples = np.random.uniform(0, 1, n)\n\n# Apply the log transformation to the samples\nresult = npips.loguni(samples, min, max)\n", "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": "\n    return np.random.loguniform(min, max, n)\n    ### END SOLUTION\n\n# Test the function\nprint(f(min=1, max=10, n=10))\n", "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": "B = pd.Series(np.zeros(len(A)))\nB[0] = a * A[0]\n\nfor i in range(1, len(A)):\n    B[i] = a * A[i] + b * B[i-1]\n", "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": "B = [A[0]*a if i==0 else A[i]*a + b*B[i-1] if i==1 else A[i]*a + b*B[i-1] + c*B[i-2] for i in range(len(A))]\n", "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": "\nresult = np.array([])\n", "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": "\n# Initialize an empty 3x0 matrix\nresult = np.empty((3,0))\n", "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": "result = np.ravel_multi_index(np.sub2ind(*dims, *index), np.array(dims))\n", "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": "result = sub2ind(dims, index)\n", "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": "df = pd.DataFrame(np.zeros((2,3), dtype='int32,float32'), index=index, columns=columns)\n", "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": "\ndef accum_np(accmap, a):\n    return np.array([np.sum(a[accmap==i]) for i in np.unique(accmap)])\n\nresult = accum_np(accmap, a)\nprint(result)\n", "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": "result = a[index]\n", "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": "\ndef accumulate(a, accmap):\n    result = np.zeros_like(a)\n    for i, j in enumerate(accmap):\n        if j >= 0:\n            result[i] += a[i+j]\n        else:\n            result[i] += a[i-j]\n    return result\n\nresult = accumulate(a, accmap)\nprint(result)\n", "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": "result = a[index]\n", "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": "def elementwise_function(element_1,element_2):\n    return (element_1 + element_2)\n\nz = np.vectorize(elementwise_function)(x, y)\n", "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": "result = np.random.choice(lista_elegir, samples, p=probabilit)\n", "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": "\n# Create a new array with the desired size\npadded_array = np.zeros((len(a) + high_index, len(a[0]) + high_index))\n\n# Copy the original array into the new array\npadded_array[low_index:len(a)+low_index, low_index:len(a[0])+low_index] = a\n\n# Now you can slice the new array as you would any other array\nresult = padded_array[1:3, 1:3]\n", "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": "\nresult = x[x >= 0]\n", "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": "result = x[np.abs(x) != np.abs(x)]\n", "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": "bin_data = [data[i:i+bin_size] for i in range(0, len(data), bin_size)]\nbin_data_mean = [np.mean(bin_data[i]) for i in range(len(bin_data))]\n", "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": "bin_data_max = [max(data[i:i+bin_size]) for i in range(0, len(data), bin_size)]\n", "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": "\ndef bin_data(data, bin_size):\n    return [data[i:i+bin_size] for i in range(0, len(data), bin_size)]\n\ndef calculate_mean(data):\n    return sum(data) / len(data)\n\nbins = bin_data(data, bin_size)\nbin_data_mean = [calculate_mean(bin) for bin in bins]\n", "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": "\ndef bin_data(data, bin_size):\n    return [data[-(bin_size):] + data[-(binumerate(data[-(bin_size):])[::-1])[0]:]]\n\ndef calculate_mean(data):\n    return [sum(data[i:i+(bin_size)])/(bin_size) for i in range(len(data)-(bin_size))]\n\nbin_data_mean = calculate_mean(bin_data(data, bin_size))\n", "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": "def bin_data(data, bin_size):\n    return [data[i:i+bin_size] for i in range(len(data)-bin_size+1)]\n\ndef calculate_mean(data):\n    return [sum(i)/len(i) for i in data]\n\ndef solve(data, bin_size):\n    bin_data = bin_data(data, bin_size)\n    bin_data_mean = calculate_mean(bin_data)\n    return bin_data_mean\n\nbin_size = 3\nprint(solve(data, bin_size))\n", "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": "def bin_data(data, bin_size):\n    bin_data_mean = []\n    for i in range(0, len(data), bin_size):\n        bin_data_mean.append(np.mean(data[i:i+bin_size]))\n    return np.array(bin_data_mean)\n\nbin_data_mean = bin_data(data, bin_size)\n", "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": "\ndef smoothclamp(x, x_min, x_max):\n    t = (x - x_min) / (x_max - x_min)\n    t = max(0, min(1, t))\n    return x_max * (1 - t**2) + x_min * t**2\n", "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": "\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0.0, 1.0)\n    return x_max * (1 - t**N) + x_min * (t**(1-N))\n", "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": "\ndef circular_correlation(a, b):\n    # Compute the circular correlation\n    # You can use the numpy function np.convolve for this\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll the array\n    # The np.roll function can be used to roll", "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": "\ndf_s = df.groupby('major').apply(lambda x: x.values.reshape(15, 5))\n\ndf_s.to_numpy()\n\n", "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": "result = df.groupby('major')['column'].apply(lambda x: x.values.reshape(1,4,5))\n", "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": "\ndef int_array_to_bin_matrix(a, m):\n    return np.array([[int(i>>n) & 1 for n in range(m-1, -1, -1)] for i in a])\n\nresult = int_array_to_bin_matrix(a, m)\n", "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": "\ndef int_to_binary_matrix(arr, m):\n    arr = np.array(arr)\n    result = np.array([[int(i>>n) & 1 for n in range(m-1, -1, -1)] for i in arr])\n    return result\n\nresult = int_to_binary_matrix(a, m)\nprint(result)\n", "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": "\ndef int_to_binary_array(a, m):\n    result = np.empty((a.shape[0], m), dtype=np.uint8)\n    for i in range(a.shape[0]):\n        result[i] = (a[i] & ((1 << m) - 1))\n    return result\n\nresult = int_to_binary_array(a, m)\n", "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": "mu = np.std(a, ddof=1)\nlower_bound = mu - 3*mu\nupper_bound = mu + 3*mu\nresult = (lower_bound, upper_bound)\n", "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": "mu = np.mean(a)\nstd_dev = np.std(a)\nlower_bound = mu - 2*std_dev\nupper_bound = mu + 2*std_dev\nresult = (lower_bound, upper_bound)\n", "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": "\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    start = mean - 3*std_dev\n    end = mean + 3*std_dev\n    return (start, end)\n\nprint(f(example_a))\n", "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": "\n# calculate the mean of the array\nmean = np.mean(a)\n\n# calculate the standard deviation of the array\nstd_dev = np.std(a)\n\n# calculate the 2nd standard deviation\nstd_dev_2 = std_dev * 2\n\n# calculate the lower and upper limit of the 2nd standard deviation interval\nlower_limit = mean - std_dev_2\nupper_limit = mean + std_dev_2\n\n# create a boolean array where the values in the array a are out of the 2nd standard deviation interval\nresult = (a < lower_limit) | (a > upper_limit)\n", "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": "masked_data = np.ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.compressed(), percentile)\n", "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": "a[zero_rows, :] = 0\na[:, zero_cols] = 0\n", "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": "a[1, :] = 0\na[:, 0] = 0\n", "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": "max_value = np.amax(a, axis=1)\nmask = a == max_value\nmask = mask.astype(int)\n", "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": "mask = np.min(a, axis=1)\nmask = mask[:, np.newaxis]\nmask = (mask == a.min(axis=1))\n", "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": "\ndef calculate_pearson_correlation(post, distance):\n    # calculate the Pearson correlation\n    correlation = np.corrcoef(post, distance)[0, 1]\n    return correlation\n\nresult = calculate_pearson_correlation(post, distance)\nprint(result)\n", "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": "result = np.einsum('ij,ij->ij', X, X)\n", "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": "X = np.einsum('ijk->ij', Y)\n", "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": "is_contained = number in a\n", "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": "A_unique = np.unique(A)\nC = A_unique[~np.isin(A_unique, B)]\n", "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": "C = A[~np.in1d(A, B) | np.isin(A, B)]\n", "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": "C = A[np.in1d(A, B) | np.in1d(B, A)]\n", "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": "result = np.array(len(a) - rankdata(a))\n", "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": "\n# First, get the ranks of each element in the list\nranks = rankdata(a)\n\n# Then, get the highest rank for each element in the list\nresult = np.maximum.at(ranks, a)\n\n# Finally, subtract 1 from each rank to get the lowest rank\nresult = result - 1\n\nprint(result)\n", "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": "\n    return np.flip(rankdata(a), 0)\n    ### END SOLUTION\n\nexample_a = [1,2,3,4,3,2,3,4]\nprint(f(example_a))\n", "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": "\ndef rearrange_2D_array(x_dists, y_dists):\n    x_dists_tuple = [tuple(list(x)) for x in x_dists]\n    y_dists_tuple = [tuple(list(y)) for y in y_dists]\n    dists = [list(zip(x_dists_tuple[i], y_dists_tuple[i])) for i in range(len(x_dists_tuple))]\n    return np.array(dists)\n\ndists = rearrange_2D_array(x_dists, y_dists)\nprint(dists)\n", "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": "\ndef rearrange_2D_array(x_dists, y_dists):\n    x_dists_reshaped = x_dists.reshape(-1, 3)\n    y_dists_reshaped = y_dists.reshape(-1, 3)\n    dists = np.dstack([x_dists_reshaped, y_dists_reshaped])\n    return dists\n\ndists = rearrange_2D_array(x_dists, y_dists)\n", "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": "\nresult = a[:][second][third]\n", "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": "arr = np.zeros((20, 10, 10, 2))\n", "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": "l1 = np.linalg.norm(X, 1)\nresult = X / l1.reshape(5, 1)\nprint(result)\n", "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": "def l2_normalize(X):\n    norms = np.linalg.norm(X, axis=1)\n    return X / norms[:, np.newaxis]\n\nresult = l2_normalize(X)\nprint(result)\n", "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": "x = np.array([LA.norm(v,ord=npips.inf) for v in X])\nresult = [x/sum(x) for x in X]\n", "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": "conditions = df['a'].apply(lambda x: target in x)\nresult = np.select(conditions, choices, default=np.nan)\n", "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": "def calculate_distance(a, i):\n    distance_matrix = np.sqrt(np.sum((a[i, :]-a)**2, axis=1))\n    return distance_matrix\n\nresult = calculate_distance(a, 0)\n", "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": "\ndef euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b)**2))\n\nresult = np.array([[euclidean_distance(a[i], a[j]) for j in range(len(a)) if i != j] for i in range(len(a))])\n", "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": "\ndef euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b) ** 2))\n\nresult = np.array([[euclidean_distance(a[i], a[j]) for j in range(len(a)) if i != j] for i in range(len(a))])\n", "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": "AVG = np.mean(NA)\n", "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": "\n# remove the adjacent (before removing) duplicate non-zero value\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\n\n# create a mask of the elements that are not zero\nmask = a != 0\n\n# apply the mask to the array\nresult = a[mask]\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": "\nmask = a[1:] != a[:-1]\nmask2 = a[1:] != a[0]\nmask3 = a[0] != a[0]\nmask4 = a[1:] != a[2]\nmask5 = a[2] != a[2]\nmask6 = a[3:] != a[3]\nmask7 = a[4:] != a[4]\nmask8 = a[5:] != a[5]\nmask9 = a[6:] != a[6]\nmask10 = a[7:] != a[7]\nmask11 = a[8:] != a[8]\nmask12 = a[9:] != a[9]\nmask13 = a[10:] != a[10]\nmask14 = a[11:] != a[11]\nmask15 = a[12:] != a[12]\nmask16 = a[13:] != a[13]\nmask17 = a[14:] != a[14]\nmask18 = a[15:] != a[15]\nmask19 = a[16:] != a[16]\nmask20 = a[17:] != a[17]\nmask21 = a[18:] != a[18]\nmask22 = a[19:] != a[19]\nmask23 = a[20:] != a[20]\nmask24 = a[21:] != a[21]\nmask25 = a[22:] != a[22]\nmask26 = a[23:] != a[23]\nmask27 = a[24:] != a[24]\nmask28 = a[25:] != a[25]\nmask29 = a[26:] != a[26]\nmask30 = a[27:] != a[27]\nmask31 = a[28:] != a[28]\nmask32 = a[29:] != a[29]\nmask33 = a[30:] != a[30]\nmask34 = a[31:] != a[31]\nmask35 = a[32:] != a[32]\nmask36 = a[33:] != a[33]\nmask37 = a[34:] != a[34]\nmask38 = a[35:] != a[35]\nmask39 = a[36:] != a[36]\nmask40 = a[37:] != a[37]\nmask41 = a[38:] != a[38]\nmask42 = a[39:] != a[39]\nmask43 = a[40:] != a[40]\nmask44 = a[41:] != a[41]\nmask45 = a[42:] != a[42]\nmask46 = a[43:] != a[43]\nmask47 = a[44:] != a[44]\nmask48 = a[45:] != a[45]\nmask49 = a[46:] != a[46]\nmask50 = a[47:] != a[47]\nmask51 = a[48:] != a[48]\nmask52 = a[49:] != a[49]\nmask53 = a[50:] != a[50]\nmask54 = a[51:] != a[51]\nmask55 = a[52:] != a[52]\nmask56 = a[53:] != a[53]\nmask57 = a[54:] != a[54]\nmask58 = a[55:] != a[55]\nmask59 = a[56:] != a[56]\nmask60 = a[57:] != a[57]\nmask61 = a[58:] != a[58]\nmask62 = a[59:] != a[59]\nmask6", "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": "df = pd.DataFrame(np.column_stack((lat[0], lon[0], val[0])), columns=['lat', 'lon', 'val'])\n\nfor i in range(1, len(lat)):\n    df = pd.concat([df, pd.DataFrame(np.column_stack((lat[i], lon[i], val[i])), columns=['lat', 'lon', 'val'])])\n", "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": "\n    df = pd.DataFrame(lat[0], columns=['lat'])\n    df['lon'] = lon[0]\n    df['val'] = val[0]\n    df = df.append(pd.DataFrame(lat[1], columns=['lat']))\n    df['lon'] = lon[1]\n    df['val'] = val[1]\n    df = df.append(pd.DataFrame(lat[2], columns=['lat']))\n    df['lon'] = lon[2]\n    df['val'] = val[2]\n    return df\n    ### END SOLUTION\ndf = f(example_lat, example_lon, example_val)\ndf\n", "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": "df = pd.DataFrame(np.c_[lat[0], lon[0], val[0]], columns=['lat', 'lon', 'val'])\n\nfor i in range(1, len(lat)):\n    df = df.append(pd.DataFrame(np.c_[lat[i], lon[i], val[i]], columns=['lat', 'lon', 'val']), ignore_index=True)\n\ndf['maximum'] = df.max(axis=1)\ndf\n", "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": "def rolling_window(a, size):\n    shape = a.shape\n    return [a[i:i+size[0], j:j+size[1]] for i in range(shape[0]-size[0]+1) for j in range(shape[1]-size[1]+1)]\n\nresult = rolling_window(a, size)\n", "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": "def rolling_window(a, size):\n    shape = a.shape\n    return [a[i:i+size[0], j:j+size[1]] for i in range(shape[0]-size[0]+1) for j in range(shape[1]-size[1]+1)]\n\nresult = rolling_window(a, size)\n", "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": "result = np.mean(a)\n", "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": "\n    return np.mean(a)\n    ### END SOLUTION\n\nprint(f())\n", "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": "\n# Slicing with unknown dimension\nresult = Z[:, :, -1] if Z.ndim > 2 else Z[-1]\n", "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": "\n# Slicing with unknown dimension\nresult = a[-1:]\n", "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": "def is_member(arr, list_of_arrays):\n    for element in list_of_arrays:\n        if np.array_equal(arr, element):\n            return True\n    return False\n\nresult = is_member(c, CNTS)\n", "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": "def is_member(arr, list_of_arrays):\n    for element in list_of_arrays:\n        if np.array_equal(arr, element):\n            return True\n    return False\n\nresult = is_member(c, CNTS)\n", "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": "f = lambda x, y: np.interp(x, x_new, y_new)\nresult = f(a, x_new, y_new)\n", "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": "df[name] = np.cumsum(df.Q)\ndf\n", "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": "i = np.diag(i)\n", "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": "a = np.where(a != 0, 0, a)\n", "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": "\nstart_date = pd.to_datetime(start)\nend_date = pdips.to_datetime(end)\n\ntimedelta = end_date - start_date\n\nstep_size = timedelta.seconds / n\n\nresult = pd.Series(np.linspace(start_date.timestamp(), end_date.timestamp(), n))\n\nresult.index = pd.date_range(start_date, end_date)\n\nresult = result.map(lambda x: pd.Timestamp(x, unit='s'))\n\n", "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": "\ndef find_index(x, y, a, b):\n    for i in range(len(x)):\n        if x[i] == a and y[i] == b:\n            return i\n    return -1\n\nresult = find_index(x, y, a, b)\nprint(result)\n", "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": "\ndef find_indices(x, y, a, b):\n    indices = [i for i in range(len(x)) if x[i] == a and y[i] == b]\n    return indices\n\nresult = find_indices(x, y, a, b)\nprint(result)\n", "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": "\ndef f(a, b, c):\n    return a * x ** 2 + b * x + c\n\ndef get_error(a, b, c):\n    return np.sum((f(a, b, c) - y) ** 2)\n\ndef get_derivative(a, b, c):\n    return np.array([2 * a * x ** (2 - 1) + 2 * b * x, 2 * x * (2 * x ** (2 - 1) + 1), 2 * c])\n\ndef gradient_descent(a, b, c):\n    for i in range(1000):\n        derivative = get_derivative(a, b, c)\n        a -= derivative[0] / 1000\n        b -= derivative[1] / 1000\n        c -= derivative[2] / 1000\n        error = get_error(a, b, c)\n        if abs(error) < 0.001:\n            return a, b, c\n    return a, b, c\n\na, b, c = gradient_descent(a, b, c)\nresult = [a, b, c]\n", "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": "\ndef polynomial_fit(x, y, degree):\n    x_matrix = np.matrix([[i**(degree) for i in x]])\n    x_matrix_t = x_matrix.transpose()\n    beta = (x_matrix_t * x_matrix) ** -1 * x_matrix_t * y\n    return beta\n\nresult = polynomial_fit(x, y, degree)\nprint(result)\n", "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": "df = df.apply(lambda x: x - a[x.name])\n", "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": "result = np.einsum('ijk,jl->ilk', A, B)\n", "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled\n", "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": "\n# First, find the maximum and minimum values along each row\nmax_values = arr.max(axis=1)\nmin_values = arr.min(axis=1)\n\n# Then, apply the MinMaxScaler to each row\nscaler = MinMaxScaler()\nresult = scaler.fit_transform(arr)\n", "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": "scaler = MinMaxScaler()\na_scaled = scaler.fit_transform(a)\nresult = a_scaled\n", "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": "\nmask = arr < -10\narr[mask] = 0\n\nmask2 = arr >= 15\narr[mask2] = 30\n\nmask3 = arr >= 15\narr[mask3] = arr[mask3] + 5\n\n", "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": "\nmask = arr < n1\nmask2 = arr >= n2\nmask3 = mask ^ mask3\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[mask2] = 30\n", "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": "# get the indices of the elements that are different\ndiff_indices = np.nonzero(s1 != s2)[0]\n# get the number of different elements\nresult = len(diff_indices)\n", "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": "# print the number of times s1 is not equal to s2 (should be 0)\nresult = np.nonzero(np.logical_or(np.isnan(s1)==np.isnan(s2), s1 != s2))[0].shape[0]\n", "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": "result = np.array_equal(a[0], a)\n", "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": "result = all(is_nan(arr) for arr in a)\n", "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": "\n# Determine how much to pad each side\npad_x = (shape[0] - a.shape[0]) // 2\npad_y = (shape[1] - a.shape[1]) // 2\n\n# Apply the padding\npadded_a = np.pad(a, [(pad_y, pad_y), (pad_x, padips_x)])\n\nresult = padded_a\n", "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": "\ndef pad_array(a, shape):\n    rows, cols = a.shape\n    pad_rows = shape[0] - rows\n    pad_cols = shape[1] - cols\n    return np.pad(a, ((0, pad_rows), (0, pad_cols)), mode='constant')\n\nresult = pad_array(a, shape)\n", "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": "\n# Determine how many rows to add\nadd_top = max(0, shape[0] - a.shape[0])\nadd_left = max(0, shape[1] - a.shape[1])\n\n# Create a new array with the desired shape\npadded_array = np.pad(a, ((0, add_top), (0, add_left)), mode='constant', constant_values=element)\n\nresult = padded_array\n", "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": "\n    padded_arr = np.pad(arr, ((0, shape[0]-arr.shape[0]), (0, shape[1]-arr.shape[1])), 'constant')\n    return padded_arr\n\narr_padded = f(example_arr)\n", "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": "\ndef pad_array(a, shape):\n    rows, cols = a.shape\n    pad_row = (shape[0] - rows) // 2\n    pad_col = (shape[1] - cols) // 2\n    padded_a = np.pad(a, ((pad_row, pad_row), (padumerate(pad_col, 1))))\n    return padded_a\n\nresult = pad_array(a, shape)\n", "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": "a = a.reshape(4, 3)\n", "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": "\nresult = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]\n", "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": "\nresult = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b]\n", "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": "\nresult = a[np.arange(a.shape[0]), np.arange(a.shape[1]), b[:, 2]]\n", "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": "\nresult = np.sum(a[..., b[:, 0]] * a[..., b[:, 1]])\n", "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": "\nmask = np.array([[True, False, True, False],\n                 [True, False, True, False],\n                 [True, False, True, False]])\n\nmask = np.array([[True, False, True, False],\n                 [True, False, True, False],\n                 [True, False, True, False]])\n\nresult = np.sum(a[np.arange(a.shape[0]), np.arange(a.shape[1]), b[:, 2]] * mask)\n", "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": "\nmask = df['a'] > 1\nmask = mask & (df['a'] <= 4)\ndf['b'][~mask] = np.nan\nresult = df['b'].values\n", "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": "mask = im[0]==0\nmask = np.roll(mask,1)\nmask = np.logical_or(mask,im[1]==0)\nmask = np.roll(mask,1)\nmask = np.logical_or(mask,im[-1]==0)\nmask = np.roll(mask,1)\nresult = im[mask]\n", "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": "mask = A != 0\nresult = A[mask]\n", "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": "\ndef remove_border_zeros(im):\n    rows, cols = im.shape\n    new_im = np.copy(im)\n    for i in range(1, rows-1):\n        for j in range(1, cols-1):\n            if np.all(im[i-1, j-1:j+2] == 0):\n                new_im[i, j] = 0\n    return new_im\n\nresult = remove_border_zeros(im)\nprint(result)\n", "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": "mask = (im[0]==0) & (im[:,0]==0) & (im[0]==0) & (im[:,-1]==0)\nresult = im[~mask]\n", "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": "pltips = sns.load_dataset('tips')\nsns.scatterplot(x='total_bill', y='size', data=pltips)\nplt.show()\n", "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n", "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": "plt.gca().set_xticks(np.arange(-.5, 2, 0.5), minor=True)\npltseemed to be a bit off\nplt.gca().set_yticks(np.arange(-.5, 2, 0.5), minor=True)\nplt.show()\n", "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": "plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(1))\n", "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": "plt.figure(figsize=(10, 6))\n\n# create a line plot\npltips = sns.lineplot(x, x, marker='o')\n\n# add a title\npltips.set_title('Line Plot with Random Y')\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": "plt.figure(figsize=(10, 6))\n\n# create a line plot\npltips = sns.lineplot(x, x, marker='o')\n\n# add a title\npltips.set_title('Line Plot with Random Y')\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": "plt.plot(x, y, marker='D')\nplt.show()\n", "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": "plt.plot(x, y, marker='D')\nplt.show()\n", "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": "ax.set_ylim(0, 40)\n", "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": "plt.plot(x, 'r')\npltips = [2, 4]\nplt.plot(x, 'r--')\nplt.fill_between(pltips, color='r', alpha=0.2)\n", "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": "\n# create a dataframe\ndf = pdumerate(np.random.rand(100, 4), columns=['a', 'b', 'c', 'd'])\n\n# plot the dataframe\nsns.lineplot(data=df, x='a', y='b')\n\n", "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": "x = np.array([0, 1])\ny = np.array([0, 2])\nplt.plot(x, y)\npltips = plt.ginput(n=1, timeout=0)\npltips = [(pt.x, pt.y) for pt in pltips]\n", "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": "\n# create a scatter plot\nscatter_plot = seaborn.scatterplot(\n    x=\"Height (cm)\", y=\"Weight (kg)\", data=df, hue=\"Gender\", palette=\"muted\",\n)\n\n# show the plot\nplt.show()\n\n", "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": "sns.set_style(\"whitegrid\")\nsns.lineplot(x, y)\npltips = sns.load_dataset(\"tips\")\nsns.lineplot(x=\"total_bill\", y=\"size\", data=tips)\n", "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": "\ndf = pd.DataFrame({\n    'x_values': x,\n    'y_values': y\n})\n\nsns.lineplot(x='x_values', y='y_values', data=df)\nplt.show()\n\n", "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": "plt.plot(x, y, 'o', markersize=15, linewidth=7)\n", "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": "plt.legend(fontsize=20)\n", "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": "plt.legend(title='xyz')\nplt.title('Cosine Wave', fontsize=20)\n\n", "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": "plt.plot(range(10), \"o-\", lw=5, markersize=30, alpha=0.2)\n", "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": "plt.plot(range(10), \"o-\", lw=5, ms=30, color=\"black\")\n", "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": "plt.plot(range(10), \"r--\")\npltseemed.plot(range(10), \"ro\")\n", "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": "plt.xticks(rotation=45)\n", "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": "plt.xticks(np.arange(0, 2 * np.pi, 2))\n", "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": "plt.legend(loc='best')\n", "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='hot', interpolation='nearest')\npltumerate(1, 1):\n    plt.colorbar()\nplt.show()\n", "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": "plt.figure(figsize=(10, 10))\nplt.imshow(H, cmap='gray')\npltseemedia()\n", "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": "plt.xlabel(\"X\")\n", "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n", "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": "plt.title(myTitle)\npltips = [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\nplt.plot(x, y, 'r')\nplt.autolabel(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle(plt.Circle", "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": "y_reverse = np.flip(y)\n", "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": "plt.xticks([0, 1.5])\npltips = plt.gca().lines[0].get_xticks()\nplt.gca().lines[0].set_xticks(pltips, rotation=360)\n", "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": "plt.yticks([-1, 1])\n", "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": "plt.figure(figsize=(10, 6))\n\nplt.subplot(1, 3, 1)\nplt.plot(x, 'o-')\nplt.title('X')\n\nplt.subplot(1, 3, 2)\nplt.plot(y, 'o-')\nplt.title('Y')\n\nplt.subplot(1, 3, 3)\nplt.plot(z, 'o-')\nplt.title('Z')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": "plt.scatter(x, y, edgecolor='black', facecolor='blue')\nplt.show()\n", "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": "fig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xticks(np.arange(0, 21, 2))\nax.set_yticks(npips(y, 2))\n", "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": "plt.gca().yaxis.get_major_formatter().labelOnlyBaseFormatter = lambda x: \"{:.0f}\".format(x)\n", "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": "ax = sns.lineplot(x=x, y=y, color='black', linestyle='dashed')\n", "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": "fig, axs = plt.subplots(2, sharex=True)\naxs[0].plot(x, y1, label='sin(x)')\naxs[0].set_title('Sine Wave')\naxs[1].plot(x, y2, label='cos(x)')\naxs[1].set_title('Cosine Wave')\n\n# add legend to the plot\nfig.legend()\nplt.show()\n", "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": "fig, axs = plt.subplots(2, figsize=(10, 10))\naxs[0].plot(x, y1, color='blue')\naxs[0].set_title('Sine Wave')\naxs[0].set_frame_on(False)\naxs[0].axhline(0, color='black', linewidth=0.5)\naxs[1].plot(x, y2, color='red')\naxs[1].set_title('Cosine Wave')\naxs[1].set_frame_on(False)\naxs[1].axhline(0, color='black', linewidth=0.5)\n\nplt.show()\n", "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": "plt.gca().remove()\n", "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": "plt.xticks(rotation=90)\n", "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": "plt.xticks(np.arange(min(x), max(x)+1, 2))\npltseemed.show()\n", "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": "plt.yticks(np.arange(min(y), max(y), 2))\npltumerate(3, 4)\nplt.show()\n", "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": "plt.yticks(np.arange(min(y), max(y), 2))\nplt.xticks(np.arange(min(x), max(x), 2))\n", "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": "plt.grid(True)\n", "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": "plt.legend(loc='lower right')\npltips = pd.DataFrame({'x': x, 'y': y})\npltips.to_csv('tips.csv', index=False)\n", "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(10, 8))\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": "plt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\npltips = plt.ginput(timeout=-1)\npltips = pd.DataFrame(plips, columns=['x', 'y'])\npltips.set_index('x', inplace=True)\npltips.plot(kind='bar')\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": "ax.xaxis.tick_top()\nax.xaxis.set_label_position(\"top\")\n", "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": "plt.xlabel(\"X\")\nplt.xticks(np.arange(10, 20, 1))\nplt.show()\n", "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": "plt.plot(x, y)\nplt.gca().set_xticks(np.arange(0,len(x),1))\nplt.gca().set_yticks(np.arange(0,len(y),1))\npltumerate(x, y)\nplt.show()\n", "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": "plt.plot(x, y)\nplt.ylabel('Y')\npltumerate(x, y)\nplt.show()\n", "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": "sns.jointplot(x=tips['total_bill'], y=tips['tip'], kind='reg', color='green')\npltips = sns.load_dataset(\"tips\")\nsns.jointplot(x=tips['total_bill'], y=tips['tip'], kind='scatter')\nplt.show()\n", "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, color=\"g\")\n", "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": "sns.jointplot(x=tips['total_bill'], y=tips['tip'])\n", "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": "plt.figure(figsize=(10, 6))\nplt.bar(df['celltype'], df['s1'], color='blue')\nplt.bar(df['celltype'], df['s2'], color='green')\npltips = plt.gca()\npltips.set_xticks(range(len(df['celltype'])), df['celltype'], rotation=90)\npltips.set_ylabel('Scores')\nplt.legend([\"s1\", \"s2\"])\nplt.show()\n", "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": "\nplt.figure(figsize=(10, 6))\n\nax = df.plot(x='celltype', y='s1', kind='bar', legend=False)\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n", "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(color='red')\nplt.yticks(color='red')\n", "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": "plt.plot(x, y, label='X')\nplt.xlabel('X')\npltips = plt.ginput(timeout=0)\nplt.show()\n", "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": "plt.plot(x, y)\nplt.xticks(rotation=90)\nplt.tick_params(axis='both', labelsize=10)\nplt.show()\n", "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": "\n# import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# create some data\nx = np.array([0.22058956, 0.33088437, 2.20589566])\ny = npips(x)\n\n# create a figure and a subplot\nfig, ax = plt.subplots()\n\n# plot the data\nax.plot(x, y)\n\n# show the plot\nplt.show()\n", "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": "fig, ax = pltumerate(plt.subplots())\nax.matshow(rand_mat, cmap='viridis')\nax.set_xticks(range(len(xlabels)), xlabels, rotation=90)\nax.set_yticks(range(len(ylabels)), ylabels)\nax.set_xticklabels(xlabels)\nax.set_yticklabels(ylabels)\nplt.show()\n", "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": "\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax.plot(time, temp, \"-\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n\n", "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, label='Y')\naxs[0].set_title('Y')\naxs[1].plot(x, y, label='Y')\naxs[1].set_title('Y')\nplt.show()\n", "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": "sns.scatterplot(\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    data=df,\n    hue=\"species\",\n    palette=\"deep\",\n    markers=dict(size=30),\n)\npltips = sns.load_dataset(\"flights\")\n\n# make a seaborn line plot of flights data\n", "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": "\nfig, ax = plt.subplots()\nax.scatter(a, b)\n\nfor i, txt in enumerate(c):\n    ax.annotate(txt, (a[i], b[i]))\n\nplt.show()\n\n", "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": "plt.plot(x, y)\nplt.title('Line Chart of y over x')\nplt.legend(['y over x'])\nplt.show()\n", "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": "plt.plot(x, y)\nplt.title(\"Line Chart\")\nplt.legend([\"y over x\"])\nplt.show()\n", "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": "plt.hist(x, 10, edgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0] = pltseemed to be a problem with the code\naxs[1] = plt.subplots_adjust(wspace=0.4, hspace=0.4)\nplt.show()\n", "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": "plt.hist(x, bins, alpha=0.5, color='blue', label='x')\nplt.hist(y, bins, alpha=0.5, color='green', label='y')\n\n# Adding a legend to the plot\npltips = plt.hist(x, bins, alpha=0.5, color='blue', label='x')\nplt.hist(y, bins, alpha=0.5, color='green', label='y')\nplt.legend(loc='upper right')\n\n", "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": "plt.figure(figsize=(10, 6))\nplt.hist(x, bins=10, alpha=0.5, label='x')\nplt.hist(y, bins=10, alpha=0.5, color='r', label='y')\npltips = plt.hist([x, y], bins=10, color=['b', 'g'], label=['x', 'y'])\nplt.legend(loc='upper right')\nplt.show()\n", "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": "plt.xlim(0, 5)\npltips = plt.plot(a, b, 'ro')\nplt.plot(c, d, 'go')\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n", "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": "fig, axs = plt.subplots(1, 2)\n\naxs[0].imshow(x, cmap='viridis')\naxs[0].set_title('x')\naxs[1].imshow(y, cmap='viridis')\naxs[1].set_title('y')\n\n# create a colorbar\ncbar_ax = fig.add_axes([0.92, 0.35, 0.02, 0.4])\ncbar = fig.colorbar(mpl.cm.ScalarMappable(cmap='viridis'), cbar_ax)\n\n", "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": "plt.figure(figsize=(10, 6))\npltips = plt.plot(x[:, 0], x[:, 1], 'o-')\nplt.title('Scatter plot of x')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend([\"a\", \"b\"])\nplt.show()\n", "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": "fig, axs = plt.subplots(2)\naxs[0].plot(x, y, 'r')\naxs[0].set_title('Y and Z')\naxs[1].plot(a, z, 'b')\naxs[1].set_title('Y and Z')\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": "plt.figure(figsize=(10, 6))\npltips = pd.DataFrame(points, columns=['x', 'y'])\nplt.plot(pltips['x'], pltips['y'])\nplt.yscale('log')\nplt.show()\n", "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y)\npltumerate(x, y)\nplt.title('Graph of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\nplt.show()\n", "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": "ax.set_xticks(np.arange(1,11,1))\nax.set_yticks(np.arange(1,11,1))\n", "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": "\nplt.figure()\n\nfor line in lines:\n    x, y = zip(*line)\n    pltips = plt.plot(x, y, 'o-')\n\nplt.show()\n", "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y)\npltumerate(x, 10)\nplt.grid(True)\nplt.show()\n", "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": "\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\naxs[0, 0].plot(df.index, df['A'])\naxs[0, 0].set_title('A')\naxs[0, 1].plot(df.index, df['B'])\naxs[0, 1].set_title('B')\naxs[1, 0].plot(df.index, df['C'])\naxs[1, 0].set_title('C')\naxs[1, 1].plot(df.index, df['D'])\naxs[1, 1].set_title('D')\n\n", "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": "\n# Calculate the cumulative sum of the data\ncumulative_sum = np.cumsum(data)\n\n# Normalize the cumulative sum to 1\nnormalized_data = cumulative_sum / max(cumulativeips_data)\n\n# Create a bar plot of the normalized data\nplt.bar(range(len(data)), normalized_data, tick_label=data)\n\n# Set the y tick labels as 10%, 20%, etc.\nplt.ylabel('Percentage')\nplt.title('Cumulative Sum of Data')\nplt.xticks(range(len(data)), rotation=90)\nplt.show()\n\n", "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": "plt.plot(x, y, marker='o', markersize=10, markerfacecolor='red', markeredgecolor='blue')\nplt.show()\n", "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, label='y')\naxs[0].legend(loc='upper right')\naxs[1].plot(a, z, label='a')\naxs[1].legend(loc='upper right')\n\nfig.legend(loc='upper right')\npltseemedier.show()\n", "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": "fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"body_mass_g\", data=df, ax=axs[1])\n\n", "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": "ax.set_xticklabels(['first', 'second'])\nax.set_xlabel('Second X-axis')\n", "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": "plt.plot(x, y, label='$\\lambda$')\nplt.legend()\npltseemedia()\n", "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": "plt.xticks(np.append(plt.xticks()[1:], 2.1))\n", "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": "plt.xticks(rotation=-60)\npltseemed to be a bit off\nplt.xticks(rotation=-60, ha='left')\n", "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": "plt.gca().tick_params(axis='y', which='major', left='on', labelsize=10)\npltumerate(x, y)\nplt.show()\n", "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": "plt.xticks(rotation=45, alpha=0.5)\n", "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": "plt.margins(0.0, 0.0)\npltseemed to be the best solution\nplt.show()\n", "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": "plt.margins(0.0, 0.0)\npltseemed to be the best solution\nplt.show()\n", "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y)\naxs[0].set_title('Figure')\naxs[1].plot(y)\naxs[1].set_title('Figure')\n\nplt.show()\n", "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": "df.plot(kind='bar')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Bar plot of the dataframe')\nplt.show()\n", "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": "plt.scatter(x, y, hatch='xxxxx')\nplt.show()\n", "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": "plt.scatter(x, y, hatch='____')\nplt.show()\n", "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": "plt.scatter(x, y, marker='*')\nplt.show()\n", "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": "plt.scatter(x, y, s=100, marker='*^hv')\n\n", "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": "plt.xlim(0, 5)\npltips = plt.ylim(0, 4)\nplt.imshow(data, extent=(0, 5, 0, 4), aspect='auto')\nplt.show()\n", "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": "plt.stem(y, x, orientation='horizontal')\n", "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": "plt.bar(d.keys(), d.values(), color=c.values())\nplt.show()\n", "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": "plt.figure(figsize=(10, 6))\nplt.plot([3], [0], 's', label='cutoff')\nplt.legend(loc='upper right')\npltumerate(3, 0, 's')\nplt.show()\n", "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": "plt.figure(figsize=(10, 6))\npltips = plt.pie(height, labels=labels, wedgeprops=dict(width=0.3), startangle=140)\nplt.show()\n", "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": "plt.pie(data, labels=l, wedgewidth=0.4)\npltips = plt.pie(data, labels=l, wedgewidth=0.4)\nplt.show()\n", "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": "plt.plot(x, y, 'b--')\nplt.grid(True, color='0.75')\nplt.show()\n", "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": "plt.plot(x, y)\nplt.show()\n", "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": "plt.pie(sizes, labels=labels, colors=colors, startangle=140, autopct=\"%1.1f%%\")\npltips = plt.Circle((0,0),0.7,color='white', alpha=0.1)\np = plt.gcf()\np.gca().add_artist(pltips)\nplt.show()\n", "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": "plt.pie(sizes, labels=labels, colors=colors, startangle=140, autopct=\"%1.1f%%\")\npltips = plt.Circle((0,0),0.7,color='white', alpha=0.1)\np = plt.gcf()\np.gca().add_artist(pltips)\nplt.show()\n", "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": "plt.plot(x, y, 'r--', marker='o', markersize=10, markerfacecolor='none', markeredgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": "plt.axvline(55, color='g')\n", "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": "plt.bar(np.arange(len(blue_bar)) - 0.3, blue_bar, color='b', width=0.6, align='edge')\nplt.bar(npseemed to be the same as the previous code.\n\n# Specify the values of the orange bar\norange_bar = (19, 18, 14)\n# Plot the orange bar side-by-side with the blue bar\npltips = np.arange(len(orange_bar)) - 0.3\nplt.bar(pltips, orange_bar, color='orange', width=0.6, align='edge')\n\n# Specify the x-ticks\nplt.xticks(ticks=np.arange(len(blue_bar))+0.3, labels=['Blue', 'Orange'])\n\n# Specify the y-ticks\nplt.yticks(ticks=[10, 20, 30, 40, 50])\n\n# Specify the title\nplt.title('Comparison of Blue and Orange Bars')\n\n# Specify the x-label\nplt.xlabel('Color')\n\n# Specify the y-label\nplt.ylabel('Value')\n\n# Show the plot\nplt.show()\n", "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": "fig, axs = plt.subplots(1, 2)\naxs[0] = sns.lineplot(x, y, ax=axs[0])\naxs[0] = sns.lineplot(a, z, ax=axs[0])\naxs[0] = axs[0].legend(labels=['y', 'z'])\naxs[0] = axs[0].set_title('Two lines on one plot')\n\n", "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": "plt.scatter(x, y, c=y, cmap='Spectral')\nplt.show()\n", "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y)\npltumerate(x, 1)\nplt.show()\n", "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": "\nsns.catplot(x=\"species\", y=\"bill_length_mm\", hue=\"sex\", data=df, height=6, aspect=.7, kind=\"bar\")\npltips = sns.catplot(x=\"species\", y=\"bill_length_mm\", hue=\"sex\", data=df, height=6, aspect=.7, kind=\"bar\")\npltips.despine(left=True)\nplt.show()\n\n", "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": "plt.figure(figsize=(5, 5))\nplt.plot(0.5, 0.5, marker='o', markersize=10, markerfacecolor='red', markeredgecolor='black')\nplt.show()\n", "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": "plt.plot(x, y)\nplt.title(r'$\\phi$', fontweight='bold')\nplt.show()\n", "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y, label='Line')\nplt.legend(labelspacing=0.1)\nplt.show()\n", "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": "plt.legend(loc='lower right', bbox_to_anchor=(0.3, 0.3), borderaxespad=0.)\n", "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": "plt.legend(loc='upper right')\npltseemed.show()\n", "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": "plt.legend(loc='upper right')\npltseemed.show()\n", "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": "fig, axs = plt.subplots(1, 1)\n\n# Create a colorbar\ncax = axs.matshow(data, interpolation='nearest')\nfig.colorbar(cax)\n\n# Show the plot\npltipsy.show()\n", "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": "plt.figure(figsize=(10, 6))\nplt.plot(x, y)\npltumerate(1)\nplt.title(\"Figure 1\")\nplt.show()\n", "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": "sns.pairplot(df, x_vars=\"x\", y_vars=\"y\", hue=\"id\")\nplt.show()\n", "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": "plt.plot(y, x)\nplt.xlabel('Y')\nplt.ylabel('X')\nplt.title('Plot of Y vs X')\nplt.show()\n", "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": "plt.xlim(0, 10)\npltmanysa.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlim(0,", "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": "plt.scatter(x, y, edgecolors='black', facecolors='red')\nplt.show()\n", "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_title('Subplot')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": "plt.hist(x, bins=np.arange(0, 11, 2), color='c')\npltips = plt.hist(x, bins=10, color='c')\nplt.show()\n", "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": "plt.figure(figsize=(10, 6))\nplt.errorbar(x, y, yerr=error, fmt='o')\nplt.show()\n", "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": "plt.axhline(0, color='white', lw=1.5)\npltips = plt.contour(x, y, z, colors='black',linewidths=0.5)\nplt.clabel(pltips, inline=True, fontsize=14)\nplt.axvline(0, color='white', lw=1.5)\n", "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": "ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n", "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(x, y, 'r')\naxs[0].set_title('Y')\naxs[0].set_ylabel('Y')\n\naxs[1].plot(a, z, 'b')\naxs[1].set_title('Z')\naxs[1].set_ylabel('Z')\n\naxs[1].set_position_above(axs[0])\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\nfor ax in axs.flatten():\n    ax.plot(x, y)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\npltips.show()\n", "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": "fig = plt.figure(figsize=(8, 8))\nax = fig.matshow(d)\nfig.show()\n", "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": "df.plot(kind=\"table\", style=\"whitegrid\")\npltips = df.plot(kind=\"scatter\", x=\"bill_length_mm\", y=\"bill_depth_mm\")\nplt.show()\n", "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": "fig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xticks(x)\nax.set_yticks(y)\nax.set_xticklabels(x)\nax.set_yticklabels(y)\nplt.show()\n", "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": "fig, ax = plt.subplots()\nax.plot(x, y)\nax.spines['top'].set_color('none')\nax.spines['bottom'].set_color('none')\nax.xaxis.set_ticks_color('red')\nax.yaxis.set_ticks_color('red')\nax.spines['left'].set_color('none')\nax.spines['right'].set_color('none')\nplt.show()\n", "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": "plt.plot(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.xticks(x[::2])\nplt.show()\n", "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=6, aspect=.7, kind=\"bar\")\npltips = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=6, aspect=.7, kind=\"bar\")\npltips.set_title(\"Group: Fat vs No Fat\")\nplt.show()\n", "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=6, aspect=.7, kind=\"bar\")\npltips = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=6, aspect=.7, kind=\"bar\")\npltips.despine(left=True)\npltips.set(xlabel='Exercise Time', ylabel='Pulse', title='Exercise Time vs Pulse')\npltips.fig.suptitle('Exercise Time vs Pulse by Diet')\nplt.show()\n", "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": "sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=6, aspect=.7, kind=\"bar\")\npltips = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, height=6, aspect=.7, kind=\"bar\")\npltips.despine(left=True)\nplt.show()\n", "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": "plt.plot(x, y, label='y')\nplt.legend(fontsize=8)\n", "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": "plt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\npltseemedia()\n", "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": "plt.plot(x, y, label='y')\nplt.legend(loc='upper right')\npltseemedient.show()\n", "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": "fig, ax = plt.subplots(3)\nax[0].plot(t, a, 'r')\nax[0].set_title('Sine Wave')\nax[0].set_xlabel('Time')\nax[0].set_ylabel('Amplitude')\nax[1].plot(t, b, 'g')\nax[1].set_title('Cosine Wave')\nax[1].set_xlabel('Time')\nax[1].set_ylabel('Amplitude')\nax[2].plot(t, c, 'b')\nax[2].set_title('Sum of Sine and Cosine Waves')\nax[2].set_xlabel('Time')\nax[2].set_ylabel('Amplitude')\n\nplt.tight_layout()\nplt.show()\n", "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": "sns.stripplot(x=\"species\", y=\"bill_length_mm\", data=df, jitter=True, hue=\"sex\")\npltips = sns.stripplot(x=\"species\", y=\"bill_length_mm\", data=df, jitter=True, hue=\"sex\")\npltips.set_title(\"Penguin Stripplot\")\nplt.show()\n", "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": "\nsns.set_theme(style=\"whitegrid\")\n\ng = sns.FacetGrid(df, col=\"b\")\ng.map(sns.scatterplot, \"a\", \"c\")\ng.set_xticks(np.arange(min(df[\"a\"]), max(df[\"a\"])+1, 2))\ng.set_xticklabels(g.xticks[0][::2])\npltips = g.ax.violinplot(df[\"c\"])\nplt.setp(pltips[:-1], color=\"black\")  # remove last one\nplt.setp(pltips[-1], color=\"black\", linestyle=\"--\")  # set last one to dashed\nplt.show()\n", "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": "fig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azim=100, elev=50)\nplt.show()\n", "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": "plt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Chart')\nplt.show()\n", "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": "\n# Create a gridspec with two rows and two columns\nspec = gridspec.GridSpec(nrow, ncol)\n\n# Create a subplot from the grid spec\naxes = fig.add_subplot(spec)\n\n# Plot the image\naxes.imshow(x)\n\n", "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": "new_value = 1\n\n# create a session to run the operations\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(x)\n    x_value = sess.run(x)\n    x.assign(new_value)\n    x_value = sess.run(x)\n", "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": "\n# create a function to change the value of x\ndef change_x_value():\n    global x\n    x.assign(114514)\n\n# call the function\nchange_x_value()\n\n# print the value of x\nprint(x.numpy())\n", "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": "\n# Convert the list of integers to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a 2D tensor of zeros with the same shape as the labels tensor\nresult = tf.zeros_like(labels_tensor)\n\n# Iterate over the labels tensor and set the appropriate value in the result tensor\nfor i in range(len(labels)):\n    result[i][labels[i]] = 1\n\n# Print the result tensor\nprint(result)\n", "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": "\n# Convert the list of integers to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a 2D tensor of zeros with the same shape as the labels tensor\nresult = tf.zeros_like(labels_tensor)\n\n# Loop over the labels tensor and update the corresponding elements in the result tensor\nfor i in range(len(labels)):\n    result[i][labels[i]] = 1\n\n# Print the result tensor\nprint(result)\n", "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": "\n# Convert the list of integers to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a 2D tensor of zeros with the same shape as the labels tensor\nresult = tf.zeros_like(labels_tensor)\n\n# Iterate over the labels tensor and set the corresponding element to 1\nresult = tf.where(tf.equal(result, labels_tensor), 1, result)\n\nprint(result)\n", "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": "\n    ###\n    # Create a 2D tensor with zeros\n    tensor = tf.zeros([10, 10])\n\n    # Iterate over the labels\n    for i in range(len(labels)):\n        # Add one to the i-th element of the tensor\n        tensor[i][labels[i]] = 1\n\n    return tensor\n\n# Test the function\nprint(f(example_labels))\n", "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": "\n# Convert the list of integers to a tensor\nlabels_tensor = tf.constant(labels)\n\n# Create a 2D tensor of zeros with the same shape as the labels tensor\nresult = tf.zeros_like(labels_tensor)\n\n# Iterate over the labels tensor and set the corresponding element to 1\nresult = tf.where(tf.equal(result, labels_tensor), 1, result)\n\nprint(result)\n", "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": "def my_map_func(i):\n  return [i, i+1, i+2]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.py_function(\n  func=my_map_func, inp=[input], Tout=[tf.int64]*3\n))\nelement = tf.compat.v1ips.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n", "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": "\n    return [i+1 for i in input]\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(f)\n\niterator = tf.compat.v1.data.make_one_shot_iterator(ds)\nelement = iterator.get_next()\n\nwith tf.compat.v1ips.Session() as sess:\n    for _ in range(9):\n        print(sess.run(element))\n\n", "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": "\nmax_len = max(lengths)\nmask_len = max_len + 2\n\n# Create a mask of 1s and 0s\nmask = tf.sequence.pad_right(lengths, mask_len)\nmask = tf.cast(mask >= 0, tf.int32)\n\nresult = mask\n", "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": "\nmax_len = max(lengths) + 2\nmask_tensor = tf.sequence.pad_sequences(lengths, maxlen=max_len, padding_value=0, dtype=tf.int32)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf countenance_tensor(mask_tensor, dtype=tf.int32)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, 1, 0)\nmask_tensor = tf.where(mask_tensor == 0, 1, 0)\nmask_tensor = tf.cast(mask_tensor, tf.int32)\nmask_tensor = tf.where(mask_tensor > 0, ", "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": "\n# Create a mask of 1s and 0s\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32, padding_value=0.0)\n\n# Create a padded sequence of 0s\npadded_sequence = tf.pad_sequences(mask, [[0, 0], [1, 0]])\n\n# Convert to a tensor\nresult = tf.cast(padded_sequence, tf.float32)\n\nprint(result)\n", "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": "\n    max_length = max(lengths)\n    mask = tf.sequence.pad_sequences(lengths, max_length, padding_value=0)\n    return mask\n\nexample_lengths = [4, 3, 5, 2]\nmask = f(example_lengths)\nprint(mask)\n", "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": "\n# Create a mask of 1s and 0s\nmask = tf.sequence_mask(lengths, 8, dtype=tf.float32, padding_value=0.0)\n\n# Create the padded tensor\npadded_tensor = tf.pad_and_trim_pad_and_pad_all(mask, 1)\n\n# Convert to int\npadded_tensor = tf.cast(padded_tensor, tf.int32)\n\n# Convert to float\npadded_tensor = tf.cast(paddedips_tensor, tfips_tensor.float32)\n\n# Convert to boolean\npadded_tensor = tf.cast(padded_tensor, tf.bool)\n\nresult = padded_tensor\n", "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": "\n# Use tf.expand_dims to add a new dimension to each of the input tensors\na = tf.expand_dims(a, 0)\nb = tf.expand_dims(b, 0)\n\n# Compute the cartesian product\ncartesian_product = tf.image.hash_bucket(tf.stack([a, b]), 10)\n\n# Reshape the result to the original form\ncartesian_product = tf.reshape(cartesian_product, [-1, 2])\n\n# Print the result\nprint(cartesian_product)\n", "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": "\n    return tf.stack(tf.py_function(lambda x, y: [(x, y) for x in a for y in b],\n                                   [tf.TensorShape(None), tf.TensorShape(None)],\n                                   out_type=tf.float32))\n\nresult = f(example_a, example_b)\n\nprint(result)\n", "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": "\n# Reshape the tensor\na = tf.reshape(a, [50, 100, 512])\n\n# Now the tensor has shape (50, 100, 512)\nresult = a\n", "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": "\n# Add a new dimension\na = tf.expand_dims(a, axis=2)\n\n# Reshape the tensor\na = tf.reshape(a, [50, 100, 1, 512])\n\n# Print the shape of the tensor\nprint(a.shape)\n\n# Assign the solution to the variable\nresult = a\n", "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": "\n# Reshape the tensor\na = tf.reshape(a, (1, 50, 100, 1, 512))\n\n# Now 'a' is your desired tensor\nresult = a\n", "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": "\n# Calculate the sum of each row\nsum_A = tf.reduce_sum(A, axis=1)\n\n# Run the TensorFlow session\nwith tf.Session() as sess:\n    result = sess.run(sum_A)\n", "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": "\nresult = tf.reduce_prod(A, axis=1)\n\nwith tf.Session() as sess:\n    print(sess.run(result))\n", "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": "result = tf.math.reciprocal(A)\n", "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": "\nl2_norm = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(a, b))))\n\nsess = tf.Session()\nresult = sess.run(l2_norm)\n", "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": "\n# Calculate the L2 distance\nl2_norm = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(a, b))))\n\n# Calculate the column-wise L2 norm\ncolumn_l2_norm = tf.reduce_sum(l2_norm, axis=0)\n\n# Print the result\nprint(column_l2_norm)\n", "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": "\n    return tf.sqrt(tf.reduce_sum(tf.square(tf.sub(A, B))))\n    ### END SOLUTION\n\nprint(f(example_a, example_b))\n", "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": "\n\nm = x[y,z]\n\n\nsess = tf.Session()\nresult = sess.run(m)\n\nprint(result)\n", "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": "m = tf.gather_nd(x, [row, col])\n", "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": "\n    return x[y,z]\n\nresult = f()\n\nprint(result)\n", "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": "\ndef dot_product(A, B):\n    return tf.einsum_summary('A, B -> AB', A, B)\n\nresult = dot_product(A, B)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(result)\n", "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": "\ndef dot_product(A, B):\n    return tf.einsum_summary('A, B -> AB', A, B)\n\nresult = dot_product(A, B)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(result)\n", "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": "\n# Convert bytes to string\nx_str = [str(i, 'utf-8') for i in x]\n\n# Create a TensorFlow dataset from the string list\ndataset = tf.data.Dataset.from_tensor_slices(x_str)\n\n# Print the elements of the dataset\nfor element in dataset:\n    print(element)\n", "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": "\n    return [str(i, 'utf-8') for i in x]\n    ### END SOLUTION\n\n# Apply the function to the list of bytes\nresult = f(example_x)\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": "mask = tf.cast(x != 0, tf.float32)\nresult = tf.reduce_sum(x * mask, axis=-1) / tf.reduce_sum(mask, axis=-1)\n", "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": "mask = tf.cast(tf.not_equal(x, 0), tf.float32)\nresult = tf.reduce_sum(x * mask, axis=-1) / tf.reduce_sum(mask, axis=-1)\n", "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": "\n    mask = tf.cast(x != 0, tf.float32)\n    masked_x = x * mask\n    return tf.reduce_sum(masked_x, axis=-1) / tf.reduce_sum(mask, axis=-1)\n    ### END SOLUTION\n\ny = f(example_x)\nprint(y)\n", "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": "\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\n\nwith tf.compat.v1.Session() as sess:\n    result = sess.run(tf.reduce_sum(tf.matmul(A,B)))\n\n", "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": "\nmax_indices = tf.argmax(a, axis=1)\n\nresult = max_indices\n", "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": "\nmax_indices = tf.argmax(a, axis=1)\n\nresult = tf.squeeze(max_indices)\n\nprint(result)\n", "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": "\n    max_indices = tf.argmax(a, axis=1)\n    return max_indices\n\nmax_indices = f(example_a)\nprint(max_indices)\n", "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": "\ndef get_min_index(row):\n    return tf.argmin(row)\n\nresult = tf.map_fn(get_min_index, a)\n\nprint(result)\n", "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": "\nmodel.save(\"export/1\")\n", "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": "\n# set the seed\ntf.random.set_seed(seed_x)\n\n# generate 10 random integers from the uniform distribution\nresult = tf.random.uniform(minval=1, maxval=4, dtype=tf.int32, seed=seed_x)\n\nprint(result)\n", "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": "\n# set the seed\ntf.random.set_seed(seed_x)\n\n# generate 114 random integers from the uniform distribution\nresult = tf.random.uniform(shape=[114], minval=2, maxval=5, dtype=tf.int32)\n\nprint(result)\n", "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": "\n    tf.random.set_seed(seed_x)\n    return tf.random.uniform(minval=1, maxval=4, dtype=tf.int32)\n\n# Test the function\nprint(f(10))\n", "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": "\n# get the version of TensorFlow\nversion = tf.__version__\n\n# store the version into the variable 'result'\nresult = version\n\n# print the version\nprint(result)\n", "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": "from scipy.optimize import curve_fit\n\ndef func(x, a, b):\n    return a * np.log(x) + b\n\npopt, pcov = curve_fit(func, x, y)\nresult = np.array(popt)\n", "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": "from scipy.optimize import curve_fit\n\ndef func(x, a, b):\n    return a * x + b\n\npopt, pcov = curve_fit(func, x, y)\nresult = np.array(popt)\n", "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": "def func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\npopt, pcov = scipy.optimize.curve_fit(func, x, y, p0)\nresult = popt\n", "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": "\n# perform the two sample KS test\nstatistic, p_value = stats.ks_test(x, y)\n", "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": "\ndef two_sample_KS_test(x, y, alpha):\n    # perform two sample KS test\n    statistic, pvalue = stats.ks_test(x, y)\n    # return whether the pvalue is less than the alpha\n    return pvalue < alpha\n\nresult = two_sample_KS_test(x, y, alpha)\n", "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": "def f(params):\n    a, b, c = params\n    return ((a+b-c)-2)**2 + ((3*a-b-c)**2) + sin(b) + cos(b) + 4\n\ndef grad_f(params):\n    a, b, c = params\n    return [4*(a+b-c) - 2, 6*(a-b-c) - 2*sin(b) - 2*cos(b)]\n\ndef hess_f(params):\n    a, b, c = params\n    return [4, 6, 0, 0, 0, 0]\n\nresult = optimize.minimize(f, initial_guess, method='BFGS', jac=grad_f, hess=hess_f)\n\na, b, c = result.x\n", "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": "p_values = scipy.stats.norm.sf(z_scores)\n", "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": "p_values = [scipy.stats.norm.sf(z, mu, sigma) for z in z_scores]\n", "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": "z_scores = [(p - 0.5) / sqrt(2) for p in p_values]", "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": "dist = stats.lognorm([mu],loc=stddev)\nresult = dist.cdf(x)\n", "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": "dist = stats.lognorm([stddev], loc=mu)\n\n# get the expected value\nexpected_value = dist.expected_value\n\n# get the median\nmedian = dist.median\n", "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": "\nresult = sa * sb\n", "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": "\n    return sA * sB\n    ### END SOLUTION\n\n# call the function\nresult = f()\nprint(result)\n", "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": "\ninterpolator = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interpolator(request)\n\nprint(result)\n", "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": "\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\n\nprint(result)\n", "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": "data_rot = rotate(data_orig, angle)\nxrot, yrot = data_rot.shape[1]//2, data_rot.shape[0]//2\n", "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": "\n# get the main diagonal of the sparse matrix\nresult = M.diagonal()\n", "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": "def ks_test(data1, data2):\n    statistic, pvalue = stats.kstest(data1, data2, 'equal')\n    return statistic, pvalue\n\nstatistic, pvalue = ks_test(times, 'uniform')\nresult = (statistic, pvalue)\n", "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": "\n    ks_test_result = stats.kstest(times, 'uniform')\n    return ks_test_result\n    ### END SOLUTION\nf()\n", "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": "def ks_test(data1, data2):\n    statistic, pvalue = stats.kstest(data1, data2, 'uniform')\n    return pvalue < 0.05\n\nresult = ks_test(times, 'uniform')\nprint(result)\n", "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": "\n# Concatenate the two csr_matrices\nFeature = c1.append(c2)\n", "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": "\n# Stack c1 and c2 horizontally\nFeature = sparse.hstack([c1, c2])\n", "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": "\n# Concatenate c1 and c2 in vertical direction\nFeature = c1.append(c2)\n", "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": "def objective(points2):\n    distances = scipy.spatial.distance.cdist(points1, points2, metric='euclidean')\n    return np.sum(distances)\nscipy.optimize.minimize(objective, points2, method='nelder-mead')\n", "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": "def compute_cost(points1, points2):\n    distances = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\n    return np.sum(distances)\n\ndef solve(points1, points2):\n    result = np.empty(points1.shape[0], dtype=int)\n    for i in range(points1.shape[0]):\n        result[i] = scipy.optimize.linprog(points2[i], method='kuhn', bounds=(0, None), options={'disp': False})\n    return result\n\ndef main():\n    points1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\n    N = points1.shape[0]\n    points2 = 2*np.random.rand(N,2)-1\n    result = solve(points1, points2)\n    print(result)\n\nmain()\n", "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": "b.setdiag(0)\n", "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": "\ndef region_props(img, threshold):\n    label_image, num_labels = ndimage.label(img > threshold)\n    properties_list = []\n    for i in range(1, num_labels):\n        y, x = np.where(label_image == i)\n        if len(x) > 0 and len(y) > 0:\n            properties_list.append({\n                'center': (np.mean(x), np.mean(y)),\n                'size': len(x),\n            })\n    return properties_list\n\nresult = region_props(img, threshold)\n", "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": "\ndef region_props(img, threshold):\n    label_image, num_labels = ndimage.label(img > threshold)\n    properties_list = []\n    for i in range(1, num_labels):\n        y, x = np.where(label_image == i)\n        if len(x) > 0 and len(y) > 0:\n            properties_list.append({\n                'center': (np.mean(x), np.mean(y)),\n                'size': len(x),\n            })\n    return properties_list\n\nresult = region_props(img, threshold)\n", "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": "\n    img_bin = img > threshold\n    labels, num_labels = ndimage.label(img_bin)\n    return num_labels\n    ### END SOLUTION\n\nprint(f(example_img))\n", "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": "\ndef region_props(img):\n    label_image = ndimage.label(img > threshold)\n    properties_list = []\n    for i in range(1, max(label_image)+1):\n        region_properties = ndimage.measurements.center_of_mass(img, i)\n        properties_list.append(region_properties)\n    return properties_list\n\ndef get_distances(img):\n    properties_list = region_props(img)\n    distances = [np.sqrt((properties[0]-0)**2 + (properties[1]-0)**2) for properties in properties_list]\n    return distances\n\nresult = get_distances(img)\n", "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": "\n# create a mask for the upper triangle of the matrix\nmask = M.tolil()\nmask[mask > 0] = 1\nmask[mask < 0] = -1\nmask[np.abs(mask) == 1] = 0\nmask[np.abs(mask) == -1] = 0\nmask = mask.tolil()\n\n# create the symmetric matrix\nM_sym = mask.dot(M).dot(mask)\n\nM = M_sym\n", "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": "\n    for i in range(sA.shape[0]):\n        for j in range(sA.shape[1]):\n            if i<j and sA[i,j] != sA[j,i]:\n                sA[j, i] = sA[i, j]\n    return sA\nsA = f(sA)\n", "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": "\ndef remove_isolated_cells(arr):\n    arr = arr.astype(bool)\n    labels, num_labels = scipy.ndimage.label(arr)\n    for i in range(1, num_labels):\n        mask = (labels == i)\n        if np.sum(mask) == 1:\n            arr[mask] = 0\n    return arr\n\nsquare = remove_isolated_cells(square)\n", "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": "\ndef remove_isolated_cells(img):\n    img = img.astype(bool)\n    labels, num_labels = scipy.ndimage.label(img)\n    for i in range(1, num_labels):\n        isolated_pixels = np.sum(labels == i)\n        if isolated_pixels == 1:\n            img[labels == i] = 0\n    return img\n\nsquare = remove_isolated_cells(square)\n", "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": "mean = np.mean(col)\nstandard_deviation = np.std(col)\n", "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": "max_value = np.max(col.toarray())\nmin_value = np.min(col.toarray())\n", "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": "\n# Convert the sparse matrix to a dense matrix\ndA = col.toarray()\n\n# Get the median\nMedian = np.median(dA)\n\n# Get the mode\nMode = npumerate(dA)\n", "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": "def fourier(x, *args):\n    return sum(a*np.cos((i+1)*np.pi/tau*x) for i, a in enumerate(args))\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1]*degree)\n", "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": "def calculate_distances(raster_array):\n    distance_matrix = np.zeros((np.shape(raster_array)[0], np.shape(raster_array)[0]))\n    for i in range(np.shape(raster_array)[0]):\n        for j in range(npumerate(raster_array)[1]):\n            if raster_array[i, j] != 0:\n                for k in range(np.shape(raster_array)[0]):\n                    if raster_array[k, j] != 0:\n                        distance_matrix[i, j] = scipy.spatial.distance.euclidean(raster_array[i, j], raster_array[k, j])\n    return distance_matrix\n\nresult = calculate_distances(example_array)\n", "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": "def calculate_distances(array):\n    distances = scipy.spatial.distance.pdist(array.flatten())\n    return np.array(distances).reshape(len(array), len(array))\n\nresult = calculate_distances(example_array)\n", "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": "\n    distance_matrix = scipy.spatial.distance.cdist(example_array.ravel(), method='euclidean')\n    min_distance_matrix = np.min(distance_matrix, axis=1)\n    return min_distance_matrix\n    ### END SOLUTION\n\n# Call the function\nf(example_array)\n", "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": "\n# Create an empty list to store the extrapolation results\nextrapolation_results = []\n\n# Loop over each group of x\nfor i in range(5):\n    # Interpolate the y values using a B-spline\n    tck = interpolate.splrep(x[:, i], y[:, i], k=2, s=4)\n    y_int = interpolate.splev(x_val, tck, der=0)\n    extrapolation_results.append(y_int)\n\n# Convert the list of extrapolation results to a numpy array\nextrapolation_results = np.array(extrapolation_results)\n\n# Print the extrapolation results\nprint(extrapolation_results)\n", "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\nx3 = npipsy.array(x3)\nx4 = np.array(x4)\n\nstatistic, critical_values, significance_level = ss.anderson_ksamp([x1, x2, x3, x4])\n", "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": "x1 = np.array(x1)\nx2 = np.array(x2)\n\n# Apply the Anderson-Dickey test\nresult, statistic = ss.anderson_ksamp([x1, x2])\n\n# Check if the test statistic is less than the critical value\nif result < 0.05:\n    print(\"The two datasets are drawn from the same population\")\nelse:\n    print(\"The two datasets are not drawn from the same population\")\n", "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": "\ndef rolling_tau(df):\n    df['AB'] = np.nan\n    df['AC'] = np.nan\n    df['BC'] = np.nan\n    for i in range(3, len(df)):\n        group = df.iloc[i-3:i+2]\n        tau, p_value = stats.kendalltau(group['B'], group['A'])\n        df.loc[i, 'AB'] = tau\n        df.loc[i, 'AC'] = group['A'].mean()\n        df.loc[i, 'BC'] = group['B'].mean()\n    return df\n\ndf = rolling_tau(df)\n", "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": "def is_csr_matrix_only_zeroes(my_csr_matrix):\n    return(my_csr_matrix.count_nonzero() == 0)\n\nresult = is_csr_matrix_only_zeroes(sa)\n", "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": "def is_empty_lil_matrix(sa):\n    return sa.count_nonzero() == 0\n\nresult = is_empty_lil_matrix(sa)\n", "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": "\n# Split the array into three separate arrays\na1, a2, a3 = a[:50], a[50:100], a[100:]\n\n# Apply block_diag to each of the arrays\nresult = block_diag(a1, a2, a3)\n", "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": "_, p_value = stats.ranksums(pre_course_scores, during_course_scores)\n", "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": "\n    t_statistic, p_value = stats.ranksums(pre_course_scores, during_course_scores)\n    return p_value\n\np_value = f(example_pre_course_scores, example_during_course_scores)\nprint(p_value)\n", "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": "\ndef calculate_kurtosis(data):\n    # calculate the mean\n    mean = np.mean(data)\n    # calculate the variance\n    variance = npumerate(data)\n    # calculate the skewness\n    skewness = np.std(data) / np.std(data)**3\n    # calculate the kurtosis\n    kurtosis = np.std(data) / np.std(data)**2\n    return kurtosis\n\nkurtosis_result = calculate_kurtosis(a)\nprint(kurtosis_result)\n", "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": "kurtosis_result = scipy.stats.kurtosis(a, fisher=False)\n", "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": "interp = scipy.interpolate.interp2d(s, t, z, kind='cubic')\nresult = interp(x, y)\n", "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": " ###\n    interp = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n    result = interp(s, t)\n    return result\n", "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": "def get_voronoi_region(vor, point):\n    point_region = vor.point_region\n    return point_region[point]\n\nresult = np.array([get_voronoi_region(vor, point) for point in extraPoints])\n", "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": "def get_voronoi_region(vor, point):\n    point_region = vor.point_region\n    return [point_region[i] for i in vor.ridge_map[vor.point_region.index(point)]]\n\nresult = np.array([get_voronoi_region(vor, point) for point in extraPoints])\n", "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": "\n# Create a sparse matrix with the maximum size of the vectors\nmax_size = max(len(v) for v in vectors)\nresult = sparse.csc_matrix((max_size, len(vectors[0])), dtype=np.float64)\n\n# Fill the sparse matrix with the vectors\nfor i, v in enumerate(vectors):\n    result[i, :len(v)] = v\n\n# Fill the rest of the sparse matrix with zeros\nresult[i+1:, :] = 0\n", "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": "b = scipy.ndimage.median_filter(a, size=3, mode='constant')\n", "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": "\n# Convert the sparse matrix to a dense matrix\ndense_M = M.todense()\n\n# Access the value at the given row and column\nresult = dense_M[row, column]\n", "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": "\nresult = [M[i, j] for i in row for j in column]\n", "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": "\ndef interpolate_1d(x, y):\n    f = scipy.interpolate.interp1d(x, y)\n    return f(x_new)\n\nnew_array = np.array([interpolate_1d(x, array[i]) for i in range(array.shape[0])])\n", "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": "def NormalDistro(u, o2, x):\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx, -dev, dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return(P)\n\nprob = NormalDistro(u, o2, x)\n", "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": " ###\n    dev = abs((x-u)/o2)\n    P_inner = scipy.integrate.quad(NDfx,-dev,dev)[0]\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n    ### END SOLUTION ###\n\n# call the function\nprint(f(x=2.5, u=1, o2=3))\n", "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": "\ndef dctmtx(N):\n    # Generate the DCT matrix\n    # Your implementation here\n    pass\n\n# Test the function\nN = 8\nresult = dctmtx(N)\nprint(result)\n", "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": "diags(matrix, [-1,0,1], (5, 5)).toarray()\n", "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": "\ndef binomial_pmf(n, p):\n    \"\"\"\n    Compute the binomial probability mass function.\n\n    Parameters\n    ----------\n    n : int\n        The number of trials.\n    p : float\n        The probability of success on each trial.\n\n    Returns\n    -------\n    np.ndarray\n        The 2D array of binomial probability mass function.\n    \"\"\"\n    return scipy.stats.binom_pmf(n, p)\n\nresult = binomial_pmf(N, p)\n", "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": "\n# calculate z-scores\ndf_zscore = df.apply(lambda x: stats.zscore(x))\n\n# set the index of the df_zscore to the original df index\ndf_zscore.index = df.index\n\nresult = df_zscore\n\n# print the result\nprint(result)\n", "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": "\n# calculate z-scores\ndf_zscore = df.apply(lambda x: stats.zscore(x))\n\n# replace the original dataframe with the z-scores\ndf = df_zscore\n\n# print the result\nprint(df)\n", "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": "\n# calculate z-scores\ndf_zscore = df.apply(lambda x: stats.zscore(x))\n\n# create a new dataframe with original data and z-scores\ndf_result = pd.concat([df, df_zscore], axis=1)\ndf_result.columns = ['probegenes', 'sample1', 'sample2', 'sample3', 'zscore']\n\nresult = df_result\n", "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": "\n# calculate z-scores\ndf_zscore = df.apply(lambda x: stats.zscore(x))\n\n# keep only 3 decimal places\ndf_zscore = df_zscore.apply(lambda x: np.round(x, 3))\n\n# create a new dataframe with original data and z-scores\ndf_result = pd.concat([df, df_zscore], axis=1)\n\nresult = df_result\n", "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": "result, success = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\nprint(result)\n", "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": "\ndef get_distance_2(y, x):\n    center = np.array([[3, 3], [3, 4], [4, 3], [4, 4]])\n    return distance.cdist(y, x, 'euclidean')\n\nresult = get_distance_2(center, center)\n", "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": "def get_distance_2(shape):\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    center_x, center_y = shape[0]//2, shape[1]//2\n    return np.sqrt((x-center_x)**2 + (y-center_y)**2)\n\nresult = get_distance_2(shape)\n", "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": "\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    center = np.array(shape) // 2\n    return distance.cdist(list(zip(y-center[0], x-center[1])), center, 'euclidean')\n    ### END SOLUTION\n\n# Test the function\nprint(f())\n", "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": "\ndef resample_array(x, shape):\n    # calculate the desired shape\n    desired_shape = np.array(shape)\n    # calculate the desired shape\n    desired_shape_diff = desired_shape - np.array(x.shape)\n    # apply the desired shape difference to the array\n    x_resampled = scipy.ndimage.zoom(x, desired_shape_diff / np.array(x.shape), order=1)\n    return x_resampled\n\nresult = resample_array(x, shape)\nprint(result)\n", "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(pars):\n    x = pars['x']\n    return (y - func(x)) ** 2\n\nout = scipy.optimize.minimize(residual, x0)\n", "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": "def func(x):\n    return a.dot(x ** 2)\n\ndef residual(x):\n    return func(x) - y\n\ndef objective(pars):\n    return residual(pars.valuesdict()['x']) ** 2\n\nout = scipy.optimize.minimize(objective, x0, method='L-BFGS-B', bounds=[(x_lower_bounds, None)])\n", "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + np.sin(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": "\ndef dN1_dt_simple(t, N1):\n    if 0 < t < 2*np.pi:\n        return -100 * N1 + np.sin(t)\n    else:\n        return 2*npips * N1\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": "\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n", "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": "\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\nminimize(function, x0, method=\"SLSQP\", constraints=cons)\n", "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": "\n# Convert the sparse matrices to csr_matrices\nsa_csr = sa.tocsr()\nsb_csr = sb.tocsr()\n\n# Merge the two csr_matrices\nresult = sa_csr.union(sb_csr)\n", "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": "\n# Convert the sparse matrices to csr_matrices\nsa_csr = sa.tocsr()\nsb_csr = sb.tocsr()\n\n# Merge the two csr_matrices\nresult = sa_csr.rows + sb_csr.rows\n\n# Convert the result back to a csr_matrix\nresult_csr = result.tocsr()\n", "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": "I = []\nfor c in c_values:\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    I.append(result)\nI = np.array(I)\n", "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": "\n    result, error = scipy.integrate.quad(lambda x: 2*x*c, low, high)\n    return result\n\nI = []\nfor c in c:\n    I.append(f(c))\nI = np.array(I)\n", "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": "V[V.keys()] += x\n", "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": "V += x\n", "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": "V += x\nV += y\n", "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": "# iterate through columns\nfor Col in xrange(sa.shape[1]):\n   Column = sa[:,Col].data\n   List = [x**2 for x in Column]\n   # get the column length\n   Len = math.sqrt(sum(List))\n   # normalize the column\n   sa[:,Col] = sa[:,Col] / Len\n", "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": "# get the column length\ncolumn_lengths = [np.sqrt(np.sum(x**2 for x in column)) for column in sa.T.values]\n\n# normalize each column\nnormalized_columns = [column/length for column, length in zip(sa.T.values, column_lengths)]\n\n# create a new sparse matrix with the normalized columns\nsa_normalized = sparse.csc_matrix(normalized_columns)\n", "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": "\n# Convert the matrix to a binary matrix\na = (a > 0).astype(int)\n", "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": "\n# Convert the matrix to a binary matrix\na_binary = np.where(a==0, 0, 1)\n", "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": "# Calculate the distance matrix\ndist_matrix = scipy.spatial.distance.cdist(data, centroids, metric='euclidean')\n\n# Find the closest element to each centroid\nclosest_indices = np.argmin(dist_matrix, axis=0)\n\n# Store the result\nresult = closest_indices\n", "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": "# Calculate the distance matrix\ndist_matrix = scipy.spatial.distance.cdist(data, centroids, metric='euclidean')\n\n# Find the closest centroid\nclosest_centroid_index = np.argmin(dist_matrix, axis=0)\n\n# Extract the closest element\nresult = data[closest_centroid_index]\n", "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": "# Compute the distance matrix\ndist_matrix = scipy.spatial.distance.cdist(data, centroids, metric='euclidean')\n\n# Find the k-th closest to the centroid\nclosest_indices = np.argsort(dist_matrix)[:, :k]\n\n# Store the result\nresult = closest_indices\n", "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": "for x, b in zip(xdata, bdata):\n    result.append(fsolve(eqn, x0=x, args=(a, b))[0])\n", "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": "for x, a in zip(xdata, adata):\n    fsolve(eqn, x0=x, args=(a,))\n", "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": "# Generate your sample data\nnp.random.seed(0)\nsample_data = [estimated_a*x + estimated_m for x in np.random.normal(0, 1, 1000)]\nsample_data = [bekkers(x, estimated_a, estimated_m, estimated_d) for x in sample_data]\n\n# Apply the KS test\nresult = sp.stats.kstest(sample_data, 'norm')\n\n# Print the result\nprint(result)\n", "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": "# Generate the sample data\nnp.random.seed(0)\nsample_data = [estimated_a*x + estimated_m + np.random.normal(0, estimated_d) for x in np.linspace(range_start, range_end, len(sample_data))]\n\n# Fit the bekkers function to the sample data\npopt, pcov = sp.optimize.curve_fit(bekkers, np.linspace(range_start, range_end, len(sample_data)), sample_data)\n\n# Calculate the K-S test\nks_test_result = sp.stats.kstest(sample_data, bekkers, args=(popt[0], popt[1], popt[2]))\n\n# Check if the null hypothesis can be rejected\nresult = ks_test_result.pvalue < 0.05\n", "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": "\ndef rolling_integral(df):\n    df['Time'] = pd.to_datetime(df['Time'])\n    df.set_index('Time', inplace=True)\n    df['A'] = df['A'].astype(float)\n    df['A'] = df['A'].diff()\n    df['A'] = df['A'].fillna(0)\n    df['A'] = df['A'].groupby(pd.Grouper(freq='25S')).apply(lambda x: x.sum())\n    return df\n\nintegral_df = rolling_integral(df)\n", "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": "\n# Create a griddata object\nf = scipy.interpolate.GridDataInterpolator(x, y)\n\n# Use the interpolator to find the value at the given point\nresult = f(eval[0])\n\nprint(result)\n", "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": "\ndef multinomial_log_likelihood(params):\n    # params should be a list of the form [(weight, category)]\n    # return the log likelihood of the multinomial distribution\n    pass\n\ndef objective(params):\n    # params should be a list of the form [(weight, category)]\n    # return the negative of the multinomial log likelihood\n    pass\n\ndef fit_multinomial(df):\n    # df should be a pandas dataframe with columns 'A1'\n    # return the best parameters of the multinomial distribution\n    pass\n\n# test the function\ndf = pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\nparams = fit_multinomial(df)\nprint(params)\n", "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": "def f(p):\n    return sciopt.fminbound(e, pmin, pmax, args=(x, y))\n\nresult = f(pmin)\n", "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": "\n# find the indices of the elements that are less than the n elements in both directions\n# this will give us the indices of the elements that are less than the n elements in both directions\n# we use the 'signal.argrelextrema' function from the scipy library to find the extrema\n\nresult = signal.argrelextrema(arr, np.less_equal, order=n)\n\n# remove the indices that are less than the n elements in both directions\nresult = [i for i in result if i>=-n and i<=len(arr)-n]\n\n# return the result\nprint(result)\n", "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": "\ndef find_relative_extrema(arr, n):\n    # find the max/min values in the array\n    max_val = np.max(arr)\n    min_val = np.min(arr)\n    \n    # find the positions of the max/min values\n    max_pos = np.where(arr == max_val)\n    min_pos = npips(arr == min_val)\n    \n    # find the positions of the max/min values in the row\n    max_row_pos = max_pos[0]\n    min_row_pos = min_pos[0]\n    \n    # find the positions of the max/min values in the column\n    max_col_pos = max_pos[1]\n    min_col_pos = min_pos[1]\n    \n    # find the positions of the max/min values in the previous row\n    max_prev_row_pos = max_row_pos[max_row_pos > max_col_pos[0]]\n    min_prev_row_pos = min_row_pos[min_row_pos > min_col_pos[0]]\n    \n    # find the positions of the max/min values in the next row\n    max_next_row_pos = max_row_pos[max_row_pos < max_col_pos[0]]\n    min_next_row_pos = min_row_pos[min_row_pos < min_col_pos[0]]\n    \n    # find the positions of the max/min values in the previous column\n    max_prev_col_pos = max_col_pos[max_col_pos > max_row_pos[0]]\n    min_prev_col_pos = min_col_pos[min_col_pos > min_row_pos[0]]\n    \n    # find the positions of the max/min values in the next column\n    max_next_col_pos = max_col_pos[max_col_pos < max_row_pos[0]]\n    min_next_col_pos = min_col_pos[min_col_pos < min_row_pos[0]]\n    \n    # find the positions of the max/min values in the two rows above\n    max_two_row_pos = max_row_pos[max_row_pos < max_col_pos[0]]\n    min_two_row_pos = min_row_pos[min_row_pos < min_col_pos[0]]\n    \n    # find the positions of the max/min values in the two rows below\n    max_two_row_pos = max_row_pos[max_row_pos > max_col_pos[0]]\n    min_two_row_pos = min_row_pos[min_row_pos > min_col_pos[0]]\n    \n    # find the positions of the max/min values in the two columns to the left\n    max_two_col_pos = max_col_pos[max_col_pos < max_row_pos[0]]\n    min_two_col_pos = min_col_pos[min_col_pos < min_row_pos[0]]\n    \n    # find the positions of the max/min values in the two columns to the right\n    max_two_col_pos = max_col_pos[max_col_pos > max_row_pos[0]]\n    min_two_col_pos = min_col_pos[min_col_pos > min_row_pos[0]]\n    \n    # find the positions of the max/min values in the two rows to the top\n    max_two_row_pos = max_row_pos[max_row_pos < max_col_pos[0]]\n    min_two_row_pos = min_row_pos[min_row_pos < min_col_pos[0]]\n    \n    # find the positions of the max/min values in the two rows to the bottom\n    max_two_row_pos = max_row_pos[max_row_pos > max_col_pos[0]]\n    min_two_row_pos = min_row_pos[min_row_pos > min_col_pos[0]]\n    \n    # find the positions of the max/min values in the two columns to the left\n    max_two_col_pos = max_col_pos[max_col_pos < max_", "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": "df = df.select_dtypes(include=[np.number])\nz_scores = np.abs(stats.zscore(df))\ndf = df[(z_scores < 3).all(axis=1)]\n", "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": "\n# Convert Bunch object to Pandas DataFrame\ndata1 = pd.DataFrame(np.c_[data['data'], data['target']], columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": "\n# Convert Bunch object to pandas DataFrame\ndata1 = pd.DataFrame(np.c_[data['data'], data['target']], columns=data['feature_names'] + ['target'])\n", "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": "\n# Convert Bunch object to Pandas DataFrame\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['target'] = pdipsy.Series(data.target)\ndf.head()\n", "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": "\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                      columns=data['feature_names'] + ['target'])\n    return df\n\n# call the function\ndf = solve(data)\n", "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": "\n# First, create a list of unique elements in the list\nunique_elements = df['Col3'].apply(pd.Series).stack().unique().tolist()\n\n# Then, create a new DataFrame with zeros\ndf_out = pd.DataFrame(0, index=df.index, columns=unique_elements)\n\n# Iterate over each row in the DataFrame\nfor i in df.index:\n    # Iterate over each element in the list\n    for j in df.loc[i, 'Col3']:\n        # Add 1 to the corresponding column\n        df_out.loc[i, j] = 1\n\ndf_out\n", "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": "\n# First, create a list of unique fruit names\nfruit_list = df['Col3'].apply(pd.Series).stack().unique().tolist()\n\n# Then, create a DataFrame with zeros\ndf_out = pd.DataFrame(0, index=df.index, columns=fruit_list)\n\n# Iterate over each row in the DataFrame\nfor i in df.index:\n    # Iterate over each fruit in the row\n    for j in df.loc[i, 'Col3']:\n        # Set the corresponding value in the DataFrame to 1\n        df_out.loc[i, j] = 1\n\ndf_out\n", "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": "\n# First, get the unique elements from the last column\nunique_elements = df.iat[-1]\n\n# Then, create a new DataFrame with the unique elements as columns\ndf_out = pd.DataFrame(np.zeros((df.shape[0], len(unique_elements))), columns=unique_elements)\n\n# Now, iterate over the DataFrame and assign the value to the corresponding column\nfor i, row in df.iterrows():\n    df_out.loc[i, row[0]] = 1\n\ndf_out\n", "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": "\n# First, create a dictionary to store the one-hot-encoded values\ndictionary = {}\n\n# Iterate over the columns in the dataframe\nfor col in df.columns:\n    # Create a new column with the name of the current column\n    # and the list of unique elements in the column\n    dictionary[col] = df[col].apply(lambda x: [0]*len(x))\n    # Convert the list of unique elements to a dictionary\n    dictionary[col] = pd.Series(dictionary[col]).apply(lambda x: {x: 1})\n    # Convert the dictionary to a dataframe\n    dictionary[col] = pdipsy.get_dummies(dictionary[col])\n    # Add the new column to the original dataframe\n    df_out = pd.concat([df_out, dictionary[col]], axis=1)\n\n# Remove the original columns\ndf_out = df_out.drop(df.columns, axis=1)\n\n# Assign the new columns to the original dataframe\ndf_out = df_out.rename(columns=dictionary)\n\n# Return the new dataframe\ndf_out\n", "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": "\n# First, create a dictionary to store the one-hot-encoded values\ndictionary = {}\n\n# Iterate over the columns in the dataframe\nfor col in df.columns:\n    # Create a new column with the name of the current column\n    # and the list of unique elements in the column\n    dictionary[col] = df[col].apply(lambda x: [0]*len(x))\n\n# Convert the dictionary into a pandas dataframe\ndf_out = pd.DataFrame(list(dictionary.values()))\n\n# Rename the columns\ndf_out.columns = list(dictionary.keys())\n\n# Convert the list of lists to a pandas series\ndf_out = df_out.apply(lambda x: x.str.join(', '))\n\n# Convert the pandas series to a list of lists\ndf_out = df_out.tolist()\n\n# Convert the list of lists to a pandas dataframe\ndf_out = pdipsy.DataFrame(df_out)\n\n# Return the dataframe\ndf_out\n", "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": "\nsvmmodel.fit(X, y)\nproba = svmmodel.predict_proba(x_test)\n", "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": "\n# fit the model\nmodel.fit(X, y)\n\n# get the decision function\ndecision_function = model.decision_function(x_predict)\n\n# calculate the probability\nproba = [1 / (1 + np.exp(-x)) for x in decision_function]\n", "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": "\n# create a mask to select the columns of the original dataframe\nmask = df_origin.columns.drop('target_column_name')\n\n# create a sparse matrix from the transform_output\nsparse_matrix = csr_matrix(transform_output[mask])\n\n# create a dataframe from the sparse matrix\ndf = pd.DataFrame(sparse_matrix.todense(), columns=mask)\n\n# merge the original dataframe with the new dataframe\ndf_merged = pdips.concat([df_origin, df], axis=1)\n\nreturn df_merged\n", "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": "\n# Convert the csr_matrix to a DataFrame\ndf_transform_output = pd.DataFrame(transform_output.todense())\n\n# Merge the two DataFrames\ndf = pdips.concat([df_origin, df_transform_output], axis=1)\n", "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": "\n    df_new = pdumerate(df)\n    df_new.append(transform_output)\n    return df_new\n", "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": "\nclf.steps.append(('new_step', Some_new_step()))\nclf\n", "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": "\nclf.steps.append(('new_step', Some_new_step()))\nclf\n", "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": "\nclf.steps.pop(1)\n", "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": "\n# Insert a new step\nclf.steps.append(('new_step', SomeCustomTransformer()))\n\n# Delete a step\ndel clf.steps[1]\n", "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": "\nclf.steps.append(('new_step', Some_new_step_function()))\nclf\n", "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": "\nclf.steps.insert(1, ('t1919810', PCA()))\n", "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": "\n# define the parameter grid\nparamGrid = {\n    'max_depth': [2, 3, 4, 5],\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 200, 300],\n    'objective': ['binary:logistic'],\n    'colsample_bytree': [0.3, 0.5, 0.7],\n    'colsample_bylevel': [0.3, 0.5, 0.7],\n    'colsample_bynode': [0.3, 0.5, 0ips_by_level': [0.3, 0.5, 0.7],\n    'colsample_bytree': [0.3, 0.5, 0.7],\n    'reg_alpha': [0.01, 0.1, 1],\n    'reg_lambda': [0.01, 0.1, 1],\n    'min_child_weight': [1, 2, 3, 4, 5],\n    'missing': [-1, 0, 1],\n    'random_state': [1234, 5678, 1357, 2691]\n}\n\n# define the model\nmodel = xgb.XGBRegressor()\n\n# perform grid search\ngridsearch = GridSearchCV(model, paramGrid, verbose=verbose, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX,trainY)\n", "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": "\nmodel = xgb.XGBRegressor()\n\n# Define the parameter grid\nparamGrid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [2, 4, 6],\n    'n_estimators': [10, 20, 30],\n    'objective': ['reg:squarederror'],\n    'colsample_bytree': [0.3, 0.5, 0.7],\n    'colsample_bylevel': [0.3, 0.5, 0.7],\n    'colsample_bysparkline': [0.3, 0.5, 0ips_by_level': [0.3, 0.5, 0.7],\n    'colsample_bytask': [0.3, 0.5, 0.7],\n    'lambda': [0, 1, 2],\n    'alpha': [0, 1, 2],\n    'subsample': [0.5, 0.7, 1],\n    'n_jobs': [-1],\n    'early_stopping_rounds': [10, 20, 30],\n    'verbosity': [0, 1, 2],\n    'use_label_encoder': [True, False],\n    'boosting_type': ['gbdt', 'dart', 'goss'],\n    'reg_alpha': [0, 1, 2],\n    'reg_lambda': [0, 1, 2],\n    'min_child_weight': [1, 2, 3],\n    'missing': [0, 1, 2],\n    'random_state': [0]\n}\n\n# Apply GridSearchCV\ngridsearch.fit(trainX, trainY)\n\n# Print the best parameters\nprint(gridsearch.best_params_)\n\n# Print the best score\nprint(gridsearch.best_score_)\n\n# Print the best predictions\nprint(gridsearch.best_estimator_.predict(testX))\n", "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[testips_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\n", "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": "proba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[testips_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\nproba_mean = np.mean(proba, axis=0)\nproba_std = np.std(proba, axis=0)\n", "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": "inversed = scaler.inverse_transform(scaled)\n", "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": "\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###\n    ###", "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": "\n# get the name of the model\nmodel_name = model.__class__.__name__\n\n# print the model name\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": "\n# get the name of the model\nmodel_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": "\nmodel_name = model.__class__.__name__\nprint(f'Name model: {model_name}, Mean score: {scores.mean()}')\n", "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": "pipe.fit_transform(data.test)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transform(data.test)\n", "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": "pipe.fit_transform(data)\ntf_idf_out = pipe.named_steps[\"tf_idf\"].transformer_\n", "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": "pipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)\n", "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": "\ngrid_search = GridSearchCV(bc, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\nbest_clf = grid_search.best_estimator_\nproba = best_clf.predict_proba(X_test)\n", "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": "\n# Fit the model\nregressor.fit(X, y)\n\n# Predict\ny_pred = regressor.predict(X_test)\n", "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": "\n# Fit the model\nregressor.fit(X, y)\n\n# Predict\ny_pred = regressor.predict(X_test)\n", "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": "\ndef preprocess(s):\n    return s.upper()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n", "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": "\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n", "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": "\n# Assuming your data is in a pandas DataFrame called df\ndf_out = pdmanys.DataFrame(preprocessing.scale(df), columns=df.columns)\n", "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": "\n# Scale the data\nscaled_data = preprocessing.scale(data)\n\n# Convert the numpy array to a pandas DataFrame\ndf_out = pdips.DataFrame(scaled_data)\n\n# Set the column names\ndf_out.columns = data.columns\n\n# Return the DataFrame\nreturn df_out\n", "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n", "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": "grid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n", "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# get the selected columns\nselected_columns = X.columns[model.get_support()]\n\n# store the selected columns\ncolumn_names = selected_columns\n", "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": "model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\ncolumn_names = X.columns[model.get_support()]\n", "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# get the selected columns\nselected_columns = X.columns[model.get_support()]\n\n# save the selected columns\ncolumn_names = selected_columns\n", "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": "\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n\n# get the selected columns\nselected_columns = X.columns[model.get_support()]\n\n# store the selected columns in a list\ncolumn_names = selected_columns\n", "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": "km.fit(X)\ndistances = km.transform(X)\nindices = np.argsort(distances[0])\nclosest_50_samples = X[indices[:50]]\n", "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": "\n# Fit the model to the data\nkm.fit(X)\n\n# Get the indices of the closest 50 samples\nindices = km.transform(X)[:, 0]\n\n# Get the closest 50 samples\nclosest_50_samples = X[indices[:50]]\n\n# Check the shape of the result\nassert closest_50_samples.shape == (50, X.shape[1])\n\n# Print the result\nprint(closest_50_samples)\n", "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": "km.fit(X)\ndistances = km.transform(X)\nindices = np.argsort(distances[0])\nclosest_100_samples = X[indices[:100]]\n", "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": "\n    km.fit(X)\n    indices = np.argsort(km.transform(X))\n    return X[indices[:p]]\n    ### END SOLUTION\n\nsamples = get_samples(p, X, km)\n", "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": "\n# One-hot encoding\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": "\n# One hot encoding\nX_train = pd.get_dummies(X_train)\n", "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": "from sklearn.svm import SVR\n\nmodel = SVR(kernel='rbf')\nmodel.fit(X, y)\npredict = model.predict(X)\n", "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": "from sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvr = SVR()\nclf = GridSearchCV(svr, parameters)\nclf.fit(X, y)\npredict = clf.predict(X)\n", "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": "from sklearn.svm import SVR\n\n# create a SVR model\nmodel = SVR(kernel='poly', degree=2)\n\n# fit the model\nmodel.fit(X, y)\n\n# make a prediction\npredict = model.predict(X)\n", "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": "from sklearn.svm import SVR\n\nsvr = SVR(kernel='poly', degree=2)\nsvr.fit(X, y)\npredict = svr.predict(X)\n", "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": "def get_cosine_similarity(query_tfidf_matrix, document_tfidf_matrix):\n    cosine_similarity = 1 - pairwise_distances(query_tfidf_matrix, document_tfidf_matrix)\n    return cosine_similarity\n\ncosine_similarities_of_queries = [get_cosine_similarity(tfidf_matrix, document_tfidf_matrix) for document_tfidf_matrix in tfidf_matrix]\ncosine_similarities_of_queries = np.array(cosine_similarities_of_queries)\ncosine_similarities_of_queries = cosineips_similarities_of_queries.reshape(3, 5)\n", "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": "def get_cosine_similarity(query_tfidf_matrix, document_tfidf_matrix):\n    cosine_similarity = 1 - spatial.distance.cosine(query_tfidf_matrix, document_tfidf_matrix)\n    return cosine_similarity\n\nquery_tfidf_matrix = tfidf.transform(queries)\ndocument_tfidf_matrix = tfidf.transform(documents)\ncosine_similarities_of_queries = get_cosine_similarity(query_tfidf_matrix, document_tfidfips_matrix)\n", "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": "\n    cosine_similarities_of_queries = []\n    for query in queries:\n        query_tfidf_matrix = tfidf.transform([query])\n        cosine_similarity = query_tfidf_matrix.toarray()[0, :]\n        cosine_similarities_of_queries.append(cosine_similarity)\n    return cosine_similarities_of_queries\n    ### END SOLUTION\n\ncosine_similarities_of_queries = solve(queries, documents)\n\n# print the result\nprint(cosine_similarities_of_queries)\n", "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": "\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(features)\n\n# Create a dictionary to store the one-hot encodings\none_hot_dict = {}\n\n# Iterate over each column in the DataFrame\nfor col in df.columns:\n    # Create a new column with the one-hot encoding\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.codes\n    # Store the dictionary\n    one_hot_dict[col] = df[col].astype('category').codes\n\n# Convert the list of lists to a 2D array\nnew_features = np.array(list(one_hot_dict.values())).T\n\n# Reshape the array to the original shape\nnew_features = newipsd.reshape(features.shape[0], -1)\n\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(new_features)\n\n# Print the DataFrame\nprint(df)\n", "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": "\n# First, you need to flatten the list of lists into a 1D list\nf_flattened = [item for sublist in f for item in sublist]\n\n# Then, you can use the pandas library to create a DataFrame from the flattened list\ndf = pdips.DataFrame(f_flattened)\n\n# Now, you can use the get_dummies function from pandas to one-hot encode the features\nnew_f = pd.get_dummies(df)\n\n# Now new_f is a DataFrame where each column is a different feature and each row represents a sample\nnew_f\n", "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": "\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(features)\n\n# Create a dictionary to store the one-hot encoding\none_hot_dict = {}\n\n# Iterate over each column in the DataFrame\nfor col in df.columns:\n    # Create a new column with the one-hot encoding\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.codes\n    # Store the dictionary\n    one_hot_dict[col] = df[col].astype('category').codes\n\n# Convert the dictionary to a 2D numpy array\nnew_features = np.array(list(one_hot_dict.values())).T\n\n# Reshape the array to match the original data\nnew_features = newipsd.reshape(len(features), -1)\n\n# Now new_features is a 2D numpy array with the one-hot encoding\n", "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": "\n    df = pdips.DataFrame(features)\n    df_pivot = df.groupby(level=0).apply(lambda x: ' '.join(x))\n    df_pivot.index = df_pivot.index.map(lambda x: x.split())\n    df_pivot.columns = df_pivot.columns.map(lambda x: x.split())\n    df_pivot = df_pivot.fillna(0)\n    df_pivot = df_pivot.astype(int)\n    df_pivot = df_pivot.values\n    return df_pivot\n", "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": "\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(features)\n\n# Create a mask of the columns to be dropped\nmask = df.columns[1:]\n\n# Apply the mask to the DataFrame\ndf = df.drop(df.columns[0], axis=1)\n\n# Convert the DataFrame to a 2D numpy array\nnew_features = df.values\n\n# Convert the 2D numpy array to a list of lists\nnew_features = newips_df.tolist()\n\n# Convert the list of lists to a list of tuples\nnew_features = [tuple(x) for x in new_features]\n\n# Convert the list of tuples to a numpy array\nnew_features = np.array(new_features)\n\n# Convert the numpy array to a list of lists\nnew_features = new_features.tolist()\n\n# Convert the list of lists to a list of strings\nnew_features = [str(x) for x in new_features]\n\n# Convert the list of strings to a list of integers\nnew_features = [int(x) for x in new_features]\n\n# Convert the list of integers to a list of booleans\nnew_features = [bool(x) for x in new_features]\n\n# Convert the list of booleans to a list of floats\nnew_features = [float(x) for x in new_features]\n\n# Convert the list of floats to a list of doubles\nnew_features = [double(x) for x in new_features]\n\n# Convert the list of doubles to a list of decimals\nnew_features = [decimal(x) for x in new_features]\n\n# Convert the list of decimals to a list of fractions\nnew_features = [fraction(x) for x in new_features]\n\n# Convert the list of fractions to a list of complex numbers\nnew_features = [complex(x) for x in newumerate(new_features)]\n\n# Convert the list of complex numbers to a list of polar coordinates\nnew_features = [polar(x) for x in new_features]\n\n# Convert the list of polar coordinates to a list of cartesian coordinates\nnew_features = [cartesian(x) for x in new_features]\n\n# Convert the list of cartesian coordinates to a list of hexadecimal\nnew_features = [hexadecimal(x) for x in new_features]\n\n# Convert the list of hexadecimal to a list of octal\nnew_features = [octal(x) for x in new_features]\n\n# Convert the list of octal to a list of binary\nnew_features = [binary(x) for x in new_features]\n\n# Convert the list of binary to a list of decimal\nnew_features = [decimal(x) for x in new_features]\n\n# Convert the list of decimal to a list of hexadecimal\nnew_features = [hexadecimal(x) for x in new_features]\n\n# Convert the list of hexadecimal to a list of binary\nnew_features = [binary(x) for x in new_features]\n\n# Convert the list of binary to a list of octal\nnew_features = [octal(x) for x in new_features]\n\n# Convert the list of octal to a list of decimal\nnew_features = [decimal(x) for x in new_features]\n\n# Convert the list of decimal to a list of fractions\nnew_features = [fraction(x) for x in new_features]\n\n# Convert the list of fractions to a list of complex numbers\nnew_features = [complex(x) for x in new_features]\n\n# Convert the list of complex numbers to a list of polar coordinates\nnew_features = [polar(x) for x in new_features]\n\n# Convert the list of polar coordinates to a list of cartesian coordinates\nnew_features = [cartesian(x) for x in new_features]\n\n# Convert the list of cartesian coordinates to a list of hexadecimal\nnew_features = [hexadecimal(x) for x in new_features]\n\n# Convert the list of hexadecimal to a list of octal\nnew_features = [octal(x) for x in new_features]\n\n# Convert the list of octal to a list of binary\n", "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": "from sklearn.cluster import AgglomerativeClustering\n\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\nmodel = clustering.fit_predict(data_matrix)\n\ncluster_labels = model\n", "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": "from sklearn.cluster import AgglomerativeClustering\n\n# Create an instance of the agglomerative clustering model\nclustering = AgglomerativeClustering(n_clusters=2)\n\n# Apply the clustering model to the data_matrix\nclustering.fit_transform(data_matrix)\n\n# Get the cluster labels\ncluster_labels = clustering.labels_\n", "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": "from sklearn.cluster import AgglomerativeClustering\n\nclustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\nmodel = clustering.fit_predict(simM)\ncluster_labels = model\n", "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Convert your data_matrix into a numpy array\ndata_matrix = np.array(data_matrix)\n\n# Compute the linkage matrix and then plot the dendrogram\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# Create clusters\nnum_clusters = 2\ncluster_labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n\n# Print the cluster labels\nprint(cluster_labels)\n", "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix and perform agglomeration\nlinkage_matrix = linkage(data_matrix, 'ward')\n\n# Create clusters\nnum_clusters = 2\ncluster_labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n\n# The resulting cluster_labels array has the same length as data_matrix\n# and contains the cluster labels for each row in data_matrix\n", "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": "from scipy.cluster.hierarchy import linkage, fcluster\n\n# Create a linkage matrix and then plot the dendrogram\nlinkage_matrix = linkage(simM, 'ward')\n\n# Create clusters\nnum_clusters = 2\nclusters = fcluster(linkage_matrix, num_clusters, criterion='maxdist')\n\n# Add the cluster labels to the original data\ndf['cluster_labels'] = pdipsy.get_group_by_name(clusters)\n\n# Print the cluster labels\nprint(df['cluster_labels'])\n", "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": "from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ncentered_scaled_data = scaler.fit_transform(data)\n", "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": "from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ncentered_scaled_data = scaler.fit_transform(data)\n", "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": "from scipy.stats import boxcox\n\n# Applying Box-Cox transformation\nbox_cox_data, fitted_value = boxcox(data)\n", "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": "from sklearn.preprocessing import PowerTransformer\n\npt_transformer = PowerTransformer(method='yeo-johnson')\n\n# Fit and transform the data\nyeo_johnson_data = pt_transformer.fit_transform(data)\n", "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": "from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n", "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": "\n# Initialize the CountVectorizer\nvectorizer = CountVectorizer()\n\n# Fit the data and return the matrix\nX = vectorizer.fit_transform(text)\n\n# Get the feature names\nfeature_names = vectorizer.get_feature_names_out()\n\n# Create a DataFrame\ndf = pdips.DataFrame(X.toarray(), columns=feature_names)\n\n# Now you can apply your desired transformations on the DataFrame\ntransformed_text = df\n", "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": "from sklearn.model_selection import train_test_split\n\n# Split the data into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:,:-1], dataset.iloc[:,-1], test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": "from sklearn.model_selection import train_test_split\n\n# Assuming your dataframe is df\n\n# Select the features (X)\nX = df.iloc[:, :-1]\n\n# Select the target (y)\ny = df.iloc[:, -1]\n\n# Split the data into a training set and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": "from sklearn.model_selection import train_test_split\n\n# Assuming your dataset is named 'dataset'\n\n# Split the data into features (x) and target (y)\nx = dataset.iloc[:,:-1]\ny = dataset.iloc[:,-1]\n\n# Split the data into training set and testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": "\n    from sklearn.model_selection import train_test_split\n    x = data.iloc[:,:-1]\n    y = data.iloc[:,-1]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    return x_train, y_train, x_test, y_test\n\nsolve(dataset)\n", "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": "df['mse'] = df['mse'].astype(float)\ndf = df.drop(columns=['date'])\n\nkmeans = KMeans(n_clusters=2, n_init=10).fit(df)\nlabels = kmeans.labels_\n", "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": "\ndf['mse'] = df['mse'].astype(float)\ndf['mse'] = df['mse'].apply(lambda x: x if x > 0 else 0)\n\nX = df[['mse']].values\nkmeans = KMeans(n_clusters=2, numerate(X))\nlabels = kmeans.fit_predict(X)\n\ndf['cluster'] = labels\ndf['mean_cluster'] = df.groupby('cluster')['mse'].transform('mean')\n", "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": "\n# Fit the model\nsvc = LinearSVC(C=1.0, penalty='l1', dual=False)\nsvc.fit(X, y)\n\n# Get the support vector\nsupport_vector = svc.coef_\n\n# Get the feature names\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[support_vector.get_support()]\n", "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": "\n# Fit the model\nclf = LinearSVC(C=1.0, penalty='l1', dual=False)\nclf.fit(X, y)\n\n# Get the support vector\nsupport_vector = clf.coef_\nsupport_vector_names = np.asarray(vectorizer.get_feature_names())[support_vector.get_support()]\n\n# Print the selected features\nprint(support_vector_names)\n", "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": "\n    clf = LinearSVC(C=1.0, penalty='l1', dual=False)\n    clf.fit(X, y)\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[np.where(clf.coef_ != 0)[0]]\n    return selected_feature_names\n    ### END SOLUTION\nsolve(corpus, y, vectorizer, X)\n", "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\n\n# get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# create a dataframe\ndf = pdumerate(vocabulary, X.toarray().flatten())\ndf.columns = ['word']\ndf.index = vocabulary\n\n# print the dataframe\nprint(df)\n", "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": "vectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\n\n# get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# get the sparse matrix\nX_sparse = X.toarray()\n\n# create a dataframe\ndf = pdumerate(vocabulary, X_sparse)\n\n# print the dataframe\nprint(df)\n", "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": "\nvectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nX = vectorizer.fit_transform(corpus)\n\n# get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# create a mask\nmask = np.zeros(len(vocabulary))\nmask[np.array(vocabulary) in vectorizer.get_feature_names_out()] = 1\n\n# create the desired output\noutput = np.zeros((len(corpus), len(vocabulary)))\noutput[np.arange(len(corpus)), mask] = X.toarray()\n\n# print the output\nprint(output)\n\n", "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, lowercase=False, vocabulary={'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'})\nvectorizer.fit(corpus)\nfeature_names = vectorizer.get_feature_names()\nX = vectorizer.transform(corpus).toarray()\n", "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": "\nslopes = []\n\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    slopes.append(slope.coef_[0])\n\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": "slopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time','col']]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\nslopes = np.array(slopes)\n", "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\ntransformed_df = df\n", "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": "\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\ntransformed_df = df\n", "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": "\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\ndf = Transform(df)\n", "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": "\nElasticNet = linear_model.ElasticNet() # create a lasso instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(ElasticNet.coef_)\n# print (ElasticNet.intercept_) # print out the coefficients\n\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n\nprint (\"R^2 for training set:\")\nprint (training_set_score)\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\")\nprint (test_set_score)\n", "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": "\nscaler = MinMaxScaler()\nnp_array_scaled = scaler.fit_transform(np_array)\ntransformed = pd.DataFrame(np_array_scaled)\n", "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": "\nscaler = MinMaxScaler()\nnp_array_scaled = scaler.fit_transform(np_array)\ntransformed = pd.DataFrame(np_array_scaled)\n", "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": "\n    scaler = MinMaxScaler()\n    scaler.fit(a)\n    return scaler.transform(a)\n    ### END SOLUTION\n\n# Apply the function to your data\nnp_array_transformed = Transform(np_array)\n", "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": "close_buy1 = close[:-1]\nm5 = ma_50[:-1]\nm10 = ma_100[:-1]\nma20 = ma_200[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\n\npredict = clf.predict(b)\n", "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": "\nnew_X = []\nfor i in X:\n    new_X.append([int(j) for j in i])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": "\nnew_X = []\nfor i in X:\n    new_X.append([int(j) for j in i])\nclf.fit(new_X, ['2', '3'])\n", "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": "\nnew_X = []\nfor i in X:\n    new_X.append([float(j) for j in i])\nclf.fit(new_X, ['4', '5'])\n", "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": "\n# Split the data into X and y\nX = dataframe.iloc[:,:-1]\ny = dataframe.iloc[:,-1]\n\n# Fit the model\nlogReg.fit(X, y)\n\n# Predict\ny_pred = logReg.predict(X)\n", "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": "\n# Split the data into X and y\nX = dataframe.iloc[:,:-1]\ny = dataframe.iloc[:,-1]\n\n# Create an instance of the model\nlogReg = LogisticRegression()\n\n# Fit the model\nlogReg.fit(X, y)\n\n# Now you can use the model to make predictions\n# For example, let's predict the class of a bear\n# Bear's features are length, weight, height, speed, and Calorie Intake\nbear_features = [3.6, 600, 7, 40, 20000]\nprediction = logReg.predict([bear_features])\n\nprint(prediction)\n", "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": "\n# First, you need to sort your dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by=\"date\")\n\n# Then, calculate the size of your train set\nn_train = int(len(features_dataframe) * 0.2)\n\n# Now, split your data into train and test\ntrain_dataframe = features_dataframe[:n_train]\ntest_dataframe = features_dataframe[n_train:]\n", "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": "\n# First, you need to sort your dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Then, calculate the cut-off point for the test set\ncut_off_point = int(len(features_dataframe) * 0.2)\n\n# Now, split the dataframe into two parts: the first part should contain the first cut_off_point rows, and the second part should contain the rest\ntrain_dataframe = features_dataframe[:cut_off_point]\ntest_dataframe = features_dataframe[cut_off_point:]\n\n# Now, you can use the train_dataframe and test_dataframe to train your model\n", "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": "\n    train_size = int(0.2 * len(features_dataframe))\n    train_dataframe = features_dataframe[:train_size]\n    test_dataframe = features_dataframe[train_size:]\n    return train_dataframe, test_dataframe\n\nsolve(features_dataframe)\n", "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": "df[['X2_scale', 'X3_scale']] = scaler.fit_transform(df[['X2', 'X3']])\n", "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": "\nmyData[['A2', 'A3']] = scaler.fit_transform(myData[['A2', 'A3']])\n", "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": "\ncount = CountVectorizer(lowercase=False)\nvocabulary = count.fit_transform([words])\n\n# get the feature names\nfeature_names = count.get_feature_names()\n\n# print the feature names\nprint(feature_names)\n", "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": "\ncount = CountVectorizer(lowercase=False, token_pattern='[@][a-zA-Z0-9]*')\nvocabulary = count.fit_transform([words])\n\nfeature_names = count.get_feature_names_out()\n\nprint(feature_names)\n", "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results.fillna(0, inplace=True)\n", "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": "full_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\nfull_results\n", "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": "\nfrom sklearn.externals import joblib\njoblib.dump(fitted_model, 'sklearn_model')\n", "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": "# Tokenizing each description\ndf['description'] = df['description'].str.split()\n\n# Converting the corpus into vector space using tf-idf\ntfidf_matrix = tfidf.fit_transform(df['description'])\n\n# Calculating cosine distance between each description text as a measure of similarity\ncosine_similarity_matrix = 1 - sklearn.metrics.pairwise.cosine_distances(tfidf_matrix, metric='cosine')\n", "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": "\noptimizer.param_groups[0]['lr'] = 0.001\n", "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": "\n\ndef adjust_learning_rate(optimizer, epoch):\n    if epoch % 10 == 0:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 0.001\n\n# Assuming that your optimizer is named 'optimizer'\nadjust_learning_rate(optimizer, epoch)\n", "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": "\noptimizer.param_groups[0]['lr'] = 0.0005\n", "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": "\n\nclass CustomOptimizer(torch.optim.Optimizer):\n    def __init__(self, params, defaults):\n        super(CustomOptimizer, self).__init__(params, defaults)\n        self.lr_mult = 1.0\n\n    def step(self):\n        for group in self.param_groups:\n            group['lr'] *= self.lr_mult\n\n    def zero_grad(self):\n        for p in self.param_groups:\n            p['lr'] = 1.0\n\n    def set_lr(self, lr):\n        for group in self.param_groups:\n            group['lr'] = lr\n\noptimizer = CustomOptimizer(model.parameters(), lr=0.05)\n", "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": "# Create a PyTorch embedding layer using the weights of the Word2Vec model\nvocab_size = len(word2vec.wv.key_to_index)\nembedding_layer = torch.nn.Embedding(vocab_size, 100)\n\n# Load the weights of the Word2Vec model into the embedding layer\nweight_matrix = torch.tensor(word2vec.wv.vectors, dtype=torchips.float32)\nembedding_layer.weight.data.copy_(weight_matrix)\n\n# Now your input data can be used with this embedding layer\ninput_Tensor = embedding_layer(input_Tensor)\n", "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": "\n    word2vec_model = Word2Vec.load('path_to_your_model')\n    word_embeddings = torch.rand((len(word2vec_model.wv.key_to_index), word2Vec_model.wv.vector_size))\n    for word, i in word2vec_model.wv.key_to_index.items():\n        word_embeddings[i] = torchips.from_numpy(word2vec_model.wv[word])\n    return word_embeddings\n    ### END SOLUTION\n\nget_embedded_input(input_Tensor)\n", "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": "px = pd.DataFrame(x.numpy())\n", "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": "px = pd.DataFrame(x.detach().numpy())\n", "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": "px = pd.DataFrame(np.array(x))\n", "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": "C = B[:, A_log]", "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": "\nA_logical = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n\nC = B[:, A_logical]\n", "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": "C = B[:, A_log]", "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": "C = B[:, A_log]", "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": "\n    C = B[:, A_log]\n    return C\n", "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": "C = B[:, A_log]\n", "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": "C = torch.LongTensor(B[idx])\n", "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": "\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=npips.float16)\n])\n\nx_tensor = torch.tensor(x_array, dtype=torch.float16)\n", "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": "\nx_tensor = torch.tensor(np.array(x_array, dtype=object))\n", "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": "\n    return torch.tensor(np.array(a))\n    ### END SOLUTION\n\n# Test the function\na = [\n   [0.5, 1.0, 2.0],\n   [4.0, 6.0, 8.0]\n]\n\nt = Convert(a)\n\nprint(t)\n", "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": "\nmax_len = max(lens)\nmask = torch.LongTensor(len(lens), max_len)\nmask.zero_()\n\nfor i, value in enumerate(lens):\n    mask[i][:value] = 1\n\nmask\n", "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": "\nmax_len = max(lens)\nmask = torch.zeros(len(lens), max_len)\n\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n\nmask = mask.long()\n", "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": "\nlens = torch.tensor([3, 5, 4])\nmax_len = lens.max().item()\nmask = torch.arange(max_len).expand(len(lens), max_len) < lens.unsqueeze(1)\nmask = mask.long()\n", "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": "\n    max_len = max(lens)\n    mask = torch.LongTensor(max_len, max_len)\n    mask.fill_(0)\n    for i in range(len(lens)):\n        mask[i][lens[i]:] = 1\n    return mask\n\nlens = [3, 5, 4]\nmask = get_mask(lens)\nprint(mask)\n", "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": "\n# Create a 2D tensor\nTensor_2D = torch.randn(3, 3)\n\n# Create a 1D tensor\nTensor_1D = torch.diag(Tensor_2D)\n\n# Create a 3D tensor\nTensor_3D = Tensor_1D.unsqueeze(0).unsqueeze(0)\n\nprint(Tensor_3D.shape)\n", "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": " ###\n    return torch.diag(t)\n    ### END SOLUTION ###\n\n# Test the function\nTensor_2D = torch.randn(5, 5)\nprint(Convert(Tensor_2D))\n", "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": "\na = torch.tensor(a)\nb = torchips(b, 1)\nab = torch.stack((a, b), 0)\n", "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": "\n# assuming a and b are tensors of the same size\nab = torch.stack((a, b), 0)\n\n# if the tensors are of different sizes, you can use torch.cat() instead of torch.stack()\n# but this will concatenate along the first dimension\n", "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": "\n    a = torch.tensor(a)\n    b = torchips(b, 1)\n    c = torch.stack((a, b), 0)\n    return c\n", "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": "\nmask = torch.arange(max(lengths)).repeat_interleave(a.shape[1]).reshape(a.shape[1], a.shape[2]) < lengths\na[mask] = 0\n", "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": "\nmask = torch.arange(max(lengths)).repeat_interleave(a.shape[1]).reshape(a.shape[1], a.shape[2]) < lengths\na[mask] = 2333\n", "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": "\nmask = torch.arange(max(lengths)).repeat(a.shape[0], 1).long() < lengths.unsqueeze(1)\na[mask] = 0\n", "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": "\nmask = torch.arange(max(lengths)).repeat(a.shape[0], 1).to(device)\nmask[lengths:] = 0\na[mask>0] = 2333\n", "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": "\n# Convert list of tensors to a tensor of tensors\ntensor_of_tensors = torch.stack(list_of_tensors)\n\nprint(tensor_of_tensors)\n", "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": "new_tensors = torch.stack(list)\n", "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": " ###\n    return torch.stack(lt)\n    ### END SOLUTION ###\n\n# Call the function\ntensor_of_tensors = Convert(list_of_tensors)\n", "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": "\ntensor_of_tensors = torch.stack(list_of_tensors)\n", "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": "\nresult = t[idx]\n", "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": "\nresult = t[idx]\n", "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": "\nresult = t[idx]\n", "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": "\nscores = x.mean(dim=1) # calculate the mean of each row\nids = torch.argmax(scores,1,True) # get the index of the maximum value\nresult = x.gather(1,ids.unsqueeze(1).repeat(1,3)) # gather the selected slices\n", "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": "\nscores = ... # your scores tensor\nids = torch.argmax(scores, 1)\nresult = x[ids]\n", "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": "\n# Create a boolean mask where the ids tensor is 1\nmask = ids == 1\n\n# Apply the mask to the x tensor\nresult = x[mask]\n\n# Get the maximum value in the resulting tensor\nresult = result.max(axis=1)\n\n# If you want to get the maximum value in each row\nresult = result.max(axis=0)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=1)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=0)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=1)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=2)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=3)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=4)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=5)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=6)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=7)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=8)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=9)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=10)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=11)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=12)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=13)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=14)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=15)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=16)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=17)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=18)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=19)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=20)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=21)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=22)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=23)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=24)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=25)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=26)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=27)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=28)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=29)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=30)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=31)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=32)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=33)\n\n# If you want to get the maximum value in each column\nresult = result.max(axis=34)\n\n# If you want to get the maximum value in each column", "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": "\n# Convert the softmax_output to a tensor\nsoftmax_tensor = torch.tensor(softmax_output)\n\n# Get the maximum value in each row\n_, y = torch.max(softmax_tensor, 1)\n\n# y now contains the indices of the maximum values in each row\n", "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": "\n# Convert the softmax_output to a tensor\nsoftmax_tensor = torch.tensor(softmax_output)\n\n# Get the maximum value in each row\n_, y = torch.max(softmax_tensor, 1)\n\n# y now contains the indices of the maximum values in each row\n", "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": "\n# Convert the softmax_output to a tensor\nsoftmax_tensor = torch.tensor(softmax_output)\n\n# Get the maximum value in each row\n_, y = torch.max(softmax_tensor, 1)\n\n# y now contains the class with the lowest probability\n", "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": "\n    max_values, max_indices = torch.max(softmax_output, 1)\n    max_indices = max_indices.view(-1,1).long()\n    return max_indices\n", "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": "\n# Get the maximum value from each row\nmax_values = torch.max(softmax_output, 1)\n\n# Create a tensor of zeros with the same size as softmax_output\ny = torch competing_class_tensor = torch.zeros_like(softmax_output)\n\n# Fill the tensor with the index of the highest value\ny.scatter_(0, max_values[1].view(-1, 1), torch.ones_like(max_values[0]))\n\nreturn y\n", "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": "def cross_entropy2d(input, target):\n    input = input.view(-1, input.size(-1))\n    target = target.view(-1)\n    log_p = F.log_softmax(input, dim=1)\n    log_p = log_p[target.long()]\n    loss = -log_p.sum()\n    return loss\n\nimages = Variable(torch.randn(5, 3, 4, 4))\nlabels = Variable(torch.LongTensor(5, 4, 4).random_(3))\nloss = cross_entropy2d(images, labels)\nprint(loss.data.item())\n", "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": "cnt_equal = torch.sum((A == B).bool())\n", "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": "cnt_equal = (A == B).sum()\n", "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": "cnt_not_equal = torch.sum((A != B).bool())\n", "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": "\n    cnt_equal = torch.sum(A==B)\n    return cnt_equal\nA = torch.randn(1000, 1)\nB = torchips(1000, 1)\ncnt_equal = Count(A, B)\n", "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": "\nA = A.view(-1, 1)\nB = B.view(-1, 1)\ncnt_equal = (A == B).sum().item()\n", "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": "\nA = A.view(-1, 1)\nB = B.view(-1, 1)\ncnt_not_equal = torch.sum((A != B).bool())\n", "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": "\n\na_split = torch.split(a, chunk_dim, dim=3)\ntensors_31 = [torch.cat(a_split[i:i+1]) for i in range(len(a_split))]\n\nfor i, tensor in enumerate(tensors_31):\n    print(f\"Tensor {i}: {tensor.shape}\")\n", "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": "\n\na_split = torch.split(a, chunk_dim, dim=2)\ntensors_31 = [torch.cat(a_split[i:i+1]) for i in range(len(a_split))]\n\n# Check the shapes of the tensors\nfor i, tensor in enumerate(tensors_31):\n    print(f\"Shape of tensor_{i}: {tensor.shape}\")\n", "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": "\nmask = mask.bool()\noutput[mask] = clean_input_spectrogram[mask]\n", "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": "\nmask = mask.bool()\noutput[mask] = clean_input_spectrogram[mask]\n", "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmin_val = torch.min(torch.abs(x), torch.abs(y))\nsigned_min = torch.mul(min_val, torch.sign(x))\nsigned_min = torch.mul(min_val, torchumerate(y))\n", "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": "\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\nmax_val = torch.max(torch.abs(x), torch.abs(y))\nsigned_max = max_val * torch.sign(x) * torch.sign(y)\n", "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": "\n    x_sign = torch.sign(x)\n    y_sign = torch.sign(y)\n    min_val = torch.min(torch.abs(x), torch.abs(y))\n    min_val_sign = torch.sign(min_val)\n    return x_sign * min_val_sign, y_sign * min_val_sign\n\nx_sign, y_sign = solve(x, y)\n", "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": "def predict_allCharacters(input):\n    output = MyNet(input)\n    conf, classes = torch.max(output.reshape(1, 3), 1)\n    class_names = '012'\n    return conf, class_names[classes.item()]\n\nconfidence_score, class_name = predict_allCharacters(input)\n", "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": "\n# get the last column of a and the first column of b\nlast_a_col = a[:, -1]\nfirst_b_col = b[:, 0]\n\n# calculate the average of the last column of a and the first column of b\naverage = (last_a_col + first_b_col) / 2\n\n# create a mask to select the elements that should be averaged\nmask = torch.arange(a.shape[1]) < average.shape[0]\n\n# apply the mask to the elements of a and b\nresult = a * mask.unsqueeze(1) + b * (1 - mask.unsqueeze(1))\n\n# remove the last column of a and the first column of b\nresult = result[:, :-1]\n\n# return the result\nreturn result\n", "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": "\n    a_last_col = a[:, -1]\n    b_first_col = b[:, 0]\n    result = a_last_col[:, None] + b_first_col\n    return result\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torchips.Tensor([[5, 6, 7], [5, 6, 7]])\n\na = solve(a, b)\nprint(a)\n", "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": "\n# Create a new tensor with zeros\nnew_t = torch.zeros_like(t)\n\n# Stack the original tensor and the new tensor\nresult = torch.stack([t, new_t])\n\nprint(result)\n", "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": "\n# Create a tensor with zeros\nt_zeros = torch.zeros_like(t)\n\n# Stack the original tensor with the zeros\nresult = torch.stack([t_zeros, t, t_zeros, t])\n\nprint(result)\n", "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": "\n# Reshape the tensor to the desired shape\nt = t.reshape(1, 4, 2)\n\n# Create a new tensor with the desired value\nnew = torch.full((1, 4), -1)\n\n# Stack the tensors\nresult = torch.stack([t, new])\n\nprint(result)\n", "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": "result = torch.bmm(data, W).squeeze()\nresult = result.view(10, 2, 3)\n", "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
