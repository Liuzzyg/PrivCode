{"task_id": "BigCodeBench/0", "solution": "import itertools\nfrom random import shuffle\ndef task_func(numbers=list(range(1, 11))):\n    # Calculate all permutations of the list\n    permutations = list(itertools.permutations(numbers))\n\n    # Initialize a list to store the sums of absolute differences\n    sums = []\n\n    # For each permutation, shuffle the list and calculate the sum of absolute differences\n    for permutation in permutations:\n        shuffle(permutation)\n        sums.append(sum(abs(a - b) for a, b in zip(permutation, permutation[1:])))\n\n    # Return the average of the sums\n    return sum(sums) / len(sums)"}
{"task_id": "BigCodeBench/1", "solution": "import collections\nimport random\nimport string\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length should be a non-negative number\")\n\n    # Generate a random string of the specified length composed of uppercase and lowercase letters\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n\n    # Count the occurrence of each character in this string\n    char_count = collections.Counter(random_string)\n\n    return dict(char_count)"}
{"task_id": "BigCodeBench/2", "solution": "import random\nimport statistics\ndef task_func(LETTERS):\n    # Create a dictionary with random integers as values\n    dict_values = {letter: [random.randint(1, 100) for _ in range(10)] for letter in LETTERS}\n\n    # Calculate the mean of each list in the dictionary\n    dict_values_mean = {letter: statistics.mean(values) for letter, values in dict_values.items()}\n\n    # Sort the dictionary by the mean values in descending order\n    sorted_dict = dict(sorted(dict_values_mean.items(), key=lambda item: item[1], reverse=True))\n\n    return sorted_dict\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']"}
{"task_id": "BigCodeBench/3", "solution": "import random\nimport numpy as np\ndef task_func(LETTERS):\n    # Create a dictionary where keys are specified letters and values are lists of random integers\n    dict_ = {letter: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for letter in LETTERS}\n\n    # Calculate the mean of these integers for each key and return a dictionary of these means\n    mean_dict = {letter: np.mean(values) for letter, values in dict_.items()}\n\n    return mean_dict\nLETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']"}
{"task_id": "BigCodeBench/4", "solution": "from collections import Counter\nimport itertools\ndef task_func(d):\n    # Flatten the list of lists\n    flat_list = list(itertools.chain.from_iterable(d.values()))\n    \n    # Count the occurrence of each integer\n    count_dict = Counter(flat_list)\n    \n    # Return the resulting dictionary\n    return count_dict\nd = {1: [1, 2, 3, 4], 2: [2, 2, 3, 4], 3: [1, 1, 1, 2, 2, 2]}"}
{"task_id": "BigCodeBench/5", "solution": "import random\nimport math\ndef task_func(LETTERS=[chr(i) for i in range(97, 123)]):\n    result = {}\n    for letter in LETTERS:\n        numbers = [random.randint(0, 100) for _ in range(random.randint(1, 10))]\n        result[letter] = math.stddev(numbers)\n    return result"}
{"task_id": "BigCodeBench/6", "solution": "import os\nimport re\nimport glob\ndef task_func(pattern, log_dir='/var/log/'):\n    # Check if the log directory exists\n    if not os.path.isdir(log_dir):\n        return None\n\n    # Get all log files in the directory\n    log_files = glob.glob(log_dir + '/*.log')\n\n    # If no log files match the pattern, return None\n    if not log_files:\n        return None\n\n    # Find the latest log file that matches the pattern\n    latest_log_file = max(log_files, key=lambda x: os.path.getmtime(x), default=None)\n\n    # If no log file is found, return None\n    if latest_log_file is None:\n        return None\n\n    return latest_log_file"}
{"task_id": "BigCodeBench/7", "solution": "import csv\nimport collections\nimport operator\ndef task_func(csv_file_path):\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        header = next(reader)\n        assert len(header) == 2, \"CSV file should have two columns: 'product' and 'quantity'\"\n        product_to_quantity = collections.defaultdict(int)\n        for row in reader:\n            product, quantity = row\n            product_to_quantity[product] += int(quantity)\n\n    # Find the product with the highest cumulative sales\n    top_selling_product = max(product_to_quantity.items(), key=operator.itemgetter(1))[0]\n\n    return top_selling_product"}
{"task_id": "BigCodeBench/8", "solution": "from collections import Counter\nimport itertools\nfrom random import randint\ndef task_func(T1, RANGE=100):\n    # Convert elements in 'T1' to integers\n    T1 = [int(i) for i in T1]\n    \n    # Generate a list of random integers\n    random_list = [randint(0, RANGE) for _ in range(sum(T1))]\n    \n    # Count the occurrences of each number in the generated list\n    counter = Counter(random_list)\n    \n    return counter\nT1 = ['1', '2', '3', '4', '5']"}
{"task_id": "BigCodeBench/9", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(list_of_pairs):\n    # Create a pandas DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n\n    # Set the title of the barplot\n    plt.title('Category vs Value')\n\n    # Create a bar chart of categories vs. values\n    sns.barplot(x='Category', y='Value', data=df)\n\n    # Return the DataFrame and the Axes\n    return df, plt.gca()\nlist_of_pairs = [('A', 10), ('B', 20), ('C', 30), ('D', 40)]"}
{"task_id": "BigCodeBench/10", "solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\ndef task_func(T1, RANGE=100):\n    if not T1:\n        raise statistics.StatisticsError(\"T1 is empty\")\n\n    T1 = [int(i) for i in T1]\n    total_sum = sum(T1)\n\n    random_list = [random.randint(1, RANGE) for _ in range(total_sum)]\n\n    for i in T1:\n        if i > RANGE or i < 1:\n            raise ValueError(f\"Element {i} in T1 is out of range (1, {RANGE})\")\n\n    mean = statistics.mean(random_list)\n    median = statistics.median(random_list)\n    mode = statistics.mode(random_list)\n\n    return (mean, median, mode)\nT1 = ['1', '2', '3', '4', '5']"}
{"task_id": "BigCodeBench/11", "solution": "import numpy as np\nimport itertools\nimport random\ndef task_func(T1, max_value=100):\n    # Convert elements in 'T1' to integers\n    T1 = [int(i) for sublist in T1 for i in sublist]\n\n    # Create a list of random integers\n    random_list = [random.randint(0, max_value) for _ in range(sum(T1))]\n\n    # Calculate the 25th, 50th, and 75th percentiles\n    p25 = np.percentile(random_list, 25)\n    p50 = np.percentile(random_list, 50)\n    p75 = np.percentile(random_list, 75)\n\n    # Return the 25th, 50th, and 75th percentiles as a tuple\n    return (p25, p50, p75)\nT1 = [['10', '20', '30'], ['40', '50', '60'], ['70', '80', '90']]"}
{"task_id": "BigCodeBench/12", "solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    try:\n        # Check if the script file exists\n        if not os.path.isfile(script_name):\n            raise FileNotFoundError(\"The script file does not exist.\")\n\n        # Get the start time\n        start_time = datetime.now()\n\n        # Run the script\n        process = subprocess.run([\"bash\", script_name], capture_output=True, text=True, check=True)\n\n        # Get the end time\n        end_time = datetime.now()\n\n        # Get the exit status\n        exit_status = process.returncode\n\n        # Write to the log file\n        with open(log_file, 'w') as f:\n            json.dump({\n                'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n                'end_time': end_time.strftime('%Y-%m-%d %H:%M:%S'),\n                'exit_status': exit_status\n            }, f)\n\n    except FileNotFoundError as fnf_error:\n        print(f\"FileNotFoundError: {fnf_error}\")\n    except subprocess.CalledProcessError as cpe_error:\n        print(f\"RuntimeError: {cpe_error}\")"}
{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    filenames = []\n    try:\n        for file in ftp.nlst():\n            with open(file, 'wb') as f:\n                ftp.retrbinary('RETR ' + file, f.write)\n            filenames.append(file)\n    except Exception as e:\n        raise Exception(f\"Failed to download files from server {ftp_server}: {str(e)}\")\n\n    return filenames"}
{"task_id": "BigCodeBench/14", "solution": "import configparser\nimport os\nimport shutil\nimport zipfile\ndef task_func(config_file_path, archive_dir='/home/user/archive'):\n    try:\n        # Read the configuration file\n        config = configparser.ConfigParser()\n        config.read(config_file_path)\n\n        # Get the project directory from the configuration file\n        project_dir = config['Project']['directory']\n\n        # Check if the project directory exists\n        if not os.path.exists(project_dir):\n            raise FileNotFoundError(f\"The specified project directory {project_dir} does not exist.\")\n\n        # Get the basename of the project directory\n        project_basename = os.path.basename(project_dir)\n\n        # Create the archive file path\n        archive_file_path = os.path.join(archive_dir, f\"{project_basename}.zip\")\n\n        # Check if the archive directory exists\n        if not os.path.exists(archive_dir):\n            raise FileNotFoundError(f\"The specified archive directory {archive_dir} does not exist.\")\n\n        # Create the ZIP archive\n        with zipfile.ZipFile(archive_file_path, 'w') as zipf:\n            zipf.write(project_dir, arcname=os.path.basename(project_dir))\n\n        return True\n\n    except FileNotFoundError as e:\n        print(f\"FileNotFoundError: {e}\")\n        return False\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    with open(commands_file_path, 'r') as file:\n        commands = csv.reader(file)\n        command_list = list(commands)\n\n    output_files = []\n\n    for i, command in enumerate(command_list):\n        command = command[0]\n        output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n        output_file = open(output_file_path, 'w')\n\n        try:\n            output = subprocess.run(command, shell=True, check=True, stdout=output_file, stderr=subprocess.STDOUT)\n            output_files.append(output_file_path)\n        except subprocess.CalledProcessError as e:\n            error_message = f\"Command failed with exit code {e.returncode}:\\n{e.output}\"\n            output_file.write(error_message)\n            output_files.append(output_file_path)\n\n        output_file.close()\n\n    return output_files"}
{"task_id": "BigCodeBench/16", "solution": "import os\nimport glob\nimport subprocess\ndef task_func(directory, backup_dir='/path/to/backup'):\n    try:\n        if not os.path.exists(directory):\n            raise FileNotFoundError(\"The specified directory does not exist.\")\n\n        log_files = glob.glob(os.path.join(directory, '*.log'))\n\n        if not log_files:\n            return 'No logs found to backup'\n\n        backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n\n        subprocess.run(['tar', '-czvf', backup_file] + log_files, check=True)\n\n        for log_file in log_files:\n            os.remove(log_file)\n\n        return backup_file\n\n    except FileNotFoundError as e:\n        return str(e)"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            print(f\"Process found. Restarting {process_name}.\")\n            # Terminate the process\n            subprocess.Popen(['taskkill', '/F', '/IM', proc.info['name']])\n            time.sleep(1)  # Wait for the process to terminate\n            return f\"Process {process_name} has been restarted.\"\n\n    # If the process is not running, start it\n    print(f\"Process not found. Starting {process_name}.\")\n    subprocess.Popen([process_name])\n    time.sleep(1)  # Wait for the process to start\n    return f\"Process {process_name} has been started.\""}
{"task_id": "BigCodeBench/18", "solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\ndef task_func(file):\n    if not os.path.exists(file):\n        return []\n    if not file.lower().endswith('.csv'):\n        return []\n    try:\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            rows = list(reader)\n    except Exception as e:\n        return []\n    if not rows:\n        return []\n    base_name = os.path.splitext(file)[0]\n    split_dir = base_name + '_split'\n    if not os.path.exists(split_dir):\n        os.makedirs(split_dir)\n    split_files = []\n    for i in range(10):  # Assuming 10 splits\n        split_file = f'{split_dir}/split_{i}.csv'\n        with open(split_file, 'w') as f:\n            for j in range(i, len(rows), 10):\n                random.shuffle(rows[j:j+10])\n                writer = csv.writer(f)\n                writer.writerows(rows[j:j+10])\n        split_files.append(split_file)\n    return split_files"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    try:\n        if not os.path.exists(directory):\n            raise FileNotFoundError\n\n        file_list = glob.glob(os.path.join(directory, \"*\"))\n\n        if len(file_list) == 0:\n            return None\n\n        zip_file_path = os.path.join(directory, 'files.zip')\n\n        with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n            for file in file_list:\n                zipf.write(file, os.path.relpath(file, directory))\n\n        return zip_file_path\n\n    except FileNotFoundError:\n        print(\"The specified directory does not exist.\")\n        return None"}
{"task_id": "BigCodeBench/20", "solution": "import ast\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert the string representations of dictionaries in a specific column ('dict_column') to Python dictionaries\n    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n\n    # Visualize the data with Seaborn's pairplot\n    pair_grid = sns.pairplot(df, vars=['dict_column'])\n\n    # Return the DataFrame and the PairGrid\n    return df, pair_grid"}
{"task_id": "BigCodeBench/21", "solution": "import psutil\nimport platform\ndef task_func():\n    # Get system details\n    os_info = platform.system()\n    arch_info = platform.machine()\n    mem_info = psutil.virtual_memory()\n\n    # Calculate memory usage\n    total_mem = mem_info.total / (1024.0 ** 2)  # Convert bytes to MB\n    used_mem = mem_info.used / (1024.0 ** 2)  # Convert bytes to MB\n    mem_usage_percent = (used_mem / total_mem) * 100\n\n    # Return as dictionary\n    return {\n        'OS': os_info,\n        'Architecture': arch_info,\n        'Memory Usage': f'{mem_usage_percent}%'\n    }"}
{"task_id": "BigCodeBench/22", "solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\ndef task_func(l1, l2, K=10):\n    # Combine two lists by alternating their elements, even if they are of different lengths.\n    combined = [item for pair in zip_longest(l1, l2, fillvalue=None) for item in pair if item is not None]\n\n    # Create a random sample of size K from the combined list.\n    sample = choices(combined, k=K)\n\n    # Calculate the frequency of each element in the sample.\n    counter = collections.Counter(sample)\n\n    return counter\nl1 = [1, 2, 3, 4, 5]\nl2 = [6, 7, 8, 9, 10, 11, 12]\nK = 5"}
{"task_id": "BigCodeBench/23", "solution": "import numpy as np\nfrom itertools import zip_longest\ndef task_func(l1, l2, THRESHOLD = 0.5):\n    # Combine the two lists\n    combined = l1 + l2\n    \n    # Calculate the absolute difference of each element from the predefined threshold\n    diffs = [abs(x - THRESHOLD) for x in combined]\n    \n    # Return the element closest to the threshold\n    return combined[diffs.index(min(diffs))]\nl1 = [1, 2, 3, 4, 5]\nl2 = [6, 7, 8, 9, 10]"}
{"task_id": "BigCodeBench/24", "solution": "import base64\nimport hashlib\nimport os\nimport unittest\ndef task_func(password, SALT_LENGTH = 32):\n    if password is None or password == \"\":\n        raise ValueError(\"Password cannot be None or empty\")\n\n    salt = os.urandom(SALT_LENGTH)\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\n\n    return (base64.b64encode(salt), base64.b64encode(hashed_password))"}
{"task_id": "BigCodeBench/25", "solution": "import base64\nimport json\nimport zlib\ndef task_func(data_dict):\n    # Serialize the dictionary to a JSON string\n    json_str = json.dumps(data_dict)\n\n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode())\n\n    # Encode the compressed data with base64\n    base64_data = base64.b64encode(compressed_data)\n\n    # Return the base64 encoded string\n    return base64_data.decode()\ndata_dict = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}"}
{"task_id": "BigCodeBench/26", "solution": "import base64\nfrom cryptography.fernet import Fernet\ndef task_func(message, encryption_key):\n    # Encrypt the message using Fernet encryption\n    cipher_suite = Fernet(encryption_key)\n    encrypted_message = cipher_suite.encrypt(message.encode())\n\n    # Base64 encode the encrypted message\n    base64_encoded_message = base64.b64encode(encrypted_message)\n\n    return base64_encoded_message"}
{"task_id": "BigCodeBench/27", "solution": "import json\nimport base64\nfrom datetime import datetime\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    # Add current timestamp\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n\n    # Serialize the dictionary to a JSON-formatted string\n    json_str = json.dumps(data)\n\n    # Encode the JSON string using base64 with ASCII character encoding\n    base64_bytes = base64.b64encode(json_str.encode('ascii'))\n\n    # Convert the base64 bytes to a string\n    base64_str = base64_bytes.decode('ascii')\n\n    return base64_str\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}"}
{"task_id": "BigCodeBench/28", "solution": "import requests\nimport json\nimport base64\ndef task_func(data, url=\"http://your-api-url.com\"):\n    # Convert the dictionary into a JSON-formatted string\n    json_data = json.dumps(data)\n\n    # Encode the JSON-formatted string in base64 format\n    base64_data = base64.b64encode(json_data.encode('utf-8'))\n\n    # Create a dictionary to hold the base64 data\n    base64_data_dict = {'data': base64_data.decode('utf-8')}\n\n    # Convert the dictionary into a JSON-formatted string\n    json_data_base64 = json.dumps(base64_data_dict)\n\n    # Send a POST request to the API endpoint\n    response = requests.post(url, data=json_data_base64)\n\n    # Return the response object\n    return response"}
{"task_id": "BigCodeBench/29", "solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\ndef task_func(data):\n    # Standardize the data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    # Encode the standardized data in base64 format\n    ascii_str = base64.b64encode(scaled_data.astype(np.uint8))\n\n    return \"str: The base64-encoded ASCII string representation of the standardized data is \" + ascii_str.decode('utf-8')\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": "BigCodeBench/30", "solution": "import json\nimport os\nimport re\ndef task_func(file_path, attribute, INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": str},  \n            \"age\": {\"type\": int},   \n            \"email\": {\"type\": str}  \n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n    \"\"\"\n    Validate the structure and contents of a JSON file against predefined schema rules and retrieve a specified attribute from the JSON object.\n    Ensures that all required fields exist, match their defined types, and checks the validity of the email format using a regular expression.\n    Raises ValueError if the file does not exist, required attributes are missing, types do not match, or the email format is invalid.\n    The function returns:\n    Any: The value of the specified attribute, consistent with the type defined in the JSON schema.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(file_path):\n        raise ValueError(\"The file does not exist.\")\n\n    # Load the JSON file\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Check if the required attributes exist\n    for required_attribute in INPUT_JSON['required']:\n        if required_attribute not in data:\n            raise ValueError(f\"The required attribute {required_attribute} is missing.\")\n\n    # Check the type of the specified attribute\n    if not isinstance(data[attribute], INPUT_JSON['properties'][attribute]['type']):\n        raise ValueError(f\"The type of the attribute {attribute} does not match the defined type.\")\n\n    # Check the validity of the email format\n    if not re.match(EMAIL_REGEX, data['email']):\n        raise ValueError(\"The email format is invalid.\")\n\n    # Return the specified attribute\n    return data[attribute]"}
{"task_id": "BigCodeBench/31", "solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nPUNCTUATION = set(punctuation)\ndef task_func(text):\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter words that start with '$' and consist only of punctuation\n    filtered_words = [word for word in words if word[0] == '$' and not any(char in PUNCTUATION for char in word)]\n\n    # Count the frequency of each word\n    word_freq = nltk.FreqDist(filtered_words)\n\n    # Create a barplot\n    if word_freq:\n        word_freq.plot(kind='bar')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Frequency of words beginning with the $ character')\n        return plt.gca()\n    else:\n        return None"}
{"task_id": "BigCodeBench/32", "solution": "import requests\nfrom bs4 import BeautifulSoup\ndef task_func(url, tag):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    result = soup.find(tag)\n    if result:\n        return result.text\n    else:\n        return None"}
{"task_id": "BigCodeBench/33", "solution": "import numpy as np\nfrom functools import reduce\ndef task_func(list_of_pairs):\n    # Extract the second value from each tuple\n    second_values = [pair[1] for pair in list_of_pairs]\n    \n    # Calculate the product of the second values\n    product = reduce(lambda x, y: x * y, second_values)\n    \n    # Convert the product to a numpy array\n    product_array = np.array([product])\n    \n    return product_array\nlist_of_pairs = [(1, 2), (3, 4), (5, 6)]"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n\n    # Remove whitespace\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Remove single character words\n    text = re.sub(r'\\b\\w\\b', '', text)\n\n    # Check if there are words available to generate a word cloud\n    if len(text.split()) == 0:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Generate word cloud\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(text) \n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    return WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(text)"}
{"task_id": "BigCodeBench/35", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace all elements in DataFrame columns that do not exist in the target_values array with zeros\n    for col in df.columns:\n        if col not in target_values:\n            df[col] = df[col].replace(to_replace=0, value=0)\n\n    # Plot the distribution of each column after replacing\n    for col in df.columns:\n        if col in target_values:\n            plt.figure(figsize=(10, 6))\n            sns.countplot(data=df, x=col)\n            plt.title(f'Distribution of {col}')\n            plt.show()\n\n    # Return the Axes object of the plotted data\n    return plt.gca()"}
{"task_id": "BigCodeBench/36", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTARGET_VALUES = np.array([1, 3, 4])\ndef task_func(df):\n    # Replace all elements in DataFrame columns that do not exist in the TARGET_VALUES array with zeros\n    df.replace(to_replace=[i for i in df.columns if i not in TARGET_VALUES], \n               value=0, \n               inplace=True)\n    \n    # Perform a Box-Cox transformation on each column (if data is not constant, add 1 to account for zeros)\n    for col in df.columns:\n        if df[col].nunique() > 1:\n            df[col] = stats.boxcox(df[col])[0] + 1\n    \n    # Display the resulting KDE plots\n    fig, axs = plt.subplots(len(df.columns), figsize=(10, len(df.columns)*2))\n    for i, col in enumerate(df.columns):\n        df[col].plot.kde(ax=axs[i])\n        axs[i].set_title(f'KDE plot for {col}')\n    plt.tight_layout()\n    plt.show()\n    \n    return df, fig"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    # Split the data into features and target variable\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n\n    # Train a random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get feature importances\n    importances = clf.feature_importances_\n\n    # Create a dataframe with feature importances\n    df_importances = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the bar plot\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n\n    return clf"}
{"task_id": "BigCodeBench/38", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\ndef task_func(data_matrix):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(data_matrix, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n\n    # Check if the DataFrame has the correct number of features\n    if len(data_matrix.columns) != len(FEATURE_NAMES):\n        raise ValueError(\"DataFrame should have the same number of features as FEATURE_NAMES\")\n\n    # Check if all features are numeric\n    if not data_matrix.apply(lambda x: pd.api.types.is_numeric_dtype(x)).all():\n        raise ValueError(\"All features should be numeric\")\n\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n\n    # Calculate the mean of each row\n    mean_data = pd.DataFrame(standardized_data.mean(axis=1), columns=['Mean'])\n\n    # Concatenate the original DataFrame and the mean DataFrame\n    result_data = pd.concat([data_matrix, mean_data], axis=1)\n\n    # Plot the distribution of the mean values\n    plt.figure(figsize=(10, 6))\n    plt.hist(result_data['Mean'], bins=20, color='blue')\n    plt.title('Distribution of Means')\n    plt.xlabel('Mean')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return result_data, mean_data"}
{"task_id": "BigCodeBench/39", "solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\nALPHA = 0.05\ndef task_func(data_matrix):\n    # Calculate the mean value of each row in a 2D data matrix\n    means = np.mean(data_matrix, axis=1)\n\n    # Run a t-test from a sample against the population value\n    t_stat, p_value = ttest_1samp(means, popmean=np.mean(means))\n\n    # Record the mean values that differ significantly\n    significant_indices = [i for i, val in enumerate(means) if val != np.mean(means) and p_value < ALPHA]\n\n    # Create a lineplot with the mean of rows in red. Its label is 'Means'\n    plt.plot(range(len(means)), means, 'r--', label='Means')\n\n    # Create a line plot with the significant_indices (those with a pvalue less than ALPHA) on the x-axis and the corresponding means on the y-axis. This plot should be blue. Its label is 'Significant Means'\n    plt.plot(significant_indices, means[significant_indices], 'bo', label='Significant Means')\n\n    # Create an horizontal line which represent the mean computed on the whole 2D matrix. It should be in green. Its label is 'Population Mean'\n    plt.axhline(np.mean(data_matrix), color='g', label='Population Mean')\n\n    plt.legend()\n    plt.show()\n\n    return significant_indices, plt.gca()"}
{"task_id": "BigCodeBench/40", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(data_matrix):\n    # Calculate the Z-values of a 2D data matrix\n    z_scores = pd.DataFrame(zscore(data_matrix, axis=1), columns=data_matrix.columns)\n\n    # Calculate the mean value of each row\n    z_scores['Mean'] = z_scores.mean(axis=1)\n\n    # Visualize the correlation matrix of the Z-values with a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(z_scores.corr(), annot=True, cmap='coolwarm')\n    ax = plt.gca()\n    ax.set_xticklabels(z_scores.columns, rotation=90)\n    ax.set_yticklabels(z_scores.columns)\n\n    plt.show()\n\n    return z_scores, ax"}
{"task_id": "BigCodeBench/41", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\ndef task_func(data_matrix):\n    # Convert the data_matrix to a pandas DataFrame\n    df = pd.DataFrame(data_matrix)\n\n    # Calculate the skewness of each row\n    df['Skewness'] = df.apply(lambda row: skew(row.dropna()))\n\n    # Plot the distribution\n    fig, ax = plt.subplots()\n    df['Skewness'].plot(kind='hist', ax=ax)\n    ax.set_title('Skewness Distribution')\n    ax.set_xlabel('Skewness')\n    ax.set_ylabel('Frequency')\n\n    return df, ax"}
{"task_id": "BigCodeBench/42", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data_matrix, n_components=2):\n    # Apply PCA with n_components components\n    pca = PCA(n_components=n_components)\n    pca.fit(data_matrix)\n\n    # Calculate the mean value of each component\n    mean_values = pca.mean_\n\n    # Calculate the cumulative explained variance of the components\n    cumulative_explained_variance = pca.explained_variance_ratio_.cumsum()\n\n    # Create a DataFrame to store the PCA transformed data and the mean of each component\n    pca_data = pd.DataFrame(pca.transform(data_matrix), columns=['Component 1', 'Component 2'])\n    pca_data['Mean'] = mean_values\n\n    # Create a plot of the cumulative explained variance\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, n_components+1), cumulative_explained_variance, marker='o')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.grid(True)\n    plt.tight_layout()\n\n    return pca_data, plt"}
{"task_id": "BigCodeBench/43", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n\n    # Calculate the mean of each numeric column\n    df_mean = df.mean()\n\n    # Replace NaN values with the mean of the column\n    df_replaced = df.fillna(df_mean)\n\n    # Create a dictionary to store the statistics\n    statistics = {}\n\n    # Calculate the statistics for each numeric column\n    for col in df_replaced.select_dtypes(include=[np.number]).columns:\n        statistics[col] = {\n            'count': df_replaced[col].count(),\n            'mean': df_replaced[col].mean(),\n            'std': df_replaced[col].std(),\n            'min': df_replaced[col].min(),\n            '25%': df_replaced[col].quantile(0.25),\n            '50%': df_replaced[col].quantile(0.50),\n            '75%': df_replaced[col].quantile(0.75),\n            'max': df_replaced[col].max()\n        }\n\n    # Create a list to store the Axes objects\n    axes_list = []\n\n    # Create a subplot for each numeric column\n    for i, (col, stats) in enumerate(statistics.items()):\n        fig, ax = plt.subplots(1, 1)\n        sns.histplot(df_replaced[col], bins=10, ax=ax)\n        axes_list.append(ax)\n        ax.set_title(f'Distribution of {col}')\n        ax.set_xlabel(col)\n        ax.set_ylabel('Frequency')\n        plt.tight_layout()\n        plt.show()\n\n    return statistics, axes_list"}
{"task_id": "BigCodeBench/44", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    # Normalize numeric columns\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Replace missing values with column's average\n    df_normalized.fillna(df_normalized.mean(), inplace=True)\n\n    # Create a figure and a set of subplots\n    fig, axs = plt.subplots(df.select_dtypes(include=[np.number]).shape[1])\n\n    # Draw a box plot for each column\n    for i in range(df.select_dtypes(include=[np.number]).shape[1]):\n        axs[i].boxplot(df_normalized.iloc[:, i])\n        axs[i].set_title(f'Box plot for column {i+1}')\n\n    return df_normalized, axs"}
{"task_id": "BigCodeBench/45", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame):\n    # Exclude non-numeric columns\n    df = df.select_dtypes(include=[np.number])\n\n    # Replace missing values with column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n\n    # Create a DataFrame with the first two principal components\n    principal_df = pd.DataFrame(data=principal_components, columns=['principal component 1', 'principal component 2'])\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(principal_df['principal component 1'], principal_df['principal component 2'])\n    ax.set_xlabel('principal component')\n    ax.set_ylabel('principal component 2')\n    plt.show()\n\n    return principal_df, ax"}
{"task_id": "BigCodeBench/46", "solution": "from scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    # Replace missing values with the column's average\n    for col in df.columns:\n        if df[col].isnull().any():\n            df[col].fillna(df[col].mean(), inplace=True)\n    \n    # Calculate z-scores for numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = df[numeric_cols].apply(zscore)\n    \n    # Plot histograms for each numeric column\n    fig, axs = plt.subplots(len(numeric_cols), figsize=(10, len(numeric_cols)*5))\n    for i, col in enumerate(numeric_cols):\n        df[col].hist(bins=10, ax=axs[i])\n        axs[i].set_title(f'Histogram of {col}')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df, axs"}
{"task_id": "BigCodeBench/47", "solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    # Standardize numeric columns\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Replace missing values with column's average\n    df_scaled.fillna(df_scaled.mean(), inplace=True)\n\n    # Create a correlation matrix\n    corr = df_scaled.corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10,8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return df_scaled, plt.gca()"}
{"task_id": "BigCodeBench/48", "solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(n, output_path=None):\n    # Generate n random Unix timestamps\n    timestamps = [time.time() for _ in range(n)]\n\n    # Convert Unix timestamps to strings formatted as UTC DATE_FORMAT\n    timestamps = [datetime.utcfromtimestamp(ts).strftime(DATE_FORMAT) for ts in timestamps]\n\n    # Plot a histogram of the distribution of the generated timestamps\n    plt.hist(timestamps, bins='auto', color='blue')\n    plt.title(\"Histogram of Unix Timestamps\")\n    plt.xlabel(\"Timestamp\")\n    plt.ylabel(\"Frequency\")\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return timestamps\nn = 1000"}
{"task_id": "BigCodeBench/49", "solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetime_objs = [datetime.fromtimestamp(int(ts)).strftime(DATE_FORMAT) for ts in timestamps]\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetime_objs\n    })\n\n    # Draw a histogram\n    plt.figure(figsize=(10, 6))\n    plt.hist(df['Datetime'], bins=10)\n    plt.xlabel('Datetime')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Datetime')\n    ax = plt.gca()\n\n    return df, ax"}
{"task_id": "BigCodeBench/50", "solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\ndef task_func(timestamp):\n    # Convert Unix timestamp to datetime object\n    dt_object = datetime.fromtimestamp(timestamp, pytz.utc)\n\n    # Create a DataFrame with 'Timezone' and 'Datetime' as column names\n    df = pd.DataFrame(\n        {\n            'Timezone': TIMEZONES,\n            'Datetime': [dt_object.astimezone(pytz.timezone(tz)).strftime(DATE_FORMAT) for tz in TIMEZONES]\n        }\n    )\n\n    # Draw a bar chart\n    fig, ax = plt.subplots()\n    df.plot(x='Timezone', y='Datetime', ax=ax, kind='bar', figsize=(10,5))\n    ax.set_title(f\"Datetime = f({TIMEZONES[0]})\")\n    ax.set_xlabel('Timezone')\n    ax.set_ylabel('Datetime')\n    plt.show()\n\n    return df, ax"}
{"task_id": "BigCodeBench/51", "solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on specified 'Age' and 'Height' conditions\n    filtered_df = df[(df['Age'] >= age) & (df['Height'] >= height)]\n    \n    # If the filtered dataframe has less than 3 columns, add a column 'Cluster' with 0 for each row\n    if len(filtered_df.columns) < 3:\n        filtered_df['Cluster'] = 0\n    else:\n        # Otherwise, do a KMeans clustering (by Age and Height) with 3 clusters\n        kmeans = KMeans(n_clusters=3, random_state=0).fit(filtered_df[['Age', 'Height']])\n        \n        # Add a column 'Cluster' to the dataframe which corresponds to the cluster index of the cluster to which each row belongs to\n        filtered_df['Cluster'] = kmeans.labels_\n    \n    # Plot a scatter plot of the 'Age' and 'height' and colored by the cluster indices\n    if len(filtered_df.columns) < 3:\n        ax = None\n    else:\n        ax = filtered_df[['Age', 'Height', 'Cluster']].plot(kind='scatter', x='Age', y='Height', c='Cluster', figsize=(10, 6))\n        ax.set_xlabel('Age')\n        ax.set_ylabel('Height')\n        ax.set_title('KMeans Clustering based on Age and Height')\n    \n    return filtered_df, ax"}
{"task_id": "BigCodeBench/52", "solution": "import pandas as pd\nimport regex as re\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\ndef task_func(text):\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Convert to lower case\n    text = text.lower()\n\n    # Tokenize the text\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Count the frequency of each word\n    word_freq = pd.Series(words).value_counts()\n\n    return word_freq\ntext = \"This is a test. This is only a test.\""}
{"task_id": "BigCodeBench/53", "solution": "import pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\ndef task_func(text):\n    data = re.findall(r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\", text)\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    return df\ntext = \"\"\"\nName: John Doe, Email: john.doe@example.com, Age: 30, Country: USA\nName: Jane Smith, Email: jane.smith@example.com, Age: 28, Country: Canada\nName: Alice Johnson, Email: alice.johnson@example.com, Age: 35, Country: USA\n\"\"\""}
{"task_id": "BigCodeBench/54", "solution": "import pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(text):\n    # Split the text into sentences\n    sentences = re.split('\\.', text)\n\n    # Remove empty sentences\n    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n\n    # Create a vectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    X = vectorizer.fit_transform(sentences)\n\n    # Create a DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\ntext = \"This is the first sentence. This is the second sentence. This is the third sentence. This is an empty sentence.\""}
{"task_id": "BigCodeBench/55", "solution": "import re\nimport pandas as pd\nSTOPWORDS = [\"those\", \"are\", \"the\", \"words\", \"to\", \"ignore\"]\ndef task_func(text):\n    # Split the text into sentences\n    sentences = re.split('(?<=[^A-Z].[.?]) +(?=[A-Z])', text)\n    \n    # Initialize an empty dictionary\n    sentence_dict = {}\n    \n    # Iterate over the sentences\n    for i, sentence in enumerate(sentences, start=1):\n        # Remove punctuation and convert to lower case\n        sentence = re.sub(r'[^\\w\\s]', '', sentence).lower()\n        \n        # Split the sentence into words\n        words = sentence.split()\n        \n        # Remove stopwords from the words\n        words = [word for word in words if word not in STOPWORDS]\n        \n        # Count the number of words\n        num_words = len(words)\n        \n        # Add the sentence and number of words to the dictionary\n        sentence_dict[f'Sentence {i}'] = num_words\n    \n    # Convert the dictionary to a pandas Series\n    series = pd.Series(sentence_dict)\n    \n    return series\ntext = \"Those are the words to ignore. These are not. This is a test.\""}
{"task_id": "BigCodeBench/56", "solution": "import pandas as pd\nimport regex as re\ndef task_func(text):\n    # Use regex to find all lines with 'Score: ' and 'Category: '\n    lines = re.findall(r'Score: (\\d+), Category: (.+)', text)\n\n    # Convert the scores to integers\n    lines = [(int(score), category) for score, category in lines]\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(lines, columns=['Score', 'Category'])\n\n    return df\ntext = \"\"\"\nScore: 85, Category: Math\nScore: 90, Category: Science\nScore: 78, Category: English\n\"\"\""}
{"task_id": "BigCodeBench/57", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(csv_file_path: str, title: str):\n    # Load the data from the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr = df.corr()\n\n    # Round the correlation to 2 decimals\n    corr = corr.round(2)\n\n    # Create a heatmap\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)\n\n    # Set the title\n    ax.set_title(title)\n\n    # Show the plot\n    plt.show()\n\n    # Return the correlation dataframe and the Axes object\n    return corr, ax"}
{"task_id": "BigCodeBench/58", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, num_samples):\n    # Generate random samples\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot normal distribution\n    x = np.linspace(stats.norm.ppf(0.01), stats.norm.ppf(0.99), 100)\n    ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', label='Normal distribution')\n\n    # Plot histogram\n    ax.hist(samples, bins=20, density=True, alpha=0.5, label='Histogram')\n\n    # Set title and labels\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n\n    return fig"}
{"task_id": "BigCodeBench/59", "solution": "import wikipedia\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\ndef task_func(page_title):\n    try:\n        # Check if the page exists\n        page = wikipedia.page(page_title)\n\n        # Get the text of the page\n        text = page.text\n\n        # Generate a word cloud\n        wordcloud = WordCloud().generate(text)\n\n        # Plot the word cloud\n        fig, ax = plt.subplots()\n        ax.imshow(wordcloud, interpolation='bilinear')\n        ax.axis(\"off\")\n        plt.show()\n\n        return ax\n\n    except wikipedia.exceptions.PageError:\n        print(f\"No wikipedia page found for the title: {page_title}\")\n        return None"}
{"task_id": "BigCodeBench/60", "solution": "import json\nimport pandas as pd\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(result)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n\n    # Convert the DataFrame to a JSON file\n    df.to_json(json_file_path, orient='records')\n\n    return None"}
{"task_id": "BigCodeBench/61", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(input_list):\n    # Extract the 'from_user' values\n    from_user_values = [d[key] for d in input_list for key in d if key == 'from_user']\n\n    # Calculate the square root of the 'from_user' values\n    sqrt_values = np.sqrt(from_user_values)\n\n    # Round each square root value to 2 decimals\n    sqrt_values = np.round(sqrt_values, 2)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the square root values\n    ax.plot(from_user_values, sqrt_values)\n\n    # Annotate the graph with the current date and time\n    ax.set_title(PLOT_TITLE)\n    ax.set_xlabel(X_LABEL)\n    ax.set_ylabel(Y_LABEL)\n    ax.text(0.6, 0.6, datetime.now().strftime(TIME_FORMAT), ha='center', va='center', transform=ax.transAxes)\n\n    # Show the plot\n    plt.show()\n\n    # Return the list of square root values\n    return sqrt_values"}
{"task_id": "BigCodeBench/62", "solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    # Check if the result is a list\n    if not isinstance(result, list):\n        raise ValueError(\"The result should be a list\")\n\n    # Check if the colors list is valid\n    if not all(isinstance(c, str) and len(c) == 1 and c in \"brgcmyk\" for c in colors):\n        raise ValueError(\"The colors list should contain valid color codes\")\n\n    # Draw a histogram\n    plt.figure(figsize=(10, 6))\n    sns.histplot(result, palette=colors)\n    plt.show()\n\n    # Return None\n    return None"}
{"task_id": "BigCodeBench/63", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(car_dict):\n    # Create a DataFrame\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n\n    # Plot the distribution of vehicle colors\n    plt.figure(figsize=(10, 5))\n    ax = df['Color'].value_counts().plot(kind='bar', color='skyblue')\n\n    # Set plot title\n    plt.title('Distribution of Vehicle Colors')\n\n    # Set x-axis label\n    plt.xlabel('Color')\n\n    # Set y-axis label\n    plt.ylabel('Count')\n\n    # Show the plot\n    plt.show()\n\n    return df, ax\ncar_dict = {'Toyota': 'Red', 'Honda': 'Blue', 'Mercedes': 'Red', 'BMW': 'Black', 'Audi': 'Black'}"}
{"task_id": "BigCodeBench/64", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n\n    # Check if the DataFrame has the correct columns\n    if set(data.columns) != set(COLUMNS):\n        raise ValueError(\"DataFrame should have columns: \" + \", \".join(COLUMNS))\n\n    # Create a pivot table with 'col1', 'col2' and 'col3' as the index and columns\n    pivot_table = pd.pivot_table(data, values='col3', index=['col1', 'col2'], aggfunc='count')\n\n    # Create a heatmap visualization\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(pivot_table, cmap='viridis', ax=ax)\n\n    return pivot_table, ax"}
{"task_id": "BigCodeBench/65", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Get the last column name\n    last_column = df.columns[-1]\n    \n    # Create a line chart\n    ax = df.groupby(df.columns[:-1]).plot(kind='line')\n    \n    # Set the x-label\n    plt.xlabel('-'.join(df.columns[:-1]))\n    \n    # Set the y-label\n    plt.ylabel(last_column)\n    \n    # Return the DataFrame and Axes object\n    return df, ax\ndata = [['a', 1, 2], ['b', 3, 4], ['c', 5, 6]]"}
{"task_id": "BigCodeBench/66", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # 1. Build a pandas DataFrame by using list of elements\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # 2. Create a new dataframe by grouping the values in the column 'col3' by ['col1', 'col2']\n    df_grouped = df.groupby(['col1', 'col2']).mean().reset_index()\n    \n    # 3. Reset the index of the newly created dataframe\n    df_grouped.set_index(['col1', 'col2'], inplace=True)\n    \n    # 4. Create a distribution plot of the 'col3' column of the previous dataframe using seaborn\n    plt.figure(figsize=(10,6))\n    sns.distplot(df_grouped['col3'], bins=30, kde=False)\n    plt.xlabel('col3')\n    \n    # Return the DataFrame and the plot object\n    return df_grouped, plt.gca()"}
{"task_id": "BigCodeBench/67", "solution": "import pandas as pd\nimport re\nimport os\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    # List to store file names and their sizes\n    file_list = []\n\n    # Iterate over all files in the directory\n    for file_name in os.listdir(dir_path):\n        # Check if the file name starts with the given pattern\n        if re.match(pattern, file_name):\n            # Get the size of the file\n            file_size = os.path.getsize(os.path.join(dir_path, file_name))\n            # Add the file name and size to the list\n            file_list.append([file_name, file_size])\n\n    # Convert the list to a pandas DataFrame\n    df = pd.DataFrame(file_list, columns=['File', 'Size'])\n    # Sort the DataFrame by size in ascending order\n    df.sort_values(by='Size', inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/68", "solution": "import pandas as pd\nimport seaborn as sns\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter the lines in which the employee ID begins with a prefix\n    df = df[df['Employee ID'].str.startswith(emp_prefix)]\n\n    # Draw a histogram of its age\n    ax = sns.histplot(df['Age'], bins=30)\n\n    return df, ax"}
{"task_id": "BigCodeBench/69", "solution": "import random\nimport matplotlib.pyplot as plt\nSALARY_RANGE = (20000, 100000)\ndef generate_random_salary(department):\n    num_employees = len(department)\n    salaries = [random.uniform(SALARY_RANGE[0], SALARY_RANGE[1]) for _ in range(num_employees)]\n    return salaries\ndef task_func(department):\n    salaries = generate_random_salary(department)\n    plt.hist(salaries, bins='auto', alpha=0.7, rwidth=0.85, color='#0504aa', density=True)\n    plt.title('Salary Distribution in EMPXX Department')\n    plt.xlabel('Salary')\n    plt.ylabel('Number of Employees')\n    return plt.gca()"}
{"task_id": "BigCodeBench/70", "solution": "import pandas as pd\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nCOLUMNS = ['email', 'list', 'sum', 'mean']\ndef task_func(json_file):\n    # Load e-mail data from a JSON file\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Check if there is e-mail data\n    if not data:\n        return pd.DataFrame(columns=COLUMNS), None\n\n    # Convert the data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate the sum and mean of the list associated with each e-mail\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n\n    # Plot the sum and mean values for each email\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='email', y=['sum', 'mean'], ax=ax)\n    ax.set_title('Sum and Mean of Lists for Each Email')\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n    ax.legend(title='Type', loc='upper right')\n\n    return df, ax"}
{"task_id": "BigCodeBench/71", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\ndef task_func(csv_file):\n    # Load e-mail data from a CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert it into a Pandas DataFrame\n    df = pd.DataFrame(df.to_dict())\n\n    # Calculate the sum, mean, and standard deviation of the list associated with each e-mail\n    df['sum'] = df['list'].apply(lambda x: sum(ast.literal_eval(x)))\n    df['mean'] = df['list'].apply(lambda x: np.mean(ast.literal_eval(x)))\n    df['std'] = df['list'].apply(lambda x: np.std(ast.literal_eval(x)))\n\n    # Draw a histogram of the mean values\n    sns.histplot(df['mean'], bins=30)\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/72", "solution": "import pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\ndef task_func(directory):\n    # Get all csv files in the directory\n    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n    \n    # If there is no csv file in the directory, return an empty dataframe\n    if not csv_files:\n        return pd.DataFrame()\n    \n    # Get the file with the longest filename\n    longest_file = max(csv_files, key=len)\n    \n    # Load the csv file\n    df = pd.read_csv(os.path.join(directory, longest_file))\n    \n    # Convert the 'list' column to a list\n    df['list'] = df['list'].apply(ast.literal_eval)\n    \n    # Calculate the sum, mean and median of the list associated with each email\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['median'] = df['list'].apply(np.median)\n    \n    # If there is no csv file in the directory, return None instead of an empty plot\n    if not os.path.exists(os.path.join(directory, longest_file)):\n        return None\n    \n    # Create a histogram of the median\n    plt.hist(df['median'], bins=20)\n    plt.xlabel('Median')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Median')\n    plt.show()\n    \n    return df"}
{"task_id": "BigCodeBench/73", "solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Query the database for the email data\n    query = \"SELECT email, list FROM EmailData\"\n    df = pd.read_sql_query(query, conn)\n\n    # Convert the 'list' column to a list of lists\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate the sum, mean, and variance for each email\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Return the DataFrame and Axes object\n    return df, plt.figure().add_subplot(111)"}
{"task_id": "BigCodeBench/74", "solution": "import socket\nimport requests\ndef task_func(host):\n    if host is None or host == '':\n        raise ValueError(\"'host' is None or an empty string\")\n\n    try:\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(\"There is a problem with the hostname\")\n\n    try:\n        response = requests.get(f\"https://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        raise ConnectionError(\"There is a problem with the geolocation service\")\n    except requests.exceptions.ConnectionError as errc:\n        raise ConnectionError(\"There is a problem with the geolocation service\")\n    except requests.exceptions.Timeout as errt:\n        raise ConnectionError(\"There is a problem with the geolocation service\")\n\n    return {\n        \"ip_address\": ip_address,\n        \"geolocation\": response.json()\n    }"}
{"task_id": "BigCodeBench/75", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"'df' should be a pandas DataFrame\")\n    if not df.empty:\n        raise ValueError(\"'df' should be empty\")\n    if not isinstance(sales_lower_bound, int) or not isinstance(sales_upper_bound, int):\n        raise ValueError(\"'sales_lower_bound' and 'sales_upper_bound' should be integers\")\n    if sales_lower_bound > sales_upper_bound:\n        raise ValueError(\"'sales_lower_bound' should be less than 'sales_upper_bound'\")\n\n    if fruits is None:\n        fruits = list(itertools.product(range(1, 11), repeat=3))\n    if days is None:\n        days = pd.date_range(start='01-01-2022', periods=100)\n    if seed is not None:\n        np.random.seed(seed)\n\n    sales = np.random.randint(sales_lower_bound, sales_upper_bound, size=(len(days), len(fruits)))\n    df_sales = pd.DataFrame(sales, index=days, columns=fruits)\n\n    g = sns.boxplot(data=df_sales)\n\n    return df_sales, g"}
{"task_id": "BigCodeBench/76", "solution": "import random\nimport string\nfrom django.http import HttpResponse\ndef task_func(request, session_expire_time):\n    # Generate a random session key\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Check if the session key contains both letters and digits\n    if not (any(c.isalpha() for c in session_key) and any(c.isdigit() for c in session_key)):\n        raise ValueError(\"Session key does not contain both letters and digits.\")\n\n    # Set the session key in a cookie\n    response = HttpResponse(\"Session key generated successfully.\")\n    response.set_cookie(key='session_key', value=session_key, max_age=session_expire_time)\n\n    return response"}
{"task_id": "BigCodeBench/77", "solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\ndef task_func(data):\n    # Check if the input data is a dictionary\n    if not isinstance(data, dict):\n        raise ValueError(\"Input data is not a dictionary.\")\n\n    # Check if the dictionary contains the required keys\n    if 'username' not in data or 'password' not in data:\n        raise KeyError(\"Username or password not found in the input data.\")\n\n    # Check if the password is a base64-encoded SHA-256 hash\n    try:\n        password_hash = base64.b64decode(data['password']).decode()\n    except (binascii.Error, UnicodeDecodeError, ValueError):\n        raise ValueError(\"Invalid base64-encoded SHA-256 hash.\")\n\n    # Authenticate the credentials\n    if data['username'] == 'admin' and password_hash == hashlib.sha256('password'.encode()).hexdigest():\n        return HttpResponse('Login successful.')\n    else:\n        return HttpResponseBadRequest('Login failed.')"}
{"task_id": "BigCodeBench/78", "solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\ndef task_func(request: HttpRequest, header: list, csv_data: list) -> FileResponse:\n    output = io.StringIO()\n    writer = csv.writer(output)\n    writer.writerow(header)\n    for data in csv_data:\n        writer.writerow(data)\n    response = FileResponse(content_type='text/csv')\n    response['Content-Disposition'] = 'attachment; filename=\"output.csv\"'\n    response.write(output.getvalue())\n    return response"}
{"task_id": "BigCodeBench/79", "solution": "import zipfile\nimport io\nfrom django.http import FileResponse\nfrom django.conf import settings\ndef task_func(request, file_paths):\n    # Create a in-memory file-like object\n    buffer = io.BytesIO()\n\n    # Create a zip file\n    with zipfile.ZipFile(buffer, 'w') as zf:\n        for file_path in file_paths:\n            # Add each file to the zip file\n            zf.write(file_path)\n\n    # Reset the file pointer to the start\n    buffer.seek(0)\n\n    # Create a FileResponse object with the zip file as the content\n    response = FileResponse(buffer, as_attachment=True, filename='my_files.zip')\n\n    return response"}
{"task_id": "BigCodeBench/80", "solution": "from flask import Flask, render_template, request\nimport json\nimport logging\ndef task_func(template_folder):\n    # Create a Flask application instance\n    app = Flask(__name__)\n\n    # Define a route at the root ('/') which handles POST requests\n    @app.route('/', methods=['POST'])\n    def handle_post_request():\n        # Log the information request data as a JSON\n        logging.info(request.get_json())\n\n        # Render the 'index.html' template with the provided data\n        return render_template('index.html', data=request.get_json())\n\n    return app"}
{"task_id": "BigCodeBench/81", "solution": "from flask import Flask, render_template\nfrom flask_restful import Resource, Api\nimport requests\ndef task_func(api_url, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class MyResource(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return response.json()\n\n    api.add_resource(MyResource, '/api')\n\n    return app"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n\n    class User(UserMixin, dict):\n        def __init__(self, id):\n            self.id = id\n            self.username = ''\n            self.password = ''\n\n        def __repr__(self):\n            return '<User %r>' % self.username\n\n        def set_password(self, password):\n            self.password = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        user = User(user_id)\n        user.username = 'test'\n        user.set_password('test')\n        return user\n\n    @app.route('/')\n    def index():\n        return render_template('index.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = User.query.filter_by(username=form.username.data).first()\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('dashboard'))\n\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('index'))\n\n    @app.route('/dashboard')\n    @login_required\n    def dashboard():\n        return render_template('dashboard.html')\n\n    return app"}
{"task_id": "BigCodeBench/83", "solution": "from flask import Flask\nfrom flask_mail import Mail, Message\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__)\n    mail = Mail(app)\n\n    app.config['MAIL_SERVER'] = smtp_server\n    app.config['MAIL_PORT'] = smtp_port\n    app.config['MAIL_USERNAME'] = smtp_user\n    app.config['MAIL_PASSWORD'] = smtp_password\n    app.config['MAIL_USE_TLS'] = True\n    app.config['MAIL_DEFAULT_SENDER'] = smtp_user\n\n    mail.init_app(app)\n\n    def send_email(to, subject, template):\n        msg = Message(\n            subject,\n            recipients=[to],\n            body=template,\n            sender=app.config['MAIL_DEFAULT_SENDER']\n        )\n        mail.send(msg)\n\n    return app, send_email"}
{"task_id": "BigCodeBench/84", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    # Checking if the inputs are valid\n    if not isinstance(products, list) or not all(isinstance(i, str) for i in products):\n        raise TypeError(\"products should be a list of strings\")\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples should be a positive integer\")\n    if sales_lower > sales_upper:\n        raise ValueError(\"sales_lower should be less than or equal to sales_upper\")\n    if not all(isinstance(i, (int, float)) for i in [sales_lower, sales_upper, profit_margin_min, profit_margin_max]):\n        raise TypeError(\"sales_lower, sales_upper, profit_margin_min, and profit_margin_max should be numeric\")\n\n    # Generating random data\n    np.random.seed(random_seed)\n    sales = np.random.uniform(sales_lower, sales_upper, n_samples)\n    profit_margin = np.random.uniform(profit_margin_min, profit_margin_max, n_samples)\n    profit = sales * (1 + profit_margin)\n\n    # Creating a DataFrame\n    df = pd.DataFrame({\n        'product': products,\n        'sales': sales,\n        'profit': profit\n    })\n\n    # Aggregating by product and sorting by profit\n    df_agg = df.groupby('product').agg({'sales': 'sum', 'profit': 'sum'}).sort_values('profit', ascending=False)\n\n    return df_agg"}
{"task_id": "BigCodeBench/85", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import timedelta, datetime\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, random_seed=42):\n    if end_date < start_date:\n        raise ValueError(\"'end_date' is before 'start_date'\")\n\n    np.random.seed(random_seed)\n\n    # Generate a list of dates between start_date and end_date\n    dates = pd.date_range(start=start_date, end=end_date)\n\n    # Generate random temperature, humidity, and wind speed for each date\n    df = pd.DataFrame({\n        'Date': dates,\n        'Temperature': np.random.uniform(-10, 40, len(dates)),\n        'Humidity': np.random.uniform(0.2, 1, len(dates)),\n        'Wind Speed': np.random.uniform(0, 20, len(dates))\n    })\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot('Date', 'Temperature', data=df)\n    ax.plot('Date', 'Humidity', data=df)\n    ax.plot('Date', 'Wind Speed', data=df)\n\n    return df, ax"}
{"task_id": "BigCodeBench/86", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random scores for the students\n    scores = {student: np.random.randint(0, 100) for student in students}\n\n    # Convert the scores to a pandas DataFrame\n    df = pd.DataFrame.from_dict(scores, orient='index', columns=['Score'])\n    df.index.name = 'Student'\n\n    # Sort the DataFrame by score\n    df = df.sort_values('Score')\n\n    # Plot the scores\n    plt.figure(figsize=(10, 6))\n    df['Score'].plot(kind='bar')\n    plt.xlabel('Student')\n    plt.ylabel('Score')\n    plt.title('Sorted Scores')\n\n    return df, plt"}
{"task_id": "BigCodeBench/87", "solution": "import pandas as pd\nfrom random import choices, seed\ndef task_func(products, ratings, weights, random_seed=42):\n    seed(random_seed)\n    choices_result = choices(products, weights, k=len(ratings))\n    df = pd.DataFrame(list(zip(choices_result, ratings)), columns=['Product', 'Rating'])\n    df.sort_values(by='Rating', ascending=False, inplace=True)\n    return df\nproducts = ['Product1', 'Product2', 'Product3', 'Product4', 'Product5']\nratings = [3, 4, 2, 5, 1]\nweights = [0.1, 0.1, 0.2, 0.2, 0.3]"}
{"task_id": "BigCodeBench/88", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, seed=42):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate a list of dates between start_date and end_date\n    dates = pd.date_range(start=start_date, end=end_date)\n\n    # Generate a list of random sales between 0 and 500\n    sales = np.random.randint(0, 501, len(dates))\n\n    # Create a DataFrame\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n\n    # Plot the sales over time\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Date'], df['Sales'])\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.title('Sales over time')\n    plt.grid(True)\n\n    return df, plt\nstart_date = '2022-01-01'\nend_date = '2022-01-31'"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Standardize the data\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data_scaled[:, column]))\n\n    # Remove outliers\n    data_no_outliers = data_scaled[z_scores <= outlier_z_score]\n\n    # Get the indices of the outliers\n    outlier_indices = np.where(z_scores > outlier_z_score)\n\n    # Plot the data before and after outlier removal\n    plt.figure(figsize=(10, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(data[:, column], data[:, -1])\n    plt.title('Data with Outliers')\n    plt.xlabel(column)\n    plt.ylabel('Last Column')\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_no_outliers[:, column], data_no_outliers[:, -1])\n    plt.title('Data without Outliers')\n    plt.xlabel(column)\n    plt.ylabel('Last Column')\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_no_outliers, outlier_indices"}
{"task_id": "BigCodeBench/90", "solution": "import numpy as np\nimport math\ndef task_func(data, target, k):\n    if not isinstance(k, int) or k <= 0:\n        raise ValueError(\"'k' must be a positive integer\")\n\n    distances = []\n    for point in data:\n        lat1, lon1 = math.radians(point[0]), math.radians(point[1])\n        lat2, lon2 = math.radians(target[0]), math.radians(target[1])\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n        distance = 6371 * c\n        distances.append((distance, point))\n\n    distances.sort()\n\n    return [(round(point[0], 2), round(point[1], 2)) for distance, point in distances[:k]]\ndata = [(40.7128, 74.0060), (40.7130, 74.0060), (40.7130, 74.0070), (40.7128, 74.0070)]\ntarget = (40.7128, 74.0065)\nk = 3"}
{"task_id": "BigCodeBench/91", "solution": "from scipy.stats import linregress\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data, column1, column2):\n    try:\n        data.loc[:, column1]\n        data.loc[:, column2]\n    except KeyError:\n        raise ValueError(\"One or both specified columns do not exist in the DataFrame.\")\n\n    slope, intercept, r_value, p_value, std_err = linregress(data[column1], data[column2])\n\n    fig, ax = plt.subplots()\n    ax.scatter(data[column1], data[column2])\n    ax.plot(data[column1], intercept + slope * data[column1], 'r')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.set_title('Scatter plot of data')\n\n    return (slope, intercept, r_value, p_value, std_err), ax"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n\n    labels = kmeans.labels_\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    centroids = kmeans.cluster_centers_\n    centroid_collection = PathCollection([centroids], sizes=20, linestyles='-', colors='r')\n    ax.add_collection(centroid_collection)\n\n    plt.title('K-Means Clustering')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n\n    plt.show()\n\n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n\n    transformed_data = pca.transform(data)\n\n    df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(df[f'PC{n_components+1}'], df[f'PC{n_components+2}'])\n    ax.set_xlabel(f'PC{n_components+1}')\n    ax.set_ylabel(f'PC{n_components+2}')\n    ax.set_title('Scatter plot of the transformed data')\n\n    return df, ax"}
{"task_id": "BigCodeBench/94", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, num_samples):\n    # Generate samples\n    samples = norm.rvs(loc=mean, scale=std_dev, size=num_samples)\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, density=True)\n\n    # Plot PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title\n    title = \"Fit results: mean = %.2f, std = %.2f\" % (mean, std_dev)\n    ax.set_title(title)\n\n    # Return figure and samples\n    return fig, samples"}
{"task_id": "BigCodeBench/95", "solution": "import pandas as pd\nfrom random import randint, uniform, seed\ndef task_func(categories=None, months=None, random_seed=42):\n    if categories is None or months is None or not isinstance(categories, list) or not isinstance(months, list) or not categories or not months:\n        raise ValueError(\"Both 'categories' and 'months' must be provided as lists and cannot be empty.\")\n\n    seed(random_seed)\n\n    sales_data = []\n    for category in categories:\n        for month in months:\n            sales = randint(100, 500) + uniform(0, 1)\n            sales_data.append({'Month': month, 'Category': category, 'Sales': sales})\n\n    df = pd.DataFrame(sales_data)\n    return df"}
{"task_id": "BigCodeBench/96", "solution": "import csv\nfrom collections import Counter\nimport operator\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file, delimiter=csv_delimiter)\n        words = [word for row in reader for word in row]\n    counter = Counter(words)\n    sorted_counter = sorted(counter.items(), key=operator.itemgetter(1), reverse=True)\n    return sorted_counter\ncsv_file = 'test.csv'\ncsv_delimiter = ','"}
{"task_id": "BigCodeBench/97", "solution": "import math\nimport itertools\nfrom functools import reduce\ndef task_func(numbers):\n    # Generate all possible combinations of the provided numbers in a given list for each possible length\n    combinations = [list(itertools.combinations(numbers, r)) for r in range(len(numbers) + 1)]\n    \n    # Compute the product of the numbers in the combination\n    products = [reduce(lambda x, y: x * y, comb) for comb in combinations]\n    \n    # Compute the logarithm of each product\n    log_products = [math.log(prod) for prod in products]\n    \n    # Sum these logarithms to produce the final result\n    result = sum(log_products)\n    \n    return result"}
{"task_id": "BigCodeBench/98", "solution": "import random\nimport string\nfrom collections import Counter\ndef task_func(num_strings, string_length):\n    # Generate a list of random strings\n    random_strings = [''.join(random.choice(string.ascii_lowercase) for _ in range(string_length)) for _ in range(num_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    char_freq = Counter(''.join(random_strings))\n\n    # Sort the characters by their frequency and return as a list of tuples\n    return sorted(char_freq.items(), key=lambda x: x[1], reverse=True)"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate pair plot\n    sns.set(style=\"whitegrid\")\n    pair_plot = sns.pairplot(iris_df, hue='species', vars=iris.feature_names)\n\n    # Set title and labels for axes\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16)\n    for ax in pair_plot.axes.flatten():\n        ax.set_xlabel(ax.xaxis.label.get_text(), fontsize=14)\n        ax.set_ylabel(ax.yaxis.label.get_text(), fontsize=14)\n\n    # Return the figure\n    return pair_plot.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate a list of random numbers for the past 30 days\n        dates = [(datetime.now() - pd.DateOffset(days=i)).date() for i in range(30)]\n        values = [random.randint(1, 100) for _ in range(30)]\n\n        # Create a DataFrame\n        df = pd.DataFrame({\n            'Date': dates,\n            'Value': values\n        })\n\n        # Sort the DataFrame by date\n        df.sort_values('Date', inplace=True)\n\n        # Create a line plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['Date'], df['Value'])\n        plt.title('Random Time Series Data')\n        plt.xlabel('Date')\n        plt.ylabel('Value')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n\n        # Show the plot\n        plt.show()\n\n        return plt.gca()\n\n    except Exception as e:\n        raise ValueError(\"An error occurred while generating the data or plot.\") from e"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    np.random.seed(seed)\n    try:\n        boston = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n\n        # Define the features\n        features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n        boston.columns = features\n\n        # Compute the correlation matrix\n        corr = boston.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Set up the matplotlib figure\n        f, ax = plt.subplots(figsize=(11, 9))\n\n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n        plt.title(\"Correlation Heatmap of Boston Housing Dataset\")\n\n        # Save the figure\n        plt.savefig('correlation_heatmap.png')\n\n        return ax\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/102", "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\ndef task_func():\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n\n    # Create a DataFrame\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = pd.Series(diabetes.target)\n\n    # Create a pairplot\n    figure = sns.pairplot(df)\n\n    return figure, df"}
{"task_id": "BigCodeBench/103", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(temperatures):\n    if not isinstance(temperatures, pd.DataFrame):\n        raise ValueError(\"Input DataFrame is not in the expected format or empty.\")\n\n    # Filter out the rows where the temperature is not between 60 and 80\n    filtered_temperatures = temperatures[(temperatures['Temperature (\u00b0C)'] >= 60) & (temperatures['Temperature (\u00b0C)'] <= 80)]\n\n    # Plot the temperature\n    plt.figure(figsize=(10, 6))\n    plt.plot(filtered_temperatures['Date'], filtered_temperatures['Temperature (\u00b0C)'])\n    plt.title('Daily Temperatures in New York')\n    plt.xlabel('Date')\n    plt.ylabel('Temperature (\u00b0C)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/104", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not all(col in df.columns for col in ['Date (ordinal)', 'Value']):\n        raise ValueError(\"'df' lacks required columns\")\n\n    colors = cycle(groups)\n    ax = None\n    for group in groups:\n        group_df = df[df['Group'] == group]\n        ax = group_df.plot(kind='scatter', x='Date (ordinal)', y='Value', color=next(colors), label=group)\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    plt.legend()\n    return ax"}
{"task_id": "BigCodeBench/105", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\ndef task_func(df):\n    # Check if dataframe is empty\n    if df.empty:\n        raise ValueError(\"Dataframe is empty.\")\n\n    # Check if required columns are present\n    required_columns = ['date']\n    for col in required_columns:\n        if col not in df.columns:\n            raise ValueError(f\"Column {col} is missing in the dataframe.\")\n\n    # Check if 'date' column is in datetime format\n    if not isinstance(df['date'][0], datetime.datetime):\n        raise ValueError(\"The 'date' column is not in datetime format.\")\n\n    # Convert 'date' column to ordinal format\n    df['date'] = df['date'].map(lambda date: date.toordinal())\n\n    # Generate a correlation matrix\n    corr = df.corr()\n\n    # Generate a pair plot\n    g = sns.PairGrid(df)\n    g.map_upper(plt.scatter, edgecolor=\"w\")\n    g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n    g.map_diag(sns.kdeplot, lw=3, legend=False)\n\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return corr, g"}
{"task_id": "BigCodeBench/106", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if 'df' is a valid DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a valid DataFrame\")\n\n    # Check if 'date' column exists in the DataFrame\n    if 'date' not in df.columns:\n        raise ValueError(\"'df' should contain a 'date' column\")\n\n    # Check if 'date' column is in datetime format\n    if not pd.api.types.is_datetime64(df['date']):\n        raise ValueError(\"'date' column should be in datetime format\")\n\n    # Convert 'date' column to ordinal\n    df['date'] = df['date'].dt.to_period('D')\n\n    # Perform linear regression\n    lr_model = LinearRegression()\n    lr_model.fit(df['date'].values.reshape(-1, 1), df['value'])\n\n    # Predict values\n    predictions = lr_model.predict(df['date'].values.reshape(-1, 1))\n\n    # Plot original values and predictions\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['date'], df['value'], color='blue', label='Original values')\n    plt.plot(df['date'], predictions, color='red', label='Predicted values')\n    plt.title('Value vs Date (Linear Regression Prediction)')\n    plt.xlabel('Date (ordinal)')\n    plt.ylabel('Value')\n    plt.legend()\n\n    return lr_model, predictions, plt"}
{"task_id": "BigCodeBench/107", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(df, n_clusters=3, random_state=0):\n    # Check if DataFrame is empty or lacks required columns\n    if df.empty or 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame is empty or lacks required columns.\")\n\n    # Convert 'date' column to ordinal\n    df['date'] = pd.to_datetime(df['date']).map(dt.datetime.toordinal)\n\n    # Perform KMeans clustering on 'date' and 'value' columns\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(df[['date', 'value']])\n\n    # Create a scatter plot of the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['date'], df['value'], c=kmeans.labels_)\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    plt.colorbar(scatter)\n    return ax"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if 'df' is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a DataFrame\")\n\n    # Check if 'df' has required columns\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a column named 'value'\")\n\n    # Check if 'value' column contains valid data types\n    if not df['value'].apply(lambda x: isinstance(x, (int, float))).all():\n        raise ValueError(\"'value' column should contain numeric data\")\n\n    # Check if 'freq' is a valid frequency string\n    if freq not in ['D', 'M', 'A']:\n        raise ValueError(\"'freq' should be a valid frequency string. Valid options are 'D', 'M', 'A'\")\n\n    # Check if 'decomposition_model' is 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    result = seasonal_decompose(df['value'], model=decomposition_model)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(3, 1, figsize=(10, 10))\n    result.plot(ax=ax[0])\n    ax[0].set_title('Trend')\n    result.seasonal.plot(ax=ax[1])\n    ax[1].set_title('Seasonality')\n    result.resid.plot(ax=ax[2])\n    ax[2].set_title('Residuals')\n    plt.tight_layout()\n\n    return result, ax"}
{"task_id": "BigCodeBench/109", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, items=None, locations=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' must be a DataFrame\")\n    if not set(df.columns).issuperset(['Item', 'Location']):\n        raise ValueError(\"'df' must have 'Item' and 'Location' columns\")\n\n    if items is None and locations is None:\n        items = ['Item1', 'Item2', 'Item3']\n        locations = ['Location1', 'Location2', 'Location3']\n\n    df = df[df['Item'].isin(items) & df['Location'].isin(locations)]\n\n    plt.figure(figsize=(10,6))\n    df.groupby(['Item', 'Location']).size().unstack().plot(kind='bar', ax=plt.gca())\n    plt.title('Distribution of Items across Locations')\n    plt.xlabel('Location')\n    plt.ylabel('Count')\n    plt.legend(title='Item', loc='upper right')\n    plt.show()\n    return plt.gca()"}
{"task_id": "BigCodeBench/110", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if 'df' is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a DataFrame\")\n\n    # Check if 'Date' and 'Sales' columns exist in 'df'\n    if 'Date' not in df.columns or 'Sales' not in df.columns:\n        raise ValueError(\"'df' should have 'Date' and 'Sales' columns\")\n\n    # Check if 'df' has any data to plot\n    if df.empty:\n        raise ValueError(\"'df' has no data to plot\")\n\n    # Create a line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'])\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/111", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if 'df' is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a DataFrame\")\n\n    # Check if 'Date', 'Time', and 'Temperature' columns exist in 'df'\n    required_columns = ['Date', 'Time', 'Temperature']\n    for column in required_columns:\n        if column not in df.columns:\n            raise ValueError(f\"'df' should contain columns '{column}'\")\n\n    # Convert 'Date' and 'Time' columns to datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 6))\n    heatmap = sns.heatmap(df[['Date', 'Time', 'Temperature']].pivot_table(index='Date', columns='Time', values='Temperature'), cmap='coolwarm', linewidths=0.5, cbar=True)\n\n    return heatmap"}
{"task_id": "BigCodeBench/112", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a pandas DataFrame\")\n    if 'Status' not in df.columns:\n        raise ValueError(\"'df' does not contain the 'Status' column\")\n\n    status_counts = df['Status'].value_counts()\n\n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%')\n\n    return ax"}
{"task_id": "BigCodeBench/113", "solution": "import json\nfrom collections import Counter\nimport random\ndef task_func(my_dict, keys):\n    if len(keys) != 10:\n        raise ValueError(\"'keys' does not contain exactly 10 unique elements.\")\n\n    # Add 10 random elements based on the 'keys' parameter\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    # Save the JSON representation of the updated dictionary\n    with open('updated_dictionary.json', 'w') as f:\n        json.dump(my_dict, f)\n\n    # Count the occurrences of each key\n    key_frequencies = Counter(my_dict.keys())\n\n    # Save the counts of each key to a separate text file\n    with open('key_frequencies.txt', 'w') as f:\n        for key, count in key_frequencies.items():\n            f.write(f\"{key}: {count}\\n\")\n\n    return my_dict, 'updated_dictionary.json', 'key_frequencies.txt'"}
{"task_id": "BigCodeBench/114", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(my_dict):\n    # Check if 'array' key exists in the dictionary\n    if 'array' not in my_dict:\n        raise KeyError(\"The dictionary does not contain a 'array' key.\")\n\n    # Check if the value of the 'array' key is a numpy array\n    try:\n        array = my_dict['array']\n        if not isinstance(array, np.ndarray):\n            raise TypeError(\"The value of the 'array' key is not a numpy array.\")\n    except KeyError:\n        raise KeyError(\"The dictionary does not contain a 'array' key.\")\n    except TypeError:\n        raise TypeError(\"The value of the 'array' key is not a numpy array.\")\n\n    # Normalize the numpy array\n    scaler = MinMaxScaler()\n    normalized_array = scaler.fit_transform(array)\n\n    # Add the normalized array to the dictionary\n    my_dict['normalized_array'] = normalized_array\n\n    return my_dict"}
{"task_id": "BigCodeBench/115", "solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list `numbers` is empty\")\n    array = np.array(numbers)\n    mode_val, _ = mode(array)\n    entropy_val = entropy(array)\n    return {'mode': mode_val.mode[0], 'entropy': entropy_val}"}
{"task_id": "BigCodeBench/116", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, sample_size):\n    # Generate a numpy array of random samples drawn from a normal distribution\n    samples = np.random.normal(mu, sigma, sample_size)\n\n    # Plot a histogram of the generated samples\n    plt.hist(samples, bins=30, alpha=0.75, color='blue')\n    plt.title('Histogram of Generated Samples')\n    plt.xlabel('Sample values')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return samples\nmu = 0\nsigma = 1\nsample_size = 1000"}
{"task_id": "BigCodeBench/117", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be a positive integer\")"}
{"task_id": "BigCodeBench/118", "solution": "import os\nimport shutil\ndef task_func(directory, backup_directory):\n    # Check if the source directory exists\n    if not os.path.isdir(directory):\n        print(f\"Source directory {directory} does not exist.\")\n        return []\n\n    # Check if the backup directory exists\n    if not os.path.isdir(backup_directory):\n        # If the backup directory does not exist, create it\n        os.makedirs(backup_directory)\n\n    # List all JSON files in the source directory\n    json_files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n\n    # Copy each JSON file to the backup directory\n    copied_files = []\n    for json_file in json_files:\n        source_file = os.path.join(directory, json_file)\n        target_file = os.path.join(backup_directory, json_file)\n        shutil.copy2(source_file, target_file)\n        copied_files.append(target_file)\n\n    # Return the list of paths to the copied files\n    return copied_files"}
{"task_id": "BigCodeBench/119", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = x**2\n\n    plt.plot(x, y)\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Check if start_date and end_date are datetime instances\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime instances\")\n\n    # Check if start_date is later than end_date\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than end_date\")\n\n    # Set the seed for the random number generator\n    random_seed(seed)\n\n    # Generate a range of dates between start_date and end_date, inclusive\n    date_range = pd.date_range(start=start_date, end=end_date)\n\n    # Return the generated series\n    return date_range\nstart_date = datetime(2020, 1, 1)\nend_date = datetime(2020, 12, 31)\nseed = 42"}
{"task_id": "BigCodeBench/121", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(my_list, seed=42):\n    # Check if 'my_list' is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n\n    # Generate a random seed\n    np.random.seed(seed)\n\n    # Define the categories\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\n    # Generate random sales data\n    sales_data = {category: np.random.randint(100, 1000, 10) for category in categories}\n\n    # Create a DataFrame\n    df = pd.DataFrame(sales_data)\n\n    # Add an item \"12\" to the list\n    my_list.append(\"12\")\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n\n    return df, ax"}
{"task_id": "BigCodeBench/122", "solution": "import numpy as np\nimport random\ndef task_func(my_list):\n    # Append a random integer between 0 and 100 to the list\n    my_list.append(random.randint(0, 100))\n\n    # Generate a numpy array of random floating-point numbers\n    numpy_array = np.random.random(len(my_list))\n\n    return numpy_array\nmy_list = [1, 2, 3, 4, 5]"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n\n    if not os.path.isdir(file_dir):\n        raise FileNotFoundError(\"No files found in the specified directory\")\n\n    file_list = glob.glob(file_dir + '/*' + file_ext)\n    if not file_list:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n\n    df = pd.DataFrame()\n    for file_name in file_list:\n        temp_df = pd.read_csv(file_name)\n        df = pd.concat([df, temp_df])\n\n    df.loc[len(df.index)] = [12]\n\n    return df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    for i in my_list:\n        if not isinstance(i, (int, float)):\n            raise ValueError(\"'my_list' should only contain numeric values (int or float)\")\n    seed(seed)\n    start_time = time.time()\n    my_list.append(12)\n    random_list = [randint(1, 100) for _ in range(sum(my_list))]\n    end_time = time.time()\n    time_taken = end_time - start_time\n    plt.hist(random_list, bins=range(1, 101), edgecolor='black')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    plt.show()\n    return time_taken, plt"}
{"task_id": "BigCodeBench/125", "solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of a given set of letters of length 'n'\n    combinations = [''.join(x) for x in itertools.combinations(LETTERS, n)]\n\n    # Count the occurrences of each letter in these combinations\n    letter_counts = defaultdict(int)\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n\n    # Save the results in a JSON file\n    random_number = random.randint(0, 100)\n    filename = f\"prefix_{random_number}.json\"\n    with open(filename, 'w') as f:\n        json.dump(dict(letter_counts), f)\n\n    return filename\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nn = 3"}
{"task_id": "BigCodeBench/126", "solution": "import pandas as pd\nfrom random import randint, seed\nimport statistics\nimport numpy as np\ndef task_func(animals=None, seed=42):\n    seed(seed)\n    random_seed(seed)\n    np.random.seed(seed)\n\n    if animals is None:\n        animals = ['lion', 'tiger', 'elephant', 'giraffe', 'zebra']\n\n    data = []\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        mean = statistics.mean(counts)\n        median = statistics.median(counts)\n        std_dev = statistics.stdev(counts)\n        data.append([animal, mean, median, std_dev])\n\n    df = pd.DataFrame(data, columns=['Animal', 'Mean', 'Median', 'Standard Deviation'])\n\n    return df"}
{"task_id": "BigCodeBench/127", "solution": "import os\nimport shutil\nimport glob\nimport hashlib\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    # Check if the root directory exists\n    if not os.path.isdir(ROOT_DIR):\n        print(f\"Error: Root directory {ROOT_DIR} does not exist.\")\n        return\n\n    # Check if the destination directory exists\n    if not os.path.isdir(DEST_DIR):\n        print(f\"Error: Destination directory {DEST_DIR} does not exist.\")\n        return\n\n    # Get a list of all files in the root directory\n    files = glob.glob(os.path.join(ROOT_DIR, '*'))\n\n    # Initialize a counter for the number of files moved\n    count = 0\n\n    # Iterate over each file in the root directory\n    for file in files:\n        # Skip if the file is a directory\n        if os.path.isdir(file):\n            continue\n\n        # Calculate the MD5 hash of the file\n        with open(file, 'rb') as f:\n            md5_hash = hashlib.md5(f.read()).hexdigest()\n\n        # If the hash matches the specified hash, move the file to the destination directory\n        if md5_hash == SPECIFIC_HASH:\n            shutil.move(file, DEST_DIR)\n            count += 1\n\n    # Return the number of files moved\n    return count"}
{"task_id": "BigCodeBench/128", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\ndef task_func(POINTS=100):\n    # Initialize the coordinates\n    x, y = 0, 0\n    # Initialize the list to store the coordinates\n    x_coords = [x]\n    y_coords = [y]\n\n    # Generate random directions and steps\n    for _ in range(POINTS):\n        # Choose a random direction (0, 1, 2, 3 for north, east, south, west)\n        direction = randint(0, 3)\n\n        # Calculate the step size\n        if direction == 0:\n            step_size = 1\n        elif direction == 1:\n            step_size = 1\n        elif direction == 2:\n            step_size = -1\n        else:\n            step_size = -1\n\n        # Update the coordinates\n        x += step_size\n        y += step_size\n\n        # Append the new coordinates to the list\n        x_coords.append(x)\n        y_coords.append(y)\n\n    # Create a figure and a plot\n    fig, ax = plt.subplots()\n\n    # Plot the coordinates\n    ax.plot(x_coords, y_coords)\n\n    # Return the figure\n    return fig"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        \n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table')\n    \n    if not table:\n        raise ValueError(\"No table data found on the page or the page content cannot be parsed.\")\n    \n    headers = [th.text.strip() for th in table.find_all('th')]\n    data = []\n    \n    for tr in table.find_all('tr'):\n        tds = tr.find_all('td')\n        row = [td.text.strip() for td in tds]\n        data.append(row)\n    \n    df = pd.DataFrame(data, columns=headers)\n    \n    return df"}
{"task_id": "BigCodeBench/130", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\nimport random\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_str = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the specified size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    byte_str += salt\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256(byte_str)\n\n    # Return the base64-encoded salt and the SHA256 hash\n    return (base64.b64encode(salt), base64.b64encode(hash_obj.digest()))\nhex_str = '4141414141414141414141414141414141414141414141414141414141414141'\nsalt_size = 16"}
{"task_id": "BigCodeBench/131", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\nimport random\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_str = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the given size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte string\n    byte_str += salt\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256(byte_str)\n\n    # Return the base64-encoded salt and the SHA256 hash\n    return (base64.b64encode(salt), base64.b64encode(hash_obj.digest()))\nhex_str = '4141414141414141414141414141414141414141414141414141414141414141'\nsalt_size = 16"}
{"task_id": "BigCodeBench/132", "solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(hex_str):\n    # Check if the input is a valid hex string\n    try:\n        bytes_obj = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n    \n    # Count the frequency of each byte\n    byte_freq = {}\n    for byte in bytes_obj:\n        if byte in byte_freq:\n            byte_freq[byte] += 1\n        else:\n            byte_freq[byte] = 1\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(byte_freq.items()), columns=['Byte Value', 'Frequency'])\n    \n    # Create a matplotlib plot\n    fig, ax = plt.subplots()\n    ax.bar(df['Byte Value'], df['Frequency'])\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/133", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Normalize the last column\n    scaler = MinMaxScaler()\n    df[df.columns[-1]] = scaler.fit_transform(df[df.columns[-1]].values.reshape(-1, 1))\n    \n    # Plot the normalized data\n    plt.figure(figsize=(10, 6))\n    plt.title(f'Normalized Data of {df.columns[-1]}')\n    plt.xlabel('Index')\n    plt.ylabel('Normalized Value')\n    plt.plot(df[df.columns[-1]])\n    plt.show()\n    \n    return df, plt"}
{"task_id": "BigCodeBench/134", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    last_column = df.iloc[:, -1]\n    \n    plt.hist(last_column, bins=bins)\n    plt.title(f'Histogram of {last_column.name}')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    \n    ax = plt.gca()\n    \n    return ax"}
{"task_id": "BigCodeBench/135", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if len(df.columns) < 2:\n        raise ValueError(\"Input DataFrame has no columns\")\n\n    # Get the last column\n    last_column = df.iloc[:, -1]\n\n    # Impute missing values with mean\n    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n    imputed_column = imputer.fit_transform(df.iloc[:, :-1])\n\n    # Create a new DataFrame with the imputed column\n    imputed_df = pd.DataFrame(imputed_column, columns=df.columns[:-1])\n\n    # Create a box plot to visualize the distribution of data in the last column\n    _, ax = plt.subplots()\n    sns.boxplot(x=imputed_df.iloc[:, -1], ax=ax)\n    ax.set_title('Box plot of the last column of the imputed DataFrame')\n    plt.show()\n\n    return imputed_df, ax"}
{"task_id": "BigCodeBench/136", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    pca = PCA(n_components=2)\n    pca.fit(df)\n\n    df_pca = pd.DataFrame(data=pca.transform(df), columns=['Principal Component 1', 'Principal Component 2'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(df_pca['Principal Component 1'], df_pca['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return df_pca, ax"}
{"task_id": "BigCodeBench/137", "solution": "import pandas as pd\nfrom scipy.stats import skew\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.columns.nlevels != 1:\n        raise ValueError(\"Input DataFrame has no columns\")\n    \n    skewness = skew(df.iloc[:, -1])\n    \n    return float(skewness)"}
{"task_id": "BigCodeBench/138", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' must be a DataFrame\")\n    if 'Letters' not in df.columns:\n        raise ValueError(\"'df' must contain a column named 'Letters'\")\n    if not all(c in letters for c in df['Letters']):\n        raise ValueError(\"'df' 'Letters' column must contain only English uppercase letters\")\n\n    letter_freq = df['Letters'].value_counts()\n\n    fig, ax = plt.subplots()\n    ax.bar(letter_freq.index, letter_freq.values)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if input is a non-empty DataFrame\n    if df.empty:\n        raise ValueError(\"Input is not a non-empty DataFrame\")\n\n    # Check if there are numeric columns in the DataFrame\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if numeric_cols.empty:\n        raise ValueError(\"DataFrame does not contain any numeric columns\")\n\n    # Create a list to store the Axes objects\n    axes_list = []\n\n    # Iterate over each numeric column\n    for col in numeric_cols:\n        # Create a histogram\n        ax = df[col].hist(bins=10, edgecolor='black')\n\n        # Set the title of the plot\n        ax.set_title(col)\n\n        # Set the x-axis label\n        ax.set_xlabel('Value')\n\n        # Set the y-axis label\n        ax.set_ylabel('Frequency')\n\n        # Append the Axes object to the list\n        axes_list.append(ax)\n\n    # Return the list of Axes objects\n    return axes_list"}
{"task_id": "BigCodeBench/140", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, cols):\n    # Check if 'df' is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n\n    # Check if 'cols' is a list\n    if not isinstance(cols, list):\n        raise ValueError(\"'cols' is not a list\")\n\n    # Check if columns in 'cols' exist in 'df'\n    for col in cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' does not exist in 'df'\")\n\n    # Standardize specified numeric columns\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n\n    return df"}
{"task_id": "BigCodeBench/141", "solution": "import numpy as np\nimport pandas as pd\nimport statistics\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"'rows' must be a positive integer greater than 0.\")\n\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, (rows, 6))\n    df = pd.DataFrame(data, columns=columns)\n\n    mean_dict = {col: statistics.mean(df[col]) for col in columns}\n    median_dict = {col: statistics.median(df[col]) for col in columns}\n\n    return df, mean_dict, median_dict"}
{"task_id": "BigCodeBench/142", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(0, 2*np.pi, 1000)\n    sine_y = np.sin(x)\n    cosine_y = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n    axs[0].plot(x, sine_y)\n    axs[0].set_title('Sine function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n\n    axs[1].plot(x, cosine_y)\n    axs[1].set_title('Cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n\n    return fig, axs"}
{"task_id": "BigCodeBench/143", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate x values\n    x = np.linspace(-10, 10, 400)\n\n    # Generate corresponding y values\n    y = 2*x + 1\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the equation y = 2x + 1\n    ax.plot(x, y, label='y=2x+1')\n\n    # Plot the solution at x = 2\n    ax.scatter(2, 5, color='green', label='Solution at x=2')\n\n    # Set the title and labels\n    ax.set_title('Solution of the equation y=2x+1 at x=2')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    # Add a legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/144", "solution": "import ipaddress\nimport requests\ndef task_func(ip_range, timeout):\n    try:\n        ip_range = ipaddress.ip_network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided\")\n\n    active_ips = []\n\n    for ip in ip_range.hosts():\n        try:\n            response = requests.get(f\"http://{ip}/\", timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(ip))\n        except (requests.exceptions.RequestException, requests.exceptions.Timeout):\n            continue\n\n    return active_ips"}
{"task_id": "BigCodeBench/145", "solution": "import csv\nfrom ipaddress import IPv4Network\ndef task_func(ip_range, csv_path):\n    with open(csv_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        for ip in IPv4Network(ip_range):\n            writer.writerow([str(ip)])\n    return csv_path"}
{"task_id": "BigCodeBench/146", "solution": "import subprocess\nfrom ipaddress import IPv4Network\ndef task_func(ip_range):\n    ip_dict = {}\n    for ip in IPv4Network(ip_range):\n        ip_str = str(ip)\n        try:\n            # Ping the IP\n            result = subprocess.run(['ping', '-c', '1', ip_str], \n                                    stdout=subprocess.PIPE, \n                                    stderr=subprocess.PIPE, \n                                    check=True)\n            ip_dict[ip_str] = True\n        except subprocess.CalledProcessError:\n            # If ping command fails, set the IP as inactive\n            ip_dict[ip_str] = False\n    return ip_dict"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    ip_network = IPv4Network(ip_range)\n    ip_addresses = ip_network.hosts()\n    result = {}\n\n    for ip in ip_addresses:\n        t = Thread(target=check_port, args=(str(ip), port,))\n        t.start()\n        t.join()\n\n    return result\ndef check_port(ip, port):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(5)\n            s.connect((ip, port))\n            result = True\n        result = False\n    except (socket.gaierror, socket.error, socket.timeout):\n        result = False\n    finally:\n        return result"}
{"task_id": "BigCodeBench/148", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df"}
{"task_id": "BigCodeBench/149", "solution": "import pandas as pd\nimport numpy as np\nDEFAULT_COLUMNS = ['Element', 'Count']\ndef task_func(elements, include_index=False):\n    # Create a list to store the results\n    result = []\n\n    # Iterate over the elements\n    for i, element in enumerate(elements, start=1):\n        # Count the number of characters in the element\n        count = len(element)\n\n        # Append the result to the list\n        result.append({'Index': i, 'Element': element, 'Count': count})\n\n    # Convert the list to a DataFrame\n    df = pd.DataFrame(result, columns=DEFAULT_COLUMNS)\n\n    # If the index is to be included, add it as the first column\n    if include_index:\n        df = df[DEFAULT_COLUMNS + [c for c in df.columns if c != 'Count']]\n\n    return df"}
{"task_id": "BigCodeBench/150", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(product_dict, product_keys):\n    # Create a DataFrame from the product dictionary\n    df = pd.DataFrame.from_dict(product_dict, orient='index', columns=['Quantity', 'Price', 'Profit'])\n\n    # Calculate the average price and profit for all considered products\n    df['Average Price'] = df['Price'] * df['Quantity']\n    df['Average Profit'] = df['Profit'] * df['Quantity']\n\n    # Calculate the total profit for each product\n    df['Total Profit'] = df['Profit'] * df['Quantity']\n\n    # Calculate the average profit for each product\n    df['Average Profit'] = df['Average Profit'] / df['Quantity']\n\n    # Calculate the total profit for all products\n    total_profit = df['Total Profit'].sum()\n\n    # Calculate the average profit for all products\n    average_profit = df['Average Profit'].sum()\n\n    # Plot a bar chart of the profit for each product\n    if df.empty:\n        return None\n    else:\n        df.plot(kind='bar', x='Product', y='Average Profit', ax=None)\n        plt.show()\n        return df, plt.gca()\nproduct_dict = {'Product1': {'Quantity': 10, 'Price': 100, 'Profit': 20},\n                'Product2': {'Quantity': 20, 'Price': 50, 'Profit': 10},\n                'Product3': {'Quantity': 15, 'Price': 75, 'Profit': 5}}\nproduct_keys = ['Product1', 'Product2', 'Product3']"}
{"task_id": "BigCodeBench/151", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data_dict, data_keys):\n    if not all(key in data_dict for key in data_keys):\n        raise ValueError(\"No keys in 'data_keys' are found in 'data_dict'\")\n\n    df = pd.DataFrame(data_dict)\n    scaler = MinMaxScaler()\n    normalized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    fig, ax = plt.subplots()\n    normalized_df.plot(ax=ax)\n\n    return normalized_df, ax"}
{"task_id": "BigCodeBench/152", "solution": "import pandas as pd\nimport numpy as np\nfrom random import randint\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\ndef task_func():\n    # Create a DataFrame with students and courses\n    df = pd.DataFrame(np.random.randint(0, 101, size=(len(STUDENTS), len(COURSES))), columns=STUDENTS, index=COURSES)\n\n    # Calculate the average grade for each student\n    df['Average'] = df.mean(axis=1)\n\n    return df"}
{"task_id": "BigCodeBench/153", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(data):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n\n    # Check if the DataFrame has a column named 'Category'\n    if 'Category' not in data.columns:\n        raise ValueError(\"DataFrame should have a column named 'Category'\")\n\n    # Create a LabelEncoder object\n    le = LabelEncoder()\n\n    # Apply the LabelEncoder to the 'Category' column\n    data['Encoded'] = le.fit_transform(data['Category'])\n\n    # Return the DataFrame\n    return data"}
{"task_id": "BigCodeBench/154", "solution": "import re\nimport os\nimport glob\nimport mimetypes\ndef task_func(directory, file_pattern, suffix):\n    # Use glob to find all files in the directory that match the pattern and have the given suffix\n    files = glob.glob(os.path.join(directory, f\"*{file_pattern}*{suffix}\"))\n\n    # Initialize an empty dictionary to store the file names and their corresponding MIME types\n    file_mime_types = {}\n\n    # Iterate over each file\n    for file in files:\n        # Get the file name\n        file_name = os.path.basename(file)\n\n        # Get the MIME type using the mimetypes module\n        mime_type = mimetypes.guess_type(file_name)[0]\n\n        # Add the file name and MIME type to the dictionary\n        file_mime_types[file_name] = mime_type\n\n    # Return the dictionary\n    return file_mime_types"}
{"task_id": "BigCodeBench/155", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Compute the average of each row\n    df['Average'] = df.mean(axis=1)\n\n    # Plot the averages\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['Average'])\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average')\n    ax.set_title('Average of each row')\n\n    return df, ax\ndata = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]])"}
{"task_id": "BigCodeBench/156", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Normalize the data using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n    \n    # Create a DataFrame from the normalized data\n    df = pd.DataFrame(normalized_data, columns=data.columns)\n    \n    # Add a new column 'Average' to the DataFrame\n    df['Average'] = df.mean(axis=1)\n    \n    # Calculate the average of each row\n    df['Average'] = df.mean(axis=1)\n    \n    # Create a bar plot of the average values\n    df['Average'].plot(kind='bar')\n    plt.show()\n    \n    return df, plt.gca()\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})"}
{"task_id": "BigCodeBench/157", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Check if the input data is a 2D array\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data is not a 2D array or if it contains non-numeric data.\")\n\n    # Calculate the average of values across each row\n    data['Average'] = data.mean(axis=1)\n\n    # Create a correlation matrix\n    corr = data.corr()\n\n    # Create a heatmap of the correlation matrix\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", ax=ax)\n\n    return data, ax"}
{"task_id": "BigCodeBench/158", "solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\nimport os\ndef task_func(url_str, file_path):\n    # Fetch JSON data from a given URL\n    response = urllib.request.urlopen(url_str)\n    data = response.read()\n\n    # Decode the JSON data\n    json_data = json.loads(data)\n\n    # Compress the JSON data into a gzip file\n    with open(file_path, 'wb') as f_out:\n        with gzip.GzipFile(fileobj=f_out, mode='w') as gz:\n            gz.write(json.dumps(json_data).encode('utf-8'))\n\n    # Return the path to the compressed gzip file\n    return file_path\nurl_str = 'https://jsonplaceholder.typicode.com/posts'\nfile_path = 'test.json.gz'"}
{"task_id": "BigCodeBench/159", "solution": "import struct\nimport io\nimport gzip\nimport numpy as np\ndef task_func(newArray):\n    # Convert the numpy array to bytes\n    bytesArray = newArray.tobytes()\n\n    # Create a BytesIO object\n    buffer = io.BytesIO()\n\n    # Compress the bytesArray using gzip\n    with gzip.GzipFile(fileobj=buffer, mode='wb') as gz:\n        gz.write(bytesArray)\n\n    # Get the bytes of the compressed data\n    compressedData = buffer.getvalue()\n\n    return compressedData\nnewArray = np.array([1, 2, 3, 4, 5])"}
{"task_id": "BigCodeBench/160", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n    if len(data.columns) != 8:\n        raise ValueError(\"Input data should have exactly eight columns\")\n    \n    # Compute the average of each row\n    data['Average'] = data.mean(axis=1)\n    \n    # Plot the distribution of the averages\n    plt.figure(figsize=(10,6))\n    sns.distplot(data['Average'], kde=False, bins=30)\n    ax = plt.gca()\n    \n    # Evaluate the normality of the averages\n    average_values = data['Average'].values\n    stat, p_value = stats.normaltest(average_values)\n    \n    return data, ax, p_value"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Regular expression to match the log entry\n    regex = r'^(\\w+):\\s*\\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\](?:\\s*-)?\\s*(.*)$'\n\n    # List to store the log entries\n    log_entries = []\n\n    # Open the log file\n    with open(log_file, 'r') as f:\n        for line in f:\n            match = re.match(regex, line)\n            if match:\n                # If the timestamp is valid, append the log entry to the list\n                try:\n                    timestamp = datetime.strptime(match.group(2), '%Y-%m-%d %H:%M:%S')\n                    log_entries.append({\n                        'type': match.group(1),\n                        'timestamp': timestamp,\n                        'message': match.group(3)\n                    })\n                except ValueError:\n                    raise ValueError(f'Invalid timestamp in log entry: {line}')\n\n    # If no valid log entries were found, raise an exception\n    if not log_entries:\n        raise ValueError('No valid log entries found')\n\n    # Convert the log entries to a pandas DataFrame\n    df = pd.DataFrame(log_entries)\n\n    # Write the DataFrame to a CSV file\n    csv_file = log_file.rstrip('.txt') + '.csv'\n    df.to_csv(csv_file, index=False)\n\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    if not text:\n        return None\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return None\n\n    lengths = [len(word) for word in words]\n\n    fig, ax = plt.subplots()\n    ax.hist(lengths, bins=np.arange(min(lengths), max(lengths)+1, rwidth), edgecolor='black')\n    ax.set_title('Histogram of Word Lengths')\n    ax.set_xlabel('Length')\n    ax.set_ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/163", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(rows=5, cols=5):\n    # Generate a DataFrame with random numerical data\n    np.random.seed(0)\n    df = pd.DataFrame(np.random.randint(0,100,size=(rows, cols)), columns=list('ABCDEF'))\n\n    # Check if the number of columns exceeds the number of available categories\n    if cols > len(df.columns):\n        raise ValueError(\"The number of columns exceeds the number of available categories\")\n\n    # Sort the DataFrame by the last column\n    df = df.sort_values(by=df.columns[-1], ascending=False)\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/164", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_labels=5, data_range=(0, 1)):\n    # Generate random numeric data across a specified range for a given number of categories\n    np.random.seed(0)\n    data = np.random.uniform(data_range[0], data_range[1], num_labels)\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Data'])\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n\n    # Set the labels and title\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Values')\n\n    # Show the plot\n    plt.show()\n\n    # Return the figure\n    return fig"}
{"task_id": "BigCodeBench/165", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    # Create a DataFrame with random integer values within a specified range for categories 'A' through 'E'\n    df = pd.DataFrame({\n        'A': [randint(rand_range[0], rand_range[1]) for _ in range(num_rows)],\n        'B': [randint(rand_range[0], rand_range[1]) for _ in range(num_rows)],\n        'C': [randint(rand_range[0], rand_range[1]) for _ in range(num_rows)],\n        'D': [randint(rand_range[0], rand_range[1]) for _ in range(num_rows)],\n        'E': [randint(rand_range[0], rand_range[1]) for _ in range(num_rows)]\n    })\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n\n    # Set the labels and title\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Values')\n    ax.set_title('Stacked Bar Chart')\n\n    # Show the plot\n    plt.show()\n\n    return fig"}
{"task_id": "BigCodeBench/166", "solution": "import pandas as pd\nfrom datetime import datetime\nimport holidays\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    # Check if start_date and end_date are datetime objects\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date should be datetime objects\")\n\n    # Check if start_date is after end_date\n    if start_date > end_date:\n        raise ValueError(\"start_date should be before end_date\")\n\n    # Fetch public holidays for the specified country\n    country_holidays = holidays.CountryHoliday(country)\n\n    # Generate a list of business days between start_date and end_date, excluding weekends and public holidays\n    business_days = pd.date_range(start=start_date, end=end_date, freq='B')\n    business_days = business_days[~business_days.isin(country_holidays)]\n\n    return business_days.tolist()\nstart_date = datetime(2023, 1, 1)\nend_date = datetime(2023, 12, 31)\ncountry = 'US'"}
{"task_id": "BigCodeBench/167", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_types=5, integer_range=(0, 100)):\n    # Generate a DataFrame with random integer values across a specified number of categories\n    df = pd.DataFrame(\n        {f'Category_{i}': [randint(*integer_range) for _ in range(randint(1, 10))] for i in range(num_types)}\n    )\n\n    # Calculate the sum of each category\n    df['Total'] = df.sum(axis=1)\n\n    # Sort the DataFrame by the 'Total' column in descending order\n    df = df.sort_values(by='Total', ascending=False)\n\n    # Reverse the DataFrame to get the categories in descending order of their total values\n    df = df.iloc[::-1]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.barh(df.index, df['Total'], color='skyblue')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Category')\n    ax.set_title('Horizontal Stacked Bar Chart')\n\n    return fig, ax"}
{"task_id": "BigCodeBench/168", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_groups=5, data_size=5, labels=None):\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.randint(0, 100, (data_size, num_groups))\n    df = pd.DataFrame(data, columns=labels)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Create a stacked bar chart\n    df.plot(kind='bar', stacked=True, ax=ax)\n\n    # Save the figure to a file\n    filename = 'test_plot.png'\n    fig.savefig(filename)\n\n    return fig, df, filename"}
{"task_id": "BigCodeBench/169", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\ndef task_func(image, sigma=2):\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input is not a numpy array\")\n    if sigma <= 0:\n        raise ValueError(\"Sigma is non-positive\")\n\n    # Apply Gaussian filter\n    filtered_image = gaussian_filter(image, sigma)\n\n    # Create a figure with two subplots\n    fig, ax = plt.subplots(1, 2)\n\n    # Plot the original image\n    ax[0].imshow(image, cmap='gray')\n    ax[0].set_title('Original')\n\n    # Plot the filtered image\n    ax[1].imshow(filtered_image, cmap='gray')\n    ax[1].set_title('Filtered')\n\n    plt.tight_layout()\n    plt.show()\n\n    return filtered_image\nimage = np.random.rand(100, 100)"}
{"task_id": "BigCodeBench/170", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url, sort_by_column=\"title\"):\n    try:\n        response = requests.get(csv_url)\n        if response.status_code != 200:\n            raise Exception(\"Exception: If the response status code is not 200.\")\n        else:\n            data = StringIO(response.content.decode('utf-8'))\n            df = pd.read_csv(data)\n            sorted_df = df.sort_values(by=[sort_by_column])\n            return sorted_df\n    except Exception as e:\n        print(e)"}
{"task_id": "BigCodeBench/171", "solution": "import random\nimport pandas as pd\nimport collections\ndef task_func(vegetable_dict, seed=0):\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Create a dictionary to map vegetables to people\n    people_to_vegetables = {person: list(vegetable_dict.keys()) for person in vegetable_dict.values()}\n\n    # Reverse the dictionary to map people to vegetables\n    vegetables_to_people = {vegetable: person for person, vegetables in people_to_vegetables.items() for vegetable in vegetables}\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(vegetables_to_people.items()), columns=['Vegetable', 'Person'])\n\n    # Assign random counts to each vegetable\n    df['Count'] = [random.randint(1, 10) for _ in df['Vegetable']]\n\n    # Calculate the total count\n    total_count = df['Count'].sum()\n\n    # Calculate the percentage occurrence of each vegetable\n    df['Percentage'] = df['Count'] / total_count * 100\n\n    return df\nvegetable_dict = {'Alice': ['Carrot', 'Potato'], 'Bob': ['Tomato', 'Cabbage', 'Spinach'], 'Charlie': ['Carrot', 'Tomato']}"}
{"task_id": "BigCodeBench/172", "solution": "import json\nfrom datetime import datetime\ndef is_weekend(date_str):\n    date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n    return date.weekday() >= 5  # 5 is Saturday and 6 is Sunday\ndef task_func(json_data):\n    utc_datetime = json.loads(json_data)['utc_datetime']\n    return is_weekend(utc_datetime)"}
{"task_id": "BigCodeBench/173", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(country_dict):\n    # Generate a list of random GDP values\n    gdp_values = np.random.randint(1000000000, 10000000000, len(country_dict))\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(country_dict.items()), columns=['Country', 'GDP'])\n\n    # Add the GDP values to the DataFrame\n    df['GDP'] = gdp_values\n\n    return df\ncountry_dict = {'Country1': 1200000000, 'Country2': 1500000000, 'Country3': 2000000000}"}
{"task_id": "BigCodeBench/174", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, key, min_value, max_value):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"The input data is not a pandas DataFrame\")\n\n    new_column = pd.Series(np.random.randint(min_value, max_value, size=data.shape[0]))\n    data[key] = new_column\n\n    return data"}
{"task_id": "BigCodeBench/175", "solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    # Check if DataFrame is empty\n    if df.empty:\n        return plt.gca()\n\n    # Check if DataFrame has the necessary columns\n    if not all(col in df.columns for col in ['Title', 'Views', 'Likes']):\n        return plt.gca()\n\n    # Search for videos with titles containing \"how\" or \"what\"\n    df = df[(df['Title'].str.contains(r'\\b(how|what)\\b', case=False))]\n\n    # If no videos match the search criteria, return an empty plot\n    if df.empty:\n        return plt.gca()\n\n    # Calculate like ratios\n    df['Like Ratio'] = df['Likes'] / df['Views']\n\n    # Generate a bar plot of the like ratios\n    fig, ax = plt.subplots()\n    ax.bar(df['Title'], df['Like Ratio'])\n    ax.set_xlabel('Video Titles')\n    ax.set_ylabel('Like Ratio')\n\n    return ax"}
{"task_id": "BigCodeBench/176", "solution": "import re\nimport socket\ndef task_func(ip_addresses: list) -> dict:\n    result = {}\n    for ip in ip_addresses:\n        try:\n            hostname = socket.gethostbyaddr(ip)\n            if hostname:\n                result[ip] = hostname[0]\n            else:\n                result[ip] = None\n        except socket.error:\n            result[ip] = None\n    return result\nip_addresses = [\"8.8.8.8\", \"192.0.2.0\", \"10.10.10.10\", \"127.0.0.1\", \"1234567890123456789012345678901234567890\"]"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\ndef task_func(df):\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Extract articles with \"like\" or \"what\" in the title\n    df = df[df['Title'].str.contains(r'\\b(like|what)\\b', case=False)]\n\n    # Initialize an empty dictionary to store the word frequency\n    word_freq = {}\n\n    # Iterate over each article\n    for index, row in df.iterrows():\n        # Remove punctuation and convert to lower case\n        content = re.sub(r'[^\\w\\s]', '', row['Content'].lower())\n\n        # Tokenize the content and count the frequency of each word\n        words = nltk.word_tokenize(content)\n        for word in words:\n            if word not in word_freq:\n                word_freq[word] = 0\n            word_freq[word] += 1\n\n    return word_freq"}
{"task_id": "BigCodeBench/178", "solution": "import re\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(ip_address):\n    if not re.match(IP_REGEX, ip_address):\n        return 'Invalid IP address received'\n    else:\n        return ip_address"}
{"task_id": "BigCodeBench/179", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ndef task_func(df):\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        return\n\n    # Filter out articles without titles containing \"how\" or \"what\"\n    df = df[(df['Title'].str.contains(r'\\b(how|what)\\b'))]\n\n    if df.empty:\n        return\n\n    # Extract the words from the content\n    content = df['Content'].str.cat(sep=' ')\n    words = re.findall(r'\\b\\w+\\b', content)\n\n    # Calculate the TF-IDF scores\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(words)\n    tfidf_scores = np.mean(X.toarray(), axis=0)\n\n    # Plot the TF-IDF scores\n    plt.figure(figsize=(10, 5))\n    plt.bar(vectorizer.get_feature_names_out(), tfidf_scores, color='b', align='center')\n    plt.ylabel('TF-IDF Score')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    ax = plt.gca()\n\n    return ax"}
{"task_id": "BigCodeBench/180", "solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    # Check if the image file exists\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"Image file not found at {img_path}\")\n\n    # Open the image file\n    img = Image.open(img_path)\n\n    # List to store the Axes and pixel values of the scaled images\n    scaled_images = []\n\n    # Scale the image and display each scaled image\n    for scale in scale_factors:\n        # Resize the image\n        img_resized = resize(img, scale)\n\n        # Create a new figure\n        fig, ax = plt.subplots()\n\n        # Display the resized image\n        ax.imshow(img_resized)\n\n        # Get the pixel values of the resized image\n        img_pixels = np.array(img_resized)\n\n        # Append the Axes and pixel values to the list\n        scaled_images.append((ax, img_pixels))\n\n    return scaled_images"}
{"task_id": "BigCodeBench/181", "solution": "from django.http import JsonResponse\nfrom django.conf import settings\nimport random\nimport time\ndef task_func(data, min_delay, max_delay):\n    # Generate a random delay between min_delay and max_delay\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n\n    # Simulate the latency of the network by adding a random delay\n    time.sleep(random.uniform(0.1, 0.5))\n\n    # Return a Django HttpResponse with JSON data\n    return JsonResponse(data)"}
{"task_id": "BigCodeBench/182", "solution": "import re\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(df):\n    # Filter articles by case-insensitive keyword \"how\" or \"what\"\n    df = df[df['title'].str.lower().str.contains(r'\\b(how|what)\\b')]\n\n    # Vectorize the content using CountVectorizer\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(df['content'])\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(X)\n\n    # Get cluster labels for the filtered articles\n    labels = kmeans.labels_\n\n    return labels"}
{"task_id": "BigCodeBench/183", "solution": "from django.http import JsonResponse, HttpResponse\nimport uuid\ndef task_func(request):\n    # Generate a UUID\n    uuid_var = uuid.uuid4()\n\n    # Create a dictionary with the UUID\n    data = {\n        'uuid': str(uuid_var)\n    }\n\n    # Return a JSON response with the UUID\n    return JsonResponse(data)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub('[^A-Za-z\\s]+', '', x))\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n\n    # Vectorize the text data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Create a DataFrame with each word as a column and their count as rows\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df"}
{"task_id": "BigCodeBench/185", "solution": "import pandas as pd\nimport numpy as np\nimport folium\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict) or any(not isinstance(v, tuple) or len(v) != 2 for v in dic.values()):\n        raise ValueError(\"'Lon' or 'Lat' keys are missing in the dictionary, or if their values are not tuples.\")\n\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"Cities should be a list of strings.\")\n\n    df = pd.DataFrame(cities, columns=['City'])\n    df['Longitude'] = np.random.uniform(dic['Lon'][0], dic['Lon'][1], len(cities))\n    df['Latitude'] = np.random.uniform(dic['Lat'][0], dic['Lat'][1], len(cities))\n\n    map_obj = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=5)\n\n    for i in range(len(cities)):\n        folium.CircleMarker(location=[df.iloc[i]['Latitude'], df.iloc[i]['Longitude']], popup=df.iloc[i]['City'], radius=5).add_to(map_obj)\n\n    return (map_obj, df)"}
{"task_id": "BigCodeBench/186", "solution": "from geopy.distance import geodesic\nimport folium\ndef task_func(dic):\n    if not dic:\n        raise ValueError(\"Input dictionary is empty\")\n\n    m = folium.Map(location=[48.8566, 2.3522])\n\n    for key, value in dic.items():\n        m.add_marker(location=[value['lat'], value['lon']],\n                      popup=key,\n                      icon=folium.Icon(icon='cloud'))\n\n    distances = {}\n\n    for key, value in dic.items():\n        distances[key] = geodesic([dic[key]['lat'], dic[key]['lon']], [dic[value]['lat'], dic[value]['lon']]).kilometers\n\n    return m, distances"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not all(isinstance(v, tuple) and len(v) == 2 for v in dic.values()):\n        raise ValueError(\"If 'Lon' or 'Lat' keys are missing in the dictionary, or if their values are not tuples.\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"Cities should be a list of strings.\")\n    \n    # Generate random coordinates\n    random_coords = [(np.random.uniform(dic['Lon'][0], dic['Lon'][1]), np.random.uniform(dic['Lat'][0], dic['Lat'][1])) for _ in cities]\n    \n    # Create GeoDataFrame\n    gdf = gpd.GeoDataFrame(\n        geometry=[Point(xy) for xy in random_coords],\n        columns=['Coordinates'],\n        index=cities,\n    )\n    \n    return gdf"}
{"task_id": "BigCodeBench/188", "solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\ndef task_func(dic):\n    # Instantiate the geolocator\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n\n    # Create a map object\n    m = folium.Map(location=[37.7749, -122.4194], zoom_start=13)\n\n    # Iterate over the dictionary\n    for index, row in dic.iterrows():\n        # If the location is a string, resolve it to a latitude and longitude\n        if isinstance(row['location'], str):\n            location = geolocator.geocode(row['location'])\n            if location is not None:\n                row['latitude'], row['longitude'] = location.latitude, location.longitude\n\n        # Add a marker for the location\n        folium.CircleMarker(\n            location=[row['latitude'], row['longitude']],\n            radius=5,\n            color='blue',\n            fill=True,\n            popup=row['location'],\n            fill_color='blue',\n            fill_opacity=0.5,\n            parse_html=False\n        ).add_to(m)\n\n    # Return the map object\n    return m"}
{"task_id": "BigCodeBench/189", "solution": "import re\nimport json\nimport requests\ndef task_func(data_url: str) -> list:\n    try:\n        response = requests.get(data_url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as err:\n        return \"Invalid url input\"\n\n    data = json.loads(response.text)\n    names = re.findall(r'[^\\[\\]]+', str(data))\n    return names"}
{"task_id": "BigCodeBench/190", "solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\ndef task_func(csv_input):\n    # Check if csv_input is a file path\n    if isinstance(csv_input, str):\n        with open(csv_input, 'r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n    # Check if csv_input is a StringIO object\n    elif isinstance(csv_input, StringIO):\n        data = csv_input.getvalue().split('\\n')\n        data = [row.split(',') for row in data if row]\n    else:\n        raise TypeError(\"csv_input should be a file path or StringIO object\")\n\n    # Create a new database connection\n    conn = sqlite3.connect(DATABASE_NAME)\n\n    # Create a new cursor\n    cur = conn.cursor()\n\n    # Create a new table\n    cur.execute(f'CREATE TABLE IF NOT EXISTS {TABLE_NAME} ({\",\".join(data[0])})')\n\n    # Insert data into the table\n    for row in data[1:]:\n        cur.execute(f'INSERT INTO {TABLE_NAME} VALUES ({\",\".join([\"?\"]*len(row))})', row)\n\n    # Commit the transaction\n    conn.commit()\n\n    # Close the connection\n    conn.close()\n\n    # Read data from the table and return it as a DataFrame\n    df = pd.read_sql_query(f'SELECT * FROM {TABLE_NAME}', conn)\n\n    return df"}
{"task_id": "BigCodeBench/191", "solution": "import random\nfrom scipy import stats\ndef task_func(animals, mean):\n    # Initialize an empty dictionary to store animal types and their sales\n    sales_dict = {}\n\n    # Generate a random number of customers\n    num_customers = stats.poisson.rvs(mu=mean)\n\n    # For each customer, randomly choose an animal type and add it to the sales_dict\n    for _ in range(num_customers):\n        animal_type = random.choice(animals)\n        if animal_type in sales_dict:\n            sales_dict[animal_type] += 1\n        else:\n            sales_dict[animal_type] = 1\n\n    return sales_dict\nanimals = ['cat', 'dog', 'bird', 'fish', 'lizard', 'cow', 'horse', 'zebra']\nmean = 5"}
{"task_id": "BigCodeBench/192", "solution": "import re\nimport smtplib\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    # Extract names\n    names = re.findall(r'\\[.*?\\]', text)\n    names = [name.strip('[]') for name in names]\n\n    # Send email\n    if smtp is None:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n\n    message = \"Subject: Extracted Names\\n\\n\" + \"\\n\".join(names)\n    smtp.sendmail(email_address, recepient_address, message)\n    smtp.quit()"}
{"task_id": "BigCodeBench/193", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\ndef task_func(rows, columns):\n    df = pd.DataFrame(columns=[f'col{i}' for i in range(columns)])\n    for col in df.columns:\n        df[col] = df[col].apply(lambda x: random_data_generator(DATA_TYPES))\n    return df\ndef random_data_generator(data_types):\n    data_type = choice(data_types)\n    if data_type == str:\n        return ''.join(choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5))\n    elif data_type == int:\n        return np.random.randint(0, 10)\n    elif data_type == float:\n        return np.random.random()\n    elif data_type == list:\n        return [np.random.randint(0, 10) for _ in range(np.random.randint(1, 6))]\n    elif data_type == tuple:\n        return tuple(np.random.randint(0, 10) for _ in range(np.random.randint(1, 6)))\n    elif data_type == dict:\n        return {np.random.randint(0, 10): np.random.randint(0, 10) for _ in range(np.random.randint(1, 6))}\n    elif data_type == set:\n        return set(np.random.randint(0, 10, np.random.randint(1, 6)))"}
{"task_id": "BigCodeBench/194", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\ndef task_func(data_size):\n    # Generate random numeric data\n    data = np.random.rand(data_size)\n\n    # Select a random color for the histogram bars\n    color = np.random.choice(BAR_COLOR)\n\n    # Create a histogram of the data\n    plt.hist(data, color=color)\n    plt.show()\n\n    return data, color"}
{"task_id": "BigCodeBench/195", "solution": "import subprocess\nimport platform\nimport time\ndef task_func(url):\n    if platform.system() == \"Windows\":\n        subprocess.Popen([\"start\", url])\n    elif platform.system() == \"Darwin\":\n        subprocess.Popen([\"open\", url])\n    else:\n        subprocess.Popen([\"xdg-open\", url])\n\n    time.sleep(1)  # Wait for the process to start\n\n    if platform.system() == \"Windows\":\n        process = subprocess.Popen([\"tasklist\"], stdout=subprocess.PIPE)\n    else:\n        process = subprocess.Popen([\"pgrep\", \"-u\", \"$USER\", \"-f\", \"chrome\"], stdout=subprocess.PIPE)\n\n    output, _ = process.communicate()\n    return int(output.decode().strip())"}
{"task_id": "BigCodeBench/196", "solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef task_func(length, range_limit=100, seed=0):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit should be greater than 1\")\n\n    random.seed(seed)\n    random_numbers = [random.randint(0, range_limit) for _ in range(length)]\n\n    ax = sns.histplot(random_numbers, bins=range_limit)\n    plt.show()\n\n    return ax, random_numbers"}
{"task_id": "BigCodeBench/197", "solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(l1, l2, N=10):\n    # Calculate the differences between the respective elements of the lists\n    differences = [abs(a-b) for a, b in zip(l1, l2)]\n    \n    # Get the N biggest differences\n    biggest_differences = heapq.nlargest(N, differences)\n    \n    # Square the differences\n    squared_differences = [diff**2 for diff in biggest_differences]\n    \n    # Calculate the square root of the differences\n    root_differences = [math.sqrt(diff) for diff in squared_differences]\n    \n    # Plot the differences\n    fig, ax = plt.subplots()\n    ax.plot(root_differences)\n    \n    return ax"}
{"task_id": "BigCodeBench/198", "solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    avg = statistics.mean(data)\n    greater_than_avg = [x for x in data if x > avg]\n    count = len(greater_than_avg)\n\n    return np.array(greater_than_avg), count\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nvalue = 6"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Check if utc_datetime is a datetime object\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    # Check if all other parameters are in the expected format\n    if not all(isinstance(city, str) and isinstance(condition, str) for city, condition in zip(cities, weather_conditions)):\n        raise ValueError(\"cities and weather_conditions must be lists of strings\")\n\n    if not all(isinstance(tz, str) for tz in timezones.values()):\n        raise ValueError(\"timezones must be a dictionary where each value is a string\")\n\n    # Set the seed for the random number generator\n    set_seed(seed)\n\n    # Generate a random weather condition\n    def get_weather_condition():\n        return weather_conditions[randint(0, len(weather_conditions) - 1)]\n\n    # Generate a random timezone\n    def get_timezone():\n        return pytz.timezone(timezones[cities[randint(0, len(cities) - 1)]])\n\n    # Generate a weather report for each city\n    reports = []\n    for city, tz in timezones.items():\n        report = {\n            'City': city,\n            'Local Time': utc_datetime.astimezone(get_timezone()).strftime('%Y-%m-%d %H:%M:%S %Z%z'),\n            'Weather Condition': get_weather_condition()\n        }\n        reports.append(report)\n\n    # Convert the reports to a pandas DataFrame\n    df = pd.DataFrame(reports)\n\n    return df"}
{"task_id": "BigCodeBench/200", "solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    numbers = [random.random() for _ in range(n)]\n\n    # Calculate the average of the generated numbers\n    average = sum(numbers) / n\n\n    # Find numbers greater than the average\n    greater_numbers = [num for num in numbers if num > average]\n\n    # Count how many are greater than or equal to the specified value\n    count = len([num for num in numbers if num >= value])\n\n    # Sort the numbers\n    numbers.sort()\n\n    # Plot the sorted numbers\n    plt.plot(numbers)\n    plt.show()\n\n    # Return the list of numbers greater than the average and the count of numbers greater than or equal to the specified value\n    return greater_numbers, count\nn = 100\nvalue = 0.5"}
{"task_id": "BigCodeBench/201", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df, column, value):\n    if column not in df.columns:\n        raise ValueError(\"Column does not exist in the DataFrame\")\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value is not a number\")\n    \n    column_data = df[column].dropna()\n    \n    if column_data.empty:\n        raise ValueError(\"Column is empty after dropping NaN values\")\n    \n    avg = np.mean(column_data)\n    greater_than_avg = column_data[column_data > avg]\n    count = len(greater_than_avg)\n    \n    fig, ax = plt.subplots()\n    ax.hist(greater_than_avg, bins='auto', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of values greater than average')\n    plt.show()\n    \n    return (greater_than_avg.values, count, ax)"}
{"task_id": "BigCodeBench/202", "solution": "import re\nimport json\nfrom collections import Counter\ndef task_func(json_str, top_n=10):\n    # Extract all URLs from the string-serialized JSON dict\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    urls = re.findall(url_pattern, json_str)\n\n    # Count the occurrences of each URL\n    url_counts = Counter(urls)\n\n    # Return a dict with the URLs as keys and the number of times they appear as values\n    return dict(url_counts.most_common(top_n))\njson_str = '{\"urls\": [\"http://example.com\", \"http://test.com\", \"http://example.com\", \"http://test.com\", \"http://sub.test.com\", \"http://sub.test.com\", \"http://sub.sub.test.com\"]}'"}
{"task_id": "BigCodeBench/203", "solution": "import json\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    if not input_data:\n        return \"No input data provided\"\n\n    try:\n        data = json.loads(input_data)\n    except json.JSONDecodeError:\n        return \"Invalid JSON format\"\n\n    if not isinstance(data, dict) or 'emails' not in data:\n        return \"Invalid data format. Expected a dictionary with 'emails' key\"\n\n    emails = data.get('emails')\n    if not isinstance(emails, list):\n        return \"Invalid data format. 'emails' key should be a list\"\n\n    receivers = []\n    for email in emails:\n        if not isinstance(email, str):\n            return \"Invalid data format. Each email should be a string\"\n        receivers.append(email)\n\n    message = MIMEMultipart()\n    message['From'] = email_address\n    message['To'] = ', '.join(receivers)\n    message['Subject'] = 'Extracted Names'\n\n    body = 'Subject: Extracted Names\\n\\n'\n    for receiver in receivers:\n        body += receiver + '\\n'\n\n    message.attach(MIMEText(body, 'plain'))\n\n    if not smtp:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n\n    smtp.send_message(message)\n    smtp.quit()\n\n    return \"Emails sent successfully\""}
{"task_id": "BigCodeBench/204", "solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Calculate mean\n    mean = np.mean(L)\n\n    # Calculate median\n    median = np.median(L)\n\n    # Calculate mode\n    count_dict = Counter(L)\n    max_count = max(list(count_dict.values()))\n    mode = [k for k, v in count_dict.items() if v == max_count]\n\n    # Calculate standard deviation\n    std_dev = np.std(L)\n\n    # Visualize the data\n    plt.hist(L, bins='auto', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the list')\n    plt.show()\n\n    return {'mean': mean, 'median': median, 'mode': mode, 'std_dev': std_dev, 'plot': plt}\nL = [1, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5, 5]"}
{"task_id": "BigCodeBench/205", "solution": "import subprocess\nfrom multiprocessing import Pool\ndef execute_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\ndef task_func(commands):\n    if not commands:\n        return []\n    with Pool() as p:\n        outputs = p.map(execute_command, commands)\n    return outputs"}
{"task_id": "BigCodeBench/206", "solution": "import csv\nimport json\nimport os\ndef task_func(file_name):\n    try:\n        if not os.path.exists(file_name):\n            raise FileNotFoundError\n        with open(file_name, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            json_data = [row for row in csv_reader]\n            json_file_name = os.path.splitext(file_name)[0] + '.json'\n            with open(json_file_name, 'w') as json_file:\n                json.dump(json_data, json_file)\n            return json_file_name\n    except FileNotFoundError:\n        print(f\"File {file_name} not found.\")\n        return None"}
{"task_id": "BigCodeBench/207", "solution": "import re\nimport requests\ndef task_func(input):\n    # Extract API endpoint from the input string\n    api_endpoint = re.search(r'https?://[^\\s]+', input)\n    if api_endpoint:\n        api_endpoint = api_endpoint.group(0)\n    else:\n        raise ValueError(\"No API endpoint found in the input string.\")\n\n    # Send a GET request to the API endpoint\n    response = requests.get(api_endpoint)\n\n    # Return the response data in JSON format\n    return response.json()"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.randint(low=-1, high=2, size=elements)\n\n    stats = {\n        'count': random_walk.size,\n        'mean': random_walk.mean(),\n        'std_dev': random_walk.std(),\n        'min': random_walk.min(),\n        '25th_percentile': np.percentile(random_walk, 25),\n        '50th_percentile': np.percentile(random_walk, 50),\n        '75th_percentile': np.percentile(random_walk, 75),\n        '95th_percentile': np.percentile(random_walk, 95),\n        'max': random_walk.max()\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Value')\n\n    return stats, ax"}
{"task_id": "BigCodeBench/209", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Generate a scatter plot\n    x = [i[0] for i in data]\n    y = [i[1] for i in data]\n    plt.scatter(x, y)\n\n    # Find the tuple with the maximum value at index 1\n    max_tuple = max(data, key=itemgetter(1))\n\n    # Highlight the tuple with the maximum value\n    plt.plot(max_tuple[0], max_tuple[1], 'ro')\n\n    # Set the title, labels, and legend\n    plt.title('Max Tuple Highlighted')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend(['Max Tuple'])\n\n    return plt.gca()\ndata = [(1, 2), (2, 3), (3, 4), (4, 5), (5, 6)]"}
{"task_id": "BigCodeBench/210", "solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Count the frequency of each letter\n    letter_counts = collections.Counter(data)\n\n    # Sort the letters by their count in descending order\n    sorted_letters = sorted(letter_counts.items(), key=itemgetter(1), reverse=True)\n\n    # Separate the letters and their counts\n    letters, counts = zip(*sorted_letters)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(letters, counts)\n\n    # Highlight the letter with the maximum count\n    max_count_index = counts.index(max(counts))\n    ax.bar(letters[max_count_index], max(counts), color='r')\n\n    # Add labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n\n    # Add a legend\n    ax.legend(['Letter Counts', 'Max Value Letter'])\n\n    # Show the plot\n    plt.show()\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n\n    # Save the zip file to the specified directory\n    with open(os.path.join(destination_directory, 'temp.zip'), 'wb') as f:\n        f.write(response.content)\n\n    # Extract the zip file contents to the specified directory\n    with zipfile.ZipFile(os.path.join(destination_directory, 'temp.zip'), 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Return the list of extracted files\n    return os.listdir(destination_directory)\nurl = 'http://example.com/sample.zip'\ndestination_directory = '/path/to/extract/to'\nheaders = {'User-Agent': 'Mozilla/5.0'}"}
{"task_id": "BigCodeBench/212", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Generate a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data[:, 0], data[:, 1])\n\n    # Set the x-axis label\n    ax.set_xlabel('x')\n\n    # Set the y-axis label\n    ax.set_ylabel('y')\n\n    # Set the title\n    ax.set_title('Points with Max Y Point Highlighted')\n\n    # Find the point with the maximum y-value\n    max_y_point = data[np.argmax(data[:, 1])]\n\n    return ax, max_y_point\ndata = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])"}
{"task_id": "BigCodeBench/213", "solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\ndef task_func(intervals=100, seed=0):\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Generate a list of random numbers over the specified intervals\n    numbers = [random.random() for _ in range(intervals)]\n\n    # Calculate the elapsed time\n    elapsed_time = [time.time() - start for start in numbers]\n\n    # Plot the numbers as a function of elapsed time\n    plt.figure(figsize=(10, 6))\n    plt.plot(elapsed_time, numbers)\n    plt.xlabel('Elapsed Time')\n    plt.ylabel('Number')\n    plt.title('Generated Numbers as a Function of Elapsed Time')\n\n    # Calculate the kurtosis of the generated numbers\n    kurt = kurtosis(numbers)\n\n    # Return the Axes object and the kurtosis value\n    return plt.gca(), kurt"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low > range_high:\n        raise ValueError(\"ValueError: If range_low is not less than range_high.\")\n\n    random.seed(seed)\n    image = np.zeros(image_size, dtype=np.uint8)\n    for i in range(image_size[0]):\n        for j in range(image_size[1]):\n            for k in range(image_size[2]):\n                image[i, j, k] = random.randint(range_low, range_high)\n\n    plt.figure(figsize=(10, 10))\n    ax = plt.subplot(111, projection='3d')\n    ax.imshow(image)\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/215", "solution": "import requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nHEADERS = {\n    'accept': 'application/json'\n}\ndef task_func(url, parameters):\n    try:\n        # Send a GET request to the API endpoint\n        response = requests.get(url, headers=HEADERS, params=parameters)\n\n        # Check if the request was successful\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get data from the API endpoint. Status code: {response.status_code}\")\n\n        # Load the data into a pandas DataFrame\n        data = response.json()\n        df = pd.DataFrame(data)\n\n        # Check if the data is empty\n        if df.empty:\n            raise Exception(\"The data is empty\")\n\n        # Check if the data is valid\n        try:\n            df.dropna(inplace=True)\n        except Exception as e:\n            raise Exception(f\"Invalid data. Error: {str(e)}\")\n\n        # Generate a correlation matrix\n        corr_matrix = df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\n        # Set up the matplotlib figure\n        fig, ax = plt.subplots(figsize=(11, 9))\n\n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,\n                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n        # Return the Axes object\n        return df, ax\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None, None"}
{"task_id": "BigCodeBench/216", "solution": "import os\nimport json\nfrom collections import Counter\ndef task_func(json_dir_path, word_count):\n    # Initialize an empty counter\n    word_counter = Counter()\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(json_dir_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the JSON file\n            with open(os.path.join(json_dir_path, filename), 'r') as f:\n                # Load the JSON data\n                data = json.load(f)\n\n                # Extract the text content from the JSON data\n                text = ' '.join(str(item) for item in data.values())\n\n                # Split the text into words and update the counter\n                word_counter.update(text.split())\n\n    # Get the most common words and their counts\n    most_common_words = word_counter.most_common(word_count)\n\n    # Return the most common words and their counts\n    return most_common_words"}
{"task_id": "BigCodeBench/217", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate a sample from a normal distribution\n    sample = np.random.normal(mu, sigma, sample_size)\n\n    # Calculate the empirical mean and standard deviation of the sample\n    empirical_mean = np.mean(sample)\n    empirical_std = np.std(sample)\n\n    # Create a range of x values for the probability density function\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n\n    # Calculate the probability density function\n    pdf = stats.norm.pdf(x, mu, sigma)\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(sample, bins=20, density=True, alpha=0.6, color='g')\n\n    # Plot the probability density function\n    ax.plot(x, pdf, 'k', linewidth=2)\n\n    # Set the title\n    ax.set_title(f'Normal Distribution with $\\\\mu = {mu:.2f}, \\\\sigma = {sigma:.2f}$')\n\n    return ax, empirical_mean, empirical_std"}
{"task_id": "BigCodeBench/218", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\ndef task_func(df, dict_mapping, plot_histogram=False):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a DataFrame\")\n    \n    # Check if FEATURES and TARGET columns exist in the DataFrame\n    if set(FEATURES + [TARGET]).issubset(df.columns):\n        pass\n    else:\n        raise ValueError(\"FEATURES and TARGET columns not in the input DataFrame\")\n    \n    # Replace values according to the dictionary mapping\n    df.replace(dict_mapping, inplace=True)\n    \n    # Standardize specified features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    # Optionally, draw a histogram of the target variable\n    if plot_histogram:\n        plt.hist(df[TARGET], bins=20)\n        plt.show()\n    \n    return df"}
{"task_id": "BigCodeBench/219", "solution": "import math\nimport statistics\nimport numpy as np\ndef task_func(input_list):\n    # Sort the list in ascending order based on the degree value of its elements\n    sorted_list = sorted(input_list, key=lambda x: x.degree)\n\n    # Calculate the mean, median, and mode of both the sorted list and the same for the magnitude of the fast fourier transform of the degree values\n    mean_sorted_list = round(statistics.mean(sorted_list), 2)\n    median_sorted_list = round(statistics.median(sorted_list), 2)\n    mode_sorted_list = statistics.mode(sorted_list)\n\n    # Calculate the mean, median, and mode of the same for the magnitude of the fast fourier transform of the degree values\n    magnitude_sorted_list = [abs(x.magnitude) for x in sorted_list]\n    mean_magnitude_sorted_list = round(np.mean(magnitude_sorted_list), 2)\n    median_magnitude_sorted_list = round(np.median(magnitude_sorted_list), 2)\n    mode_magnitude_sorted_list = statistics.mode(magnitude_sorted_list)\n\n    # Return the results as a tuple\n    return (mean_sorted_list, median_sorted_list, mode_sorted_list, mean_magnitude_sorted_list, median_magnitude_sorted_list, mode_magnitude_sorted_list)"}
{"task_id": "BigCodeBench/220", "solution": "from random import choice\nimport turtle\nimport time\ndef task_func(colors):\n    # Set up the turtle screen\n    screen = turtle.Screen()\n    screen.bgcolor(\"white\")\n\n    # Create a turtle object\n    turtle_obj = turtle.Turtle()\n    turtle_obj.speed(1)\n\n    # Draw the squares\n    for i in range(5):\n        color = choice(colors)\n        turtle_obj.color(color)\n        turtle_obj.forward(100)\n        turtle_obj.right(90)\n        time.sleep(1)\n\n    # Close the turtle screen\n    turtle.done()\ncolors = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\"]"}
{"task_id": "BigCodeBench/221", "solution": "import numpy as np\nfrom scipy import stats\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\ndef task_func(df, dct):\n    if not all(feature in df.columns for feature in FEATURES):\n        return \"Invalid input: DataFrame does not contain all required features\"\n\n    for feature in FEATURES:\n        if feature not in dct:\n            return f\"Invalid input: Missing mapping for feature {feature}\"\n\n        try:\n            df[feature] = df[feature].replace(dct[feature])\n        except KeyError:\n            return f\"Invalid input: Mapping for feature {feature} is not a valid key in the dictionary\"\n\n    result = {}\n\n    for feature in FEATURES:\n        try:\n            data = df[feature].values\n            mean = np.mean(data)\n            median = np.median(data)\n            mode = stats.mode(data)[0][0]\n            variance = np.var(data)\n            result[feature] = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance}\n        except Exception as e:\n            return f\"Error in calculation for feature {feature}: {str(e)}\"\n\n    return result"}
{"task_id": "BigCodeBench/222", "solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(list_input):\n    # Sort the list in ascending order based on the degree value of its elements\n    sorted_list = sorted(list_input, key=lambda x: abs(x))\n\n    # Calculate the cumulative sum of the sorted list\n    cumulative_sum = np.cumsum(sorted_list)\n\n    # Draw a line chart of the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumulative_sum)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.set_title('Cumulative Sum of Sorted List')\n\n    return cumulative_sum, ax\nlist_input = [3, -2, 1, -1, 2, -3]"}
{"task_id": "BigCodeBench/223", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df, dct, columns=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    if columns is not None:\n        for col in columns:\n            if col in df.columns:\n                df[col] = df[col].map(dct.get(col))\n            else:\n                print(f\"Warning: Column {col} not found in DataFrame. Skipping...\")\n    else:\n        for col, dct_col in dct.items():\n            if col in df.columns:\n                df[col] = df[col].map(dct_col)\n            else:\n                print(f\"Warning: Column {col} not found in DataFrame. Skipping...\")\n\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n\n    for col in df.columns:\n        if df[col].dtype == 'float64':\n            df[col] = (df[col] - df[col].mean()) / df[col].std()\n\n    return df"}
{"task_id": "BigCodeBench/224", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    x = np.arange(range_start, range_end, step)\n    sin_x = np.sin(x)\n    cos_x = np.cos(x)\n    abs_diff = np.abs(sin_x - cos_x)\n    mean_fft = np.mean(np.abs(fft(abs_diff)))\n    median_fft = np.median(np.abs(fft(abs_diff)))\n\n    def generate_tuples():\n        for i in range(len(x)):\n            yield (x[i], sin_x[i], cos_x[i], abs_diff[i])\n\n    ax = plt.figure().gca()\n    plt.plot(x, sin_x, label='sin(x)')\n    plt.plot(x, cos_x, label='cos(x)')\n    plt.plot(x, abs_diff, label='|sin(x) - cos(x)|')\n    plt.legend()\n    plt.show()\n\n    return generate_tuples(), ax, mean_fft, median_fft"}
{"task_id": "BigCodeBench/225", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    if columns:\n        for col in columns:\n            if col in df.columns:\n                df[col] = df[col].map(dct)\n            else:\n                print(f\"Column {col} not found in DataFrame\")\n    else:\n        df = df.replace(dct)\n\n    if plot_histograms:\n        for col in df.columns:\n            df[col].hist(bins=50)\n            plt.title(f\"Histogram of {col}\")\n            plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/226", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(range_start=0, range_end=10, step=0.1):\n    def exp_generator(range_start, range_end, step):\n        x = range_start\n        while x <= range_end:\n            yield (x, math.exp(x))\n            x += step\n\n    # Generate the sequence\n    exp_seq = list(exp_generator(range_start, range_end, step))\n\n    # Extract the x and e^x values\n    x_values = [item[0] for item in exp_seq]\n    e_values = [item[1] for item in exp_seq]\n\n    # Plot the exponential function\n    plt.plot(x_values, e_values)\n    plt.xlabel('x')\n    plt.ylabel('e^x')\n    plt.title('Exponential function')\n    plt.grid(True)\n    plt.show()\n\n    # Return the generator object\n    return exp_seq, plt"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    try:\n        # Read audio data\n        data, sample_rate = sf.read(audio_file)\n    except FileNotFoundError:\n        print(\"The specified audio file does not exist.\")\n        return\n\n    # Calculate SPL\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Normalize data\n    data = data / np.max(np.abs(data))\n\n    # Create matrix\n    matrix = np.reshape(data, (M, N))\n\n    # Generate spectrogram\n    fig, ax = plt.subplots()\n    Sxx, freqs, bins, im = ax.specgram(matrix, Fs=sample_rate)\n\n    # Adjust SPL to spectrogram\n    im.set_interpolation('bilinear')\n    im.set_clim(0, SPL)\n\n    ax.set_title(f'Spectrogram with SPL = {SPL:.2f}')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency')\n\n    plt.show()\n\n    return matrix, fig"}
{"task_id": "BigCodeBench/228", "solution": "import pandas as pd\nimport numpy as np\nimport pandas.api.types as pd_types\ndef task_func(df, dct):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Check if dct is a dictionary\n    if not isinstance(dct, dict):\n        raise ValueError(\"dct is not a dictionary.\")\n\n    # Check if dct keys match columns in df\n    if set(dct.keys()) != set(df.columns):\n        raise ValueError(\"dct keys do not match columns in df.\")\n\n    # Replace values in df with values from dct\n    df = df.replace(dct)\n\n    # Calculate Pearson correlation coefficients\n    corr_matrix = df.corr(method='pearson')\n\n    return corr_matrix"}
{"task_id": "BigCodeBench/229", "solution": "import json\nimport random\nfrom datetime import datetime, timedelta\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\ndef task_func(file_path, num_entries, seed=None):\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Generate a list of dictionaries\n    data = []\n    for i in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(['login', 'logout', 'download', 'upload'])\n        timestamp = datetime.now() - timedelta(seconds=random.randint(0, 60*60*24))\n        data.append({'user': user, 'action': action, 'timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S')})\n\n    # Write the data to a JSON file\n    with open(file_path, 'w') as f:\n        json.dump(data, f)\n\n    return file_path"}
{"task_id": "BigCodeBench/230", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\ndef task_func(df):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        return \"Invalid input\"\n\n    # Check if DataFrame has the required columns\n    for col in COLUMNS:\n        if col not in df.columns:\n            return \"Invalid input\"\n\n    # Create histogram\n    fig, ax1 = plt.subplots()\n    df.hist(column='Score', ax=ax1, rwidth=0.8)\n    ax1.set_title('Histogram of Scores')\n\n    # Create boxplot\n    df.boxplot(column='Score', by='Country', ax=ax1)\n\n    # Create a new figure for boxplot\n    fig2, ax2 = plt.subplots()\n    sns.boxplot(x='Country', y='Score', data=df, ax=ax2)\n    ax2.set_title('Boxplot of Scores by Country')\n\n    # Return the figure\n    return fig"}
{"task_id": "BigCodeBench/231", "solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\ndef task_func(obj_list) -> Axes:\n    # Calculate mean and standard deviation\n    mu = sum(obj.value for obj in obj_list) / len(obj_list) if obj_list else 0\n    std = (sum((obj.value - mu) ** 2 for obj in obj_list) / len(obj_list)) ** 0.5 if obj_list else 0\n\n    # Plot histogram\n    plt.hist([obj.value for obj in obj_list], bins='auto', alpha=0.7, rwidth=0.85)\n\n    # Plot custom normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Return the Axes\n    ax = plt.gca()\n    return ax"}
{"task_id": "BigCodeBench/232", "solution": "import pandas as pd\nimport collections\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Exclude duplicate customer names\n    df.drop_duplicates(subset='Customer Name', inplace=True)\n\n    # Calculate total sales\n    total_sales = df['Sales'].sum()\n\n    # Calculate most popular sales category\n    most_popular_category = df.groupby('Sales Category')['Sales Category'].apply(lambda x: x.value_counts().idxmax()).values[0]\n\n    return {'Total Sales': total_sales, 'Most Popular Category': most_popular_category}\ndf = pd.DataFrame({\n    'Customer Name': ['John', 'Jane', 'John', 'Jane', 'Jane', 'John'],\n    'Sales': [100, 200, 150, 300, 250, 150],\n    'Sales Category': ['Electronics', 'Electronics', 'Furniture', 'Furniture', 'Electronics', 'Furniture']\n})"}
{"task_id": "BigCodeBench/233", "solution": "import random\nimport matplotlib.pyplot as plt\nclass Object:\n    value = 0\n    def __init__(self, value=None):\n        if value is None:\n            self.value = random.gauss(0, 1)\n        else:\n            self.value = value\ndef task_func(obj_list, attr, num_bins=30, seed=0):\n    random.seed(seed)\n    values = [getattr(obj, attr) for obj in obj_list]\n    plt.hist(values, num_bins, edgecolor='black')\n    plt.title('Histogram of attribute values')\n    plt.xlabel('Attribute Value')\n    plt.ylabel('Count')\n    return plt.gca()\nnum_objects = 1000\nobj_list = [Object() for _ in range(num_objects)]"}
{"task_id": "BigCodeBench/234", "solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    \n    # Remove duplicates\n    df = df.drop_duplicates()\n    \n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['age'], df['score'])\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df['age'], df['score'])\n    \n    # Add regression line\n    x = np.linspace(df['age'].min(), df['age'].max(), 100)\n    y = slope * x + intercept\n    ax.plot(x, y, 'r')\n    \n    ax.set_title('Linear Regression')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Score')\n    \n    return fig, ax"}
{"task_id": "BigCodeBench/235", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate a normal distribution with given mean and standard deviation\n    data = np.random.normal(mu, sigma, num_samples)\n\n    # Create a histogram\n    plt.hist(data, num_bins, density=True, color='b', alpha=0.5)\n\n    # Calculate the x values for the PDF\n    x = np.linspace(data.min(), data.max(), num_bins)\n\n    # Calculate the PDF\n    y = (1/(sigma * np.sqrt(2 * np.pi))) * np.exp(-(x - mu)**2 / (2 * sigma**2))\n\n    # Plot the PDF\n    plt.plot(x, y, color='r', label='PDF')\n\n    # Overlay a second order polynomial function on the histogram fitted bin-wise using ordinary least squares (OLS) regression"}
{"task_id": "BigCodeBench/236", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\ndef task_func(df):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Drop duplicate 'Name' entries\n    df.drop_duplicates(subset='Name', keep='first', inplace=True)\n\n    # Split the data into 'Age' and 'Score'\n    X = df[['Age', 'Score']]\n    y = df['Category']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a Random Forest Classifier\n    clf = RandomForestClassifier(random_state=42)\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = clf.predict(X_test)\n\n    # Calculate the accuracy of the predictions\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy"}
{"task_id": "BigCodeBench/237", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, save_plot=False, plot_path=None):\n    if save_plot and plot_path is None:\n        raise ValueError(\"If save_plot is True, plot_path must be provided.\")\n\n    coordinates_2d = data[:, :3]\n\n    pca = PCA(n_components=2)\n    pca.fit(coordinates_2d)\n    coordinates_2d = pca.transform(coordinates_2d)\n\n    if save_plot:\n        fig, ax = plt.subplots()\n        ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n        ax.set_xlabel('First Principal Component')\n        ax.set_ylabel('Second Principal Component')\n        ax.set_title('2D PCA plot')\n\n        if plot_path is not None:\n            plt.savefig(plot_path)\n\n        return coordinates_2d, ax\n    else:\n        return coordinates_2d"}
{"task_id": "BigCodeBench/238", "solution": "import matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Standardize 'Age' and 'Score' columns\n    scaler = StandardScaler()\n    df[['Age', 'Score']] = scaler.fit_transform(df[['Age', 'Score']])\n\n    # Remove duplicate entries based on 'Name'\n    df = df.drop_duplicates(subset='Name', keep='first')\n\n    # Plot a scatter plot of standardized 'Age' and 'Score'\n    fig, ax = plt.subplots()\n    ax.scatter(df['Age (standardized)'], df['Score (standardized)'])\n    ax.set_title('Scatter Plot of Standardized Age and Score')\n    ax.set_xlabel('Age (standardized)')\n    ax.set_ylabel('Score (standardized)')\n    plt.show()\n\n    return df, ax"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [i[0] for i in original]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    mean = np.mean(numeric_values)\n    std_dev = np.std(numeric_values)\n    min_val = np.min(numeric_values)\n    max_val = np.max(numeric_values)\n\n    stats_dict = {\n        \"mean\": mean,\n        \"standard deviation\": std_dev,\n        \"minimum\": min_val,\n        \"maximum\": max_val\n    }\n\n    # Generate a histogram with an overlaid PDF\n    plt.figure(figsize=(10,6))\n    plt.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Compute PDF\n    x = np.linspace(min_val, max_val, 100)\n    y = stats.norm.pdf(x, mean, std_dev)\n\n    # Overlay PDF\n    plt.plot(x, y, label=f\"Mean: {mean:.2f}, Std Dev: {std_dev:.2f}\")\n    plt.legend()\n    plt.show()\n\n    return numeric_values, stats_dict\noriginal = [(10,), (20,), (30,), (40,), (50,)]"}
{"task_id": "BigCodeBench/240", "solution": "import pandas as pd\nfrom random import uniform\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    # Generate a random dataset of floating-point numbers\n    data = {column_name: [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]}\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original')\n    ax.plot(normalized_array, label='Normalized')\n\n    # Set the labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Original and Normalized Arrays')\n\n    # Add a legend\n    ax.legend()\n\n    # Show the plot\n    plt.show()\n\n    # Return the numpy arrays\n    return original_array, normalized_array, ax\noriginal = [1, 2, 3, 4, 5]"}
{"task_id": "BigCodeBench/242", "solution": "import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(image_path, kernel_size):\n    if not isinstance(image_path, str):\n        raise ValueError(\"image_path should be a string\")\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"kernel_size should be a positive integer\")\n    try:\n        img = cv2.imread(image_path, 1)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The specified image file does not exist\")\n\n    blurred_img = cv2.blur(img, (kernel_size, kernel_size))\n\n    fig, axs = plt.subplots(1, 2)\n    axs[0].imshow(img)\n    axs[0].set_title('Original Image')\n    axs[1].imshow(blurred_img)\n    axs[1].set_title('Blurred Image')\n    plt.show()\n\n    return blurred_img, axs"}
{"task_id": "BigCodeBench/243", "solution": "import pandas as pd\nimport random\nN_DATA_POINTS = 10000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\ndef task_func(n_data_points=N_DATA_POINTS):\n    if n_data_points == 0:\n        return pd.DataFrame(columns=['Value'])\n    else:\n        data = [round(random.uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(n_data_points)]\n        df = pd.DataFrame(data, columns=['Value'])\n        return df"}
{"task_id": "BigCodeBench/244", "solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\ndef task_func(original):\n    if not original:\n        return np.array([]), np.array([]), None\n\n    # Create a numpy array from the \"original\" list\n    original_array = np.array(original)\n\n    # Calculate Fast Fourier Transform (FFT)\n    fft_data = fft(original_array)\n\n    # Record the original and FFT data\n    original_magnitude = np.abs(fft_data)\n    fft_magnitude = np.abs(fft_data)\n\n    # Plot the histogram of the magnitude of the FFT data\n    fig, ax = plt.subplots()\n    ax.hist(fft_magnitude, bins=256, color='gray')\n    ax.set_title('FFT Magnitude Histogram')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return original_array, fft_data, ax"}
{"task_id": "BigCodeBench/245", "solution": "import pandas as pd\nimport random\nfrom scipy import stats\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate a random dataset of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Convert the list to a pandas Series\n    series = pd.Series(data)\n\n    # Calculate statistical measures (mean, median, mode)\n    mean = round(series.mean(), 3)\n    median = round(series.median(), 3)\n    mode = stats.mode(series)[0][0]\n\n    # Return the dictionary with keys 'mean', 'median', 'mode' and their corresponding calculated values\n    return {'mean': mean, 'median': median, 'mode': mode}"}
{"task_id": "BigCodeBench/246", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\nANGLES = np.arange(0, 2*np.pi, 0.01)\ndef task_func(n_waves, seed=0):\n    if n_waves < 1:\n        return [], np.array([]), None\n\n    np.random.seed(seed)\n\n    # Generate sine waves\n    waves = [np.sin(ANGLES) + 0.5*np.random.normal(size=len(ANGLES)) for _ in range(n_waves)]\n\n    # Mix the waves\n    mixed_signal = sum(waves)\n\n    # Calculate FFT\n    fft_data = fft(mixed_signal)\n\n    # Plot the histogram of the magnitude of the FFT data\n    plt.figure(figsize=(10, 6))\n    plt.hist(np.abs(fft_data), bins=256, color='gray', edgecolor='black')\n    plt.title('Histogram of the magnitude of the FFT data')\n    plt.xlabel('Magnitude')\n    plt.ylabel('Frequency')\n\n    plt.grid(True)\n    ax = plt.gca()\n\n    return waves, np.abs(fft_data), ax"}
{"task_id": "BigCodeBench/247", "solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\nN_DATA_POINTS = 5000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\ndef task_func(n_data_points=N_DATA_POINTS, min_value=MIN_VALUE, max_value=MAX_VALUE):\n    if max_value < min_value:\n        raise ValueError(\"max_value should be greater than min_value\")\n\n    # Generate a random dataset of floating point numbers\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Normalized Value'])\n\n    # Normalize the data using standard scaling\n    scaler = StandardScaler()\n    df['Normalized Value'] = scaler.fit_transform(df['Normalized Value'].values.reshape(-1, 1))\n\n    return df"}
{"task_id": "BigCodeBench/248", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data_list is empty.\")\n\n    unzipped_data = list(itertools.chain(*data_list))\n\n    plt.figure()\n    plt.plot(unzipped_data)\n    plt.title('The plot with the unzipped numerical values')\n    plt.xlabel('Position')\n    plt.ylabel('Numerical values')\n    plt.show()\ndata_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/249", "solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate a random set of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Value'])\n    \n    # Divide the data into train and test sets based on a given test size\n    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n    \n    return (train_df, test_df)"}
{"task_id": "BigCodeBench/250", "solution": "import numpy as np\nimport itertools\nimport json\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    # Calculate the mean of the numeric values for each position in the provided data list\n    mean_values = {f'Position {i}': np.mean(data_list[i] for i in range(len(data_list))) for i in range(len(data_list))}\n\n    # Export the results to a specified JSON file\n    with open(json_file_name, 'w') as f:\n        json.dump(mean_values, f)\n\n    return mean_values\ndata_list = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]"}
{"task_id": "BigCodeBench/251", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data is not a DataFrame\")\n    \n    # Assuming the data is a DataFrame with a column 'Job'\n    job_counts = data['Job'].value_counts()\n    \n    fig, ax = plt.subplots()\n    ax.pie(job_counts, labels=job_counts.index, autopct='%1.1f%%')\n    \n    return fig"}
{"task_id": "BigCodeBench/252", "solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle, islice\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\ndef task_func(data, labels):\n    # If there are more data series than the predefined colors, cycle through the colors\n    if len(data) > len(COLORS):\n        colors = cycle(COLORS)\n    else:\n        colors = cycle(COLORS[:len(data)])\n\n    # If there are more data series than colors + labels, use 'black'\n    if len(data) > len(COLORS) + len(labels):\n        colors = cycle(['black'] * len(data) - len(COLORS) - len(labels))\n\n    ax = plt.figure().add_subplot(111)\n\n    for d, l in zip_longest(data, labels, fillvalue=' '):\n        if d is not None:\n            ax.plot(d, color=next(colors))\n        if l is not None:\n            ax.text(len(d), next(colors), l, color='black')\n\n    plt.show()\n\n    return ax"}
{"task_id": "BigCodeBench/253", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\ndef task_func(ax):\n    # Generate a random sine wave\n    t = np.linspace(0, 2 * np.pi, 1000)\n    wave = np.sin(t)\n\n    # Select a random color\n    color = random.choice(COLORS)\n\n    # Set a random position for radial labels\n    r = np.random.rand()\n\n    # Plot the sine wave\n    ax.plot(t, wave, color)\n\n    # Output the color code as a string\n    color_code = color\n\n    return color_code"}
{"task_id": "BigCodeBench/254", "solution": "import json\nimport math\ndef task_func(decimal_value, precision=2):\n    # Calculate the square root of the decimal value\n    root = round(decimal_value ** 0.5, precision)\n\n    # Encode the result as a JSON string\n    json_str = json.dumps(root)\n\n    return json_str"}
{"task_id": "BigCodeBench/255", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nFUNCTIONS = [np.sin, np.cos, np.tan]\ndef task_func(ax, func_index):\n    if not isinstance(ax, plt.axes._axes.Axes):\n        raise ValueError(\"Input ax must be an Axes object\")\n\n    x = np.linspace(0, 2 * np.pi, 100)\n    y = FUNCTIONS[func_index](x)\n\n    ax.plot(x, y)\n\n    # Set radial ticks\n    ax.set_xticks(np.arange(0, 2 * np.pi, 45))\n\n    return ax"}
{"task_id": "BigCodeBench/256", "solution": "import json\nimport random\nimport hashlib\nimport time\nfrom datetime import datetime\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    # Check if utc_datetime is a datetime object\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    # Check if salt is a string\n    if not isinstance(salt, str):\n        raise ValueError(\"salt must be a string\")\n\n    # Generate a random lowercase alphanumeric password\n    random.seed(seed)\n    password = ''.join(random.choice('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789') for _ in range(password_length))\n\n    # Combine the password with the salt and the UTC datetime\n    combined = password + salt + str(utc_datetime.utcnow())\n\n    # Hash the combined string using SHA-256\n    hashed_password = hashlib.sha256(combined.encode()).hexdigest()\n\n    # Return the hashed password as a JSON string\n    return json.dumps({\"password\": hashed_password})\nutc_datetime = datetime.utcnow()\nsalt = 'my_salt'"}
{"task_id": "BigCodeBench/257", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(ax, num_turns):\n    # Convert the number of turns to radians\n    turn_rad = np.deg2rad(num_turns * 45)\n\n    # Generate the x and y coordinates for the spiral\n    x = np.cos(np.linspace(0, 2 * np.pi, 1000))\n    y = np.sin(np.linspace(0, 2 * np.pi, 1000))\n\n    # Add the spiral to the axes\n    ax.plot(x, y)\n\n    # Set the radial ticks\n    ax.set_xticks(np.arange(min(x), max(x), turn_rad))\n    ax.set_yticks(np.arange(min(y), max(y), turn_rad))\n\n    return ax"}
{"task_id": "BigCodeBench/258", "solution": "import json\nimport random\nimport time\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\ndef task_func(utc_datetime, seed=0):\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = time.time()\n    return json.dumps(person)"}
{"task_id": "BigCodeBench/259", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, num_points):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be an Axes object\")\n    if num_points < 0:\n        raise ValueError(\"Input num_points must be a non-negative number\")\n\n    # Generate random points\n    theta = np.random.uniform(0, 2 * np.pi, num_points)\n    r = np.random.uniform(0, 1, num_points)\n    points = np.column_stack([r * np.cos(theta), r * np.sin(theta)])\n\n    # Plot points\n    ax.scatter(points[:, 0], points[:, 1])\n\n    # Set radial ticks\n    ax.set_rticks(np.arange(0, 1.1, 0.1))\n    ax.set_rticks(np.arange(0, 1.1, 0.1), color='black')\n\n    return ax"}
{"task_id": "BigCodeBench/260", "solution": "import json\nimport os\nimport glob\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\ndef task_func(directory):\n    # Get all JSON files in the directory\n    json_files = glob.glob(f'{directory}/*.json')\n\n    # Initialize count\n    count = 0\n\n    # Iterate over each JSON file\n    for json_file in json_files:\n        with open(json_file, 'r') as f:\n            data = json.load(f)\n\n        # Check if key exists\n        if KEY not in data:\n            data[KEY] = VALUE\n            count += 1\n\n        # Write updated data back to file\n        with open(json_file, 'w') as f:\n            json.dump(data, f)\n\n    # Return count\n    return count"}
{"task_id": "BigCodeBench/261", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.PolarAxes):\n        raise TypeError(\"The 'ax' parameter should be a PolarAxes object.\")\n    if radius < 0:\n        raise ValueError(\"The radius should be a non-negative number.\")\n    \n    theta = np.linspace(0, 2*np.pi, 100)\n    r = radius * np.ones_like(theta)\n    \n    ax.plot(theta, r)\n    ax.set_rticks([0.5, 1, 1.5, 2])\n    ax.grid(True)\n    \n    return ax"}
{"task_id": "BigCodeBench/262", "solution": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(dictionary, new_key, new_value):\n    # Add new key-value pair to the dictionary\n    dictionary[new_key] = new_value\n    print(\"dict:\", dictionary)\n\n    # Count the frequency of each value in the dictionary\n    value_counts = collections.Counter(dictionary.values())\n\n    # Create a bar plot of the distribution of values\n    plt.figure(figsize=(10,6))\n    sns.barplot(x=list(value_counts.keys()), y=list(value_counts.values()))\n    plt.title('Distribution of Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    ax = plt.gca()\n    print(\"matplotlib.axes.Axes:\", ax)\n    plt.show()\ndictionary = {'a': 1, 'b': 2, 'c': 2, 'd': 3, 'e': 4, 'f': 1}"}
{"task_id": "BigCodeBench/263", "solution": "import os\nimport glob\nimport shutil\nimport time\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\ndef task_func(my_path: str, days_old: int) -> str:\n    # Get the current time\n    current_time = time.time()\n\n    # Get the list of all files with the specified extensions in the given directory\n    for extension in FILE_EXTENSIONS:\n        files = glob.glob(f\"{my_path}/**/*{extension}\", recursive=True)\n\n        # Iterate over all files\n        for file in files:\n            # Get the file's modification time\n            file_time = os.path.getmtime(file)\n\n            # If the file is older than 'days_old'\n            if current_time - file_time > days_old * 24 * 60 * 60:\n                # Get the file's directory\n                file_dir = os.path.dirname(file)\n\n                # Get the archive subdirectory path\n                archive_dir = os.path.join(file_dir, 'archive')\n\n                # If the archive subdirectory does not exist, create it\n                if not os.path.exists(archive_dir):\n                    os.makedirs(archive_dir)\n\n                # Move the file to the archive subdirectory\n                shutil.move(file, archive_dir)\n\n    # Return the path of the archive subdirectory\n    return archive_dir"}
{"task_id": "BigCodeBench/264", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number\")\n\n    np.random.seed(seed)\n    data = np.random.normal(loc=value, scale=value, size=n)\n    dictionary[key] = data\n\n    plt.hist(data, bins=bins, edgecolor='black')\n    plt.title('Histogram of generated dataset')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return dictionary, pd.Series(data), plt.gca()"}
{"task_id": "BigCodeBench/265", "solution": "import collections\nimport json\nimport os\ndef task_func(data, json_file_name='data.json'):\n    # Add a new key \"a\" with the value 1 to the input dictionary\n    data['a'] = 1\n\n    # Calculate the frequency of its values\n    freq = collections.Counter(data.values())\n\n    # Save the updated dictionary and its frequency distribution to a JSON file\n    with open(json_file_name, 'w') as f:\n        json.dump(data, f)\n\n    return os.path.abspath(json_file_name)\ndata = {'apple': 'fruit', 'banana': 'fruit', 'grape': 'fruit', 'mango': 'fruit', 'orange': 'fruit'}"}
{"task_id": "BigCodeBench/266", "solution": "import os\nimport os.path\nimport csv\nimport collections\nFILE_NAME = 'file_sizes.csv'\ndef task_func(my_path):\n    # Create a dictionary to store file sizes\n    file_sizes = collections.defaultdict(int)\n\n    # Iterate over all files in the directory\n    for foldername, subfolders, filenames in os.walk(my_path):\n        for filename in filenames:\n            # Get the full file path\n            file_path = os.path.join(foldername, filename)\n            # Get the file size\n            file_size = os.path.getsize(file_path)\n            # Add the file size to the dictionary\n            file_sizes[file_path] += file_size\n\n    # Write the file sizes to a CSV file\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['File Path', 'File Size'])\n        for file_path, file_size in file_sizes.items():\n            writer.writerow([file_path, file_size])\n\n    # Return the path of the CSV file\n    return FILE_NAME"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # Generate a signal based on the values in \"data\"\n    signal = [value for key, value in data.items()]\n\n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft = fftpack.fft(signal)\n\n    # Plot and return the FFT of the signal\n    plt.figure(figsize=(10, 6))\n    plt.plot(np.abs(fft))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    ax = plt.gca()\n    ax.set_xticks(np.arange(0, len(signal), 100), min(data), max(data), 10)\n    ax.set_xticklabels(np.arange(0, len(signal), 100), rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return fft, plt"}
{"task_id": "BigCodeBench/268", "solution": "import collections\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\ndef task_func(n_keys, n_values):\n    # Generate keys\n    keys = random.sample(LETTERS, n_keys)\n\n    # Generate values\n    values = list(range(1, n_values + 1))\n\n    # Create dictionary\n    dictionary = dict(zip(keys, values))\n\n    return dictionary"}
{"task_id": "BigCodeBench/269", "solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    # Add key \"a\" with value of 1\n    data_dict['a'] = 1\n\n    # Conduct statistical analysis on its values\n    values = list(data_dict.values())\n    mean = np.mean(values)\n    median = np.median(values)\n    mode = stats.mode(values)\n\n    # Round the mean to 2 decimal places\n    mean = round(mean, 2)\n\n    # Normalize the values using MinMaxScaler to a range of (0, 1)\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(np.array(values).reshape(-1, 1))\n\n    # Plot a histogram of the normalized values\n    plt.hist(normalized_values, bins='auto', alpha=0.7, rwidth=0.85, color='#0504aa', density=True)\n    plt.title(\"Histogram of Normalized Values\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis='y', alpha=0.75)\n    plt.show()\n\n    return data_dict, {'mean': mean, 'median': median, 'mode': mode[0][0]}\ndata_dict = {'x': 1, 'y': 2, 'z': 3}"}
{"task_id": "BigCodeBench/270", "solution": "import re\nfrom collections import Counter\ndef task_func(sentence):\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    word_counts = Counter(words)\n    return dict(word_counts)"}
{"task_id": "BigCodeBench/271", "solution": "import random\nimport string\nimport hashlib\nimport time\ndef task_func(data_dict: dict, seed=0) -> dict:\n    # Generate a random salt\n    random.seed(seed)\n    salt = ''.join(random.choice(string.ascii_lowercase) for _ in range(5))\n\n    # Process the dictionary\n    for key, value in data_dict.items():\n        # Concatenate the value with the salt\n        concat_str = value + salt\n        # Hash the concatenated string using SHA-256\n        hashed_str = hashlib.sha256(concat_str.encode()).hexdigest()\n        # Update the value with the hashed string\n        data_dict[key] = hashed_str\n\n    # Add a 'timestamp' key with the current UNIX timestamp as its value\n    data_dict['timestamp'] = int(time.time())\n\n    return data_dict\ndata_dict = {'apple': 'fruit', 'carrot': 'vegetable'}"}
{"task_id": "BigCodeBench/272", "solution": "import cgi\nimport http.server\nimport json"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json"}
{"task_id": "BigCodeBench/275", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(n):\n    if n < 1:\n        raise ValueError(\"Input n should be greater than 0\")\n    else:\n        pairs = list(combinations(range(1, n+1), 2))\n        return [tuple(pair) for pair in pairs]"}
{"task_id": "BigCodeBench/276", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    # Calculate the distribution of the maximum values of each row in the matrix\n    max_values = np.max(matrix, axis=1)\n    skewness, kurtosis = stats.skewtest(max_values)\n\n    # Record the histogram and the estimate of the core density of the distribution\n    plt.hist(max_values, bins='auto', alpha=0.7, rwidth=0.85)\n    plt.title('Histogram of Maximum Values')\n    plt.xlabel('Maximum Value')\n    plt.ylabel('Frequency')\n\n    # Return the skew, kurtosis, and the histogram plot\n    return skewness, kurtosis, plt.gca()\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": "BigCodeBench/277", "solution": "import random\nfrom itertools import combinations\nimport math\ndef task_func(n):\n    if n < 2:\n        return None\n\n    # Generate n random points within the unit square\n    points = [(random.uniform(0, 1), random.uniform(0, 1)) for _ in range(n)]\n\n    # Find the pair of closest points\n    closest_pair = None\n    min_distance = float('inf')\n    for point1, point2 in combinations(points, 2):\n        distance = math.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n        if distance < min_distance:\n            min_distance = distance\n            closest_pair = (point1, point2)\n\n    return closest_pair"}
{"task_id": "BigCodeBench/278", "solution": "import numpy as np\nfrom sympy import symbols, solve\ndef task_func(precision=2, seed=0):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random coefficients\n    a = np.random.uniform(-10, 10)\n    b = np.random.uniform(-10, 10)\n    c = np.random.uniform(-10, 10)\n\n    # Define the symbols\n    x = symbols('x')\n\n    # Define the equation\n    equation = a*x**2 + b*x + c\n\n    # Solve the equation\n    solutions = solve(equation, x)\n\n    # Extract the real and imaginary parts\n    real_solutions = [sol.evalf(precision) for sol in solutions if sol.is_real]\n    imaginary_solutions = [sol.evalf(precision) for sol in solutions if sol.is_imaginary]\n\n    # Return the solutions as a tuple\n    return tuple(real_solutions + imaginary_solutions)"}
{"task_id": "BigCodeBench/279", "solution": "import random\nfrom collections import Counter\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\ndef task_func(x=1):\n    hands = []\n    drawn_cards = []\n    for _ in range(x):\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n        drawn_cards.append(Counter(hand))\n    return hands, Counter(drawn_cards)"}
{"task_id": "BigCodeBench/280", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    n = len(signal)\n\n    # Calculate the DFT\n    dft = fft(signal)\n\n    # Round the DFT values to the specified precision\n    dft_rounded = np.round(np.abs(dft), precision)\n\n    # Create the original signal plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(np.arange(n), signal, label='Original Signal')\n    plt.plot(np.arange(n), dft_rounded, label='Transformed Signal')\n    plt.legend()\n    plt.title('Original Signal vs Transformed Signal')\n    plt.xlabel('Index')\n    plt.ylabel('Amplitude')\n    original_ax = plt.gca()\n\n    # Create the transformed signal plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(np.arange(n), np.abs(dft), label='Transformed Signal')\n    plt.legend()\n    plt.title('Transformed Signal')\n    plt.xlabel('Index')\n    plt.ylabel('Amplitude')\n    transformed_ax = plt.gca()\n\n    return dft_rounded, (original_ax, transformed_ax)\nsignal = np.random.rand(100)"}
{"task_id": "BigCodeBench/281", "solution": "import re\nimport os\nfrom collections import Counter\ndef task_func(folder_path: str) -> dict:\n    ip_addresses = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(folder_path):\n        # Check if the file is a log file\n        if filename.endswith(\".log\"):\n            # Open the file and read its content\n            with open(os.path.join(folder_path, filename), 'r') as file:\n                content = file.read()\n\n                # Use a regular expression to find all IP addresses in the file\n                ip_addresses.extend(re.findall(r'\\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b', content))\n\n    # Count the occurrences of each IP address\n    ip_counts = Counter(ip_addresses)\n\n    return dict(ip_counts)"}
{"task_id": "BigCodeBench/282", "solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\ndef task_func(file_path, onpick):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The image file {file_path} does not exist.\")\n\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist = hist.reshape(-1, hist.shape[2])\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    x = hist[:, 0]\n    y = hist[:, 1]\n    z = hist[:, 2]\n\n    ax.scatter(x, y, z)\n\n    ax.set_xlabel('R')\n    ax.set_ylabel('G')\n    ax.set_zlabel('B')\n\n    plt.show()\n\n    return ax"}
{"task_id": "BigCodeBench/283", "solution": "import os\nimport json\nfrom collections import Counter\ndef task_func(json_files_path='./json_files/', key='name'):\n    # Initialize an empty dictionary to store the counts\n    count_dict = {}\n\n    # Iterate over all json files in the specified directory\n    for filename in os.listdir(json_files_path):\n        if filename.endswith('.json'):\n            # Open and load the json file\n            with open(os.path.join(json_files_path, filename), 'r') as f:\n                data = json.load(f)\n\n            # Extract the value of the specified key from each json object\n            values = [obj.get(key) for obj in data]\n\n            # Count the occurrences of each value\n            count_dict = Counter(values) + count_dict\n\n    return count_dict"}
{"task_id": "BigCodeBench/284", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\ndef task_func(url):\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n\n    soup = BeautifulSoup(browser.response().read(), 'html.parser')\n\n    links = [urljoin(url, a.get('href')) for a in soup.find_all('a', href=True)]\n\n    return links"}
{"task_id": "BigCodeBench/285", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\ndef task_func(url, form_id, data):\n    # Create a browser object\n    browser = mechanize.Browser()\n\n    # Submit the form\n    browser.open(url)\n    browser.select_form(id=form_id)\n    browser.submit()\n\n    # Get the response page\n    response_page = browser.response().geturl()\n\n    # If the page has no title, return 'No Title'\n    if 'No Title' in response_page:\n        return 'No Title'\n\n    # Parse the page with BeautifulSoup\n    soup = BeautifulSoup(browser.response().read(), 'html.parser')\n\n    # Extract the title\n    title = soup.title.string\n\n    # Return the title\n    return title"}
{"task_id": "BigCodeBench/286", "solution": "from collections import Counter\nimport os\nimport csv\ndef task_func(output_file, test_directory):\n    # Check if the directory exists\n    if not os.path.exists(test_directory):\n        print(\"Error: The directory does not exist.\")\n        return 0\n\n    # Check if the output file exists\n    if os.path.exists(output_file):\n        os.remove(output_file)\n\n    # Initialize counter\n    total_words = 0\n\n    # Iterate over all .txt files in the directory\n    for filename in os.listdir(test_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(test_directory, filename), 'r') as f:\n                words = f.read().split()\n                total_words += len(words)\n\n    # Write the counts to a CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([\"Word\", \"Count\"])\n        writer.writerow([\"Total\", total_words])\n\n    return total_words"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(directory):\n    total_words = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                total_words += len(words)\n    return total_words"}
{"task_id": "BigCodeBench/288", "solution": "import collections\nimport json\nimport os\ndef task_func(directory_path: str) -> dict:\n    result = collections.defaultdict(int)\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory_path, filename), 'r') as f:\n                data = json.load(f)\n                for key in data.keys():\n                    result[key] += 1\n    return dict(result)"}
{"task_id": "BigCodeBench/289", "solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(X, y, n_splits, batch_size, epochs):\n    # Scale the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Initialize the k-fold cross-validation\n    kf = KFold(n_splits=n_splits)\n\n    # Initialize the training history list\n    train_history = []\n\n    # Iterate over the k-fold cross-validation\n    for train_index, test_index in kf.split(X_scaled):\n        # Split the data\n        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        # Reshape the labels to binary format\n        y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n        y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)\n\n        # Define the model\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(20, activation='relu', input_shape=(X_train.shape[1],)),\n            tf.keras.layers.Dense(2, activation='softmax')\n        ])\n\n        # Compile the model\n        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n        # Train the model\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n\n        # Append the training history to the list\n        train_history.append(history.history)\n\n    return train_history"}
{"task_id": "BigCodeBench/290", "solution": "import nltk\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(directory_path):\n    total_count = 0\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                text = file.read().lower()\n                words = nltk.word_tokenize(text)\n                filtered_words = [word for word in words if word.isalpha()]\n                non_stop_words = [word for word in filtered_words if word not in STOPWORDS]\n                counter = Counter(non_stop_words)\n                total_count += sum(counter.values())\n    return total_count"}
{"task_id": "BigCodeBench/291", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\ndef task_func(mu, sigma, seed=0):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, 1000)\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.distplot(samples, kde=False, color='blue', bins=30)\n\n    plt.title('Normal Distribution')\n    plt.xlabel('Samples')\n    plt.ylabel('Frequency')\n\n    plt.show()\n\n    return ax\nmu = 0\nsigma = 1"}
{"task_id": "BigCodeBench/292", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Scale the 'Age' and 'Income' columns between 0 and 1 for each group by 'id'\n    scaler = MinMaxScaler()\n    df[['Age', 'Income']] = scaler.fit_transform(df[['Age', 'Income']])\n\n    # Create a histogram of the 'Income' column\n    plt.hist(df['Income'], bins=10, edgecolor='black')\n    plt.title('Histogram of Income')\n    plt.xlabel('Income')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/293", "solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(elements, subset_size):\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n\n    fig, ax = plt.subplots()\n    ax.hist(sums, bins=range(min(sums), max(sums)+1))\n    ax.set_title('Histogram of subsets sums')\n    ax.set_xlabel('Sum')\n    ax.set_ylabel('Frequency')\n\n    return ax, subsets, sums\nelements = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nsubset_size = 3"}
{"task_id": "BigCodeBench/294", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Check if the DataFrame has the required columns\n    if not all(col in df.columns for col in ['id', 'age', 'income']):\n        raise ValueError(\"The DataFrame does not have the 'id', 'age', and 'income' columns.\")\n\n    # Standardize the 'age' and 'income' columns for each group by 'id'\n    df_std = df.groupby('id').apply(lambda x: StandardScaler().fit_transform(x[['age', 'income']]))\n\n    # Convert the numpy array back to a DataFrame\n    df_std = pd.DataFrame(df_std, columns=['age', 'income'])\n\n    return df_std"}
{"task_id": "BigCodeBench/295", "solution": "import itertools\nimport statistics\ndef task_func(elements, subset_size):\n    # Generate all subsets\n    subsets = list(itertools.combinations(elements, subset_size))\n\n    # Initialize lists to store sums\n    sums = []\n\n    # Calculate sums of each subset and append to sums list\n    for subset in subsets:\n        sums.append(sum(subset))\n\n    # Calculate mean, median, and mode\n    mean_sum = statistics.mean(sums)\n    median_sum = statistics.median(sums)\n    mode_sum = statistics.mode(sums)\n\n    # Return results as a dictionary\n    return {\"mean\": mean_sum, \"median\": median_sum, \"mode\": mode_sum}"}
{"task_id": "BigCodeBench/296", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    if df.empty:\n        return plt.gca()\n    df['value'].value_counts().plot(kind='bar')\n    plt.title('Value Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Count')\n    return plt.gca()"}
{"task_id": "BigCodeBench/297", "solution": "import itertools\nimport collections\ndef task_func(elements, subset_size):\n    # Generate all 2-element subsets of a tuple\n    subsets = list(itertools.combinations(elements, subset_size))\n\n    # Count the occurrences of each sum in the subsets\n    sum_counts = collections.Counter(sum(subset) for subset in subsets)\n\n    # Return the dictionary with the sums and their counts\n    return dict(sum_counts)\nelements = (1, 2, 3, 4, 5)\nsubset_size = 2"}
{"task_id": "BigCodeBench/298", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport datetime\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    # Check if 'Date' and 'Value' columns exist\n    if not all(col in df.columns for col in ['Date', 'Value']):\n        raise KeyError(\"DataFrame does not have the 'Date' and 'Value' columns.\")\n\n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Split 'Value' column into several columns\n    df = pd.concat([df, pd.DataFrame(df.pop('Value').tolist(), columns=[f'Value_{i}' for i in range(len(df))])], axis=1)\n\n    # Scale the columns using StandardScaler\n    scaler = StandardScaler()\n    df[COLUMNS[1:]] = scaler.fit_transform(df[COLUMNS[1:]])\n\n    # Set the 'Date' column as index\n    df.set_index('Date', inplace=True)\n\n    # If 'plot' is True, create a bar chart\n    if plot:\n        df.plot(kind='bar')\n        plt.xlabel('Date')\n        plt.ylabel('Scaled Value')\n        plt.title('Scaled Values Over Time')\n        plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/299", "solution": "import itertools\nimport math\nfrom pandas import Series\ndef task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements) or subset_size == 0:\n        return 1\n\n    subsets = list(itertools.combinations(elements, subset_size))\n    sum_list = [sum(subset) for subset in subsets]\n    product = 1\n    for num in sum_list:\n        product *= num\n\n    sum_list.sort(reverse=True)\n    if len(sum_list) > top_n:\n        sum_list = sum_list[:top_n]\n\n    return product, Series(sum_list)\nelements = [1, 2, 3, 4, 5]\nsubset_size = 2"}
{"task_id": "BigCodeBench/300", "solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if 'Date' and 'Value' columns exist\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise KeyError(\"The DataFrame does not have the 'Date' and 'Value' columns.\")\n\n    # Convert 'Date' to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Split 'Value' lists into separate columns\n    df = df.join(df.pop('Value').apply(pd.Series).add_prefix('Value_'))\n\n    # Calculate Z-scores\n    df['Z-Score'] = df['Value_1'].apply(zscore)\n\n    # Create a box plot for Z-scores over time\n    plt.figure(figsize=(10,6))\n    plt.boxplot(df['Z-Score'].values)\n    plt.xlabel('Date')\n    plt.ylabel('Z-Score')\n    plt.title('Z-Scores Over Time')\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/301", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string\n    date = parse(date_str)\n\n    # Convert the date to the timezone of the from_tz\n    from_tz = pytz.timezone(from_tz)\n    date = from_tz.localize(date)\n\n    # Calculate the year of the date\n    year = date.year\n\n    # Find the closest solar cycle year\n    solar_cycle_year = SOLAR_CYCLE_YEARS[np.argmin(np.abs(SOLAR_CYCLE_YEARS - year))]\n\n    # Calculate the difference in years\n    years_diff = year - solar_cycle_year\n\n    # Calculate the solar activity using a cosine function\n    solar_activity = 0.5 * (1 + math.cos(math.pi * years_diff / 11))\n\n    return solar_activity"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ndef task_func(df, plot=False):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if 'Value' column exists\n    if 'Value' not in df.columns:\n        raise ValueError(\"DataFrame does not contain 'Value' column\")\n\n    # Check if 'Value' column is a list\n    if not isinstance(df['Value'][0], list):\n        raise ValueError(\"Invalid 'Value' column. It should be a list of lists\")\n\n    # Split lists in 'Value' column into separate columns\n    df = pd.concat([df.drop(['Value'], axis=1), df['Value'].apply(pd.Series)], axis=1)\n\n    # Calculate Pearson correlation coefficient\n    corr_matrix = df.corr()\n\n    # If plot is True, create a heatmap\n    if plot:\n        fig, ax = plt.subplots(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)\n        ax.set_title('Correlation Heatmap')\n        plt.show()\n\n    return corr_matrix"}
{"task_id": "BigCodeBench/303", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string\n    date = parse(date_str)\n\n    # Convert the date to the specified timezone\n    date = date.astimezone(pytz.timezone(from_tz))\n\n    # Calculate the number of years since the reference year\n    years_since_reference = date.year - MOON_PHASES_YEARS[0]\n\n    # Calculate the index of the reference year\n    reference_index = np.where(MOON_PHASES_YEARS == date.year)[0][0]\n\n    # Calculate the fraction of the year that has passed since the reference year\n    fraction_since_reference = years_since_reference / (MOON_PHASES_YEARS.size - reference_index)\n\n    # Calculate the moon phase\n    moon_phase = (1 - fraction_since_reference) * 2\n\n    return moon_phase"}
{"task_id": "BigCodeBench/304", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if df.empty:\n        return 0,0\n\n    # Convert lists to separate columns\n    for col in df.columns:\n        if isinstance(df[col][0], list):\n            df[col] = pd.Series(df[col])\n\n    # Perform PCA\n    pca = PCA()\n    pca.fit(df)\n\n    # Get explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(explained_variance_ratio)), explained_variance_ratio)\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n    ax.set_title('Explained Variance Ratio of Principal Components')\n\n    return explained_variance_ratio, ax"}
{"task_id": "BigCodeBench/305", "solution": "from collections import Counter\nimport itertools\nimport random\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\ndef task_func(list_of_lists, seed=0):\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # If a list is empty, fill it with a random sample from the alphabet\n    if not list_of_lists:\n        list_of_lists = [random.sample(ALPHABET, len(ALPHABET)) for _ in range(10)]\n\n    # Flatten the list of lists and count the frequency of each letter\n    letters = [letter for sublist in list_of_lists for letter in sublist]\n    counter = Counter(letters)\n\n    return counter\nlist_of_lists = [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\nseed = 1"}
{"task_id": "BigCodeBench/306", "solution": "import os\nimport logging\ndef task_func(directory):\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The specified directory {directory} does not exist.\")\n\n    removed_files = []\n    removed_count = 0\n\n    try:\n        for filename in os.listdir(directory):\n            if 'jquery' in filename:\n                file_path = os.path.join(directory, filename)\n                os.remove(file_path)\n                removed_files.append(file_path)\n                removed_count += 1\n                logging.info(f\"Removed file: {file_path}\")\n        return removed_count, removed_files\n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n        raise"}
{"task_id": "BigCodeBench/307", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = [random.randint(0, 100) for _ in range(5)]\n        combined_data.extend(sublist)\n\n    sns.histplot(combined_data, bins=10)\n    plt.show()\n    return plt.gca()\nlist_of_lists = [[1, 2, 3], [], [4, 5, 6, 7, 8]]"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\nGRADES = {student: [random.randint(0, 100) for _ in FIELDS] for student in STUDENTS}\ndef task_func(additional_fields = []):\n    # Generate additional fields if any\n    if additional_fields:\n        FIELDS.extend(additional_fields)\n\n    # Create a DataFrame with students' grades and their average grades\n    df = pd.DataFrame(GRADES, index=STUDENTS)\n    df['Average'] = df.mean(axis=1)\n\n    # Add a column for the average grade per subject\n    df['Average Grade'] = df.mean(axis=0)\n\n    return df"}
{"task_id": "BigCodeBench/309", "solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_lists, seed=42):\n    random.seed(seed)\n    # If any inner list is empty, fill it with five random integers between 0 and 100\n    for i in range(len(list_of_lists)):\n        if not list_of_lists[i]:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(5)]\n\n    # Scale the values to a (0,1) range using MinMaxScaler\n    scaler = MinMaxScaler()\n    list_of_lists = scaler.fit_transform(list_of_lists)\n\n    return list_of_lists\nlist_of_lists = [[1, 2, 3], [], [4, 5, 6, 7, 8]]"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate simulated data\n    data = [\n        [f'Person{i}', random.randint(20, 80), round(random.uniform(150, 200), 2), round(random.uniform(40, 100), 2)]\n        for i in range(PEOPLE_COUNT)\n    ]\n\n    # Calculate average values\n    average_age = mean([d[1] for d in data])\n    average_height = mean([d[2] for d in data])\n    average_weight = mean([d[3] for d in data])\n\n    # Append average values to data\n    data.append([COLUMNS[0], average_age, average_height, average_weight])\n\n    # Write to CSV file\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return f'The path of the created CSV file is: {filename}'"}
{"task_id": "BigCodeBench/311", "solution": "import numpy as np\nimport random\nfrom scipy import stats\ndef task_func(list_of_lists, size=5, seed=0):\n    # Set the seed for reproducibility\n    random.seed(seed)\n\n    # If list_of_lists is empty, generate a list of size with random integers\n    if not list_of_lists:\n        list_of_lists = [random.randint(0, 100) for _ in range(size)]\n\n    # Flatten the list of lists and convert it to a numpy array\n    values = np.array(list_of_lists)\n\n    # Calculate the mean\n    mean = np.mean(values)\n\n    # Calculate the median\n    median = np.median(values)\n\n    # Calculate the mode\n    mode = stats.mode(values)[0][0]\n\n    # Return the statistics as a dictionary\n    return {\"mean\": mean, \"median\": median, \"mode\": mode}"}
{"task_id": "BigCodeBench/312", "solution": "import random\nimport matplotlib.pyplot as plt\nimport numpy as np\nDISTRIBUTION_SIZE = 1000\ndef task_func(bins=30):\n    # Generate a Gaussian distribution\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(distribution, bins=bins, density=True)\n    ax.set_title('Histogram of Gaussian Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return (distribution, ax)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    # Create a dictionary to store the subdirectories and their corresponding files\n    sub_dir_dict = {}\n\n    # Iterate over all files in the directory\n    for file_name in os.listdir(directory):\n        # Get the file extension\n        file_extension = os.path.splitext(file_name)[1]\n\n        # Check if the file is a text file\n        if file_extension == '.txt':\n            # Get the first text in the file\n            with open(os.path.join(directory, file_name), 'r') as file:\n                first_text = file.readline().strip()\n\n            # If the first text is not enclosed in square brackets, create a subdirectory for it\n            if not re.match(r'\\[.*?\\]', first_text):\n                # Create the subdirectory\n                sub_dir = os.path.join(directory, first_text)\n                os.makedirs(sub_dir, exist_ok=True)\n\n                # Add the file to the subdirectory\n                sub_dir_dict.setdefault(first_text, []).append(file_name)\n\n    # Return the directory path with organized files and the dictionary of subdirectories\n    return directory, sub_dir_dict"}
{"task_id": "BigCodeBench/314", "solution": "import socket\nimport ssl\nimport http.client\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    try:\n        context = ssl.create_default_context()\n        with socket.create_connection((SERVER_NAME, SERVER_PORT)) as sock:\n            with context.wrap_socket(sock, server_hostname=SERVER_NAME) as ssock:\n                request = http.client.Request(path)\n                response = ssock.getresponse(request)\n                response_body = response.read().decode()\n        return str(response_body)\n    except ssl.SSLError:\n        print(\"SSL handshake error occurred.\")\n        return None"}
{"task_id": "BigCodeBench/315", "solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\ndef task_func(dir, api_key, recipient_email):\n    try:\n        if not os.path.exists(dir):\n            raise FileNotFoundError(\"The specified directory does not exist.\")\n\n        message = Mail(\n            from_email='your_email@example.com',\n            to_emails=recipient_email,\n            subject='List of files in a directory',\n            plain_text_content='Here is the list of files in the directory: ' + dir)\n\n        try:\n            sg = SendGridAPIClient(api_key)\n            response = sg.send(message)\n            return response.status_code in range(200, 300)\n        except HTTPError as e:\n            print(f'HTTP error occurred: {e.message}')\n            return False\n        except Exception as e:\n            print(f'An error occurred: {e.message}')\n            return False\n    except FileNotFoundError as e:\n        print(f'FileNotFoundError: {e.message}')\n        return False"}
{"task_id": "BigCodeBench/316", "solution": "import pandas as pd\nimport random\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\ndef task_func(value_range=(0, 100)):\n    # Generate a random number within the specified range for each category\n    random_numbers = [random.randint(value_range[0], value_range[1]) for _ in CATEGORIES]\n\n    # Create a DataFrame\n    df = pd.DataFrame({'Category': CATEGORIES, 'Count': random_numbers})\n\n    return df"}
{"task_id": "BigCodeBench/317", "solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\ndef task_func(example_str):\n    # Extract all texts not enclosed in square brackets into a string\n    text = re.sub(r'\\[\\s*([^\\]]+)\\s*\\]', r'\\1', example_str)\n\n    # Split the text into words\n    words = text.split()\n\n    # Create a TfidfVectorizer object\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the text data\n    X = vectorizer.fit_transform(words)\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Create a dictionary with words as keys and TF-IDF scores as values\n    tfidf_dict = dict(zip(feature_names, X.toarray()[0]))\n\n    return tfidf_dict"}
{"task_id": "BigCodeBench/318", "solution": "import random\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(points_count=1000, radius=1):\n    # Generate random points within a circle\n    points = [(random.uniform(0, radius), random.uniform(0, radius)) for _ in range(points_count)]\n\n    # Plot the points\n    fig, ax = plt.subplots(1, 1, aspect='equal')\n    ax.scatter(*zip(*points))\n\n    return ax"}
{"task_id": "BigCodeBench/319", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\ndef task_func(example_str, top_n=30):\n    # Extract all texts that are not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Split the text into words\n    words = text.split()\n\n    # Create a frequency distribution of the words\n    fdist = FreqDist(words)\n\n    # Get the top_n most common words\n    top_words = fdist.most_common(top_n)\n\n    # Create a frequency distribution plot\n    plt.figure(figsize=(10,5))\n    fdist.plot()\n    plt.title('Frequency Distribution')\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    Axes = plt.gca()\n\n    # Return the top_n most common words and their frequencies\n    return Axes, dict(top_words)"}
{"task_id": "BigCodeBench/320", "solution": "import subprocess\nimport os\nimport random\ndef task_func(directory, file_list):\n    if not os.path.exists(directory) or not os.path.isdir(directory):\n        return \"Error: Directory does not exist\"\n    if not file_list or not isinstance(file_list, list):\n        return \"Error: File list is not a list\"\n    if not all(isinstance(i, str) for i in file_list):\n        return \"Error: File list contains non-string elements\"\n    if not any(os.path.exists(os.path.join(directory, i)) for i in file_list):\n        return \"Error: One or more files in the list do not exist\"\n\n    file_path = os.path.join(directory, random.choice(file_list))\n    process = subprocess.Popen(file_path, shell=True)\n    return process.pid, process.returncode"}
{"task_id": "BigCodeBench/321", "solution": "import pandas as pd\nimport re\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text):\n    # Extract names surrounded by non-square brackets\n    names = re.findall(r'\\[.*?\\]', text)\n\n    # Count the frequency of each extracted name\n    name_freq = pd.Series(names).value_counts()\n\n    # Create a bar chart of the name frequencies\n    if not name_freq.empty:\n        name_freq.plot(kind='bar')\n        plt.show()\n        ax = plt.gca()\n    else:\n        ax = None\n\n    # Calculate skewness and kurtosis\n    skewness = np.nan if name_freq.empty else stats.skew(name_freq)\n    kurtosis = np.nan if name_freq.empty else stats.kurtosis(name_freq)\n\n    return name_freq, ax, skewness, kurtosis"}
{"task_id": "BigCodeBench/322", "solution": "import subprocess\nimport os\nimport shutil\nimport sys\nDIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\Backup'\ndef task_func(filename):\n    # Check if the source directory exists\n    if not os.path.exists(DIRECTORY):\n        print(f\"Source directory {DIRECTORY} does not exist.\")\n        return -1\n\n    # Check if the backup directory exists\n    if not os.path.exists(BACKUP_DIRECTORY):\n        print(f\"Backup directory {BACKUP_DIRECTORY} does not exist.\")\n        return -1\n\n    # Create a backup of the file\n    backup_file = os.path.join(BACKUP_DIRECTORY, filename)\n    shutil.copy2(os.path.join(DIRECTORY, filename), backup_file)\n\n    # Check if the backup was successful\n    if not os.path.exists(backup_file):\n        print(f\"Backup of {filename} failed.\")\n        return -1\n\n    # Execute the file\n    process = subprocess.Popen([filename], shell=True)\n    process.communicate()\n\n    # Check the exit code\n    exit_code = process.returncode\n    if exit_code != 0:\n        print(f\"Execution of {filename} failed with exit code {exit_code}.\")\n        return exit_code\n\n    print(f\"Backup of {filename} was successful.\")\n    return 0"}
{"task_id": "BigCodeBench/323", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians should be greater than 0\")\n    if num_gaussians > len(set(re.findall(r\"\\[.*?\\]|\\b\\w+\\b\", text))):\n        raise ValueError(\"num_gaussians is greater than the number of unique words\")\n\n    words = re.findall(r\"\\[.*?\\]|\\b\\w+\\b\", text)\n    word_freq = Counter(words)\n\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    gmm.fit(word_freq.values().reshape(-1, 1))\n\n    return {\"mean\": gmm.means_.flatten(), \"variance\": gmm.covariances_.flatten()}\ntext = \"Hello [world] how are you [doing]?\""}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    # List to store the exit codes\n    exit_codes = []\n\n    # List to store the threads\n    threads = []\n\n    # Iterate over the list of files\n    for file in file_list:\n        # Create a new thread for each file\n        t = threading.Thread(target=run_file, args=(file,))\n        threads.append(t)\n        t.start()\n\n    # Wait for all threads to complete\n    for t in threads:\n        t.join()\n\n    # Return the list of exit codes\n    return exit_codes\ndef run_file(file):\n    # Run the file as a subprocess\n    process = subprocess.run([\"python3\", file], capture_output=True, text=True)\n\n    # Store the exit code\n    exit_code = process.returncode\n\n    # Print the output\n    print(f\"File: {file}, Exit Code: {exit_code}\")\n\n    # Append the exit code to the list\n    global exit_codes\n    exit_codes.append(exit_code)\nfile_list = [\"file1.py\", \"file2.py\", \"file3.py\"]"}
{"task_id": "BigCodeBench/325", "solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\ndef task_func(directory_path: str, regex_pattern: str = r'\\\\(.+?\\\\)|\\\\w') -> dict:\n    result_dict = {}\n\n    # Get all files in the directory\n    files = glob.glob(f\"{directory_path}/*\")\n\n    # Iterate over each file\n    for file in files:\n        # Check if the file is a text file\n        if Path(file).suffix == \".txt\":\n            # Read the file\n            with open(file, \"r\") as f:\n                content = f.read()\n\n            # Extract matches using the regular expression\n            matches = re.findall(regex_pattern, content)\n\n            # Create a dictionary entry for the file\n            result_dict[os.path.basename(file)] = matches\n\n    return result_dict"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    result = []\n    for file_path in glob.glob(os.path.join(directory_path, '*.bat')):\n        file_name = os.path.basename(file_path)\n        try:\n            output = subprocess.run(file_path, shell=True, capture_output=True, text=True, check=True)\n            exit_code = output.returncode\n        except subprocess.CalledProcessError as e:\n            print(f\"Error executing {file_name}: {str(e)}\")\n            exit_code = None\n        result.append((file_name, exit_code))\n    return result"}
{"task_id": "BigCodeBench/327", "solution": "import csv\nimport re\nfrom collections import Counter\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    with open(file_path, 'r') as file:\n        data = csv.reader(file)\n        next(data)  # Skip header\n        counts = Counter()\n        for row in data:\n            for item in row:\n                matches = re.findall(regex_pattern, item)\n                counts.update(matches)\n        return dict(counts)"}
{"task_id": "BigCodeBench/328", "solution": "import collections\nimport random\nfrom queue import PriorityQueue\ndef task_func(number_teams=5):\n    # Create a priority queue to store teams and their points\n    pq = PriorityQueue()\n\n    # Generate random points for each team\n    for i in range(1, number_teams + 1):\n        points = random.randint(1, 100)  # Assuming points are between 1 and 100\n        team_name = f\"Team {i}\"\n        pq.put((points, team_name))\n\n    # Create an empty dictionary to store the sorted teams\n    sorted_teams = collections.OrderedDict()\n\n    # Pop teams from the priority queue and add them to the sorted dictionary\n    while not pq.empty():\n        points, team_name = pq.get()\n        sorted_teams[team_name] = points\n\n    # Sort the dictionary by points in descending order\n    sorted_teams = collections.OrderedDict(sorted(sorted_teams.items(), key=lambda item: item[1], reverse=True))\n\n    return sorted_teams"}
{"task_id": "BigCodeBench/329", "solution": "import re\nimport json\nimport os\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    matches = {}\n    for key, value in data.items():\n        if isinstance(value, str):\n            matches[key] = re.findall(regex_pattern, value)\n        elif isinstance(value, list):\n            matches[key] = [re.findall(regex_pattern, str(item)) for item in value]\n\n    return {file_path: matches}"}
{"task_id": "BigCodeBench/330", "solution": "import heapq\nimport random\ndef task_func(list_length:5, k:int):\n    # Generate a list of random integers\n    random_list = [random.randint(1, 100) for _ in range(list_length)]\n\n    # Use heapq to find the k largest numbers\n    k_largest_numbers = heapq.nlargest(k, random_list)\n\n    return random_list, k_largest_numbers\nlist_length = 10\nk = 3"}
{"task_id": "BigCodeBench/331", "solution": "import bisect\nimport random\ndef task_func(num, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a random list of integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    random_list.sort()\n\n    # Insert the number into the sorted list\n    bisect.insort(random_list, num)\n\n    return random_list"}
{"task_id": "BigCodeBench/332", "solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\ndef task_func(text: str) -> dict:\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Convert to lower case\n    text = text.lower()\n\n    # Tokenize the text\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in stopwords.words('english')]\n\n    # Count the words\n    word_counts = Counter(words)\n\n    return dict(word_counts)\ntext = \"This is a test text. This text is used for testing the function.\""}
{"task_id": "BigCodeBench/333", "solution": "import heapq\nimport random\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a list of random integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n\n    # Use heapq to find the k smallest numbers\n    k_smallest_numbers = heapq.nsmallest(k, random_list)\n\n    return random_list, k_smallest_numbers"}
{"task_id": "BigCodeBench/334", "solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\ndef task_func(documents):\n    # Tokenize the documents\n    tokenized_documents = [word_tokenize(doc) for doc in documents]\n\n    # Create a TfidfVectorizer object\n    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n\n    # Fit and transform the documents\n    X = vectorizer.fit_transform(tokenized_documents)\n\n    # Create a DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\ndocuments = [\"This is the first document.\", \"This document is the second document.\", \"And the third one.\"]"}
{"task_id": "BigCodeBench/335", "solution": "import collections\nfrom queue import PriorityQueue\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(string_length=100):\n    # Generate a random string of a given length from a predefined list of letters\n    random_string = ''.join(random.choice(LETTERS) for _ in range(string_length))\n\n    # Count the frequency of each letter\n    letter_freq = collections.Counter(random_string)\n\n    # Return an ordered dictionary sorted by frequency in descending order\n    return collections.OrderedDict(sorted(letter_freq.items(), key=lambda x: x[1], reverse=True))"}
{"task_id": "BigCodeBench/336", "solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\ndef task_func(pattern, directory, extensions):\n    # Create a list to store the file paths\n    file_paths = []\n\n    # Get the absolute path of the directory\n    dir_path = os.path.abspath(directory)\n\n    # Use the glob function to get all files in the directory\n    for file_name in glob.glob(os.path.join(dir_path, '*'), recursive=True):\n        # Check if the file is a directory\n        if os.path.isfile(file_name):\n            # Check if the file is an allowed extension\n            if Path(file_name).suffix.lower() in extensions:\n                # Open the file\n                with open(file_name, 'r', encoding='utf-8-sig') as f:\n                    # Read the file content\n                    content = f.read()\n                    # Check if the content contains the pattern\n                    if re.search(pattern, content, re.IGNORECASE):\n                        # If it does, add the file path to the list\n                        file_paths.append(file_name)\n\n    # Return the list of file paths\n    return file_paths"}
{"task_id": "BigCodeBench/337", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col):\n    # Check if the 'Value' column has non-numeric values\n    if not df[value_col].apply(lambda x: isinstance(x, (int, float))).all():\n        raise TypeError(\"The 'Value' column should only contain numeric values.\")\n\n    # Get the number of groups\n    num_groups = len(df[group_col].unique())\n\n    # Get the colors\n    colors = COLORS[:num_groups]\n\n    # Create the bar chart\n    plt.figure(figsize=(10,6))\n    plt.bar(df[group_col], df[value_col], color=colors)\n\n    # Add error bars\n    plt.errorbar(df[group_col], df[value_col], yerr=df['Value_Error'], color='black', fmt='o')\n\n    # Set labels\n    plt.xlabel(group_col)\n    plt.ylabel(value_col)\n    plt.title(f'Bar chart of {value_col} by {group_col}')\n\n    # Show the plot\n    ax = plt.gca()\n    return ax"}
{"task_id": "BigCodeBench/338", "solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\ndef task_func(elements):\n    # Generate a random alphanumeric string of length 5\n    def generate_random_string(length=5):\n        return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n\n    # Format each string in the list\n    formatted_elements = [f\"% {i}s\" % (generate_random_string(),) for i in elements]\n\n    # Count the occurrence of each character in the formatted strings\n    char_count = {}\n    for element in formatted_elements:\n        for char in element:\n            if char in char_count:\n                char_count[char] += 1\n            else:\n                char_count[char] = 1\n\n    # Plot the histogram\n    plt.figure(figsize=(10, 5))\n    plt.bar(char_count.keys(), char_count.values())\n    plt.xlabel('Characters')\n    plt.ylabel('Count')\n    plt.title('Histogram of Character Occurrence')\n    ax = plt.gca()\n\n    return formatted_elements, ax, char_count\nelements = ['Hello', 'World', 'Python', 'Matplotlib', 'Pandas']"}
{"task_id": "BigCodeBench/339", "solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\ndef task_func(req_data, secret_key):\n    # Check if req_data is a dictionary\n    if not isinstance(req_data, dict):\n        raise TypeError(\"req_data should be a dictionary\")\n\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(req_data)\n\n    # Generate the HMAC signature\n    h = hmac.new(secret_key.encode(), json_str.encode(), hashlib.sha256)\n\n    # URL encode the signature\n    url_encoded_sig = urllib.parse.quote_plus(h.hexdigest())\n\n    return url_encoded_sig"}
{"task_id": "BigCodeBench/340", "solution": "import json\nimport hashlib\nimport blake3\ndef task_func(req_data):\n    # Hashes the request data with BLAKE3\n    blake3_hash = blake3.new(digest_size=16)\n    blake3_hash.update(req_data.encode())\n    blake3_hash_hex = blake3_hash.hexdigest()\n\n    # Generates an MD5 hash of the BLAKE3 hash\n    md5_hash = hashlib.md5(blake3_hash_hex.encode())\n    md5_hash_hex = md5_hash.hexdigest()\n\n    return (blake3_hash_hex, md5_hash_hex)\nreq_data = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a DataFrame.\")\n\n    # Check if DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"Input df must not be empty.\")\n\n    # Check if column exists in DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"Column {col} not found in DataFrame.\")\n\n    # Generate a figure object\n    fig = plt.figure()\n\n    # Add a subplot for the histogram\n    ax1 = fig.add_subplot(211)\n    sns.histplot(df[col], ax=ax1, kde=True)\n    ax1.set_title('Histogram with KDE')\n\n    # Add a subplot for the box plot\n    ax2 = fig.add_subplot(212)\n    sns.boxplot(df[col], ax=ax2)\n    ax2.set_title('Box plot')\n\n    return fig"}
{"task_id": "BigCodeBench/342", "solution": "import string\nimport random\nimport re\ndef task_func(elements, pattern, seed=100):\n    # Set the seed for randomness\n    random.seed(seed)\n\n    # Replace each character in each element of the Elements list with a random character\n    formatted_elements = [''.join(random.choice(string.ascii_letters) for _ in el) for el in elements]\n\n    # Join the formatted elements into a single string\n    formatted_string = ' '.join(formatted_elements)\n\n    # Compile the regex pattern\n    regex_pattern = re.compile(pattern)\n\n    # Search for the regex pattern in the formatted string\n    search_result = bool(regex_pattern.search(formatted_string))\n\n    return formatted_elements, search_result"}
{"task_id": "BigCodeBench/343", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b', 'y', 'm']\ndef task_func(df, col, title=None):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a DataFrame\")\n\n    # Check if DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"Input df must not be empty\")\n\n    # Check if column exists in DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"DataFrame must contain the specified column: {col}\")\n\n    # Create pie chart\n    ax = df[col].value_counts().plot(kind='pie', \n                                     autopct='%1.1f%%', \n                                     colors=COLORS, \n                                     startangle=140, \n                                     shadow=True)\n\n    # Add title if provided\n    if title is not None:\n        plt.title(title)\n\n    return ax"}
{"task_id": "BigCodeBench/344", "solution": "import os\nimport shutil\ndef task_func(src_folder, backup_dir):\n    try:\n        # Check if source folder exists\n        if not os.path.exists(src_folder):\n            raise ValueError(\"Source folder does not exist.\")\n\n        # Backup the source folder\n        shutil.copytree(src_folder, backup_dir)\n\n        # Delete the source folder\n        if os.path.exists(src_folder):\n            shutil.rmtree(src_folder)\n\n        return True\n\n    except Exception as e:\n        # If an error occurs while deleting the source folder, return False\n        print(f\"Error occurred: {e}\")\n        return False"}
{"task_id": "BigCodeBench/345", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col1, col2):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if DataFrame contains specified columns\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"DataFrame does not contain columns {col1} and {col2}\")\n\n    # Check if DataFrame contains non-numeric data\n    if not df[col1].apply(lambda x: isinstance(x, (int, float))).all() or not df[col2].apply(lambda x: isinstance(x, (int, float))).all():\n        raise TypeError(\"DataFrame contains non-numeric data\")\n\n    # Create scatter plot\n    sns.set(style=\"whitegrid\")\n    ax = sns.scatterplot(data=df, x=col1, y=col2)\n\n    # Add regression line\n    m, b = np.polyfit(df[col1], df[col2], 1)\n    ax.plot(df[col1], m*df[col1] + b, color=\"red\")\n\n    plt.show()\n    return ax"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script {script_path} does not exist\")\n\n    try:\n        if wait:\n            process = subprocess.run(script_path, args=args, timeout=10)\n            return process.returncode\n        else:\n            subprocess.run(script_path, args=args)\n            return None\n    except subprocess.CalledProcessError as e:\n        print(f\"Script {script_path} raised an exception: {str(e)}\", file=sys.stderr)\n        return None"}
{"task_id": "BigCodeBench/347", "solution": "import pandas as pd\nimport re\nimport numpy as np\nPATTERN = r\"([a-fA-F\\d]{32})\"\ndef task_func(df, column):\n    if column not in df.columns:\n        raise KeyError(f\"Column {column} does not exist in the DataFrame\")\n\n    matches = df[column].apply(lambda x: re.findall(PATTERN, str(x)))\n    unique_matches = pd.Series(matches.values.flatten()).apply(lambda x: len(x))\n\n    return unique_matches"}
{"task_id": "BigCodeBench/348", "solution": "import subprocess\nimport os\nimport signal\nimport time\ndef task_func(process_name: str) -> int:\n    # Get a list of all running processes\n    running_processes = [proc for proc in os.listdir(os.path.join('/proc')) if proc.startswith(process_name)]\n\n    # If no processes are found, return 0\n    if not running_processes:\n        return 0\n\n    # Initialize counter for processes stopped\n    count = 0\n\n    # Send termination signal to each running process\n    for proc in running_processes:\n        try:\n            # Get process id\n            pid = int(proc)\n\n            # Send termination signal\n            os.kill(pid, signal.SIGTERM)\n\n            # Wait for 1 second\n            time.sleep(1)\n\n            # Increment counter\n            count += 1\n        except (ValueError, OSError):\n            pass\n\n    return count"}
{"task_id": "BigCodeBench/349", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories):\n    # Create an empty DataFrame\n    df = pd.DataFrame(columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n\n    # Loop through the product list\n    for product in product_list:\n        # Generate a random quantity between 1 and 100\n        quantity = random.randint(1, 100)\n        # Generate a random revenue between 10 and 100\n        revenue = random.randint(10, 100) * quantity\n\n        # Append the product, category, quantity sold, and revenue to the DataFrame\n        df = df.append({'Product': product, 'Category': categories[product], 'Quantity Sold': quantity, 'Revenue': revenue}, ignore_index=True)\n\n    return df\nproduct_list = ['Product1', 'Product2', 'Product3']\ncategories = {'Product1': 'Category1', 'Product2': 'Category2', 'Product3': 'Category3'}"}
{"task_id": "BigCodeBench/350", "solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\ndef task_func(src_folder, dst_folder):\n    # Check if source folder exists\n    if not os.path.exists(src_folder):\n        return {'success': False, 'message': f'Source folder {src_folder} does not exist.', 'failed_files': []}\n\n    # Check if destination folder exists\n    if not os.path.exists(dst_folder):\n        return {'success': False, 'message': f'Destination folder {dst_folder} does not exist.', 'failed_files': []}\n\n    # Get all files in source folder\n    files = glob(os.path.join(src_folder, '*'))\n\n    failed_files = []\n    for file in files:\n        # Check if file is a regular file\n        if os.path.isfile(file):\n            # Compress file using gzip\n            compressed_file = file + '.gz'\n            subprocess.run(['gzip', '-c', file, '-o', compressed_file])\n\n            # Check if compression was successful\n            if os.path.exists(compressed_file):\n                # Move compressed file to destination folder\n                shutil.move(compressed_file, dst_folder)\n            else:\n                failed_files.append(file)\n        else:\n            failed_files.append(file)\n\n    # Check if all files were moved successfully\n    if not failed_files:\n        return {'success': True, 'message': 'All files were compressed and moved successfully.', 'failed_files': failed_files}\n    else:\n        return {'success': False, 'message': 'Some files failed to compress or move.', 'failed_files': failed_files}"}
{"task_id": "BigCodeBench/351", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    # Create a DataFrame\n    df = pd.DataFrame(columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n\n    # Populate the DataFrame\n    for product, category in zip(product_list, categories):\n        df = df.append({'Product': product, \n                        'Category': category, \n                        'Quantity Sold': random.randint(min_value, max_value),\n                        'Revenue': random.randint(min_value, max_value)}, ignore_index=True)\n\n    return df\nproduct_list = ['Product1', 'Product2', 'Product3']\ncategories = ['Category1', 'Category2', 'Category3']"}
{"task_id": "BigCodeBench/352", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(text_dict, word_keys, top_k=2):\n    if top_k < 0:\n        raise ValueError(\"top_k should not be a negative integer\")\n\n    # Calculate the frequency of the words in the dictionary\n    word_freq = Counter(text_dict)\n\n    # Get the top_k most common words\n    top_words = word_freq.most_common(top_k)\n\n    # Create a DataFrame from the top_words\n    df = pd.DataFrame(top_words, columns=['word', 'frequency'])\n\n    # Create a bar chart\n    ax = df.plot(kind='bar', x='word', y='frequency', legend=False)\n\n    return ax, dict(top_words)\ntext_dict = {'word1': 10, 'word2': 20, 'word3': 30, 'word4': 25, 'word5': 15}\nword_keys = ['word1', 'word2', 'word3', 'word4', 'word5', 'word6']"}
{"task_id": "BigCodeBench/353", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    # Generate random quantity sold and revenue for each product\n    random.seed(0)\n    quantity_sold = [random.randint(min_value, max_value) for _ in product_list]\n    revenue = [random.randint(min_value, max_value) for _ in product_list]\n\n    # Calculate total revenue for each product\n    total_revenue = [q * r for q, r in zip(quantity_sold, revenue)]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Product': product_list,\n        'Category': categories,\n        'Quantity Sold': quantity_sold,\n        'Revenue': revenue,\n        'Total Revenue': total_revenue\n    })\n\n    return df\nproduct_list = ['Product A', 'Product B', 'Product C']\ncategories = ['Category A', 'Category B', 'Category C']"}
{"task_id": "BigCodeBench/354", "solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(sentences_dict, word_keys):\n    # Create a dictionary to store the word frequencies\n    word_freq = collections.defaultdict(int)\n\n    # Iterate over each sentence\n    for sentence in sentences_dict.values():\n        # Tokenize the sentence into words\n        words = sentence.split()\n\n        # Iterate over each word in the sentence\n        for word in words:\n            # If the word is in the word_keys list, increment its frequency\n            if word.lower() in word_keys:\n                word_freq[word.lower()] += 1\n\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(list(word_freq.items()), columns=['word', 'frequency'])\n\n    # Sort the DataFrame by frequency in descending order\n    df = df.sort_values(by='frequency', ascending=False)\n\n    # Create a bar chart\n    plt.figure(figsize=(10, 5))\n    plt.bar(df['word'], df['frequency'])\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Word Frequencies')\n    plt.xticks(rotation=90)\n    plt.show()\n\n    # Return the Axes object of the bar chart\n    return plt.gca()"}
{"task_id": "BigCodeBench/355", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\ndef task_func(amplitude, frequency, time):\n    # Generate the time points\n    t = np.linspace(0, time, int(time*1000), False)\n\n    # Generate the complex wave\n    wave = amplitude * np.cos(2 * np.pi * frequency * t) + \\\n           amplitude * np.sin(2 * np.pi * frequency * t)\n\n    # Apply a Hann window\n    window = get_window('hann', 'periodic', False, time)\n    wave = wave * window\n\n    # Plot the wave\n    fig, ax = plt.subplots()\n    ax.plot(t, wave.real, label='Real part')\n    ax.plot(t, wave.imag, label='Imaginary part')\n    ax.set_title('Complex Wave with Hann Window')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    plt.show()\n\n    return wave, fig, ax"}
{"task_id": "BigCodeBench/356", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\ndef task_func(x, y):\n    # Check if x and y are numpy ndarray\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"x and y should be numpy ndarray\")\n\n    # Check if x and y have the same length\n    if x.shape != y.shape:\n        raise ValueError(\"x and y should have the same length\")\n\n    # Calculate the phase values\n    phase_values = np.angle(x + 1j*y)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the phase values\n    ax.plot(x, phase_values)\n\n    # Return the axes object and the phase values array\n    return ax, phase_values"}
{"task_id": "BigCodeBench/357", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input x must be a numpy.ndarray\")\n\n    real_part = norm.rvs(loc=0, scale=1, size=x.shape)\n    imag_part = norm.rvs(loc=2, scale=2, size=x.shape)\n\n    complex_distribution = np.add(real_part + 1j * imag_part, x)\n\n    return complex_distribution\nx = np.linspace(-10, 10, 1000)"}
{"task_id": "BigCodeBench/358", "solution": "import itertools\nimport json\nimport datetime\ndef task_func(json_list, r):\n    # Check if json_list is a valid JSON\n    try:\n        data = json.loads(json_list)\n    except json.JSONDecodeError:\n        raise Exception(\"Invalid JSON\")\n\n    # Check if json_list is empty\n    if not data:\n        raise Exception(\"JSON data is empty\")\n\n    # Check if json_list has 'number_list' key\n    if 'number_list' not in data:\n        raise Exception(\"JSON data does not have 'number_list' key\")\n\n    # Extract the 'number_list' from json_list\n    number_list = data['number_list']\n\n    # Check if 'number_list' is a list\n    if not isinstance(number_list, list):\n        raise Exception(\"'number_list' is not a list\")\n\n    # Generate all possible combinations of r elements from the 'number_list'\n    combinations = list(itertools.combinations(number_list, r))\n\n    # Return the combinations\n    return [tuple(comb) for comb in combinations]\nr = 2"}
{"task_id": "BigCodeBench/359", "solution": "from scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(data_dict, data_keys):\n    # Check if both keys exist in the dictionary\n    if all(key in data_dict for key in data_keys):\n        # Calculate the correlation\n        corr, _ = stats.pearsonr(data_dict[data_keys[0]], data_dict[data_keys[1]])\n\n        # Create a scatter plot\n        plt.scatter(data_dict[data_keys[0]], data_dict[data_keys[1]])\n        plt.xlabel(data_keys[0])\n        plt.ylabel(data_keys[1])\n        plt.title('Scatter plot of the two data series')\n        plt.show()\n\n        return corr, plt\n    else:\n        raise ValueError(\"One or both keys do not exist in the dictionary\")\ndata_dict = {'x': [1, 2, 3, 4, 5], 'y': [2, 3, 4, 5, 6]}\ndata_keys = ['x', 'y']"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    try:\n        if not os.path.exists(file_location):\n            raise FileNotFoundError(\"The file does not exist at the specified path.\")\n        \n        if not os.path.exists(sheet_name):\n            raise ValueError(\"The specified sheet does not exist in the workbook.\")\n        \n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n        \n        mean_std_dict = {}\n        for col in df.columns:\n            mean_std_dict[col] = {'mean': df[col].mean(), 'std': df[col].std()}\n        \n        fig, ax = plt.subplots()\n        ax.bar(mean_std_dict.keys(), mean_std_dict.values(), color='blue')\n        ax.set_title('Mean and Standard Deviation')\n        ax.set_xlabel('Columns')\n        ax.set_ylabel('Values')\n        \n        return fig\n    \n    except FileNotFoundError as e:\n        print(str(e))\n    except ValueError as e:\n        print(str(e))"}
{"task_id": "BigCodeBench/361", "solution": "import pandas as pd\nimport logging\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    try:\n        # Read data from Excel file\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n    except FileNotFoundError:\n        logging.error(\"FileNotFoundError: The file does not exist at the specified path.\")\n        return\n    except ValueError:\n        logging.error(\"ValueError: The specified sheet name is not found in the file.\")\n        return\n\n    # Check if the dataframe is empty\n    if df.empty:\n        logging.error(\"ValueError: The Excel file does not contain any data.\")\n        return\n\n    # Convert the dataframe to a CSV file\n    df.to_csv(csv_file_location, index=False)\n\n    # Calculate the sum of each column\n    column_sums = df.sum()\n\n    # Return the dictionary with the sum of each column\n    return column_sums.to_dict()"}
{"task_id": "BigCodeBench/362", "solution": "import pandas as pd\nimport os\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    # Check if the original file exists\n    if not os.path.exists(original_file_location):\n        raise FileNotFoundError(f\"The original file at the path {original_file_location} does not exist.\")\n\n    # Check if the specified sheet exists in the workbook\n    if not pd.ExcelFile(original_file_location).sheet_names.__contains__(sheet_name):\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the workbook.\")\n\n    # Copy data from the original Excel file to a new one\n    df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n\n    # Write the data to a new Excel file\n    df.to_excel(new_file_location, index=False)\n\n    # Return the data as a pandas DataFrame\n    return df"}
{"task_id": "BigCodeBench/363", "solution": "from multiprocessing import Pool\nimport math\ndef calculate_factorial(number: int) -> tuple:\n    if not isinstance(number, int):\n        raise ValueError(\"The input number is not an integer.\")\n    if number < 0:\n        raise ValueError(\"The input number is negative.\")\n    return number, math.factorial(number)\ndef task_func(numbers: list) -> dict:\n    with Pool() as p:\n        result = p.map(calculate_factorial, numbers)\n    return dict(result)\nnumbers = [1, 2, 3, 4, 5, -1, 'a']"}
{"task_id": "BigCodeBench/364", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\ndef task_func(df):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Split the data into features and target\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Train a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model"}
{"task_id": "BigCodeBench/365", "solution": "from collections import Counter\nimport json\nimport random\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\ndef task_func(n, file_name, seed=77):\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Generate n random words\n    selected_words = random.sample(WORDS, n)\n\n    # Count the frequency of each word\n    word_counts = Counter(selected_words)\n\n    # Create a dictionary with the word counts\n    word_dict = dict()\n    for word, count in word_counts.items():\n        word_dict[word] = count\n\n    # Write the dictionary to a JSON file\n    with open(file_name + '.json', 'w') as f:\n        json.dump(word_dict, f)\n\n    # Return the name of the JSON file\n    return file_name + '.json'"}
{"task_id": "BigCodeBench/366", "solution": "import matplotlib.pyplot as plt\nimport random\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\ndef task_func(number_list, bins):\n    # Generate a random color for each bin\n    color = random.choice(COLORS)\n\n    # Create a histogram plot\n    plt.hist(number_list, bins=bins, color=color)\n\n    # Set plot title and labels\n    plt.title('Histogram')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n\n    # Return the axes object\n    return plt.gca()"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(i, datetime) for i in activities):\n        raise TypeError(\"The activities should be datetime objects.\")\n\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n\n    days = list(day_counts.keys())\n    counts = list(day_counts.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # Select a random file from the list\n    random_file = random.choice(files)\n\n    # Construct the full path to the random file\n    random_file_path = os.path.join(src_dir, random_file)\n\n    # Move the random file to the destination directory\n    shutil.move(random_file_path, dest_dir)\n\n    # Return the name of the file moved\n    return random_file\nsrc_dir = '/path/to/source/directory'\ndest_dir = '/path/to/destination/directory'\nseed = 100"}
{"task_id": "BigCodeBench/369", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    # Fit the data to a Gaussian distribution\n    mu, std = stats.norm.fit(l)\n\n    # Create a histogram of the data\n    plt.hist(l, bins=20, density=True, alpha=0.6, color='g')\n\n    # Create a Gaussian distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Set the title and labels\n    plt.title(f\"Fit results: mu = {round(mu, 2)},  std = {round(std, 2)}\")\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n\n    # Return the Axes object\n    return plt.gca()\nl = np.random.normal(0, 1, 1000)"}
{"task_id": "BigCodeBench/370", "solution": "import os\nimport re\nimport json\nimport glob\ndef task_func(directory_path: str) -> list:\n    try:\n        os.chdir(directory_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The specified directory does not exist.\")\n\n    json_files = glob.glob('*.json')\n    processed_files = []\n\n    for file in json_files:\n        with open(file, 'r') as f:\n            data = json.load(f)\n\n        escaped_data = {k: re.escape(v) for k, v in data.items()}\n\n        with open(file, 'w') as f:\n            json.dump(escaped_data, f)\n\n        processed_files.append(file)\n\n    return processed_files"}
{"task_id": "BigCodeBench/371", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(l):\n    # Create a DataFrame from the input list\n    df = pd.DataFrame(l, columns=['Values'])\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    scaled_values = scaler.fit_transform(df['Values'].values.reshape(-1, 1))\n\n    # Create a new DataFrame with the scaled values\n    scaled_df = pd.DataFrame(scaled_values, columns=['Scaled Values'])\n\n    return scaled_df\nl = [[1], [2], [3], [4], [5]]"}
{"task_id": "BigCodeBench/372", "solution": "import re\nimport glob\nfrom docx import Document\ndef task_func(directory_path: str) -> int:\n    # Get all .docx files in the directory\n    docx_files = glob.glob(f\"{directory_path}/*.docx\")\n\n    # Initialize counter\n    count = 0\n\n    # Process each .docx file\n    for docx_file in docx_files:\n        # Open the document\n        document = Document(docx_file)\n\n        # Iterate over each paragraph\n        for paragraph in document.paragraphs:\n            # Find all double quotes in the text\n            matches = re.findall(r'\"([^\"]*)\"', paragraph.text)\n\n            # Replace each double quote with a backslash-wrapped version\n            for match in matches:\n                paragraph.text = paragraph.text.replace(match, r'\\\"' + match + r'\\\"')\n\n        # Save the document\n        document.save(docx_file)\n\n        # Increment the counter\n        count += 1\n\n    # Return the number of .docx files processed\n    return count"}
{"task_id": "BigCodeBench/373", "solution": "from scipy.optimize import curve_fit\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(l, x_data):\n    def func(x, a, b, c):\n        return a*x**2 + b*x + c\n\n    params, params_cov = curve_fit(func, x_data, l)\n    fitted_values = func(x_data, *params)\n\n    if plot:\n        plt.figure(figsize=(10, 6))\n        plt.plot(x_data, l, 'o', label='Original data')\n        plt.plot(x_data, fitted_values, '-', label='Fitted curve')\n        plt.legend()\n        plt.show()\n\n    return params, fitted_values\nx_data = np.linspace(0, 10, 100)\nl = x_data**2 + 2*x_data + 1"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(directory_path + '*.xlsx')\n\n    # Initialize counter\n    count = 0\n\n    # Process each file\n    for file in xlsx_files:\n        # Load the workbook\n        wb = load_workbook(filename=file)\n\n        # Get all worksheets in the workbook\n        worksheets = wb.sheetnames\n\n        # Process each worksheet\n        for worksheet in worksheets:\n            # Get the workbook\n            ws = wb[worksheet]\n\n            # Get all cells in the worksheet\n            cells = ws.iter_rows()\n\n            # Process each cell\n            for row in cells:\n                for cell in row:\n                    # If the cell contains a double quote, prefix it with a double backslash\n                    if re.search(r'\"', cell.value):\n                        cell.value = '\\\\\"' + cell.value.replace('\"', '\\\\\"') + '\\\\\"'\n\n            # Save the workbook\n            wb.save(filename=file)\n\n        # Increment the counter\n        count += 1\n\n    # Return the number of Excel files processed\n    return count"}
{"task_id": "BigCodeBench/375", "solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    # Perform Principal Component Analysis (PCA)\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(l)\n\n    # Create a DataFrame for the PCA result\n    pca_df = pd.DataFrame(data=pca_result, columns=['First Principal Component', 'Second Principal Component'])\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['First Principal Component'], pca_df['Second Principal Component'])\n\n    # Set x and y labels\n    ax.set_xlabel('First Principal Component')\n    ax.set_ylabel('Second Principal Component')\n\n    # Set the title of the plot\n    ax.set_title('PCA Result')\n\n    return ax"}
{"task_id": "BigCodeBench/376", "solution": "import nltk\nimport re\nfrom collections import Counter\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(text):\n    # Split the text into words\n    words = nltk.word_tokenize(text)\n\n    # Convert them to lowercase\n    words = [word.lower() for word in words]\n\n    # Remove punctuation marks\n    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n\n    # Remove common stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Calculate the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq\ntext = \"This is a test text. This text is used to test the function.\""}
{"task_id": "BigCodeBench/377", "solution": "from texttable import Texttable\nimport os\nimport psutil\ndef task_func():\n    # Create a table\n    table = Texttable()\n    table.set_cols_align([\"l\", \"r\"])\n    table.set_cols_valign([\"m\", \"m\"])\n    table.header([\"Item\", \"Value\"])\n\n    # Add system information\n    cpu_usage = psutil.cpu_percent()\n    memory_usage = psutil.virtual_memory().percent\n    disk_usage = psutil.disk_usage('/').percent\n\n    table.add_row([\"CPU Usage (%)\", cpu_usage])\n    table.add_row([\"Memory Usage (%)\", memory_usage])\n    table.add_row([\"Disk Usage (%)\", disk_usage])\n\n    return table.draw()"}
{"task_id": "BigCodeBench/378", "solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\ndef task_func(data_dir='./data/'):\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"The specified directory {data_dir} does not exist.\")\n\n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n    if not csv_files:\n        raise ValueError(f\"There are no CSV files in the specified directory {data_dir}.\")\n\n    table = Texttable()\n    table.set_cols_align([\"l\", \"r\", \"r\"])\n    table.header(['File Name', 'Number of Rows', 'Number of Columns'])\n\n    for csv_file in csv_files:\n        try:\n            df = pd.read_csv(csv_file)\n            row = [csv_file, df.shape[0], df.shape[1]]\n            table.add_row(row)\n        except pd.errors.EmptyDataError:\n            print(f\"Skipping empty CSV file: {csv_file}\")\n\n    return table.draw()"}
{"task_id": "BigCodeBench/379", "solution": "import pandas as pd\nimport numpy as np\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length):\n    # Generate a Pandas DataFrame with specified length and random data\n    df = pd.DataFrame(np.random.randint(0,100,size=(length, len(COLUMNS))), columns=COLUMNS)\n    \n    # Record the data\n    print(\"DataFrame:\")\n    print(df)\n\n    return df"}
{"task_id": "BigCodeBench/380", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    # Get all files in the directory\n    files = os.listdir(directory)\n\n    # Create a dictionary to store files by extension\n    ext_dict = {}\n\n    # Iterate over all files\n    for file in files:\n        # Get the file extension\n        ext = os.path.splitext(file)[1]\n\n        # If the extension is not in the dictionary, add it\n        if ext not in ext_dict:\n            ext_dict[ext] = []\n\n        # Add the file to the corresponding list\n        ext_dict[ext].append(file)\n\n    # Create a new directory for each extension\n    for ext, files in ext_dict.items():\n        new_dir = os.path.join(directory, ext)\n        os.makedirs(new_dir, exist_ok=True)\n\n        # Move the files to the new directory\n        for file in files:\n            shutil.move(os.path.join(directory, file), new_dir)\n\n    # Print None to indicate that the task is complete\n    print(None)"}
{"task_id": "BigCodeBench/381", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    # Check if file exists\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load data\n    try:\n        df = pd.read_csv(file_path)\n    except pd.errors.ParserError:\n        raise ValueError(\"The file is not a valid CSV file.\")\n\n    # Check if target column exists\n    if target_column not in df.columns:\n        raise ValueError(f\"The target column {target_column} does not exist in the CSV file.\")\n\n    # Drop rows with any NaN values\n    df = df.dropna()\n\n    # Check for NaN, infinity, and too large values\n    if df.isnull().any().any():\n        raise ValueError(\"The input data contains NaN, infinity or a value too large for dtype('float32').\")\n\n    # Check for values too large for dtype('float32')\n    if df.applymap(lambda x: x if pd.api.types.is_numeric_dtype(df[target_column].dtype) else pd.np.nan).isnull().any().any():\n        raise ValueError(\"The input data contains NaN, infinity or a value too large for dtype('float32').\")\n\n    # Define X and y\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n\n    # Train a random forest model\n    rf = RandomForestClassifier(n_estimators=100, random_state=seed)\n    rf.fit(X, y)\n\n    # Get feature importances\n    importances = rf.feature_importances_\n\n    # Create a DataFrame for the feature importances\n    df_importances = pd.DataFrame({'feature': X.columns, 'importance': importances})\n\n    # Sort the DataFrame by importance\n    df_importances = df_importances.sort_values('importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='importance', y='feature', data=df_importances)\n    plt.title('Feature Importances')\n    plt.tight_layout()\n\n    return rf, importances"}
{"task_id": "BigCodeBench/382", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(length):\n    # Generate a normal distribution\n    distribution = norm.rvs(loc=0, scale=1, size=length)\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(distribution, bins='auto', density=True)\n    \n    # Plot the probability density function\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, loc=0, scale=1)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    return distribution, ax"}
{"task_id": "BigCodeBench/383", "solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\ndef task_func(text, n, top_k):\n    # Create a counter of n-grams\n    ngram_counter = Counter(text[i:i+n] for i in range(len(text) - n + 1))\n\n    # Get the top k n-grams\n    top_ngrams = ngram_counter.most_common(top_k)\n\n    # Print the top k n-grams\n    for ngram, count in top_ngrams:\n        print(f\"{ngram}: {count}\")\ntext = \"this is a test text for the n-grams task\""}
{"task_id": "BigCodeBench/384", "solution": "import collections\nimport random\nimport itertools\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe', 'Horse', 'Rabbit', 'Snake', 'Zebra']\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n\n    # Reverse the dictionary\n    reversed_dict = {v: k for k, v in animal_dict.items()}\n\n    # Count the occurrences of each animal name\n    counter = collections.Counter(itertools.chain(*reversed_dict.values()))\n\n    # Add random counts to the counts\n    for animal in ANIMALS:\n        if animal in counter:\n            count = counter[animal]\n            counter[animal] = random.randint(1, max_count) + count\n\n    return reversed_dict, counter\nanimal_dict = {'John': 'Dog', 'Emma': 'Cat', 'George': 'Elephant', 'Adam': 'Bear'}"}
{"task_id": "BigCodeBench/385", "solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry', 'Fig', 'Grape', 'Honeydew', 'Indian Prune', 'Jackfruit']\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit\n    fruit_counts = Counter(FRUITS)\n\n    # Create a bar chart\n    plt.figure(figsize=(10, 5))\n    plt.bar(fruit_counts.keys(), fruit_counts.values())\n    plt.xlabel('Fruit')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Fruits')\n    plt.xticks(rotation=90)\n    plt.show()\n\n    # Return the dictionary\n    return fruit_counts\nfruit_dict = {'Alice': 'Apple', 'Bob': 'Banana', 'Charlie': 'Cherry', 'David': 'Date', 'Eve': 'Elderberry', 'Frank': 'Fig', 'Grace': 'Grape', 'Helen': 'Honeydew', 'Igor': 'Indian Prune', 'John': 'Jackfruit'}"}
{"task_id": "BigCodeBench/386", "solution": "import numpy as np\nimport pandas as pd\nimport random\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length, min_value = 0, max_value = 100):\n    # Generate a random DataFrame\n    df = pd.DataFrame(np.random.randint(low=min_value, high=max_value, size=(length, len(COLUMNS))), columns=COLUMNS)\n\n    # Calculate the CDF\n    cdf = (df.T + 1).cumsum()\n\n    # Return the DataFrame with CDF\n    return df, cdf"}
{"task_id": "BigCodeBench/387", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\ndef task_func(city_dict, max_range=1000000, seed=0):\n    # Seed the random number generator\n    np.random.seed(seed)\n\n    # Generate a dictionary with city names as keys and randomly generated populations as values\n    city_populations = {city: np.random.randint(1, max_range) if city in CITIES else -1 for city in city_dict.keys()}\n\n    # Plot the population data\n    cities = list(city_populations.keys())\n    populations = list(city_populations.values())\n    fig, ax = plt.subplots()\n    ax.bar(cities, populations)\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('Population of Cities')\n\n    return city_populations, ax\ncity_dict = {'New York': 1000000, 'London': 2000000, 'Beijing': 3000000, 'Tokyo': 4000000, 'Sydney': 5000000, 'Paris': 6000000, 'Berlin': 7000000, 'Moscow': 8000000, 'Madrid': 9000000, 'Rome': 10000000}"}
{"task_id": "BigCodeBench/388", "solution": "import collections\nimport pandas as pd\ndef task_func(my_tuple, path_csv_files):\n    # Initialize an empty dictionary to store the results\n    result = {}\n\n    # Iterate over each CSV file\n    for path in path_csv_files:\n        # Read the CSV file\n        df = pd.read_csv(path)\n\n        # Iterate over each column in the CSV file\n        for column in df.columns:\n            # If the column is in the tuple, count the occurrences\n            if column in my_tuple:\n                # Count the occurrences of each value in the column\n                counts = collections.Counter(df[column])\n\n                # Add the counts to the result dictionary\n                result[column] = dict(counts)\n\n    # Return the result dictionary\n    return result"}
{"task_id": "BigCodeBench/389", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    # List of files moved\n    files_moved = []\n\n    # Check all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file is a regular file\n        if os.path.isfile(os.path.join(directory, filename)):\n            # Check if the filename contains \"like\" or \"what\"\n            if re.search(r'(like|what)', filename):\n                # Create the \"Interesting Files\" subdirectory if it doesn't exist\n                if not os.path.exists(os.path.join(directory, 'Interesting Files')):\n                    os.makedirs(os.path.join(directory, 'Interesting Files'))\n                \n                # Move the file to the \"Interesting Files\" subdirectory\n                shutil.move(os.path.join(directory, filename), os.path.join(directory, 'Interesting Files', filename))\n                \n                # Add the moved file's name to the list of moved files\n                files_moved.append(filename)\n\n    # Output the list of moved files\n    print(\"List of files moved:\")\n    for file in files_moved:\n        print(file)"}
{"task_id": "BigCodeBench/390", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    if not csv_url_dict:\n        raise ValueError(\"The dictionary is empty.\")\n    if \"URL\" not in csv_url_dict:\n        raise ValueError(\"The key 'URL' does not exist in the dictionary.\")\n\n    response = requests.get(csv_url_dict[\"URL\"])\n    data = StringIO(response.text)\n    df = pd.read_csv(data)\n\n    if sort_by_column in df.columns:\n        df = df.sort_values(by=[sort_by_column])\n\n    return df"}
{"task_id": "BigCodeBench/391", "solution": "import os\nimport glob\nimport shutil\ndef task_func(directory, archive_dir='archive'):\n    # Check if the directory exists\n    if not os.path.exists(directory):\n        return False, [f\"Directory {directory} does not exist\"]\n\n    # Check if the archive directory exists\n    if not os.path.exists(archive_dir):\n        return False, [f\"Archive directory {archive_dir} does not exist\"]\n\n    # Get all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # If no JSON files found in the directory\n    if not json_files:\n        return True, []\n\n    # Create a list to store error messages\n    error_messages = []\n\n    # Move each JSON file to the archive directory\n    for json_file in json_files:\n        try:\n            shutil.move(json_file, archive_dir)\n        except Exception as e:\n            error_messages.append(f\"Failed to move {json_file}: {str(e)}\")\n\n    return False, error_messages"}
{"task_id": "BigCodeBench/392", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col, group_name):\n    if group_name not in df.columns:\n        raise ValueError(f\"The group_name {group_name} does not exist in the dataframe.\")\n\n    group_data = df.groupby(group_col)[value_col].sum()\n    group_names = df[group_col].unique()\n\n    fig, ax = plt.subplots()\n    ax.bar(group_names, group_data, color=COLORS)\n    ax.set_title(f'Bar chart of {value_col} for {group_name}')\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n\n    return ax"}
{"task_id": "BigCodeBench/393", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig, axs = plt.subplots(2, figsize=(10, 10))\n\n    axs[0].hist(samples, bins=20, density=True)\n    axs[0].set_title('Histogram')\n\n    axs[1].plot(stats.norm.ppf(np.linspace(0, 1, num_samples)), stats.norm.ppf(np.linspace(0, 1, num_samples)), 'r-', label='Normal Distribution')\n    axs[1].plot(samples, np.zeros(num_samples) - 0.01, 'go', label='Samples')\n    axs[1].legend()\n    axs[1].set_title('Q-Q Plot')\n\n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/394", "solution": "import collections\nimport string\nimport random\ndef task_func(length, seed=0):\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Generate a random string of the given length\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n\n    # Calculate the frequency of each character in the string\n    frequency_dict = collections.Counter(random_string)\n\n    # Return the dictionary\n    return frequency_dict"}
{"task_id": "BigCodeBench/395", "solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    try:\n        # Check if directory exists\n        if not os.path.exists(directory):\n            raise FileNotFoundError(\"The specified directory does not exist.\")\n\n        # Get all text files matching the pattern\n        files = glob.glob(os.path.join(directory, file_pattern))\n\n        # Check if any files are found\n        if not files:\n            raise ValueError(\"No files matching the pattern are found.\")\n\n        # Extract numeric data from each file\n        data = []\n        for file in files:\n            with open(file, 'r') as f:\n                content = f.read()\n                matches = re.findall(regex, content)\n                data.append(matches)\n\n        # Compile the data into a pandas DataFrame\n        df = pd.DataFrame(data, columns=['Numeric Data'])\n        df['Filename'] = files\n\n        return df\n\n    except FileNotFoundError as e:\n        print(e)\n    except ValueError as e:\n        print(e)"}
{"task_id": "BigCodeBench/396", "solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, sample_size, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    ax = plt.gca()\n    ax.hist(sample, density=True, alpha=0.5, bins=20, color='g')\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n    return ax"}
{"task_id": "BigCodeBench/397", "solution": "import re\nimport urllib.request\nimport json"}
{"task_id": "BigCodeBench/398", "solution": "import json\nimport os\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    if isinstance(data, list):\n        for item in data:\n            if not isinstance(item, dict):\n                return False\n        return True\n    else:\n        return False"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be a positive number\")\n\n    time = np.linspace(0, 1, sample_size)\n    sine_wave = np.sin(2 * np.pi * frequency * time)\n    cosine_wave = np.cos(2 * np.pi * frequency * time)\n\n    fig, ax = plt.subplots()\n    ax.plot(time, sine_wave, label='Sine Wave')\n    ax.plot(time, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Sine and Cosine Waves')\n    ax.legend()\n\n    return fig, ax"}
{"task_id": "BigCodeBench/400", "solution": "import json\nfrom glob import glob\ndef task_func(directory, string):\n    # List to store file paths containing the string within their JSON data\n    json_files = []\n\n    # Recursive function to search for JSON files in a directory and its subdirectories\n    def search_files(dir):\n        nonlocal json_files\n        for file in glob(dir + '/*.json'):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                if string in str(data):\n                    json_files.append(file)\n            search_files(file)\n\n    # Start the search\n    search_files(directory)\n\n    return json_files"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    # Initialize Flask app\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables\n    mail_server = os.getenv('MAIL_SERVER', 'localhost')\n    mail_port = os.getenv('MAIL_PORT', 25)\n    mail_use_tls = os.getenv('MAIL_USE_TLS', False)\n    mail_username = os.getenv('MAIL_USERNAME', None)\n    mail_password = os.getenv('MAIL_PASSWORD', None)\n\n    # Initialize Flask-Mail instance\n    mail = Mail(app, server=mail_server, port=mail_port, use_tls=mail_use_tls, username=mail_username, password=mail_password)\n\n    # Return Flask-Mail instance and app's mail configurations\n    return (mail, {'server': mail_server, 'port': mail_port, 'use_tls': mail_use_tls, 'username': mail_username, 'password': mail_password})"}
{"task_id": "BigCodeBench/402", "solution": "import re\nimport requests\nimport json\nimport csv\nimport os\nAPI_URL = 'https://api.example.com/data'\ndef task_func(pattern):\n    # Make a GET request to the API\n    response = requests.get(API_URL)\n\n    # Check if the request was successful\n    if response.status_code != 200:\n        return \"Failed to fetch data from API\"\n\n    # Extract data from the JSON response\n    data = response.json()\n\n    # Initialize an empty list to store matched data\n    matched_data = []\n\n    # Extract data that matches the provided pattern\n    for item in data['data']:\n        if re.search(pattern, item):\n            matched_data.append(item)\n\n    # Create a temporary file to write the matched data to\n    temp_file = tempfile.mktemp(prefix='matched_data.', suffix='.csv')\n\n    # Write the matched data to the CSV file\n    with open(temp_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"ID\", \"Name\", \"Email\"])  # assuming the data keys are \"ID\", \"Name\", \"Email\"\n        writer.writerows(matched_data)\n\n    # Return the absolute path to the CSV file\n    return os.path.abspath(temp_file)"}
{"task_id": "BigCodeBench/403", "solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\ndef task_func(img_path, blur_radius=5):\n    try:\n        # Open the image\n        img = Image.open(img_path)\n\n        # Apply a blur filter\n        blurred_img = img.filter(ImageFilter.GaussianBlur(blur_radius))\n\n        # Convert to grayscale\n        gray_img = blurred_img.convert('L')\n\n        # Convert to numpy arrays\n        np_img = np.array(img)\n        np_blur_img = np.array(blurred_img)\n        np_gray_img = np.array(gray_img)\n\n        return (np_img, np_blur_img, np_gray_img)\n\n    except FileNotFoundError:\n        print(f\"FileNotFoundError: The image file {img_path} does not exist.\")\n        return None\nimg_path = 'path_to_your_image.jpg'"}
{"task_id": "BigCodeBench/404", "solution": "import cv2\nimport numpy as np\nimport os\ndef task_func(img_path):\n    try:\n        # Open the image\n        image = cv2.imread(img_path)\n\n        # Check if the image file exists\n        if not os.path.exists(img_path):\n            raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n        # Convert the image to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Find contours\n        _, contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        return (image, contours)\n\n    except FileNotFoundError as e:\n        print(e)\n        return None"}
{"task_id": "BigCodeBench/405", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(points: int):\n    # Generate a list of random numbers\n    random_numbers = [random.random() for _ in range(points)]\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n\n    # Plot the random numbers\n    ax.plot(range(points), random_numbers)\n\n    # Return the plot and the list of random numbers\n    return random_numbers, ax"}
{"task_id": "BigCodeBench/406", "solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\ndef task_func(img_path, angle):\n    try:\n        img = Image.open(img_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The image file does not exist at the specified path.\")\n\n    # Rotate the image\n    rotated_img = img.rotate(angle)\n\n    # Convert the image to numpy array\n    img_array = np.array(img)\n    rotated_img_array = np.array(rotated_img)\n\n    # Display the original and rotated images side by side\n    fig, axs = plt.subplots(1, 2)\n    axs[0].imshow(img_array)\n    axs[0].set_title('Original Image')\n    axs[1].imshow(rotated_img_array)\n    axs[1].set_title('Rotated Image')\n    plt.show()\n\n    return img_array, rotated_img_array"}
{"task_id": "BigCodeBench/407", "solution": "import os\nimport csv\nfrom openpyxl import load_workbook\ndef task_func(file_name, excel_file_path, csv_file_path):\n    # Load the Excel file\n    wb = load_workbook(filename=excel_file_path, read_only=True)\n\n    # Get the active sheet\n    sheet = wb.active\n\n    # Create a list to store the data from the Excel file\n    data = []\n\n    # Iterate over the rows in the active sheet\n    for row in sheet.iter_rows():\n        # Create a list to store the data for a row\n        row_data = []\n        for cell in row:\n            # Add the data from the cell to the row data list\n            row_data.append(cell.value)\n        # Add the row data list to the data list\n        data.append(row_data)\n\n    # Create a filename for the CSV file\n    csv_filename = os.path.splitext(file_name)[0] + '.csv'\n\n    # Create a CSV file\n    with open(os.path.join(csv_file_path, csv_filename), 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerows(data)\n\n    # Return the name of the created CSV file\n    return csv_filename"}
{"task_id": "BigCodeBench/408", "solution": "import sqlite3\nimport pandas as pd\ndef task_func(db_file: str, query: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Execute the SQL query\n    df = pd.read_sql_query(query, conn)\n\n    # Close the connection\n    conn.close()\n\n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    try:\n        # Check if the file exists\n        if not os.path.exists(os.path.join(excel_file_path, file_name + '.xlsx')):\n            raise FileNotFoundError(\"The file does not exist at the specified path.\")\n\n        # Load the Excel file\n        df = pd.read_excel(os.path.join(excel_file_path, file_name + '.xlsx'))\n\n        # Check if the column exists\n        if column_name not in df.columns:\n            raise ValueError(\"The specified column is not found in the Excel file.\")\n\n        # Calculate the mean, median, and standard deviation\n        mean = df[column_name].mean()\n        median = df[column_name].median()\n        std_dev = df[column_name].std()\n\n        # Return the results as a dictionary\n        return {'mean': mean, 'median': median, 'standard deviation': std_dev}\n\n    except FileNotFoundError as fnf_error:\n        print(fnf_error)\n    except ValueError as ve_error:\n        print(ve_error)"}
{"task_id": "BigCodeBench/410", "solution": "import os\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> pd.DataFrame:\n    # Check if the file exists\n    file_path = os.path.join(excel_directory, file_name)\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The specified file {file_path} does not exist.\")\n\n    # Check if the start_date and end_date are in the correct format\n    try:\n        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"start_date and end_date should be in the format YYYY-MM-DD\")\n\n    # Check if the column_name exists in the DataFrame\n    if column_name not in pd.read_excel(file_path).columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n\n    # Filter the data in the specific date range\n    df = pd.read_excel(file_path)\n    df = df[(df[column_name] >= start_date) & (df[column_name] <= end_date)]\n\n    return df"}
{"task_id": "BigCodeBench/411", "solution": "import pandas as pd\nimport json\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    df = pd.DataFrame(data)\n    if 'c' in df.columns:\n        df = df.drop('c', axis=1)\n    df.to_json(output_path, orient='records')\n    return output_path"}
{"task_id": "BigCodeBench/412", "solution": "import json\nimport base64\nimport unicodedata\ndef task_func(json_file: str) -> dict:\n    with open(json_file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    decoded_dict = {}\n    for key, value in data.items():\n        decoded_str = base64.b64decode(value).decode('utf-8', 'replace')\n        decoded_str = unicodedata.normalize('NFC', decoded_str)\n        decoded_dict[key] = decoded_str\n\n    return decoded_dict"}
{"task_id": "BigCodeBench/413", "solution": "import os\nfrom flask_mail import Mail\ndef task_func(app):\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Get email server details from environment variables\n    smtp_host = os.getenv('SMTP_HOST', 'localhost')\n    smtp_port = os.getenv('SMTP_PORT', '25')\n    smtp_username = os.getenv('SMTP_USERNAME', '')\n    smtp_password = os.getenv('SMTP_PASSWORD', '')\n\n    # Set email server details\n    mail.init_app(app,\n                  host=smtp_host,\n                  port=smtp_port,\n                  username=smtp_username,\n                  password=smtp_password,\n                  secure=False)  # In case of SSL, use secure=True\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD'], app.config['MAIL_DEFAULT_SENDER']"}
{"task_id": "BigCodeBench/414", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    if column in data.columns:\n        data = data.drop(column, axis=1)\n    return data"}
{"task_id": "BigCodeBench/415", "solution": "import pandas as pd\nimport codecs\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input is not a Pandas DataFrame\")\n    \n    try:\n        dataframe['UnicodeString'] = dataframe['UnicodeString'].apply(lambda x: codecs.decode(x, 'unicode_escape'))\n    except KeyError:\n        raise KeyError(\"The column 'UnicodeString' does not exist in the DataFrame\")\n    \n    return dataframe"}
{"task_id": "BigCodeBench/416", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    # Check if the data is empty or not\n    if data.empty:\n        return None\n\n    # Check if the column exists in the data\n    if column not in data.columns:\n        return None\n\n    # Check if the column is numeric\n    if pd.api.types.is_numeric(data[column]):\n        # Remove the column\n        data = data.drop(columns=column)\n\n        # Create a correlation matrix\n        corr = data.corr()\n\n        # Create a heatmap\n        fig, ax = plt.subplots(figsize=(10, 8))\n        sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)\n\n        return ax\n    else:\n        return None"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Splitting the data into a training set and a test set\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Constructing a Sequential model with one dense hidden layer and a sigmoid activation function\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compiling the model using binary cross-entropy loss and SGD optimizer with a specified learning rate\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n\n    # Fitting the model to the training data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plotting the model's training and validation loss over epochs\n    plt.figure(figsize=(12, 6))\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Test')\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Creating a Keras Sequential model with one hidden layer using a sigmoid activation function\n    model = keras.models.Sequential([\n        keras.layers.Dense(128, activation='sigmoid', input_shape=(X.shape[1],)),\n        keras.layers.Dense(64, activation='sigmoid'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compiling the model with binary cross-entropy loss and an SGD optimizer\n    model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n\n    # Fitting the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, verbose=0)\n\n    # Predicting the probabilities on the test set\n    Y_pred = model.predict(X_test)\n\n    # Computing the ROC curve\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n\n    # Computing the AUC score\n    roc_auc = auc(fpr, tpr)\n\n    # Plotting the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model"}
{"task_id": "BigCodeBench/419", "solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Constructing a Keras Sequential model with one hidden dense layer and sigmoid activation\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid')\n    ])\n\n    # Compiling the model using binary cross-entropy loss and SGD optimizer\n    model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n\n    # Fitting the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, verbose=0)\n\n    # Predicting the probabilities on the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculating the precision-recall curve\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plotting the Precision-Recall curve\n    plt.figure(figsize=(8, 6))\n    plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n    plt.title('Precision-Recall Curve')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.legend(loc='best')\n    plt.grid(True)\n    plt.show()\n\n    return model, plt"}
{"task_id": "BigCodeBench/420", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n\n    # Create a StandardScaler object\n    scaler = StandardScaler()\n\n    # Get a list of numeric columns\n    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Apply the scaler to the numeric columns\n    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n\n    # Return the scaled DataFrame\n    return data"}
{"task_id": "BigCodeBench/421", "solution": "import requests\nimport os\nimport json\nimport time"}
{"task_id": "BigCodeBench/422", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    # Check if the column to remove exists in the dataframe\n    if column_to_remove in df.columns:\n        # Remove the specified column\n        df = df.drop(column_to_remove, axis=1)\n\n    # Split the data into features and target\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n\n    # Split the data into train and test datasets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    return X_train, X_test, y_train, y_test"}
{"task_id": "BigCodeBench/423", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n    # Check if the threshold is an integer and in the range 0-255\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(f\"Threshold value {threshold} is not an integer or not in the range 0-255.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)\n\n    # Return the original and binarized images as numpy arrays\n    return (image, binary)"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(f\"The value of 'n_clusters' should be a positive integer, but got {n_clusters}.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to 2D array\n    image = image.reshape((-1, 3))\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed).fit(image)\n\n    # Get the centroids\n    centroids = kmeans.cluster_centers_\n\n    # Reshape the centroids to the original image shape\n    centroids = centroids.reshape((image.shape[0], image.shape[1], image.shape[2]))\n\n    # Create a mask to replace the centroids with the original colors\n    mask = np.ones_like(image) * -1\n    centroids = np.uint8(centroids)\n    mask[np.where(image == centroids)] = [0, 0, 0]\n\n    # Apply the mask to the original image\n    segmented_image = image + mask\n\n    return (image, segmented_image)"}
{"task_id": "BigCodeBench/425", "solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    try:\n        # Read the image\n        image = cv2.imread(image_path)\n\n        # Check if the image file exists\n        if not os.path.exists(image_path):\n            raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n\n        # Convert the image to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Create a histogram of the image pixel intensities\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n\n        # Normalize the histogram\n        cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n\n        # Display the original image and its histogram\n        plt.figure(figsize=(10, 5))\n        plt.subplot(121), plt.imshow(image, 'gray')\n        plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n        plt.subplot(122), plt.hist(hist.ravel(), bins=256, color='gray')\n        plt.title('Histogram'), plt.xticks([]), plt.yticks([])\n        plt.show()\n\n        return plt.gca()\n\n    except FileNotFoundError as e:\n        print(f\"FileNotFoundError: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/426", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    # Check if the threshold is an integer and in the range 0-255\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(\"Threshold must be an integer in the range 0-255\")\n\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"Image file not found at the path: {image_path}\")\n\n    # Read the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY)\n\n    return image, binary_image"}
{"task_id": "BigCodeBench/427", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes based on the 'id' column\n    df = pd.merge(df1, df2, on='id')\n\n    # Perform linear regression using columns specified in features to predict the target\n    lm = LinearRegression()\n    lm.fit(df[features], df[target])\n\n    # Get the coefficients and intercept\n    coefficients = lm.coef_\n    intercept = lm.intercept_\n\n    # Predict the target values\n    predicted_values = lm.predict(df[features])\n\n    # Calculate the residuals\n    residuals = df[target] - predicted_values\n\n    # Plot the residuals\n    plt.figure(figsize=(10, 6))\n    plt.scatter(predicted_values, residuals, alpha=0.5)\n    plt.title('Residuals Plot')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.show()\n\n    return {\n        'coefficients': coefficients,\n        'intercept': intercept,\n        'residuals_plot': plt\n    }"}
{"task_id": "BigCodeBench/428", "solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df1, df2):\n    # Merge the two dataframes on the 'id' column\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Scale the numeric features from df1\n    scaler = StandardScaler()\n    merged_df[df1.columns] = scaler.fit_transform(merged_df[df1.columns])\n\n    # Return a pair plot of the scaled features from df1\n    pair_plot = sns.pairplot(merged_df)\n\n    return merged_df, pair_plot"}
{"task_id": "BigCodeBench/429", "solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2):\n    # Combine the two dataframes\n    df = pd.concat([df1, df2])\n\n    # Select the top 2 features based on the f_classif score\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selector.fit(df, df['target'])\n\n    # Get the mask of the selected features\n    mask = selector.get_support()\n\n    # Get the list of selected features\n    selected_features = df.columns[mask]\n\n    # Create a correlation matrix\n    correlation_matrix = df[selected_features].corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Heatmap')\n    plt.show()\n\n    return selected_features, plt"}
{"task_id": "BigCodeBench/430", "solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge datasets\n    df = pd.concat([df1, df2])\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    kmeans.fit(df[[column1, column2]])\n\n    # Get cluster labels\n    labels = kmeans.labels_\n\n    # Create scatterplot\n    fig, ax = plt.subplots()\n    ax.scatter(df[column1], df[column2], c=labels)\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.set_title('KMeans Clustering')\n\n    return labels, ax"}
{"task_id": "BigCodeBench/431", "solution": "import cv2\nimport os\nimport numpy as np\ndef task_func(image_file: str) -> np.ndarray:\n    try:\n        if not os.path.exists(image_file):\n            raise FileNotFoundError(f\"The specified image file {image_file} does not exist.\")\n        \n        img = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n        \n        if img is None:\n            raise ValueError(f\"The image file {image_file} is not a valid image.\")\n        \n        hist = np.zeros(256, dtype=int)\n        \n        for row in img:\n            for pixel in row:\n                hist[pixel] += 1\n        \n        return hist\n    \n    except FileNotFoundError as fnf_error:\n        print(fnf_error)\n    except ValueError as ve_error:\n        print(ve_error)"}
{"task_id": "BigCodeBench/432", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the two dataframes based on the 'id' column\n    merged_df = pd.merge(df1, df2, on='id')\n\n    # Perform a chi-square independence test on the merged dataframe\n    chi2, p, dof, expected = chi2_contingency(merged_df[[column1, column2]])\n\n    # Create a heatmap of the contingency table\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(merged_df[[column1, column2]].groupby(level=0).apply(pd.Series.value_counts, args=(merged_df[[column1, column2]].groupby(level=0).size(),)).astype(float), cmap='Blues')\n    plt.title('Contingency Table')\n    plt.xlabel(column1)\n    plt.ylabel(column2)\n\n    return p, plt"}
{"task_id": "BigCodeBench/433", "solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\ndef task_func(s, signature, secret_key):\n    # Decode the base64-encoded message\n    decoded_message = base64.b64decode(s)\n\n    # Compute the HMAC SHA-1 hash of the decoded message using the provided secret key\n    hash_obj = hmac.new(secret_key.encode(), decoded_message, hashlib.sha1)\n\n    # Compare the computed hash with the provided signature\n    computed_signature = hash_obj.digest().hex()\n\n    # Return True if the provided signature matches the computed signature, False otherwise\n    return computed_signature == signature"}
{"task_id": "BigCodeBench/434", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Split the input string into lines\n    lines = s.split('\\n')\n\n    # Initialize an empty list to store the data\n    data = []\n\n    # Iterate over each line\n    for line in lines:\n        # Split the line into parts\n        parts = line.split()\n\n        # If the line is not empty\n        if parts:\n            # Get the product name\n            product_name = ' '.join(parts[:-2])\n\n            # Get the remaining parts\n            remaining_parts = parts[-2:]\n\n            # If the product name is not already in the list\n            if product_name not in data:\n                # Assign a product name to the code\n                product_name = random.choice(['Apple', 'Banana', 'Orange', 'Pear', 'Grape'])\n\n            # Get the quantity and price\n            quantity, price = remaining_parts[0].split('B')\n\n            # Convert the quantity and price to integers\n            quantity = int(quantity)\n            price = int(price)\n\n            # Get the code\n            code = remaining_parts[1]\n\n            # Get the description\n            description = ' '.join(parts[:-2])\n\n            # Append the data to the list\n            data.append([quantity, price, code, product_name, description])\n\n    # Create a DataFrame from the list\n    df = pd.DataFrame(data, columns=['Quantity', 'Price', 'Code', 'Product', 'Description'])\n\n    # Convert the Quantity and Price columns to integers\n    df['Quantity'] = df['Quantity'].astype(int)\n    df['Price'] = df['Price'].astype(int)\n\n    return df\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'"}
{"task_id": "BigCodeBench/435", "solution": "import pandas as pd\nfrom random import randint\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'Developer', 'Tester']\n    random_job_title = job_titles[randint(0, 4)]\n\n    data_dict = {\n        'Name': name,\n        'Age': age,\n        'Code': code,\n        'Salary': salary,\n        'Bio': bio,\n        'Job Title': random_job_title\n    }\n\n    data_df = pd.DataFrame(data_dict)\n\n    return data_df"}
{"task_id": "BigCodeBench/436", "solution": "import string\nimport matplotlib.pyplot as plt\ndef task_func(s):\n    if not isinstance(s, str):\n        raise ValueError(\"Input should be a string\")\n\n    s = s.lower()\n    freq_dict = {char: s.count(char) for char in string.ascii_lowercase}\n\n    letters = list(freq_dict.keys())\n    frequencies = list(freq_dict.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(letters, frequencies)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequencies')\n    plt.show()\n\n    return freq_dict, ax"}
{"task_id": "BigCodeBench/437", "solution": "import pickle\nimport os\nimport pandas as pd\ndef task_func(df, file_name=\"save.pkl\"):\n    # Save the provided Pandas DataFrame to a pickle file\n    df.to_pickle(file_name)\n\n    # Load the DataFrame from the saved file\n    loaded_df = pd.read_pickle(file_name)\n\n    # Delete the intermediate file\n    os.remove(file_name)\n\n    return loaded_df"}
{"task_id": "BigCodeBench/438", "solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list) or not all(isinstance(i, (int, float)) for i in numbers):\n        raise TypeError(\"Input is not a list of numbers\")\n\n    # Generate the plot\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as f:\n        pickle.dump(fig, f)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as f:\n        loaded_fig = pickle.load(f)\n\n    # Delete the pickle file\n    os.remove(file_path)\n\n    return loaded_fig"}
{"task_id": "BigCodeBench/439", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(P, T):\n    # Check if P and T are numpy arrays\n    if not isinstance(P, np.ndarray) or not isinstance(T, np.ndarray):\n        raise ValueError(\"P and T must be numpy arrays\")\n\n    # Check if P and T are 3D arrays\n    if P.ndim != 2 or T.ndim != 3:\n        raise ValueError(\"P and T must be 3D arrays\")\n\n    # Check if P and T have the same shape\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"P's columns must match T's rows\")\n\n    # Perform matrix-tensor multiplication\n    result = np.dot(P, T)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(result, cmap='viridis')\n    ax = plt.gca()\n    ax.set_title('Heatmap of the product of P and T')\n    plt.show()\n\n    return result"}
{"task_id": "BigCodeBench/440", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(P, T):\n    # Check if the shapes of P and T are compatible for multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"The number of features in the first matrix must be equal to the number of features in the second matrix.\")\n\n    # Perform matrix-tensor multiplication between a matrix \"P\" and a 3D tensor \"T\" using numpy\n    result = np.tensordot(P, T, axes=0)\n\n    # Normalize the resulting 2D array using sklearn's StandardScaler\n    scaler = StandardScaler()\n    result_normalized = scaler.fit_transform(result.reshape(-1, 1))\n\n    # Return a DataFrame with normalized results\n    return pd.DataFrame(result_normalized, columns=[f'feature_{i}' for i in range(result.shape[0])])\nP = np.array([[1, 2, 3], [4, 5, 6]])\nT = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])"}
{"task_id": "BigCodeBench/441", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(P, T):\n    # Check if P and T are numpy arrays\n    if not isinstance(P, np.ndarray) or not isinstance(T, np.ndarray):\n        raise ValueError(\"P and T must be numpy arrays\")\n\n    # Check if P and T have the correct shape\n    if P.shape[0] != T.shape[0]:\n        raise ValueError(\"P and T must have the same first dimension\")\n\n    # Calculate the product of P and T\n    result = np.einsum('ij,ijkl->ijkl', P, T)\n\n    # Create a 3D visualization\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result[0,:,0], result[0,:,1], result[0,:,2])\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    plt.show()\n\n    return result, ax\nP = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nT = np.array([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[2, 0, 0], [0, 2, 0], [0, 0, 2]]])"}
{"task_id": "BigCodeBench/442", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    # Check if P and T are numpy arrays\n    if not isinstance(P, np.ndarray) or not isinstance(T, np.ndarray):\n        raise ValueError(\"P and T should be numpy arrays\")\n\n    # Check if P and T have the same shape\n    if P.shape != T.shape:\n        raise ValueError(\"P and T should have the same shape\")\n\n    # Check if tensor_shape is a valid shape for T\n    if not np.array_equal(P.shape, tensor_shape):\n        raise ValueError(\"tensor_shape should be a valid shape for T\")\n\n    # Calculate the product of P and T\n    PT = np.tensordot(P, T, axes=0)\n\n    # Apply PCA to reduce the dimensionality of the result\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(PT)\n\n    # Plot the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n    plt.show()\n\n    return pca_result, ax"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(P: np.ndarray, T: np.ndarray, n_clusters: int = 3, random_state: int = 0, n_init: int = 10) -> (np.ndarray, plt.Axes):\n    # Calculate the product of matrix P and tensor T\n    PT = np.dot(P, T)\n\n    # Flatten the result\n    PT_flat = PT.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(PT_flat)\n\n    # Get the result of KMeans clustering\n    cluster_result = kmeans.labels_\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Plot the KMeans clustering\n    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2], s=250, c='red', marker='o')\n    ax.scatter(PT_flat[cluster_result == 0], PT_flat[cluster_result == 0], color='blue', s=50, marker='o')\n    ax.scatter(PT_flat[cluster_result == 1], PT_flat[cluster_result == 1], color='green', s=50, marker='o')\n    ax.scatter(PT_flat[cluster_result == 2], PT_flat[cluster_result == 2], color='yellow', s=50, marker='o')\n\n    # Set the title of the plot\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax"}
{"task_id": "BigCodeBench/444", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(n_points=100, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    points = np.random.uniform(0, 1, (n_points, 3))\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n    plt.show()\n    return points, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"Input points should be a numpy array\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input points should be a 2D array with 2 columns\")\n    if not isinstance(seed, (int, np.integer)):\n        raise TypeError(\"Seed should be an integer\")\n\n    np.random.seed(seed)\n    points = points + np.random.normal(size=points.shape) * 0.05\n    vor = Voronoi(points)\n    fig, ax = voronoi_plot_2d(vor, ax=None, show_vertices=False, cmap='Spectral', edgecolor='k')\n\n    return vor, ax\npoints = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])"}
{"task_id": "BigCodeBench/446", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    # Generate isotropic Gaussian blobs\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c=y)\n    ax.set_title('Isotropic Gaussian Blobs')\n    plt.show()\n\n    return X, y, ax"}
{"task_id": "BigCodeBench/447", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, n_components=2, random_state=None):\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    if n_components == 1:\n        fig, ax = plt.subplots()\n        ax.scatter(transformed_data[:, 0], np.zeros(len(transformed_data)))\n    else:\n        fig, ax = plt.subplots()\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n\n    plt.show()\n\n    return {\"transformed_data\": transformed_data, \"ax\": ax}"}
{"task_id": "BigCodeBench/448", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.figure(figsize=(10, 6))\n    ax = plt.subplot()\n    ax.plot(x, norm.pdf(x, mu, sigma))\n    ax.set_title(f'Normal Distribution with Mean {mu} and Standard Deviation {sigma}')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Probability Density')\n    return ax"}
{"task_id": "BigCodeBench/449", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a list to store the axes objects\n    axes_list = []\n\n    # Loop through each feature\n    for column in standardized_data.columns:\n        # Create a histogram for each feature\n        ax = standardized_data[column].plot.hist(bins=20)\n        axes_list.append(ax)\n\n    return standardized_data, axes_list"}
{"task_id": "BigCodeBench/450", "solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate a synthetic 2D dataset\n    X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Visualize the dataset\n    if plot_path is not None:\n        plt.figure(figsize=(10, 6))\n        plt.scatter(X[:, 0], X[:, 1], c=y)\n        plt.savefig(plot_path)\n        plt.close()\n        return None\n    else:\n        plt.show()\n        return X, y\nn_samples = 200\ncenters = 4\nrandom_seed = 42\nplot_path = 'test_plot.png'"}
{"task_id": "BigCodeBench/451", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    # Generate a high-dimensional dataset\n    np.random.seed(random_seed)\n    X = np.random.randn(N_SAMPLES, N_FEATURES)\n\n    # Run PCA to reduce its dimensionality\n    pca = PCA(n_components=n_components)\n    X_pca = pca.fit_transform(X)\n\n    # Draw a heatmap of the covariance matrix of the transformed data\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(X_pca.cov(), annot=True, cmap='coolwarm')\n    heatmap_axes = plt.gca()\n\n    return X_pca, heatmap_axes"}
{"task_id": "BigCodeBench/452", "solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y = datasets.make_regression(n_samples, n_features, noise=0.1, random_state=random_seed)\n\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n\n    # Fit a linear regression model to the data\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Return the predicted values along with the coefficients and intercept of the model\n    predictions = model.predict(X_test)\n    coefficients = model.coef_\n    intercept = model.intercept_\n    mse = np.mean((predictions - y_test)**2)\n\n    return (predictions, coefficients, intercept, mse)\nn_samples = 100\nn_features = 10\nrandom_seed = 42"}
{"task_id": "BigCodeBench/453", "solution": "import re\nimport string\nfrom random import choice\ndef task_func(n, pattern):\n    while True:\n        random_string = ''.join(choice(string.ascii_letters) for _ in range(n))\n        if re.match(pattern, random_string):\n            return f'str: {random_string}'"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source directory exists\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(\"Source directory does not exist\")\n\n    # Check if destination directory exists\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(\"Destination directory does not exist\")\n\n    # Get all files with the specified extension\n    files = glob.glob(f'{src_dir}/*.{ext}')\n\n    # Initialize an empty list to store the full paths of the moved files\n    moved_files = []\n\n    # Iterate over each file\n    for file in files:\n        # Get the base name of the file (without the extension)\n        base_name = os.path.basename(file)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.isfile(os.path.join(dest_dir, base_name)):\n            # Move the file to the destination directory\n            shutil.move(file, dest_dir)\n\n            # Add the full path of the moved file to the list\n            moved_files.append(os.path.join(dest_dir, base_name))\n\n    # Return the list of moved files\n    return moved_files"}
{"task_id": "BigCodeBench/455", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, n):\n    samples = stats.norm.rvs(loc=mean, scale=std_dev, size=n)\n    return samples"}
{"task_id": "BigCodeBench/456", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(normalized_data, cmap='YlGnBu')\n    plt.title('Normalized Data Heatmap')\n    plt.xlabel('Normalized Value')\n    plt.ylabel('Data Point')\n\n    return normalized_data, ax\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [6, 7, 8, 9, 10],\n    'C': [11, 12, 13, 14, 15]\n})"}
{"task_id": "BigCodeBench/457", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    if not isinstance(L, list):\n        raise TypeError(\"Input should be a list of lists of integers\")\n    for sublist in L:\n        if not isinstance(sublist, list):\n            raise TypeError(\"All elements in the list should be lists of integers\")\n        for element in sublist:\n            if not isinstance(element, int):\n                raise TypeError(\"All elements in the sublists should be integers\")\n    df = pd.DataFrame(L)\n    ax = df.hist(rwidth=0.8)\n    return ax"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    if not json_str:\n        return pd.DataFrame()\n\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    def normalize_value(value):\n        if isinstance(value, list):\n            return [normalize_value(v) * 2 for v in value]\n        elif isinstance(value, str):\n            match = re.match(r'^\\d+\\.?\\d*$', value)\n            if match:\n                return float(value) * 2\n        return value\n\n    normalized_data = {k: normalize_value(v) for k, v in data.items()}\n    return pd.DataFrame(normalized_data)\njson_str = '{\"a\": 1, \"b\": [1, 2, 3], \"c\": \"4.5\", \"d\": \"hello\"}'"}
{"task_id": "BigCodeBench/459", "solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\ndef task_func(script_dir, scripts, delay):\n    if not scripts or delay < 0:\n        raise ValueError(\"If the delay is negative or no scripts are provided.\")\n\n    start_times = []\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        start_time = datetime.now()\n        subprocess.run([\"bash\", script_path], check=True)\n        elapsed_time = datetime.now() - start_time\n        print(f\"Script {script} executed in {elapsed_time}\")\n        start_times.append(elapsed_time)\n        if delay > 0:\n            time.sleep(delay)\n\n    return start_times"}
{"task_id": "BigCodeBench/460", "solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(script_path, output_file_path):\n    try:\n        # Execute the script and capture the output\n        output = subprocess.check_output(script_path, shell=True).decode('utf-8')\n    except subprocess.CalledProcessError as e:\n        raise ValueError(\"Script failed to execute. Error: {}\".format(e))\n\n    # Check if the output is valid CSV\n    try:\n        df = pd.read_csv(output_file_path)\n    except pd.errors.ParserError:\n        raise ValueError(\"The produced CSV is invalid.\")\n\n    # Check if the CSV contains exactly 2 columns\n    if df.shape[1] != 2:\n        raise ValueError(\"The produced CSV does not contain exactly 2 columns.\")\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x=df.columns[0], y=df.columns[1], ax=ax)\n\n    return df, ax"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        return {'Error': 'Script not found'}\n\n    start_time = time.time()\n    cpu_usage = 0.0\n    memory_usage = 0\n\n    while True:\n        try:\n            process = subprocess.run([\"bash\", script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n        except subprocess.TimeoutExpired:\n            break\n        except Exception as e:\n            return {'Error': f'Execution error: {str(e)}'}\n\n        cpu_percent = process.cpu_percent(interval=1)\n        cpu_usage += cpu_percent\n\n        memory_info = psutil.Process(process.pid).memory_info()\n        memory_usage += memory_info.rss\n\n        if process.returncode != 0:\n            return {'Error': 'Script exited with non-zero status'}\n\n    elapsed_time = time.time() - start_time\n\n    cpu_usage /= elapsed_time\n    memory_usage /= elapsed_time\n\n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }"}
{"task_id": "BigCodeBench/462", "solution": "import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be greater than 0\")\n\n    random.seed(random_seed)\n\n    df = pd.DataFrame({\n        \"category\": [random.choice(categories) for _ in range(num_rows)],\n        \"value\": [random.randint(1, 100) for _ in range(num_rows)]\n    })\n\n    df['category'].value_counts().plot(kind='bar', title='Category Counts')\n    plt.show()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/463", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_str, separator=\",\", bins=20):\n    # Check if data is empty\n    if not data_str:\n        raise ValueError(\"Data is empty\")\n    \n    # Convert string to pandas series\n    try:\n        series = pd.Series(list(map(int, data_str.split(separator))))\n    except ValueError:\n        raise ValueError(\"Failed to convert data\")\n    \n    # Convert pandas series to int64\n    series = series.astype(np.int64)\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(series, bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n    \n    return series, ax"}
{"task_id": "BigCodeBench/464", "solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\ndef task_func(my_obj):\n    class MyEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            elif isinstance(obj, Decimal):\n                return float(obj)\n            else:\n                return super().default(obj)\n\n    return json.dumps(my_obj, cls=MyEncoder)"}
{"task_id": "BigCodeBench/465", "solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            return super().default(obj)\ndef task_func(my_obj):\n    try:\n        json_str = json.dumps(my_obj, cls=CustomEncoder)\n        return json_str\n    except TypeError as e:\n        raise TypeError(\"TypeError: An object of an unsupported type is encountered that cannot be serialized by both the custom and default JSON encoders.\") from e"}
{"task_id": "BigCodeBench/466", "solution": "import json\nfrom enum import Enum\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n    BLUE = 3\ndef task_func(my_obj):\n    class ColorEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, Color):\n                return obj.name\n            return json.JSONEncoder.default(self, obj)\n\n    return json.dumps(my_obj, cls=ColorEncoder)"}
{"task_id": "BigCodeBench/467", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    x = np.random.rand(n)\n    y = np.random.rand(n)\n    points = list(zip(x, y))\n\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(111)\n    ax.scatter(x, y)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Scatter plot of random points')\n\n    return fig, points"}
{"task_id": "BigCodeBench/468", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Read a CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Convert numeric values into floats\n    df[columns] = df[columns].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n\n    # Compute the cube-root of the data\n    cube_root_series = df[columns].apply(lambda x: np.cbrt(x))\n\n    # Draw a line chart of data in the specified columns\n    fig, ax = plt.subplots()\n    for column in columns:\n        ax.plot(df[column], label=column)\n    ax.legend()\n    plt.show()\n\n    return df, ax, cube_root_series"}
{"task_id": "BigCodeBench/469", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Filter out the grades that are not in the possible grades\n    filtered_grades = [grade for grade in student_grades if grade.upper() in possible_grades]\n\n    # Count the frequency of each grade\n    grade_counts = Counter(filtered_grades)\n\n    # Create a DataFrame from the counts\n    df = pd.DataFrame(list(grade_counts.items()), columns=['Grade', 'Count'])\n\n    # Sort the DataFrame by the grade\n    df.sort_values('Grade', inplace=True)\n\n    # Plot the grade distribution\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='Grade', y='Count', ax=ax)\n    ax.set_title('Grade Distribution')\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n\n    return df, ax"}
{"task_id": "BigCodeBench/470", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(myList):\n    # Bin edges are adjusted to align with integer values in myList\n    bins = np.arange(min(myList), max(myList)+1)\n\n    # Histogram bars are outlined in black\n    plt.hist(myList, bins, color='black')\n\n    # X-axis label: 'Value'\n    plt.xlabel('Value')\n\n    # Y-axis label: 'Frequency'\n    plt.ylabel('Frequency')\n\n    # Plot title: 'Histogram of Values'\n    plt.title('Histogram of Values')\n\n    # Return the Axes object of the plot\n    ax = plt.gca()\n    return ax"}
{"task_id": "BigCodeBench/471", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(myList):\n    # Count the frequency of each word in the list\n    word_counts = Counter(myList)\n\n    # Convert the Counter object to a pandas DataFrame\n    df = pd.DataFrame.from_dict(word_counts, orient='index', columns=['Count'])\n\n    # Reset the index of the DataFrame\n    df.reset_index(inplace=True)\n\n    # Rename the index column\n    df.columns = ['Word', 'Count']\n\n    return df"}
{"task_id": "BigCodeBench/472", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\ndef task_func(myList, n_clusters):\n    # Check if the input is a list\n    if not isinstance(myList, list):\n        raise ValueError(\"Input should be a list of 2D points\")\n\n    # Check if the list contains only 2D points\n    for point in myList:\n        if not isinstance(point, list) or len(point) != 2:\n            raise ValueError(\"All elements in the list should be 2D points\")\n\n    # Convert the list of 2D points to a numpy array\n    myList = np.array(myList)\n\n    # Initialize the KMeans model\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n\n    # Fit the model to the data\n    kmeans.fit(myList)\n\n    # Create a scatter plot\n    plt.scatter(myList[:, 0], myList[:, 1], c=kmeans.labels_)\n\n    # Plot the cluster centers in red\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x')\n\n    # Return the Axes object\n    return plt.gca()"}
{"task_id": "BigCodeBench/473", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(n_walks, n_steps, seed=None):\n    # Check if n_walks and n_steps are valid\n    if not isinstance(n_walks, int) or n_walks <= 0:\n        raise ValueError(\"n_walks must be a positive integer\")\n    if not isinstance(n_steps, int) or n_steps <= 0:\n        raise ValueError(\"n_steps must be a positive integer\")\n\n    # Set the seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Generate the random walks\n    walks = [np.random.choice([-1, 1], size=(n_steps,)) for _ in range(n_walks)]\n\n    # Plot the random walks\n    fig, ax = plt.subplots()\n    colors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n    for walk in walks:\n        ax.plot(walk, color=next(colors))\n\n    ax.set_xlabel(\"Steps\")\n    ax.set_ylabel(\"Position\")\n    ax.set_title(f\"{n_walks} random walks with {n_steps} steps\")\n\n    return ax"}
{"task_id": "BigCodeBench/474", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n\n    fig, ax = plt.subplots(1, 1)\n\n    ax.hist(samples, density=True, alpha=0.5, color='g', label='Histogram')\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'r', linewidth=2, label='PDF')\n\n    ax.legend()\n    plt.show()\n\n    return ax, samples"}
{"task_id": "BigCodeBench/475", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, date_format, country, country_codes=None):\n    # Check if 'data' is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a DataFrame\")\n\n    # Check if 'date_format' is a string\n    if not isinstance(date_format, str):\n        raise ValueError(\"'date_format' must be a string\")\n\n    # Check if 'country' is in 'country_codes'\n    if country not in country_codes.keys():\n        raise ValueError(\"'country' must be in 'country_codes'\")\n\n    # Check if 'country_codes' is a dictionary\n    if not isinstance(country_codes, dict):\n        raise ValueError(\"'country_codes' must be a dictionary\")\n\n    # Convert the date column to the specified format\n    data['date'] = pd.to_datetime(data['date'], format=date_format)\n\n    # Create a histogram of the date column\n    ax = data['date'].plot(kind='hist', rwidth=0.8)\n\n    # Set the title and labels\n    ax.set_title('Date Distribution')\n    ax.set_ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/476", "solution": "import matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\ndef task_func(X, Y):\n    def quadratic(X, a, b, c):\n        return a*X**2 + b*X + c\n\n    popt, pcov = curve_fit(quadratic, X, Y)\n\n    plt.scatter(X, Y)\n    plt.plot(X, quadratic(X, *popt), 'r')\n    plt.show()\n\n    return popt, plt.gca()\nX = [1, 2, 3, 4, 5]\nY = [2.5, 3.6, 4.9, 6.1, 7.4]"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random values for x and y\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n\n    # Generate random categories\n    categories = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        \"x\": x,\n        \"y\": y,\n        \"category\": categories\n    })\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df[\"x\"], df[\"y\"], c=df[\"category\"])\n\n    # Return the DataFrame and Axes object\n    return df, ax"}
{"task_id": "BigCodeBench/478", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(data_list, seed=None):\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a list to store the original and modified strings\n    original_strings = []\n    modified_strings = []\n\n    # Loop over the data list\n    for string in data_list:\n        # Remove a random comma-separated value\n        modified_string = re.sub(r',.*$', '', string)\n\n        # Add the original and modified strings to the lists\n        original_strings.append(string)\n        modified_strings.append(modified_string)\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Modified String': modified_strings\n    })\n\n    return df"}
{"task_id": "BigCodeBench/479", "solution": "import random\nimport string\nimport pandas as pd\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    random_string = ''.join(random.choice(string.ascii_lowercase) for _ in range(len(data_list[0])))\n    df = pd.DataFrame(data_list, columns=['Original String'])\n    df['Modified String'] = df['Original String'].apply(lambda x: x.replace(x[random.randint(0, len(x)-2):random.randint(0, len(x)-2)], random_string))\n    return df\ndata_list = ['Hello, world!', 'Goodbye, world!', 'Python, is, awesome']"}
{"task_id": "BigCodeBench/480", "solution": "import re\nimport random\nimport pandas as pd\ndef task_func(data_list, seed=None):\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Initialize an empty list to store the shuffled strings\n    shuffled_strings = []\n\n    # Iterate over each string in the list\n    for string in data_list:\n        # Remove leading and trailing whitespaces and split the string into substrings\n        substrings = re.findall(r'\\S+', string.strip())\n\n        # Shuffle the substrings\n        random.shuffle(substrings)\n\n        # Join the shuffled substrings back into a string\n        shuffled_string = ' '.join(substrings)\n\n        # Append the original string and shuffled string to the list\n        shuffled_strings.append((string, shuffled_string))\n\n    # Convert the list of tuples to a pandas DataFrame\n    df = pd.DataFrame(shuffled_strings, columns=['Original String', 'Shuffled String'])\n\n    return df"}
{"task_id": "BigCodeBench/481", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=42):\n    # Set the seed for randomness\n    random.seed(seed)\n\n    # Create a DataFrame with the original and randomized strings\n    df = pd.DataFrame(data_list, columns=['Original String'])\n\n    # Normalize spaces and replace commas with a space\n    df['Normalized String'] = df['Original String'].str.replace(' ', '')\n    df['Normalized String'] = df['Normalized String'].str.replace(',', ' ')\n\n    # Randomize the order of the substrings\n    df['Randomized String'] = df['Normalized String'].apply(lambda x: ' '.join(random.sample(x.split(), len(x.split()))))\n\n    return df\ndata_list = ['This is a test string', 'Another test string', 'Yet another test string']"}
{"task_id": "BigCodeBench/482", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=None):\n    random.seed(seed)\n    operations = ['remove', 'replace', 'shuffle', 'randomize']\n    df = pd.DataFrame(data_list, columns=['Original String'])\n\n    for i, row in df.iterrows():\n        original_str = row['Original String']\n        substrings = re.findall(r'\"([^\"]*)\"|\\'([^\\']*)\\'', original_str)\n        substrings = [s[0] if len(s) == 1 else s for s in substrings]\n\n        if len(substrings) > 1:\n            operation = random.choice(operations)\n            if operation == 'remove':\n                random_substring = random.choice(substrings)\n                modified_str = original_str.replace(random_substring, '')\n            elif operation == 'replace':\n                random_substring = random.choice(substrings)\n                random_string = ''.join(random.choice([c for c in random_substring if c.isalpha()]) for _ in range(5))\n                modified_str = original_str.replace(random_substring, random_string)\n            elif operation == 'shuffle':\n                random.shuffle(substrings)\n                modified_str = ' '.join(substrings)\n            else:  # operation == 'randomize'\n                random.shuffle(substrings)\n                modified_str = ' '.join(substrings)\n                random.shuffle(substrings)\n                modified_str = ' '.join(substrings)\n        else:\n            modified_str = original_str\n\n        df.loc[i, 'Modified String'] = modified_str\n\n    return df"}
{"task_id": "BigCodeBench/483", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    if not pattern:\n        return df\n\n    def reverse_words(words):\n        return ' '.join(reversed(words.split()))\n\n    df[column_name] = df[column_name].apply(lambda x: reverse_words(x) if re.search(pattern, x) else x)\n    return df"}
{"task_id": "BigCodeBench/484", "solution": "import math\nimport numpy as np\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    # Set the random seed\n    np.random.seed(random_seed)\n\n    # Generate timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate sensor readings\n    df = pd.DataFrame(index=timestamps)\n    df[columns[1:]] = np.array([[math.sin(t.timestamp()) + np.random.normal() for t in timestamps] for _ in range(3)])\n\n    # Assign sensor statuses\n    df['SensorStatus'] = np.random.choice(sensor_statuses, size=len(df))\n\n    return df\ndf = task_func(\n    start_time=pd.Timestamp('2022-01-01'),\n    end_time=pd.Timestamp('2022-01-02'),\n    step='1h',\n)"}
{"task_id": "BigCodeBench/485", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time):\n    # Define the time zones\n    time_zones = {\n        \"UTC\": pytz.utc,\n        \"America/Los_Angeles\": pytz.timezone(\"US/Pacific\"),\n        \"Europe/Paris\": pytz.timezone(\"Europe/Paris\"),\n        \"Asia/Kolkata\": pytz.timezone(\"Asia/Kolkata\"),\n        \"Australia/Sydney\": pytz.timezone(\"Australia/Sydney\")\n    }\n\n    # Create a list to store the time difference in hours\n    time_differences = []\n\n    # Loop over the date range\n    current_date = start_time\n    while current_date <= end_time:\n        # Get the time difference in hours between UTC and each time zone\n        for time_zone_name, time_zone in time_zones.items():\n            if time_zone_name != \"UTC\":\n                time_difference = (current_date.astimezone(pytz.utc) - current_date.astimezone(time_zone)).total_seconds() / 3600\n                time_differences.append(time_difference)\n\n        # Move to the next day\n        current_date += timedelta(days=1)\n\n    # Plot the time differences\n    plt.figure(figsize=(10, 6))\n    for i, (time_zone_name, time_zone) in enumerate(time_zones.items()):\n        if time_zone_name != \"UTC\":\n            plt.plot(range(len(time_differences)), time_differences, label=time_zone_name, color=f\"C{i}\")\n\n    plt.title(\"Hourly Difference Between UTC and Time Zones\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Time Difference (Hours)\")\n    plt.legend()\n    plt.grid(True)\n\n    return plt.gca()\nstart_time = datetime(2022, 1, 1)\nend_time = datetime(2022, 1, 31)"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate a time series\n    time_series = pd.Series(np.random.normal(loc=0, scale=1, size=(end_time-start_time).days))\n\n    # Add a linear trend\n    time_series += trend * time_series.index\n\n    # Convert the time series to datetime\n    time_series.index = pd.date_range(start=start_time, periods=len(time_series), freq='D')\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n\n    # Plot the time series\n    time_series.plot(ax=ax)\n\n    # Set the labels and title\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n\n    # Show the plot\n    plt.show()\n\n    # Return the Axes object\n    return ax\nstart_time = datetime(2022, 1, 1)\nend_time = datetime(2022, 1, 31)\nstep = timedelta(days=1)\ntrend = 0.5"}
{"task_id": "BigCodeBench/487", "solution": "import os\nimport pandas as pd\nimport re\ndef task_func(file_path: str) -> pd.DataFrame:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The specified log file {file_path} does not exist.\")\n\n    df = pd.DataFrame(columns=['Timestamp', 'Level', 'Message'])\n\n    with open(file_path, 'r') as f:\n        for line in f:\n            match = re.match(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{6}) - (.*) - (.*)', line)\n            if match:\n                timestamp, level, message = match.groups()\n                df = df.append({'Timestamp': timestamp, 'Level': level, 'Message': message}, ignore_index=True)\n\n    return df"}
{"task_id": "BigCodeBench/488", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(start_time, end_time, step, amplitude, period, seed=0):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate a time series with a given seasonality\n    time_series = pd.Series(np.random.normal(0, amplitude, int((end_time - start_time).seconds / step) + 1))\n\n    # Add a seasonality\n    time_series = time_series.add(np.sin(np.linspace(start_time.timestamp(), end_time.timestamp(), int((end_time - start_time).seconds / step) + 1)) * period)\n\n    # Convert the timestamp to datetime\n    time_series.index = pd.to_datetime(time_series.index, unit='s')\n\n    # Plot the time series with the seasonality\n    fig, ax = plt.subplots()\n    time_series.plot(ax=ax)\n    ax.set_xlabel('Timestamp')\n    ax.set_ylabel('Value')\n    plt.show()\n\n    return ax\nstart_time = datetime.utcnow()\nend_time = start_time + timedelta(hours=1)\nstep = 3600\namplitude = 100\nperiod = 1"}
{"task_id": "BigCodeBench/489", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(epoch_milliseconds, seed=0):\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Define the predefined list of users and activities\n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n\n    # Convert the epoch time to a datetime object\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    # Check if the start time is after the current system time\n    if start_time > datetime.now():\n        raise ValueError(\"The start time is after the current system time.\")\n\n    # Initialize an empty list to store the logs\n    logs = []\n\n    # Iterate from the start time to the current system time\n    while start_time <= datetime.now():\n        # Choose a random user and activity\n        user = random.choice(users)\n        activity = random.choice(activities)\n\n        # Generate a timestamp for the log entry\n        time = start_time + timedelta(seconds=random.randint(1, 10))\n\n        # Add the log entry to the list\n        logs.append({'User': user, 'Activity': activity, 'Time': time})\n\n        # Increment the start time by a random number of seconds\n        start_time += timedelta(seconds=random.randint(1, 10))\n\n    # Convert the logs list to a DataFrame\n    df = pd.DataFrame(logs)\n\n    return df"}
{"task_id": "BigCodeBench/490", "solution": "import xmltodict\nimport json\ndef task_func(s, file_path):\n    # Converts XML string into a dictionary\n    dict_data = xmltodict.parse(s)\n\n    # Saves the dictionary as a JSON file\n    with open(file_path, 'w') as f:\n        json.dump(dict_data, f)\n\n    return dict_data"}
{"task_id": "BigCodeBench/491", "solution": "import random\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(epoch_milliseconds, seed=None):\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Set the start date\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    # Check if the start time is negative or after the current time\n    if start_date > datetime.utcnow():\n        raise ValueError(\"The start time is negative or after the current time.\")\n\n    # Define the categories\n    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n\n    # Generate sales data for each category for the next 30 days\n    sales_data = {}\n    for category in categories:\n        sales_data[category] = [random.randint(10, 50) for _ in range(30)]\n\n    # Convert the dates to UTC\n    dates = [(start_date + timedelta(days=i)).replace(tzinfo='utc') for i in range(30)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(sales_data, index=dates)\n\n    # Plot the sales trend\n    ax = df.plot(kind='line', title='Sales Trend')\n\n    return sales_data, ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Check if epoch_milliseconds is a valid datetime\n    try:\n        epoch_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    except ValueError:\n        return \"Invalid epoch time\"\n\n    # Set the random seed\n    random.seed(random_seed)\n\n    # Generate a list of dates between the epoch time and the current time\n    date_range = pd.date_range(start=epoch_time, end=datetime.now())\n\n    # Initialize an empty DataFrame\n    sales_data = pd.DataFrame(columns=['Product', 'Date', 'Sales'])\n\n    # For each product and date in the date range\n    for product in products:\n        for date in date_range:\n            # Generate a random sales quantity between 10 and 50\n            sales = random.randint(10, 50)\n            # Append a row to the DataFrame\n            sales_data = sales_data.append({'Product': product, 'Date': date, 'Sales': sales}, ignore_index=True)\n\n    return sales_data"}
{"task_id": "BigCodeBench/493", "solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    # Set the random seed\n    random.seed(random_seed)\n\n    # Convert epoch milliseconds to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n\n    # Initialize the performance data\n    performance_data = {}\n\n    # Loop over each team\n    for team in teams:\n        # Initialize the list for the team's performance\n        team_performance = []\n\n        # Loop over each day from the start date to the current date\n        current_date = start_date\n        while current_date <= datetime.now():\n            # Generate a random performance between 0.1 and 1\n            performance = random.uniform(0.1, 1.0)\n\n            # Add the performance to the list\n            team_performance.append(performance)\n\n            # Move to the next day\n            current_date += timedelta(days=1)\n\n        # Add the team's performance data to the overall performance data\n        performance_data[team] = team_performance\n\n    # Create a figure to show the performance trend\n    fig, ax = plt.subplots()\n\n    # Loop over each team\n    for team, performances in performance_data.items():\n        # Plot the performance data\n        ax.plot(range(len(performances)), performances, label=team)\n\n    # Set the labels and title\n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Performance')\n    ax.set_title('Performance Trend Over Time')\n\n    # Show the legend\n    ax.legend()\n\n    # Show the figure\n    plt.show()\n\n    # Return the performance data and figure\n    return performance_data, fig"}
{"task_id": "BigCodeBench/494", "solution": "from datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\nimport random\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Create a Faker instance\n    fake = Faker()\n\n    # Create a dictionary to store the event details\n    event_details = {}\n\n    # Check if the epoch_milliseconds is a valid integer\n    if not isinstance(epoch_milliseconds, int):\n        raise ValueError(\"epoch_milliseconds must be an integer\")\n\n    # Convert the epoch_milliseconds to a datetime object\n    dt_object = datetime.fromtimestamp(epoch_milliseconds / 1000.0, pytz.utc)\n\n    # Generate a fake event name\n    event_name = fake.bs()\n\n    # Check if the timezones are valid\n    valid_timezones = [tz for tz in pytz.all_timezones if re.match(r'^(UTC|[A-Za-z]+/[A-Za-z]+)$', tz)]\n    if not timezones or not any(tz in valid_timezones for tz in timezones):\n        # If no valid timezones were specified or none were provided, use UTC\n        timezone = \"UTC\"\n    else:\n        # Otherwise, select a valid timezone from the provided list\n        timezone = random.choice(timezones)\n\n    # Add the event details to the dictionary\n    event_details['date'] = dt_object.date()\n    event_details['time'] = dt_object.time()\n    event_details['timezone'] = timezone\n\n    # Return the dictionary\n    return {event_name: event_details}"}
{"task_id": "BigCodeBench/495", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(days):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Create a date range for the specified number of days\n    date_range = pd.date_range(start='2023-01-01', periods=days)\n\n    # Create a DataFrame with columns for Groceries, Entertainment, Rent, Utilities, and Miscellaneous\n    df = pd.DataFrame(index=date_range)\n\n    # Fill the DataFrame with random values for each day\n    df['Groceries'] = np.random.randint(0, 100, size=days)\n    df['Entertainment'] = np.random.randint(0, 100, size=days)\n    df['Rent'] = np.random.randint(0, 100, size=days)\n    df['Utilities'] = np.random.randint(0, 100, size=days)\n    df['Miscellaneous'] = np.random.randint(0, 100, size=days)\n\n    return df"}
{"task_id": "BigCodeBench/496", "solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be greater than 0\")\n\n    np.random.seed(random_seed)\n    temperatures = np.random.randint(15, 35, days_in_past)\n    dates = [(datetime.now() - timedelta(days=i)).date() for i in range(days_in_past)]\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(dates, temperatures)\n    plt.title('Temperature Trend')\n    plt.xlabel('Date')\n    plt.ylabel('Temperature (\u00b0C)')\n\n    ax = plt.gca()\n    ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n    ax.xaxis.set_major_formatter(plt.DateFormatter('%Y-%m-%d'))\n\n    plt.tight_layout()\n    plt.show()\n\n    return ax"}
{"task_id": "BigCodeBench/497", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\ndef task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"days_in_past should be a non-negative number\")\n\n    today = datetime.now(pytz.utc)\n    target_date = today - timedelta(days=days_in_past)\n\n    weekday = calendar.day_name[target_date.weekday()]\n\n    return {\n        \"weekday (str)\": weekday\n    }"}
{"task_id": "BigCodeBench/498", "solution": "import xmltodict\nimport json\ndef task_func(s, save_json=False, json_file_path=None):\n    if not s or s.isspace():\n        raise ValueError(\"Input XML string is empty or contains only whitespace\")\n\n    parsed_xml = xmltodict.parse(s)\n\n    if save_json:\n        with open(json_file_path, 'w') as f:\n            json.dump(parsed_xml, f)\n\n    return parsed_xml"}
{"task_id": "BigCodeBench/499", "solution": "import xlwt\nimport os\nimport io\nimport csv\ndef task_func(csv_content, filename):\n    # Convert CSV content into a list of lists\n    csv_data = [row.split(',') for row in csv_content.split('\\n')]\n\n    # Create a new Excel workbook\n    workbook = xlwt.Workbook()\n\n    # Create a new worksheet\n    worksheet = workbook.add_sheet('Sheet1')\n\n    # Write the data into the worksheet\n    for i, row in enumerate(csv_data):\n        for j, cell in enumerate(row):\n            worksheet.write(i, j, cell)\n\n    # Save the workbook as an Excel file\n    workbook.save(filename)\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)\ncsv_content = 'Hello\\nWorld'"}
{"task_id": "BigCodeBench/500", "solution": "import xlwt\nimport os\nimport collections\nFIELDS = ['ID', 'Name', 'Age']\ndef task_func(values, filename):\n    # Create a new Excel workbook and add a new sheet.\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Write the column headers to the first row.\n    for i, field in enumerate(FIELDS):\n        sheet.write(0, i, field)\n\n    # Write the data to the Excel file.\n    for i, row_data in enumerate(values, start=1):\n        for j, field in enumerate(FIELDS):\n            sheet.write(i, j, row_data[field])\n\n    # Save the workbook to the specified filename.\n    workbook.save(filename)\n\n    # Return the absolute path of the created Excel file.\n    return os.path.abspath(filename)\nempty_data = []"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if json_str is a string\n    if not isinstance(json_str, str):\n        raise TypeError(\"json_str should be a string\")\n\n    # Check if json_str is a valid JSON\n    try:\n        json.loads(json_str)\n    except ValueError:\n        raise ValueError(\"json_str is not a valid JSON\")\n\n    # Convert JSON to pandas DataFrame\n    df = pd.read_json(json_str)\n\n    # If DataFrame is empty, create an empty Excel file\n    if df.empty:\n        excel_writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n        df.to_excel(excel_writer, sheet_name=sheet_name, index=False)\n        excel_writer.save()\n        return os.path.abspath(filename)\n\n    # If DataFrame is not empty, create a new Excel file\n    excel_writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n    df.to_excel(excel_writer, sheet_name=sheet_name, index=False)\n    excel_writer.save()\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed\n    random.seed(random_seed)\n\n    # Define the activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate a list of random durations for each activity\n    durations = [random.randint(0, 120) for _ in activities]\n\n    # Generate a list of random dates for each activity\n    dates = [(datetime.now() - timedelta(days=i)).date() for i in range(days_in_past)]\n\n    # Combine the lists into a DataFrame\n    df = pd.DataFrame({\n        \"Date\": dates,\n        \"Activity\": activities,\n        \"Duration\": durations\n    })\n\n    # Convert the Date column to datetime\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n    # Sort the DataFrame by Date\n    df = df.sort_values(\"Date\")\n\n    # Create a line plot\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n\n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Create a DataFrame with stock prices for the specified number of days\n    dates = pd.date_range(start=datetime.now() - pd.Timedelta(days=days_in_past), periods=days_in_past)\n    df = pd.DataFrame(index=dates, columns=stock_names)\n\n    for stock in stock_names:\n        df[stock] = np.random.rand(days_in_past)\n\n    return df"}
{"task_id": "BigCodeBench/504", "solution": "import hashlib\nimport rsa\nimport base64\ndef task_func(file_path):\n    # Read the file\n    with open(file_path, 'rb') as f:\n        file_contents = f.read()\n\n    # Hash the file contents\n    sha256 = hashlib.sha256(file_contents)\n\n    # Sign the hash with the private key\n    with open('private.pem', 'rb') as key_file:\n        private_key = rsa.PrivateKey.load_pkcs1(key_file.read())\n    signature = rsa.sign(sha256, private_key, 'SHA-256')\n\n    # Encode the signature in base64\n    base64_signature = base64.b64encode(signature)\n\n    return base64_signature"}
{"task_id": "BigCodeBench/505", "solution": "import hashlib\nimport hmac\ndef task_func(secret, message):\n    # Convert the secret key to bytes\n    secret_key = secret.encode()\n\n    # Create a new HMAC object with the secret key and the hashlib.sha256 function\n    hmac_obj = hmac.new(secret_key, msg=message.encode(), digestmod=hashlib.sha256)\n\n    # Return the HMAC as a hexadecimal string\n    return hmac_obj.hexdigest()"}
{"task_id": "BigCodeBench/506", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import BarContainer\ndef task_func(column, data):\n    if not data:\n        return {\n            'sum': np.nan,\n            'mean': np.nan,\n            'min': np.inf,\n            'max': -np.inf,\n            'plot': None\n        }\n\n    column_data = [row[column] for row in data]\n\n    result = {\n        'sum': sum(column_data),\n        'mean': np.mean(column_data),\n        'min': min(column_data),\n        'max': max(column_data),\n        'plot': None\n    }\n\n    plt.hist(column_data, bins='auto')\n    result['plot'] = plt.gcf()\n\n    return result"}
{"task_id": "BigCodeBench/507", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(column, data):\n    if isinstance(data, pd.DataFrame):\n        if column in data.columns:\n            column_data = data[column]\n            sum_data = column_data.sum()\n            mean_data = column_data.mean()\n            min_data = column_data.min()\n            max_data = column_data.max()\n            result = {\n                'sum': sum_data,\n                'mean': mean_data,\n                'min': min_data,\n                'max': max_data\n            }\n            return result\n        else:\n            raise ValueError(\"The specified column name is not valid.\")\n    else:\n        raise ValueError(\"The input data is not a DataFrame.\")\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})"}
{"task_id": "BigCodeBench/508", "solution": "import hashlib\nimport io\nimport os\ndef task_func(file_path1, file_path2):\n    try:\n        with open(file_path1, 'rb') as f:\n            file1_md5 = hashlib.md5(f.read()).hexdigest()\n\n        with open(file_path2, 'rb') as f:\n            file2_md5 = hashlib.md5(f.read()).hexdigest()\n\n        return file1_md5 == file2_md5\n\n    except FileNotFoundError:\n        print(f\"FileNotFoundError: Either {file_path1} or {file_path2} does not exist.\")\n        return False"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as f1, open(file_path2, 'r') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n\n            lines1 = [row for row in reader1]\n            lines2 = [row for row in reader2]\n\n            if not lines1 and not lines2:\n                raise ValueError(\"Both files are empty\")\n\n            diff = list(zip(range(1, len(lines1) + 1), range(1, len(lines2) + 1), ndiff(lines1, lines2)))\n\n            df = pd.DataFrame(diff, columns=['Line Number', 'Status', 'Content'])\n\n            return df\n\n    except FileNotFoundError:\n        print(f\"File {file_path1} or {file_path2} not found\")\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")"}
