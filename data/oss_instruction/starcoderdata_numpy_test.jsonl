{"content": "\"\"\"Dataset for whole slide images from TCGA data\"\"\"\nfrom pathlib import Path\nimport os\nimport h5py\nimport numpy as np\nimport os\nimport pickle\nimport random\nimport torch.utils.data as data\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport util\nimport staintools\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nfrom .base_dataset import BaseDataset\nfrom .label_mapper import TASK_SEQUENCES\nfrom .constants import COL_TCGA_SLIDE_ID, COL_TCGA_FILE_ID,\\\n        COL_TCGA_FILE_NAME, COL_TCGA_CASE_ID, COL_TCGA_LABEL,\\\n        COL_TCGA_PATCH_ID, COL_TCGA_NUM_PATCHES, COL_TCGA_INDICES,\\\n        COL_TCGA_PATH, COL_TCGA_ENTITIES, SLIDE_METADATA_FILE,\\\n        SLIDE_PKL_FILE, DEFAULT_PATCH_SIZE, TCGA_MEAN, TCGA_STD \n\n# TODO: implement flag to turn dataset into toy dataset\nclass TCGA_Google_Dataset(BaseDataset):\n    \"\"\"Dataset for TCGA classification.\"\"\"\n\n    def __init__(self, data_path, transform_args, metadata_csv,\n                 split='test', num_classes=2,\n                 resize_shape=(DEFAULT_PATCH_SIZE, DEFAULT_PATCH_SIZE),\n                 max_patches=None, tasks_to='tcga',\n                 is_training=False, filtered=True, toy=False, \n                 normalize=False, transform=None):\n        \"\"\"Initialize TCGADataset.\n\n        data directory to be organized as follows:\n            data_path\n                XXX.jpg\n                XXX.png\n                ...\n                XXX.jpg\n                metadata_dataset_name.csv\n\n        Args:\n            data_path (str): path to data directory\n            transform_args (args): arguments to transform data\n            metadata_csv (str): path to csv containing metadata information of the dataset\n            split (str): either \"train\", \"valid\", or \"test\"\n            num_classes (int): number of unique labels\n            resize_shape (tuple): shape to resize the inputs to\n            max_patches (int): max number of patches to obtain for each slide\n            tasks_to (str): corresponds to a task sequence\n            is_training (bool): whether the model in in training mode or not\n            filtered (bool): whether to filter the images\n        \"\"\"\n        #if split not in [\"train\", \"valid\", \"test\"]:\n            #raise ValueError(\"Invalid value for split. Must specify train, valid, or test.\")\n\n        super().__init__(data_path, transform_args,\n                         split, is_training, 'tcga', tasks_to)\n        print(\"split:\" + split)\n\n        self.data_path = data_path\n\n        #self.hdf5_path = os.path.join(self.data_path, \"{}.hdf5\".format(split))\n        #self.hdf5_fh = h5py.File(self.hdf5_path, \"r\")\n\n        self.split = split\n        self.is_training = is_training\n        self.dataset_name = 'tcga_google'\n        print(\"dataset: \" + self.data_path.split(\"/\")[-2])\n        self.metadata_path = os.path.join(self.data_dir, \"metadata \" + self.data_path.split(\"/\")[-2] + \".csv\")\n#        print(self.data_dir)\n        self.metadata = pd.read_csv(self.metadata_path)\n        self.toy = True\n        self.filtered = filtered \n        self.num_classes = num_classes\n        \n        print(\"number of patches in this test set:\" + str(len(self.metadata)))\n        print(self.metadata[\"label\"].value_counts())\n\n        self.label_dict = self._get_label_dict(tasks_to)\n\n        #self._set_class_weights(self.labels)\n        self.normalize = normalize\n\n        # tools for patch normalization\n        self.normalizer_with_constants = transforms.Compose([transforms.Normalize(mean = TCGA_MEAN, std = TCGA_STD)])\n        self.ToTensor = transforms.Compose([transforms.ToTensor()])\n\n \n    def __len__(self):\n        return len(self.metadata)\n\n    def _get_label_dict(self, tasks_to):\n        \"\"\"Return appropriate label dict for task\"\"\"\n        return tasks_to\n\n    def _label_conversion(self, label):\n        \"\"\"Turn string label into integer\"\"\"\n        if label == 'Cholangio':\n            label = 'CHOL'\n        if label not in self.label_dict:\n            raise ValueError(\"Invalid label: {} entered\".format(label))\n        return self.label_dict[label]\n\n    def transform_label(self, label):\n        \"\"\"Make label correct shape\"\"\"\n        label = np.array(label).astype(np.float32).reshape(1)\n        return label\n    \n    def get_patch(self, patch_name):\n        if patch_name.split(\".\")[-1] == 'png':\n            patch = Image.open(patch_name).convert('RGB')\n        else:\n            patch = Image.open(patch_name)\n        return patch\n  \n    def normalize_patch_with_constants(self, patch):\n        \"\"\" Normalize using pre-calculated data-specific constants\"\"\"\n        patch = self.normalizer_with_constants(patch)\n        return patch        \n        \n      \n    def __getitem__(self, idx):\n        \"\"\"Return element of dataset\"\"\"\n        patch_name = os.path.join(self.data_path, self.metadata.loc[idx, \"file_name\"])\n        label = self.metadata.loc[idx, \"label\"]\n        label = self._label_conversion(label)\n        label = self.transform_label(label)\n        label = torch.tensor(label, dtype=torch.float32)\n        patch = self.get_patch(patch_name)\n        # reize and convert to Tensor\n        patch = transforms.Compose([transforms.Resize([DEFAULT_PATCH_SIZE, DEFAULT_PATCH_SIZE]), transforms.ToTensor()])(patch)\n\n        if self.normalize:\n            patch = self.normalize_patch_with_constants(patch)\n        \n        patch = patch.numpy()\n        patch = patch.astype(np.float32)\n        info_dict = {'patch_name': self.metadata.loc[idx, \"file_name\"]} \n        return patch, label, info_dict\n\n"}
{"content": "# Copyright (c) 2020, Huawei Technologies.All rights reserved.\n#\n# Licensed under the BSD 3-Clause License  (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://opensource.org/licenses/BSD-3-Clause\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom torch.cuda import device\nimport torch\nimport numpy as np\nimport copy\nimport sys\nfrom common_utils import TestCase, run_tests\nfrom common_device_type import dtypes, instantiate_device_type_tests\nfrom util_test import create_common_tensor\n\n\nclass TestMaskedFillRange(TestCase):\n    def cpu_op_exec(self, input1, start, end, value, axis, dim):\n        out = input1.clone()\n        start_shape = start.shape\n        if dim == 1:\n            for i in range(0, start_shape[0]):\n                for j in range(0, start_shape[1]):\n                    for k in range(start[i, j], end[i, j]):\n                        out[k] = value[i]\n        if dim == 2:\n            for i in range(0, start_shape[0]):\n                for j in range(0, start_shape[1]):\n                    for k in range(start[i, j], end[i, j]):\n                        if axis == 0:\n                            out[k, :] = value[i]\n                        else:\n                            out[j, k] = value[i]\n        if dim == 3:\n            for i in range(0, start_shape[0]):\n                for j in range(0, start_shape[1]):\n                    for k in range(start[i, j], end[i, j]):\n                        if axis == 0:\n                            out[k, :, :] = value[i]\n                        elif axis == 1:\n                            out[:, k, :] = value[i]\n                        else:\n                            out[j, :, k] = value[i]\n        return out\n\n    def npu_op_exec(self, input1, start, end, value, axis):\n        out = torch.npu_masked_fill_range(input1, start, end, value, axis)\n        out = out.to(\"cpu\")\n        return out.detach().numpy()\n\n    def test_normalize_batch(self, device):\n        # TODO(ascend): \u8be5\u7b97\u5b50\u8fd8\u5b58\u5728\u6cdb\u5316\u95ee\u9898\uff0c \u76ee\u524d\u4fdd\u8bc1\u6a21\u578b\u573a\u666f\u6ca1\u95ee\u9898\n        # Note: \u4ee5\u4e0b\u4e3a\u6a21\u578b\u7528\u4f8b\uff1a\u6d4b\u8bd5\u901a\u8fc7\n        shape_format = [\n            [[np.float32, -1, [32, 64, 1688]], \n                [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n                    14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n                [[6, 7, 31, 9, 10, 11, 12, 19, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, \n                    26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]], [[1], torch.float32], 2],\n            [[np.float16, -1, [32, 64, 1688]], \n                [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n                    14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n                [[6, 7, 31, 9, 10, 11, 12, 19, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, \n                    26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]], [[1], torch.float16], 2],\n            [[np.int32, -1, [32, 64, 1688]], \n                [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n                    14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n                [[6, 7, 31, 9, 10, 11, 12, 19, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, \n                    26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]], [[1], torch.int32], 2],\n            [[np.int8, -1, [32, 64, 1688]], \n                [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n                    14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n                [[6, 7, 31, 9, 10, 11, 12, 19, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, \n                    26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]], [[1], torch.int8], 2],\n        ]\n        for item in shape_format:\n            axis = item[-1]\n            cpu_input1, npu_input1 = create_common_tensor(item[0], 1, 100)\n            shape = item[0][-1]\n            cpu_start = torch.tensor(item[1], dtype=torch.int32)\n            npu_start = cpu_start.npu()\n            cpu_end = torch.tensor(item[2], dtype=torch.int32)\n            npu_end = cpu_end.npu()\n            cpu_value = torch.tensor(item[3][0], dtype=item[3][1])\n            npu_value = cpu_value.npu()\n            cpu_output = self.cpu_op_exec(cpu_input1, cpu_start, cpu_end, cpu_value, axis, len(shape))\n            npu_output = self.npu_op_exec(npu_input1, npu_start, npu_end, npu_value, axis)\n            cpu_output = cpu_output.numpy()\n            cpu_output = cpu_output.astype(npu_output.dtype)\n            self.assertRtolEqual(cpu_output, npu_output)\n\ninstantiate_device_type_tests(TestMaskedFillRange, globals(), except_for='cpu')\nif __name__ == \"__main__\":\n    run_tests()\n"}
{"content": "# Copyright 2019 <NAME>\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"1-dimensional uniform binary tree tensor network.\n\nIndex ordering conventions:\n```\niso_012:\n\n  0\n  |\n(iso)\n / \\\n1   2\n```\n\niso_021:\n```\n  0\n  |\n(iso)\n / \\\n2   1\n```\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nimport copy\nimport math\nimport time\nimport contextlib\nimport tensornetwork\n\n\ndef _ascend_partial(op, iso):\n  \"\"\"Contract an operator with the rightmost index of an isometry.\n\n  For 012 (021) index ordering, this is equivalent to contracting with the\n  physical right (left). This is \"half\" of the operation needed to ascend\n  an operator via the isometry.\n  To complete, use `_complete_partial_ascend()`.\n\n  Cost: D^4.\n\n  Args:\n    op: The operator to ascend (a matrix). Dimensions must match the\n      dimensions of the lower indices of the isometry.\n    iso: The isometry (a rank-3 tensor).\n\n  Returns:\n    The result of contracting `op` with `iso`.\n  \"\"\"\n  return backend.ncon([iso, op], [(-1, -2, 1), (-3, 1)])\n\n\ndef _complete_partial_ascend(iso_op, iso):\n  \"\"\"Complete a partial operator ascension performed by `_ascend_partial()`.\n\n  This contracts with the conjugated isometry.\n\n  Cost: D^4.\n\n  Args:\n    iso_op: Operator contracted with the isometry (result of \n      `_ascend_partial()`).\n    iso: The isometry (a rank-3 tensor).\n\n  Returns:\n    The ascended operator.\n  \"\"\"\n  return backend.ncon([backend.conj(iso), iso_op], [(-1, 1, 2), (-2, 1, 2)])\n\n\ndef _ascend_op_2site_to_1site_partial(mpo_2site, iso_021):\n  \"\"\"Contract a 2-site MPO with a single isometry.\n\n  Produces an ascended (1-site) operator after completion via\n  `_complete_partial_ascend()`.\n\n  Cost: D^4.\n\n  Args:\n    mpo_2site: The 2-site MPO consisting of two lists of the same length (the\n      MPO bond dimension), one for each site, of 1-site operators.\n    iso_021: The isometry (a rank-3 tensor) with \"021\" ordering.\n\n  Returns:\n    The result of contracting the operator with the isometry.\n  \"\"\"\n  op2L, op2R = mpo_2site\n\n  M = len(op2L)  # MPO bond dimension\n\n  terms = []\n  for m in range(M):\n    # permute result to 012 order: M mild transposes\n    iso_op_mpo_L_012 = backend.ncon([iso_021, op2L[m]], [(-1, -3, 1),\n                                                               (-2, 1)])\n\n    terms.append(_ascend_partial(op2R[m], iso_op_mpo_L_012))\n  iso_op_2site_012 = sum(terms)\n\n  return iso_op_2site_012\n\n\ndef _ascend_uniform_op_to_1site_partial(op_1site, mpo_2site, iso_012, iso_021):\n  \"\"\"Contract a uniform 2-site operator with a single isometry.\n\n  A \"uniform 2-site operator\" means an operator that is a sum of of two equal\n  1-site terms and a single 2-site MPO term:\n    \"op = op_1site(0) + op_1site(1) + mpo_2site\"\n\n  Produces an ascended (1-site) operator after completion via\n  `_complete_partial_ascend()`.\n\n  Cost: D^4.\n\n  Args:\n    op_1site: The 1-site term.\n    mpo_2site: The 2-site MPO term.\n    iso_021: The isometry (a rank-3 tensor) with \"021\" ordering.\n\n  Returns:\n    res_012: The result of contracting the operator with the isometry,\n      012 ordering.\n    res_021: The result of contracting the operator with the isometry,\n      021 ordering.\n  \"\"\"\n  iso_op_2site_012 = _ascend_op_2site_to_1site_partial(mpo_2site, iso_021)\n  iso_op_1site_R_012 = _ascend_partial(op_1site, iso_012)\n  iso_op_1site_L_021 = _ascend_partial(op_1site, iso_021)\n  return iso_op_2site_012 + iso_op_1site_R_012, iso_op_1site_L_021\n\n\ndef ascend_op_1site_to_1site_R(op_1site, iso_012):\n  \"\"\"Ascends a 1-site operator from the right of an isometry.\n\n  Note: If called with an isometry using \"021\" ordering, this ascends from\n    the left instead.\n\n  Args:\n    op_1site: The 1-site operator (a matrix).\n    iso_012: The isometry (a rank-3 tensor).\n\n  Returns:\n    The ascended operator.\n  \"\"\"\n  return _complete_partial_ascend(_ascend_partial(op_1site, iso_012), iso_012)\n\n\ndef ascend_op_1site_to_1site_L(op_1site, iso_012):\n  \"\"\"Ascends a 1-site operator from the left of an isometry.\n\n  Args:\n    op_1site: The 1-site operator (a matrix).\n    iso_012: The isometry (a rank-3 tensor).\n\n  Returns:\n    The ascended operator.\n  \"\"\"\n  return ascend_op_1site_to_1site_R(\n    op_1site, backend.transpose(iso_012, (0,2,1)))\n\n\ndef ascend_uniform_op_to_1site(op_1site, mpo_2site, iso_012, iso_021):\n  \"\"\"Ascends a uniform 2-site operator through an isometry.\n\n  A \"uniform 2-site operator\" means an operator that is a sum of of two equal\n  1-site terms and a single 2-site MPO term:\n    \"op = op_1site(0) + op_1site(1) + mpo_2site\"\n\n  Args:\n    op_1site: The 1-site term.\n    mpo_2site: The 2-site MPO term.\n    iso_012: The isometry (a rank-3 tensor) with \"012\" ordering.\n    iso_021: The isometry (a rank-3 tensor) with \"021\" ordering.\n\n  Returns:\n    The ascended operator.\n  \"\"\"\n  terms_012, iso_op_1site_L_021 = _ascend_uniform_op_to_1site_partial(\n      op_1site, mpo_2site, iso_012, iso_021)\n\n  res = _complete_partial_ascend(iso_op_1site_L_021, iso_021)\n  res += _complete_partial_ascend(terms_012, iso_012)\n\n  return res\n\n\ndef ascend_op_2site_to_1site(mpo_2site, iso_012, iso_021):\n  \"\"\"Ascends a 2-site MPO through a single isometry.\n\n  Args:\n    mpo_2site: The 2-site MPO.\n    iso_012: The isometry (a rank-3 tensor) with \"012\" ordering.\n    iso_021: The isometry (a rank-3 tensor) with \"021\" ordering.\n\n  Returns:\n    The ascended operator, now a 1-site operator.\n  \"\"\"\n  iso_op_2site_012 = _ascend_op_2site_to_1site_partial(mpo_2site, iso_021)\n  return _complete_partial_ascend(iso_op_2site_012, iso_012)\n\n\ndef ascend_op_2site_to_2site(mpo_2site, iso_012, iso_021):\n  \"\"\"Ascends a 2-site MPO through a pair of isometries.\n\n  Given a pair of neighboring isometries, each with two lower indices,\n  ascends a 2-site MPO through the middle two indices to form a new 2-site\n  MPO.\n\n  Args:\n    mpo_2site: The 2-site MPO.\n    iso_012: The isometry (a rank-3 tensor) with \"012\" ordering.\n    iso_021: The isometry (a rank-3 tensor) with \"021\" ordering.\n\n  Returns:\n    The ascended operator, a 2-site MPO.\n  \"\"\"\n  def _ascend(op, iso, iso_conj):\n    return backend.ncon([iso_conj, op, iso], [(-1, 3, 1), (1, 2),\n                                                    (-2, 3, 2)])\n\n  op2L, op2R = mpo_2site\n\n  M = len(op2L)\n\n  iso_021_conj = backend.conj(iso_021)\n  op_asc_R = []\n  for m in range(M):\n    op_asc_R.append(_ascend(op2R[m], iso_021, iso_021_conj))\n\n  iso_012_conj = backend.conj(iso_012)\n  op_asc_L = []\n  for m in range(M):\n    op_asc_L.append(_ascend(op2L[m], iso_012, iso_012_conj))\n\n  return op_asc_L, op_asc_R\n\n\ndef ascend_uniform_op_local(op_1site, mpo_2site, iso_012, iso_021):\n  \"\"\"Ascends a globally uniform operator through a periodic layer of isometries.\n\n  The operator is assumed to consist of a sum of equal 1-site terms, the same\n  on every site, plus a sum of 2-site MPOs, also the same for each pair of\n  neighboring sites.\n\n  This is ascended though a uniform layer of isometries to produce an\n  ascended of the same form.\n\n  It is assumed that the layer of isometries consists of more than isometry.\n  If this is not the case, use `ascend_uniform_op_local_top()`.\n\n  Args:\n    op_1site: The 1-site term.\n    mpo_2site: The 2-site MPO term.\n    iso_012: The isometry (a rank-3 tensor) with \"012\" ordering.\n    iso_021: The isometry (a rank-3 tensor) with \"021\" ordering.\n\n  Returns:\n    op_1site: The 1-site component of the ascended operator.\n    op_2site: The 2-site MPO component of the ascended operator.\n  \"\"\"\n  op_1site = ascend_uniform_op_to_1site(op_1site, mpo_2site, iso_012, iso_021)\n  mpo_2site = ascend_op_2site_to_2site(mpo_2site, iso_012, iso_021)\n  return op_1site, mpo_2site\n\n\ndef ascend_uniform_op_local_top(op_1site, mpo_2site, iso_012, iso_021):\n  \"\"\"Ascends a globally uniform operator through the top tensor of a tree.\n\n  See `ascend_uniform_op_local()`. This ascends a globally uniform operator\n  through a periodic layer of isometries consisting of only a single isometry\n  as occurs at the top of a tree tensor network.\n\n  The result is a 1-site operator.\n\n  Args:\n    op_1site: The 1-site term.\n    mpo_2site: The 2-site MPO term.\n    iso_012: The isometry (a rank-3 tensor) with \"012\" ordering.\n    iso_021: The isometry (a rank-3 tensor) with \"021\" ordering.\n\n  Returns:\n    The ascended operator, a 1-site operator.\n  \"\"\"\n  mpo_2site = add_mpos_2site(mpo_2site, reflect_mpo_2site(mpo_2site))\n  op_1site = ascend_uniform_op_to_1site(op_1site, mpo_2site, iso_012, iso_021)\n  return op_1site\n\n\ndef ascend_uniform_op_local_many(op_1site, mpo_2site, isos):\n  \"\"\"Ascends a globally uniform operator through many layers.\n\n  Returns intermediate results.\n  See `ascend_uniform_op_local()`.\n\n  Args:\n    op_1site: The 1-site term.\n    mpo_2site: The 2-site MPO term.\n    isos: List of isometries, each representing a uniform layer through which\n      the operator is to be ascended.\n\n  Returns:\n    A list of pairs of 1-site and MPO terms. Each entry `i` is the result of\n    ascending through the layers defined by `isos[:i+1]`.\n  \"\"\"\n  ops = []\n  for l in range(len(isos)):\n    op_1site, mpo_2site = ascend_uniform_op_local(op_1site, mpo_2site, *isos[l])\n    ops.append(op_1site, mpo_2site)\n  return ops\n\n\ndef ascend_uniform_MPO_to_top(mpo_tensor_dense, isos_012):\n  \"\"\"Ascends a globally uniform MPO to the top of a tree.\n\n  Unlike the 2-site MPOs used elsewhere, this takes a dense MPO tensor of\n  rank 4 with the following ordering convention:\n\n     3\n     |\n  0--m--1\n     |\n     2\n\n  The bottom and top indices are the \"left\" and \"right\" indices of 1-site\n  operators. The left and right indices are the MPO bond indices.\n\n  Args:\n    mpo_tensor_dense: The MPO tensor.\n    isos_012: List of isometries with 012 ordering, defining the tree.\n\n  Returns:\n    A 1-site operator acting on the top space of the tree.\n  \"\"\"\n  L = len(isos_012)\n  for l in range(L):\n    # NOTE: There is no attempt to be economical with transpose here!\n    mpo_tensor_dense = backend.ncon(\n        [isos_012[l],\n         backend.conj(isos_012[l]), mpo_tensor_dense, mpo_tensor_dense],\n        [(-4, 3, 1), (-3, 4, 5), (2, -2, 5, 1), (-1, 2, 4, 3)])\n  op = backend.ncon([mpo_tensor_dense], [(1, 1, -1, -2)])\n  return op\n\n\ndef descend_state_1site_R(state_1site, iso_012):\n  \"\"\"Descends a 1-site density matrix though a single isometry to the right.\n\n  Produces a 1-site density matrix on the right site by tracing out the left\n  site.\n\n  Cost: D^4.\n\n  Args:\n    state_1site: The 1-site density matrix.\n    iso_012: Isometry (rank-3 tensor) with 012 ordering.\n\n  Returns:\n    Descended 1-site density matrix.\n  \"\"\"\n  return backend.ncon(\n    [iso_012, state_1site, backend.conj(iso_012)],\n    [(2, 3, -1), (2, 1), (1, 3, -2)])\n\n\ndef descend_state_1site_L(state_1site, iso_021):\n  \"\"\"Descends a 1-site density matrix though a single isometry to the left.\n\n  Produces a 1-site density matrix on the left site by tracing out the right\n  site.\n\n  Cost: D^4.\n\n  Args:\n    state_1site: The 1-site density matrix.\n    iso_021: Isometry (rank-3 tensor) with 021 ordering.\n\n  Returns:\n    Descended 1-site density matrix.\n\"\"\"\n  return descend_state_1site_R(state_1site, iso_021)\n\n\ndef descend_state_1site(state_1site, iso_012, iso_021):\n  \"\"\"Average descended 1-site.\n\n  The average of `descend_state_1site_R()` and `descend_state_1site_L()`.\n\n  Cost: D^4.\n\n  Args:\n    state_1site: The 1-site density matrix.\n    iso_012: Isometry (rank-3 tensor) with 012 ordering.\n    iso_021: The same isometry, but with 021 ordering.\n\n  Returns:\n    Descended 1-site density matrix.\n  \"\"\"\n  state_1L = descend_state_1site_L(state_1site, iso_021)\n  state_1R = descend_state_1site_R(state_1site, iso_012)\n  return 0.5 * (state_1L + state_1R)\n\n\ndef correlations_2pt_1s(isos_012, op):\n  \"\"\"Computes a two-point correlation function for a 1-site operator `op`.\n\n  Args:\n    isos_012: List of isometries defining the uniform tree.\n    op: The 1-site operator (matrix).\n\n  Returns:\n    cf_transl_avg: Translation-averaged correlation function (as a vector).\n    cf_1: Partially translation-averaged correlation function (as a vector).\n  \"\"\"\n  if len(op.shape) != 2:\n    raise ValueError(\"Operator must be a matrix.\")\n  nsites = 2**len(isos_012)\n  states = backend.all_states_1site_graph(isos_012)\n  expval_sq = backend.trace(states[0] @ op)**2\n  twopoints = {}\n  asc_ops = {0: op}\n  for l in range(len(isos_012)):\n    iso_012 = isos_012[l]\n    iso_021 = backend.transpose(iso_012, (0,2,1))\n    # Compute all two-point functions available at this level\n    for (site1, asc_op1) in asc_ops.items():\n      for (site2, asc_op2) in asc_ops.items():\n        asc_op12 = ascend_op_2site_to_1site(\n          ([asc_op1], [asc_op2]),\n          iso_012,\n          iso_021)\n        site2 += 2**l\n        twopoints[(site1, site2)] = (\n          backend.trace(asc_op12 @ states[l+1]) - expval_sq)\n    if l < len(isos_012) - 1:\n      asc_ops_new = {}\n      for (site, asc_op) in asc_ops.items():\n        asc_opL = ascend_op_1site_to_1site_R(asc_op, iso_021)  # R is correct.\n        asc_opR = ascend_op_1site_to_1site_R(asc_op, iso_012)\n        asc_ops_new[site] = asc_opL\n        asc_ops_new[site + 2**l] = asc_opR\n      asc_ops = asc_ops_new\n  corr_func = {}\n  for ((site1, site2), val) in twopoints.items():\n    dist = abs(site1 - site2)\n    try:\n      corr_func[dist].append(val)\n    except KeyError:\n      corr_func[dist] = [val]\n  # Final translation averaging\n  for (dist, vals) in corr_func.items():\n    corr_func[dist] = sum(vals) / len(vals)\n  dists = sorted(corr_func.keys())\n  cf_transl_avg = backend.convert_to_tensor([corr_func[d] for d in dists])\n  cf_1 = backend.convert_to_tensor([twopoints[(0,i)] for i in range(1,nsites)])\n  return cf_transl_avg, cf_1\n\n\ndef reflect_mpo_2site(mpo_2site):\n  \"\"\"Spatial reflection of a 2-site MPO.\n  \"\"\"\n  return tuple(reversed(mpo_2site))\n\n\ndef add_mpos_2site(mpo1, mpo2):\n  \"\"\"Sum of two 2-site MPOs acting on the same pair of sites.\n  \"\"\"\n  return (mpo1[0] + mpo2[0], mpo1[1] + mpo2[1])\n\n\ndef opt_energy_env_2site(isos_012, h_mpo_2site, states_1site_above):\n  \"\"\"Computes 2-site Hamiltonian contributions to the isometry environment.\n\n  This always computes the environment contribution for the isometry in the\n  first entry of `isos_012`. To compute environments for higher levels in a\n  three, supply data for the truncated tree: For level `l` call with\n  `isos_012[l:]` and the corresponding hamiltonian and states.\n\n  Args:\n    isos_012: The isometries defining the tree tensor network.\n    h_mpo_2site: The 2-site term of the uniform Hamiltonian for the bottom\n      of the network defined in `isos_012`.\n    states_1site_above: 1-site translation-averaged density matrices for each\n      level above the bottom of the network defined in `isos_012`.\n\n  Returns:\n    Environment tensor (rank 3).\n  \"\"\"\n  def _ascend_op_2site_to_2site_many(mpo_2site, isos):\n    ops = []\n    for l in range(len(isos)):\n      mpo_2site = ascend_op_2site_to_2site(mpo_2site, *isos[l])\n      ops.append(mpo_2site)\n    return ops\n\n  def _mpo_with_state(iso_012, iso_021, h_mpo_2site, state_1site):\n    \"\"\"Contract a 2-site MPO with a 1-site descended state. O(D^4)\"\"\"\n    h2L, h2R = h_mpo_2site\n\n    envL = [\n        backend.ncon(\n            [state_1site, iso_021, h, backend.conj(iso_012)],\n            [(1, 3), (1, -1, 2), (4, 2), (3, 4, -2)])  # one transpose required\n        for h in h2L\n    ]\n\n    envR = [\n        backend.ncon(\n            [state_1site, iso_012, h, backend.conj(iso_021)],\n            [(1, 3), (1, -1, 2), (4, 2), (3, 4, -2)])  # one transpose required\n        for h in h2R\n    ]\n\n    return envL, envR\n\n  def _descend_energy_env_L(env, iso_021):\n    return [descend_state_1site_L(e, iso_021) for e in env]\n\n  def _descend_energy_env_R(env, iso_012):\n    return [descend_state_1site_R(e, iso_012) for e in env]\n\n  isos_wt = isos_with_transposes(isos_012)\n  iso_012, iso_021 = isos_wt[0]\n  isos_wt_above = isos_wt[1:]\n  levels_above = len(isos_wt_above)\n\n  # Ascend two-site Hamiltonian terms to the bottom of the final isometry\n  h2s_above = _ascend_op_2site_to_2site_many(h_mpo_2site, isos_wt)\n\n  # hamiltonian with isometry opposite the gap\n  h2L, h2R = h_mpo_2site\n  iso_h2R_012 = [\n      backend.ncon([iso_021, h], [(-1, -3, 1), (-2, 1)]) for h in h2R\n  ]  # transpose to 012\n  iso_h2L_012 = [\n      backend.ncon([iso_012, h], [(-1, -2, 1), (-3, 1)]) for h in h2L\n  ]\n\n  def _compute_env(lvl, reflect=False):\n    # TODO: Could shorten this a bit by doing only left or right at one time\n    h2 = h2s_above[lvl]\n    if reflect:\n      h2 = reflect_mpo_2site(h2)\n\n    envL, envR = _mpo_with_state(*isos_wt_above[lvl], h2,\n                                 states_1site_above[lvl])\n\n    # descend envs back down to the level of the gap\n    for lvl2 in reversed(range(lvl)):\n      iso_012_l2, iso_021_l2 = isos_wt_above[lvl2]\n      if reflect:\n        envR = _descend_energy_env_L(envR, iso_021_l2)\n        envL = _descend_energy_env_R(envL, iso_012_l2)\n      else:\n        envL = _descend_energy_env_L(envL, iso_021_l2)\n        envR = _descend_energy_env_R(envR, iso_012_l2)\n\n    if reflect:\n      iso_h2_L, iso_h2_R = iso_h2R_012, iso_h2L_012\n    else:\n      iso_h2_L, iso_h2_R = iso_h2L_012, iso_h2R_012\n\n    # contract with the hamiltonian + isometry opposite the gap\n    envL = sum(\n        backend.ncon([eL, ihR], [(1, -1), (1, -2, -3)])\n        for eL, ihR in zip(envL, iso_h2_R))\n\n    envR = sum(\n        backend.ncon([eR, ihL], [(1, -1), (1, -2, -3)])\n        for eR, ihL in zip(envR, iso_h2_L))\n\n    # weight each term according to the number of occurrences\n    # in the translation-invariant tree\n    weight = 1 / 2.0**(lvl + 1)\n    return (envL + envR) * weight, weight\n\n  weightsum = 0.0\n  env_total = []\n  for lvl in range(levels_above):\n    env, weight = _compute_env(lvl)\n    weightsum += weight\n    env_total.append(env)\n\n  # Now compute the boundary term\n  env, weight = _compute_env(levels_above - 1, reflect=True)\n  weightsum += weight\n  env_total.append(env)\n\n  env_total = sum(env_total)\n\n  assert weightsum == 1.0\n\n  return env_total\n\n\ndef opt_energy_env_1site(iso_012, h_op_1site, h_mpo_2site, state_1site):\n  \"\"\"Computes 1-site Hamiltonian contributions to the isometry environment.\n\n  Args:\n    iso_012: The isometry whose environment is desired.\n    h_op_1site: The 1-site term of the uniform Hamiltonian for the bottom\n      of the layer defined by the isometry.\n    h_mpo_2site: The 2-site term of the uniform Hamiltonian for the bottom\n      of the layer defined by the isometry.\n    state_1site: 1-site translation-averaged density matrix for the top of\n      the layer defined by the isometry.\n\n  Returns:\n    Environment tensor (rank 3).\n  \"\"\"\n  iso_021 = backend.transpose(iso_012, (0, 2, 1))\n  terms_012, terms_021 = _ascend_uniform_op_to_1site_partial(\n    h_op_1site, h_mpo_2site, iso_012, iso_021)\n  terms = terms_012 + backend.transpose(terms_021, (0, 2, 1))\n  env = backend.ncon([state_1site, terms], [(1, -1), (1, -2, -3)])\n  return env\n\n\ndef opt_energy_env(isos_012,\n                   h_op_1site,\n                   h_mpo_2site,\n                   states_1site_above):\n  \"\"\"Computes the isometry environment for the energy expectation value network.\n\n  This always computes the environment contribution for the isometry in the\n  first entry of `isos_012`. To compute environments for higher levels in a\n  three, supply data for the truncated tree: For level `l` call with\n  `isos_012[l:]` and the corresponding hamiltonian terms and states.\n\n  Args:\n    isos_012: The isometries defining the tree tensor network.\n    h_op_1site: The 1-site term of the uniform Hamiltonian for the bottom\n      of the network defined in `isos_012`.\n    h_mpo_2site: The 2-site term of the uniform Hamiltonian for the bottom\n      of the network defined in `isos_012`.\n    states_1site_above: 1-site translation-averaged density matrices for each\n      level above the bottom of the network defined in `isos_012`.\n    envsq_dtype: Used to specify a different dtype for the computation of the\n      squared environment, if used.\n\n  Returns:\n    Environment tensor (rank 3).\n  \"\"\"\n  if len(isos_012) == 1:  # top of tree\n    h_mpo_2site = add_mpos_2site(h_mpo_2site, reflect_mpo_2site(h_mpo_2site))\n    env = opt_energy_env_1site(isos_012[0], h_op_1site, h_mpo_2site,\n                               states_1site_above[0])\n  else:\n    env1 = opt_energy_env_1site(isos_012[0], h_op_1site, h_mpo_2site,\n                                states_1site_above[0])\n    env2 = opt_energy_env_2site(isos_012, h_mpo_2site, states_1site_above[1:])\n    env = env1 + env2\n\n  return env\n\n\ndef _uinv_decomp(X_sq, cutoff=0.0, decomp_mode=\"eigh\"):\n  \"\"\"Computes an \"inverse\" from the square of a rectangular matrix.\n  The matrix returned is the inverse up to a unitary transformation. So not\n  really an inverse at all.\n\n  Args:\n    X_sq: A positive Hermitian matrix (the square of rectangular matrix).\n    cutoff: Threshold for pseudo-inversion.\n    decomp_mode: Can be \"eigh\" of \"svd\". The former should be slightly faster.\n\n  Returns:\n    X_uinv: An \"inverse\" of the original rectangular matrix.\n    s: The singular values of the square root of X_sq.\n  \"\"\"\n  if decomp_mode == \"svd\":\n    # hermitian, positive matrix, so eigvals = singular values\n    e, v, _ = backend.svd(X_sq)\n  elif decomp_mode == \"eigh\":\n    e, v = backend.eigh(X_sq)\n    e = backend.to_real(e)  # The values here should be real anyway\n  else:\n    raise ValueError(\"Invalid decomp_mode: {}\".format(decomp_mode))\n\n  s = backend.sqrt(e)  # singular values of the square root of X_sq\n  # NOTE: Negative values are always due to precision problems.\n  # NOTE: Inaccuracies here mean the final tensor is not exactly isometric!\n  e_pinvsqrt = backend.where(e <= cutoff, backend.zeros_like(e), 1 / s)\n\n  e_pinvsqrt_mat = backend.diag(backend.cast(e_pinvsqrt, v.dtype))\n  X_uinv = backend.matmul(v @ e_pinvsqrt_mat, v, adjoint_b=True)\n  return X_uinv, s\n\n\ndef _iso_from_envsq_decomp(env,\n                  cutoff=0.0,\n                  decomp_mode=\"eigh\",\n                  decomp_device=None,\n                  envsq_dtype=None):\n  \"\"\"Computes a new optimal isometry from the square of the environment tensor.\n\n  The precision of the result is the square root of the working precision,\n  so the working precision for this operation can be specified separately via\n  the `envsq_dtype` argument. A different device may also be specified, in case\n  the current device does not support the required precision or operations.\n  \"\"\"\n  if envsq_dtype is not None:\n    env = backend.cast(env, envsq_dtype)\n  with backend.device(decomp_device):\n    env_sq = backend.ncon(\n      [env, backend.conj(env)], [(-1, 1, 2), (-2, 1, 2)])\n    env_uinv, s = _uinv_decomp(env_sq, cutoff, decomp_mode)\n    iso_012_new = backend.ncon([env_uinv, env], [(-1, 1), (1, -2, -3)])\n  if envsq_dtype is not None:\n    iso_012_new = backend.cast(iso_012_new, dtype)\n  return iso_012_new, s\n\n\ndef _energy_expval_env(isos_012, h_op_1site, h_mpo_2site, states_1site_above):\n  \"\"\"Computes the energy using the environments. For testing.\n  \"\"\"\n  if len(isos_012) == 1:  # top of tree\n    h_mpo_2site = add_mpos_2site(h_mpo_2site, reflect_mpo_2site(h_mpo_2site))\n    env = opt_energy_env_1site(isos_012[0], h_op_1site, h_mpo_2site,\n                               states_1site_above[0])\n  else:\n    env1 = opt_energy_env_1site(isos_012[0], h_op_1site, h_mpo_2site,\n                                states_1site_above[0])\n    env2 = opt_energy_env_2site(isos_012, h_mpo_2site, states_1site_above[1:])\n    env = env1 + env2 / 2\n    # NOTE: There are *two* environments for each Ham. term spanning two\n    #       isometries. To get the correct energy we must divide env2 by 2.\n  nsites = 2**(len(isos_012) - 1)\n  return backend.ncon([backend.conj(isos_012[0]), env], [(1, 2, 3),\n                                                          (1, 2, 3)]) * nsites\n\n\ndef _iso_from_svd(u, vh):\n  return backend.ncon([u, vh], [(-1, 1), (1, -2, -3)])\n\n\ndef _iso_from_svd_decomp(env, decomp_device=None):\n  \"\"\"Isometry update using SVD of environment.\n  \"\"\"\n  with backend.device(decomp_device):\n    env_r = backend.reshape(env, (env.shape[0], -1))\n    s, u, v = backend.svd(env_r)\n    vh = backend.adjoint(v)\n    vh = backend.reshape(vh, (vh.shape[0], env.shape[1], env.shape[2]))\n    iso_new = _iso_from_svd(u, vh)\n    return iso_new, s\n\n\ndef _iso_from_svd_decomp_scipy(env):\n  \"\"\"Isometry update using SVD of environment using scipy's SVD.\n  When scipy is built with the MKL, this is the MKL SVD, which currently \n  parallelizes better than TensorFlow's SVD on CPU.\n  \"\"\"\n  env = backend.to_numpy(env)\n  env_r = env.reshape((env.shape[0], -1))\n  u, s, vh = backend.svd_np(env_r, full_matrices=False)\n  u = backend.convert_to_tensor(u)\n  s = backend.convert_to_tensor(s)\n  vh = vh.reshape((vh.shape[0], env.shape[1], env.shape[2]))\n  vh = backend.convert_to_tensor(vh)\n  return u, s, vh\n\n\ndef opt_energy_layer_once(isos_012,\n                          h_op_1site,\n                          h_mpo_2site,\n                          states_1site_above,\n                          graphed=False,\n                          decomp_mode=\"svd_full_iso\",\n                          decomp_device=None,\n                          envsq_dtype=None,\n                          timing=False):\n  \"\"\"Updates a layer of the tree via a linearized energy optimization.\n\n  Args:\n    isos_012: The isometries for the tree, beginning at the layer to be\n      updated.\n    h_op_1site: The 1-site term of the uniform Hamiltonian for the bottom\n      of the network defined in `isos_012`.\n    h_mpo_2site: The 2-site term of the uniform Hamiltonian for the bottom\n      of the network defined in `isos_012`.\n    states_1site_above: 1-site translation-averaged density matrices for each\n      level above the bottom of the network defined in `isos_012`.\n    graphed: Whether to build computational graphs of certain groups of\n      operations. This can speed up computation, but may increase memory usage.\n    decomp_mode: The decomposition used to update the isometries.\n    decomp_device: Device on which to perform the decomposition.\n    envsq_dtype: Used to specify a different dtype for the computation of the\n      squared environment and its decomposition, if used.\n    timing: Whether to gather timing information (decomps vs. environments).\n\n  Returns:\n    iso_012_new: Updated isometry for the current layer.\n    s: Singular values of the environment.\n    t_env: Time spent computing the environment (only returned if timing is\n      True).\n    t_decomp: Time spent computing the decomposition (only returned if timing\n      is True).\n  \"\"\"\n  t0 = time.time()\n  if graphed:\n    env = backend.opt_energy_env_graph(\n        isos_012,\n        h_op_1site,\n        h_mpo_2site,\n        states_1site_above)\n  else:\n    env = opt_energy_env(\n        isos_012,\n        h_op_1site,\n        h_mpo_2site,\n        states_1site_above)\n\n  if timing and backend.executing_eagerly():\n    # Hack to ensure values on GPU are ready. Only works for TensorFlow.\n    backend.to_numpy(env_sq[0,0])\n\n  t_env = time.time() - t0\n\n  t0 = time.time()\n  if decomp_mode == \"svd_full_iso\":\n    if graphed:\n      iso_012_new, s = backend._iso_from_svd_decomp_graph(\n        env, decomp_device=decomp_device)\n    else:\n      iso_012_new, s = _iso_from_svd_decomp(env, decomp_device=decomp_device)\n  elif decomp_mode == \"svd_full_iso_scipy\":\n    u, s, vh = _iso_from_svd_decomp_scipy(env)\n    if graphed:\n      iso_012_new = backend._iso_from_svd_graph(u, vh)\n    else:\n      iso_012_new = _iso_from_svd(u, vh)\n  else:\n    if graphed:\n      iso_012_new, s = backend._iso_from_envsq_decomp_graph(\n          env,\n          decomp_mode=decomp_mode,\n          decomp_device=decomp_device,\n          envsq_dtype=envsq_dtype)\n    else:\n      iso_012_new, s = _iso_from_envsq_decomp(\n          env,\n          decomp_mode=decomp_mode,\n          decomp_device=decomp_device,\n          envsq_dtype=envsq_dtype)\n\n  if timing and backend.executing_eagerly():\n    # Hack to ensure values on GPU are ready. Only works for TensorFlow.\n    backend.to_numpy(iso_012_new[0,0,0])\n\n  t_decomp = time.time() - t0\n\n  if timing:\n    return iso_012_new, s, t_env, t_decomp\n  return iso_012_new, s\n\n\ndef opt_energy_layer(isos_012,\n                     h_op_1site,\n                     h_mpo_2site,\n                     states_1site_above,\n                     itr,\n                     graphed=False,\n                     graph_level=None,\n                     decomp_mode=\"eigh\",\n                     decomp_device=None,\n                     envsq_dtype=None,\n                     timing=False):\n  \"\"\"Updates a layer of tree by doing several linearized energy optimizations.\n\n  Args:\n    isos_012: The isometries for the tree, beginning at the layer to be updated.\n    h_op_1site: The 1-site term of the uniform Hamiltonian for the bottom\n      of the network defined in `isos_012`.\n    h_mpo_2site: The 2-site term of the uniform Hamiltonian for the bottom\n      of the network defined in `isos_012`.\n    states_1site_above: 1-site translation-averaged density matrices for each\n      level above the bottom of the network defined in `isos_012`.\n    itr: How many linearized updates to do.\n    graphed: Whether to build computational graphs of certain groups of\n      operations. This can speed up computation, but may increase memory usage.\n    graph_level: If \"sweep\", use a single graph for the entire linearized\n      update. Otherwise use separate graphs for decomp. and environment.\n    decomp_mode: The decomposition used to update the isometries.\n    decomp_device: Device on which to perform the decomposition.\n    envsq_dtype: Used to specify a different dtype for the computation of the\n      squared environment and its decomposition, if used.\n    timing: Whether to gather timing information (decomps vs. environments).\n\n  Returns:\n    iso_012: Updated isometry for the current layer.\n    s: Singular values of the environment (from the final iteration).\n    t_env: Average time spent computing the environment (only returned if\n      timing is True).\n    t_decomp: Average time spent computing the decomposition (only returned\n      if timing is True).\n  \"\"\"\n  shp = isos_012[0].shape\n  if shp[0] == shp[1] * shp[2]:  # unitary, nothing to optimise\n    return isos_012[0]\n\n  iso_012 = isos_012[0]\n  s = None\n  tes, tds = 0.0, 0.0\n  for _ in range(itr):\n    if graph_level == \"sweep\":\n      if timing:\n        raise ValueError(\"Timing data not available with graph_level 'sweep'\")\n      iso_012, s = backend.opt_energy_layer_once_graph(\n          isos_012,\n          h_op_1site,\n          h_mpo_2site,\n          states_1site_above,\n          graphed=False,\n          decomp_mode=decomp_mode,\n          decomp_device=decomp_device,\n          envsq_dtype=envsq_dtype,\n          timing=False)\n    else:\n      res = opt_energy_layer_once(\n          isos_012,\n          h_op_1site,\n          h_mpo_2site,\n          states_1site_above,\n          graphed=graphed,\n          decomp_mode=decomp_mode,\n          decomp_device=decomp_device,\n          envsq_dtype=envsq_dtype,\n          timing=timing)\n      iso_012, s = res[:2]\n      if timing:\n        te, td = res[2:]\n        tes += te\n        tds += td\n\n  if timing:\n    return iso_012, s, tes / itr, tds / itr\n  return iso_012, s\n\n\ndef all_states_1site(isos_012):\n  \"\"\"Compute 1-site reduced states for all levels of a tree tensor network.\n\n  Args:\n    isos_012: The isometries definiting the tree tensor network (bottom to top).\n\n  Returns:\n    states: L+1 1-site reduced states, where L is the number of layers in the\n      tree. Bottom to top ordering.\n  \"\"\"\n  states = [backend.eye(isos_012[-1].shape[0], dtype=isos_012[0][0].dtype)]\n  for l in reversed(range(len(isos_012))):\n    iso_021 = backend.transpose(isos_012[l], (0, 2, 1))\n    states.append(descend_state_1site(states[-1], isos_012[l], iso_021))\n  return states[::-1]\n\n\ndef entanglement_specs_1site(isos_012):\n  \"\"\"1-site entanglement spectra for all levels of a tree tensor network.\n\n  Here, \"entanglement spectrum\" means the spectrum of the reduced density\n  matrix (rather than the log of that spectrum).\n\n  Args:\n    isos_012: The isometries definiting the tree tensor network (bottom to top).\n\n  Returns:\n    specs: L 1-site entanglement spectra, where L is the number of layers in\n      the tree. Bottom to top ordering.\n  \"\"\"\n  specs = []\n  state = backend.eye(isos_012[-1].shape[0], dtype=isos_012[0][0].dtype)\n  for l in reversed(range(len(isos_012))):\n    iso_021 = backend.transpose(isos_012[l], (0, 2, 1))\n    state = descend_state_1site(state, isos_012[l], iso_021)\n    e = backend.eigvalsh(state)\n    e = backend.to_real(e)\n    specs.append(e)\n  return specs[::-1]\n\n\ndef entropies_from_specs(specs):\n  \"\"\"Compute entanglement entropies from a list of entanglement spectra.\n\n  Here, \"entanglement spectrum\" means the spectrum of the reduced density\n  matrix (rather than the log of that spectrum) and \"entanglement entropy\"\n  means the von Neumann entropy using base 2 for the logarithm.\n\n  Negative entries int he entanglement spectrum are treated as zeros.\n\n  Args:\n    specs: List of entanglement spectra.\n\n  Returns:\n    entropies: List of entanglement entropies.\n  \"\"\"\n  entropies = []\n  for spec in specs:\n    spec = backend.to_numpy(spec)\n    x = spec * backend.np.log2(spec)\n    x[backend.np.isnan(x)] = 0.0  # ignore zero or negative eigenvalues\n    S = -backend.np.sum(x)\n    entropies.append(S)\n  return entropies\n\n\ndef random_isometry_cheap(D1, D2, dtype, decomp_mode=\"eigh\"):\n  \"\"\"Generate a random isometric matrix of dimension D1 x D2 more cheaply.\n\n  This uses a decomposition of the square of a random matrix to generate an\n  isometry. Since the initial matrix is random, the singular values of its\n  square should not be small, and the halving of the precision due to the\n  squaring should not be cause significant violations of the isometry property.\n\n  We require D1 <= D2.\n\n  Args:\n    D1: Left dimension.\n    D2: Right dimension.\n    dtype: Element type.\n    decomp_mode: \"eigh\" or \"svd\".\n\n  Returns:\n    V: An isometry.\n  \"\"\"\n  if not D1 <= D2:\n    raise ValueError(\"The left dimension must be <= the right dimension.\")\n  A = backend.random_normal_mat(D1, D2, dtype)\n  A_inv, _ = _uinv_decomp(\n    backend.matmul(A, A, adjoint_b=True), decomp_mode=decomp_mode)\n  return A_inv @ A\n\n\ndef random_isometry(D1, D2, dtype):\n  \"\"\"Generate a random isometric matrix of dimension D1 x D2.\n\n  We require D1 <= D2.\n\n  Args:\n    D1: Left dimension.\n    D2: Right dimension.\n    dtype: Element type.\n\n  Returns:\n    V: An isometry.\n  \"\"\"\n  if not D1 <= D2:\n    raise ValueError(\"The left dimension must be <= the right dimension.\")\n  A = backend.random_normal_mat(D2, D1, dtype)\n  Q, R = backend.qr(A)\n  r = backend.diag_part(R)\n  L = backend.diag(r / backend.cast(backend.abvals(r), dtype))\n  return backend.transpose(Q @ L, (1,0))\n\n\ndef random_tree_tn_uniform(Ds, dtype, top_rank=1):\n  \"\"\"Generate a random tree tensor network.\n\n  Args:\n    Ds: List of bond dimensions, one for each layer in the tree. The first\n      entry is the \"physical dimension\".\n    dtype: Data dtype for the tensor elements.\n    top_rank: The top dimension of the tree. A value of 1 produces a pure\n      state. A value > 1 produces an equal mixture of normalized pure states.\n\n  Returns:\n    isos: List of random isometries defining the tree tensor network.\n  \"\"\"\n  num_layers = len(Ds)\n  Ds = Ds + [top_rank]\n  isos = []\n  for j in range(num_layers):\n    if Ds[j + 1] == Ds[j]**2:\n      iso = backend.eye(Ds[j + 1], dtype=dtype)\n    else:\n      iso = random_isometry(Ds[j + 1], Ds[j]**2, dtype)\n    iso = backend.reshape(iso, (Ds[j + 1], Ds[j], Ds[j]))\n    isos.append(iso)\n  return isos\n\n\ndef expand_bonds(isos, new_Ds, new_top_rank=None):\n  \"\"\"Expand the bond dimension of a tree tensor network.\n\n  Inserts random isometry pairs on the bonds of the tree as necessary to\n  increase the bond dimension as requested. The state represented is not\n  changed by this operation.\n\n  Args:\n    isos: List of isometries defining the tree.\n    Ds: List of bond dimensions, one for each layer in the tree. The first\n      entry is the \"physical dimension\".\n    new_top_rank: The top dimension of the tree. A value of 1 produces a pure\n      state. A value > 1 produces an equal mixture of normalized pure states.\n\n  Returns:\n    isos_new: List of isometries defining the expanded tree.\n  \"\"\"\n  old_Ds = [iso.shape[1] for iso in isos] + [isos[-1].shape[0]]\n\n  if new_top_rank is None:\n    new_top_rank = old_Ds[-1]\n  new_Ds = new_Ds + [new_top_rank]\n\n  if new_Ds[0] != old_Ds[0]:\n    raise ValueError(\"Bottom dimension expansion not supported!\")\n\n  isos_new = [iso for iso in isos]\n  for i in range(len(isos)):\n    # Absorb dimension-expanding isometries on indices as needed\n    if old_Ds[i + 1] != new_Ds[i + 1]:\n      v = random_isometry(old_Ds[i + 1], new_Ds[i + 1], isos_new[i].dtype)\n      isos_new[i] = backend.ncon([v, isos_new[i]], [(1, -1), (1, -2, -3)])\n      if i + 1 < len(isos):\n        isos_new[i + 1] = backend.ncon(\n            [backend.conj(v), backend.conj(v), isos_new[i + 1]],\n            [(1, -2), (2, -3), (-1, 1, 2)])\n  return isos_new\n\n\ndef random_herm(D, dtype):\n  \"\"\"Generate a random hermitian matrix of dimension D.\n\n  Symmetrizes a random matrix with entries drawn from a normal distribution.\n\n  Args:\n    D: The dimension.\n    dtype: Element type.\n\n  Returns:\n    A random hermitian matrix.\n  \"\"\"\n  h = backend.random_normal_mat(D, D, dtype)\n  return 0.5 * (h + backend.adjoint(h))\n\n\ndef check_iso(iso):\n  \"\"\"Test the isometry property of a tree tensor network tensor.\n\n  Args:\n    iso: The supposed isometry.\n\n  Returns:\n    The norm difference between the square of iso and the identity.\n  \"\"\"\n  sq = backend.ncon([iso, backend.conj(iso)], [(-1, 1, 2), (-2, 1, 2)])\n  return backend.norm(sq - backend.eye(sq.shape[0], dtype=sq.dtype))\n\n\ndef shift_ham(H, shift=\"auto\"):\n  \"\"\"Add an identity contribution to a Hamiltonian.\n\n  H -> H - shift * I\n\n  Args:\n    H: The local Hamiltonian term (2-tuple of 1-site contributions and 2-site\n      MPO).\n    shift: Amount by which to shift the spectrum downwards. If \"auto\", computes\n      the spectrum of the local term H and shifts so that all eigenvalues are\n      less than or equal to 0.\n\n  Returns:\n    The shifted Hamiltonian.\n  \"\"\"\n  h1, (h2L, h2R) = H\n  D = h1.shape[0]\n  dtype = h1.dtype\n\n  if shift == \"auto\":\n    e1 = backend.reduce_max(backend.to_real(backend.eigvalsh(h1)))\n\n    h2 = sum([\n        backend.ncon([hl, hr], [(-1, -3), (-2, -4)])\n        for (hl, hr) in zip(h2L, h2R)\n    ])\n    h2 = backend.reshape(h2, (D**2, D**2))\n    e2 = backend.reduce_max(backend.to_real(backend.eigvalsh(h2)))\n\n    shift = backend.cast(e1 + e2, dtype)\n\n  if shift != 0.0:\n    H = (h1 - shift * backend.eye(D, dtype=dtype), (h2L, h2R))\n\n  return H, shift\n\n\ndef _full_ham_top(H):\n  \"\"\"Compute the full Hamiltonian for the layer below the top tensor.\n\n  Assuming periodic boundary conditions.\n\n  Args:\n    H: Local Hamiltonian ascended to just below the top tensor.\n\n  Return:\n    The full Hamiltonian for that layer as a dense matrix.\n  \"\"\"\n  h1, (h2L, h2R) = H\n  D = h1.shape[0]\n  dtype = h1.dtype\n\n  E = backend.eye(D, dtype=dtype)\n\n  fullH = backend.ncon([h1, E], [(-1, -3), (-2, -4)])\n  fullH += backend.ncon([E, h1], [(-1, -3), (-2, -4)])\n  for (hl, hr) in zip(h2L, h2R):\n    fullH += backend.ncon([hl, hr], [(-1, -3), (-2, -4)])\n  for (hl, hr) in zip(h2R, h2L):\n    fullH += backend.ncon([hl, hr], [(-1, -3), (-2, -4)])\n\n  return backend.reshape(fullH, (D**2, D**2))\n\n\ndef _dense_ham_term(H):\n  \"\"\"Convert the dense representation of the local Hamiltonian term.\n\n  Args:\n    H: The sparse form for the local Hamiltonian term.\n\n  Returns:\n    The dense term as a single rank-4 tensor.\n  \"\"\"\n  h1, (h2L, h2R) = H\n  D = h1.shape[0]\n  dtype = h1.dtype\n\n  E = backend.eye(D, dtype=dtype)\n\n  h = backend.ncon([h1, E], [(-1, -3), (-2, -4)])\n  for (hl, hr) in zip(h2L, h2R):\n    h += backend.ncon([hl, hr], [(-1, -3), (-2, -4)])\n\n  return h\n\n\ndef isos_with_transposes(isos_012):\n  \"\"\"Compute the transposes of all isometries in a tree.\n\n  Args:\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    A list of tuples of form (iso_012, iso_021), with iso_021 the transpose\n    (reflection) of iso_012.\n  \"\"\"\n  return list(\n    zip(isos_012, [backend.transpose(w, (0, 2, 1)) for w in isos_012]))\n\n\ndef opt_tree_energy(isos_012,\n                    H,\n                    itr,\n                    itr_l,\n                    verbose=0,\n                    graphed=False,\n                    decomp_mode=\"svd_full_iso\",\n                    decomp_device=None,\n                    envsq_dtype=None,\n                    ham_shift=\"auto\",\n                    callback=None,\n                    time_layer_updates=False):\n  \"\"\"Variationally minimize the energy of a binary tree tensor network.\n\n  Spatial uniformity is assumed: The tree tensor network consists of a single\n  isometric tensor per layer.\n\n  The Hamiltonian, assumed to be translation invariant, is provided as a\n  single nearest-neighbor term `H`. See for example `get_ham_ising()`, which\n  constructs an appropriate object for the Ising model. The size of the\n  second and third dimensions of the first-layer tensor `isos_012[0]` must\n  match the physical dimension of the Hamiltonian.\n\n  A number `itr` of variational sweeps are carried out. For each sweep, the\n  tensor specifying each layer is optimized using a linear approximation,\n  with `itr_l` iterations per layer.\n\n  Args:\n      isos_012: List of tensors specifying the tree tensor network; one\n                tensor for each layer. Assumed to be isometries.\n      H: The local term of the Hamiltonian as an MPO.\n      itr: The number of variational sweeps to perform.\n      itr_l: The number of iterations per layer. Typically, 1 is enough.\n      verbose: Set to >0 to print some status information.\n      graphed: If `True`, build a graph for a complete sweep for best\n                performance.\n      decomp_mode: Which decomposition scheme to use for tensor updates.\n      decomp_device: TensorFlow device on which to perform decompositions.\n      envsq_dtype: Data type to use for the squared environment. Only\n                    applicable if `decomp_mode` is `\"svd\"` or `\"eigh\"`.\n      ham_shift: Amount by which to shift the energies of the local \n                  Hamiltonian term. A small positive value typically improves\n                  convergence.\n      callback: A function to be called after each sweep.\n      time_layer_updates: Boolean. Whether to collect timing data for layer\n        updates, split into computation of environments and matrix\n        decompositions. The data is supplied only to the callback function.\n  Returns:\n      isos_012: The optimized tensors of the tree tensor network.\n  \"\"\"\n  with backend.device(decomp_device):\n    H, shift = shift_ham(H, ham_shift)\n  print(\"Hamiltonian shift:\", shift)\n\n  L = len(isos_012)\n\n  # Ascend through any trivial layers only once\n  bottom = 0\n  for l in range(L):\n    shp = isos_012[l].shape\n    if shp[0] == shp[1] * shp[2]:\n      if graphed:\n        H = backend.ascend_uniform_op_local_graph(\n          *H, isos_012[l], backend.transpose(isos_012[l], (0, 2, 1)))\n      else:\n        H = ascend_uniform_op_local(*H, isos_012[l], backend.transpose(\n            isos_012[l], (0, 2, 1)))\n      bottom = l + 1\n    else:\n      break\n\n  t0 = time.time()\n  for j in range(itr):\n    if graphed:\n      states = backend.all_states_1site_graph(isos_012[bottom:])\n    else:\n      states = all_states_1site(isos_012[bottom:])\n    states = [None] * bottom + states\n\n    Hl = H\n    svs = [None] * L\n    tes_sweep = 0.0\n    tds_sweep = 0.0\n    for l in range(bottom, L):\n      if verbose > 1:\n        print(\"Optimizing level {}\".format(l))\n      res = opt_energy_layer(\n          isos_012[l:],\n          *Hl,\n          states[l + 1:],\n          itr_l,\n          graphed=graphed,\n          decomp_mode=decomp_mode,\n          decomp_device=decomp_device,\n          envsq_dtype=envsq_dtype,\n          timing=time_layer_updates)\n      isos_012[l], s = res[:2]\n      svs[l] = s\n\n      if time_layer_updates:\n        tes, tds = res[2:]\n        tes_sweep += tes\n        tds_sweep += tds\n\n      if l < L - 1:\n        if graphed:\n          Hl = backend.ascend_uniform_op_local_graph(\n            *Hl, isos_012[l], backend.transpose(isos_012[l], (0, 2, 1)))\n        else:\n          Hl = ascend_uniform_op_local(\n            *Hl, isos_012[l], backend.transpose(isos_012[l], (0, 2, 1)))\n\n    if graphed:\n      H_top = backend.ascend_uniform_op_local_top_graph(\n        *Hl, isos_012[-1], backend.transpose(isos_012[-1], (0, 2, 1)))\n    else:\n      H_top = ascend_uniform_op_local_top(\n        *Hl, isos_012[-1], backend.transpose(isos_012[-1], (0, 2, 1)))\n    en = backend.trace(H_top) / (2**L) + shift * H_top.shape[0]\n\n    tes_sweep = tes_sweep / (L + 1 - bottom)\n    tds_sweep = tds_sweep / (L + 1 - bottom)\n\n    if verbose > 0:\n      minsv = backend.np.min(\n        [backend.to_numpy(sv).min() for sv in svs[bottom:]])\n      print(\"sweeps: {}, energy density: {}, min_sv: {}, run-time: {}\".format(\n          j,\n          backend.to_numpy(en).real, minsv,\n          time.time() - t0))\n\n    if callback is not None:\n      stop_request = callback(isos_012=isos_012,\n                              decomp_singular_values=svs,\n                              sweep_num=j,\n                              energy=en,\n                              time_sweep=time.time() - t0,\n                              time_env=tes_sweep,\n                              time_decomp=tds_sweep)\n      if stop_request:\n        break\n\n  return isos_012\n\n\ndef top_hamiltonian(H, isos_012):\n  \"\"\"Ascend the Hamiltonian to the single leg on the top of the tree.\n\n  In case the top rank is 1, this computes the Hamiltonian expectation value\n  of the pure state.\n\n  Args:\n    H: The local Hamiltonian term for the bottom of the tree.\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    The Hamiltonian at the top, dimension equal to the top rank of the tree.\n  \"\"\"\n  L = len(isos_012)\n  for l in range(L - 1):\n    H = ascend_uniform_op_local(\n      *H, isos_012[l], backend.transpose(isos_012[l], (0, 2, 1)))\n\n  H = ascend_uniform_op_local_top(\n    *H, isos_012[-1], backend.transpose(isos_012[-1], (0, 2, 1)))\n  \n  return H\n\n\ndef top_eigen(H, isos_012):\n  \"\"\"Compute the eigenvalue decomposition of the top Hamiltonian.\n\n  Args:\n    H: The local Hamiltonian term for the bottom of the tree.\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    ev: Eigenvalues.\n    eV: Matrix of eigenvectors.\n  \"\"\"\n  Htop = top_hamiltonian(isos_012, H)\n  return backend.eigh(Htop)\n\n\ndef apply_top_op(isos_012, top_op):\n  \"\"\"Apply an operator to the top of a tree.\n\n  Note: If the operator is not an isometry, the resulting tree will no longer\n  be isometric.\n\n  Args:\n    isos_012: The isometries defining the tree.\n    top_op: The operator to apply as a matrix. The right index will be \n      contracted with the top index of the tree.\n\n  Returns:\n    isos_012_new: Updated list of tensors defining the tree.\n  \"\"\"\n  isos_012_new = isos_012[:]\n  isos_012_new[-1] = backend.ncon(\n    [top_op, isos_012_new[-1]],\n    [(-1, 1), (1, -2, -3)])\n  return isos_012_new\n\n\ndef apply_top_vec(isos_012, top_vec):\n  \"\"\"Contract a vector with the top of a tree, converting it to a pure state.\n\n  Note: If the vector is not normalized, the tree will no longer be\n  normalized, and hence no longer isometric.\n\n  Args:\n    isos_012: The isometries defining the tree.\n    top_vec: Vector to contract with the tree top.\n\n  Returns:\n    isos_012_new: Updated list of tensors defining the tree.\n  \"\"\"\n  if len(top_vec.shape) != 1:\n    raise ValueError(\"top_purestate was not a vector!\")\n  top_op = backend.reshape(top_vec, (1, top_vec.shape[0]))\n  return apply_top_op(isos_012, top_op)\n\n\ndef top_translation(isos_012):\n  \"\"\"Ascend the physical translation operator to the top of the tree.\n\n  For top rank equal to 1, this computes a value representing the translation\n  invariance of the tree. If it is 1, the tree is completely translation\n  invariant. Similarly, for top rank > 1, the unitarity of the resulting\n  operator is a measure of the translation invariance of the mixed state.\n\n  Args:\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    T: The coarse-grained translation operator.\n  \"\"\"\n  d = isos_012[0].shape[1]\n  E2 = backend.eye(d**2, dtype=isos_012[0].dtype)\n  # Ordering: mpo_left, mpo_right, phys_bottom, phys_top\n  translation_tensor = backend.reshape(E2, (d,d,d,d))\n  return ascend_uniform_MPO_to_top(translation_tensor, isos_012)\n\n\ndef top_global_product_op(op, isos_012):\n  \"\"\"Ascend a uniform product of 1-site operators to the top of the tree.\n\n  Args:\n    op: 1-site operator (matrix) defining the global product.\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    top_op: The coarse-grained operator.\n  \"\"\"\n  d = op.shape[0]\n  Mop = backend.reshape(op, (1, 1, d, d))\n  return ascend_uniform_MPO_to_top(Mop, isos_012)\n\n\ndef top_localop_1site(op, n, isos_012):\n  \"\"\"Ascend a 1-site operator at a particular site to the top of the tree.\n\n  Args:\n    op: 1-site operator (matrix).\n    n: The site number from which to ascend.\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    top_op: The coarse-grained operator.\n  \"\"\"\n  L = len(isos_012)\n  if not (0 <= n < 2**L):\n    raise ValueError(\"Invalid site number '{}' with {} sites.\".format(n, 2**L))\n  for l in range(L):\n    if n % 2 == 0:\n      op = ascend_op_1site_to_1site_L(op, isos_012[l])\n    else:\n      op = ascend_op_1site_to_1site_R(op, isos_012[l])\n    n = n // 2\n  return op\n\n\ndef top_localop_2site(op, n, isos_012):\n  \"\"\"Ascend a 2-site MPO at a particular pair of sites to the top of the tree.\n\n  Args:\n    op: 2-site MPO (2-tuple of lists of operators).\n    n: The (leftmost) site number from which to ascend.\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    top_op: The coarse-grained operator.\n  \"\"\"\n  L = len(isos_012)\n  N = 2**L\n  if not (0 <= n < 2**L):\n    raise ValueError(\"Invalid site number '{}' with {} sites.\".format(n, N))\n  np1 = n + 1  # site number of neighbor\n  for l in range(L):\n    xn = n // 2\n    xnp1 = np1 // 2\n    if n == np1:\n      # After the ops merge, this is a 1-site op ascension.\n      # Never occurs on the first iteration.\n      if n % 2 == 0:\n        op = ascend_op_1site_to_1site_L(op, isos_012[l])\n      else:\n        op = ascend_op_1site_to_1site_R(op, isos_012[l])\n    elif (xn % 2 == 0) != (xnp1 % 2 == 0): \n      # If we are still following different paths\n      if l == L-1: #catch the outside case\n        op = ascend_op_2site_to_1site(\n          reflect_mpo_2site(op),\n          isos_012[l],\n          backend.transpose(isos_012[l], (0,2,1)))\n      else:\n        op = ascend_op_2site_to_2site(\n          op, isos_012[l], backend.transpose(isos_012[l], (0,2,1)))\n    else:  # if the paths merge\n      op = ascend_op_2site_to_1site(\n        op, isos_012[l], backend.transpose(isos_012[l], (0,2,1)))\n    n = xn\n    np1 = xnp1\n  return op\n\n\ndef top_local_ham(H, n, isos_012):\n  \"\"\"Ascend a local Hamiltonian term at a particular location to the tree top.\n\n  Keeps the 1-site and 2-site components separate.\n\n  Args:\n    H: Local Hamiltonian term in sparse representation.\n    n: The (leftmost) site number from which to ascend.\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    top_op: The coarse-grained operator.\n  \"\"\"\n  h1, h2 = H\n  h1 = top_localop_1site(h1, n, isos_012)\n  h2 = top_localop_2site(h2, n, isos_012)\n  return (h1, h2)\n\n\ndef top_ham_all_terms(H, isos_012):\n  \"\"\"Ascend all Hamiltonian terms separately to the top of the tree.\n\n  Args:\n    H: Local Hamiltonian term in sparse representation.\n    isos_012: The isometries defining the tree.\n\n  Returns:\n    top_ops: List of coarse-grained Hamiltonian terms.\n  \"\"\"\n  N = 2**len(isos_012)\n  Htop_terms = []\n  for n in range(N):\n    Htop_terms.append(top_local_ham(H, n, isos_012))\n  return Htop_terms\n\n\ndef top_ham_modes(H, isos_012, ns):\n  \"\"\"Compute the Hamiltonian density modes at the top of the tree.\n\n  Args:\n    H: Local Hamiltonian term in sparse representation.\n    isos_012: The isometries defining the tree.\n    ns: Modes to compute (list of integers).\n\n  Returns:\n    mode_ops: List of coarse-grained Hamiltonian density modes.\n  \"\"\"\n  Htop_terms = top_ham_all_terms(H, isos_012)\n  N = len(Htop_terms)\n  Hns = []\n  for n in ns:\n    Hn = sum(\n      backend.np.exp(1.j * n * j * 2*backend.np.pi / N) * h1 + \n      backend.np.exp(1.j * n * (j + 0.5) * 2*backend.np.pi / N) * h2 \n      for (j, (h1,h2)) in enumerate(Htop_terms))\n    Hns.append(Hn)\n  return Hns\n\n\ndef tree_energy_expval_check(H, isos_012):\n  \"\"\"Compute the energy at all levels in the tree.\n\n  Useful for checking consistency of ascended Hamiltonians and descended\n  states.\n\n  Args:\n    H: Local Hamiltonian term.\n    isos_012: List of isometries defining the tree.\n\n  Returns:\n    Vector of energies, one for each level plus one for the top.\n  \"\"\"\n  L = len(isos_012)\n  states = all_states_1site(isos_012)\n\n  ens = []\n  Hl = H\n  for l in range(L):\n    en = _energy_expval_env(isos_012[l:], *Hl, states[l + 1:])\n    ens.append(en / (2**L))\n    if l < L - 1:\n      Hl = ascend_uniform_op_local(*Hl, isos_012[l], backend.transpose(\n          isos_012[l], (0, 2, 1)))\n\n  H_top = ascend_uniform_op_local_top(*Hl, isos_012[-1],\n                              backend.transpose(isos_012[-1], (0, 2, 1)))\n  en = backend.trace(H_top)\n  ens.append(en / (2**L))\n\n  return backend.convert_to_tensor(ens)\n\n\ndef descend_full_state_pure(isos_012):\n  \"\"\"Compute the dense representation of the state from a pure tree.\n\n  This is an expensive operation that requires exponential memory and time\n  (in the number of sites, so doubly exponential in the number of layers!).\n\n  Args:\n    isos_012: The list of isometries defining the tree.\n\n  Returns:\n    The state as a dense tensor of rank N, where N is the number of sites.\n  \"\"\"\n  if not isos_012[-1].shape[0] == 1:\n    raise ValueError(\"Top dimension is not 1 (state not pure).\")\n\n  nisos = []\n\n  iso_top = isos_012[-1]\n  iso_top = backend.reshape(iso_top, iso_top.shape[1:])\n\n  niso = tensornetwork.Node(\n      iso_top,\n      name=\"iso_{}_0\".format(len(isos_012) - 1),\n      axis_names=[\"bL\", \"bR\"],\n      backend=backend.name)\n  nisos.append(niso)\n  sites = [niso[\"bL\"], niso[\"bR\"]]\n\n  for l in reversed(range(len(isos_012) - 1)):\n    sites_next = []\n    for (s, s_edge) in enumerate(sites):\n      niso = tensornetwork.Node(\n          isos_012[l],\n          name=\"iso_{}_{}\".format(l, s),\n          axis_names=[\"t\", \"bL\", \"bR\"],\n          backend=backend.name)\n      tensornetwork.connect(s_edge, niso[\"t\"])\n      sites_next += [niso[\"bL\"], niso[\"bR\"]]\n      nisos.append(niso)\n    sites = sites_next\n\n  nisos = nisos[::-1]\n  nstate = nisos.pop()\n  while nisos:\n    nstate = nstate @ nisos.pop()\n  nstate = nstate.reorder_edges(sites)\n  return nstate.get_tensor()\n\n\ndef get_ham_ising(dtype, J=1.0, h=1.0):\n  \"\"\"Return the local term for the critical Ising Hamiltonian.\n\n  Defines the global Hamiltonian:\n  $H = -\\sum_{i=1}^N [ J * X_i X_{i+1} + h * Z_i ]$\n\n  Args:\n    dtype: The data type.\n    J: The coupling strength.\n    h: The field strength.\n\n  Returns:\n    The Hamiltonian term, separated into a 1-site contribution and a 2-site\n    MPO.\n  \"\"\"\n  X = backend.convert_to_tensor([[0.0, 1.0], [1.0, 0.0]], dtype=dtype)\n  Z = backend.convert_to_tensor([[1.0, 0.0], [0.0, -1.0]], dtype=dtype)\n  h_mpo_2site = ([-J * X], [X])\n  h1 = -h * Z\n  return h1, h_mpo_2site\n\n\ndef _weylops(q):\n  om = backend.np.exp(2j * backend.np.pi / q)\n  U = backend.np.backend.diag([om**j for j in range(q)])\n  V = backend.np.backend.diag(backend.np.ones(q - 1), 1)\n  V[-1, 0] = 1\n  return U, V, om\n\n\ndef get_ham_potts(dtype, q, J=1.0, h=1.0):\n  \"\"\"Return the local term for the q-state Potts Hamiltonian.\n\n  Defines the global Hamiltonian:\n  $H = -\\sum_{i=1}^N \\sum_{k=1}^q [ J * U_i^k U_{i+1}^{q-k} + h * V_i^k]$\n\n  Args:\n    dtype: The data type.\n    q: Which root of unity to use. Alternatively, how many values the Potts\n      \"spins\" are able to take.\n    J: Coefficient for the nearest-neighbor terms (positive means\n      ferromagnetic).\n    h: Coefficient for the 1-site terms.\n\n  Returns:\n    The Hamiltonian term, separated into a 1-site contribution and a 2-site\n    MPO.\n  \"\"\"\n  U, V, _ = _weylops(q)\n\n  mp = backend.np.linalg.matrix_power\n\n  if backend.dtype_is_complex(dtype):\n    h2 = ([-J * mp(U, k) for k in range(1, q)],\n          [mp(U, q - k) for k in range(1, q)])\n  else:\n    # The straightforward way to build the Hamiltonian results in complex\n    # matrices in the MPO. The dense Hamiltonian is, however, real.\n    # To make the MPO real, we first build the dense 2-site term, then\n    # use an SVD to split it back into a real MPO.\n    h2_dense = sum(\n      backend.np.tensordot(-J * mp(U, k), mp(U, q-k), axes=((),()))\n      for k in range(1,q))\n    realness = backend.np.linalg.norm(h2_dense - h2_dense.real)\n    if realness > 1e-12:\n      raise ValueError(\n        \"2-site term was not real. Realness = {}\".format(realness))\n    u, s, vh = backend.svd_np(\n      h2_dense.real.backend.reshape((q**2, q**2)), full_matrices=False)\n    mpo_rank = backend.np.count_nonzero(s.round(decimals=12))\n    if mpo_rank != q - 1:\n      raise ValueError(\n        \"Error performing SVD of 2-site term. {} != {}-1\".format(mpo_rank, q))\n    h2 = ([s[i] * u[:,i].backend.reshape(q,q) for i in range(q-1)],\n          [vh[i,:].backend.reshape(q,q) for i in range(q-1)])\n\n  h1 = -h * sum(mp(V, k) for k in range(1, q))\n\n  h1 = backend.convert_to_tensor(h1, dtype=dtype)\n  h2 = (\n      [backend.convert_to_tensor(h, dtype=dtype) for h in h2[0]],\n      [backend.convert_to_tensor(h, dtype=dtype) for h in h2[1]],\n  )\n\n  return h1, h2\n\n\ndef kron_td(a, b):\n  \"\"\"Computes the Kronecker product of two matrices using tensordot.\n\n  Args:\n    a: Matrix a.\n    b: Matrix b.\n\n  Returns:\n    The Kronecker product a x b, as a matrix.\n  \"\"\"\n  if len(a.shape) != 2 or len(b.shape) != 2:\n    raise ValueError(\"Only implemented for matrices.\")\n  ab = backend.tensordot(a, b, 0)\n  ab = backend.transpose(ab, (0,2,1,3))\n  return backend.reshape(\n    ab, (a.shape[0] * b.shape[0], a.shape[1] * b.shape[1]))\n\n\ndef block_ham(H, sites_per_block):\n  \"\"\"Creates a 'blocked' Hamiltonian from an input Hamiltonian.\n\n  Blocks sites together, increasing the site dimension.\n\n  Args:\n    H: The local Hamiltonian term.\n    sites_per_block: The number of sites to block into one.\n\n  Returns:\n    The blocked local Hamiltonian term.\n  \"\"\"\n  h1, h2 = H\n  d = h1.shape[0]\n  dtype = h1.dtype\n  E = backend.eye(d, dtype=dtype)\n\n  h1_blk = None\n  for i in range(sites_per_block):\n    h1_term = h1 if i == 0 else E\n    for j in range(1, sites_per_block):\n      h1_term = kron_td(h1_term, h1 if i == j else E)\n    if h1_blk is not None:\n      h1_blk += h1_term\n    else:\n      h1_blk = h1_term\n\n  h2_dense = sum(kron_td(h2[0][i], h2[1][i]) for i in range(len(h2[0])))\n  for i in range(sites_per_block - 1):\n    h1_term = h2_dense if i == 0 else E\n    j = 2 if i == 0 else 1\n    while j < sites_per_block:\n      h1_term = kron_td(h1_term, h2_dense if i == j else E)\n      j += 2 if i == j else 1\n    h1_blk += h1_term\n  del(h2_dense)\n\n  E_big = backend.eye(d**(sites_per_block - 1), dtype=dtype)\n  h2_0 = [kron_td(E_big, h) for h in h2[0]]\n  h2_1 = [kron_td(h, E_big) for h in h2[1]]\n\n  return h1_blk, (h2_0, h2_1)\n\n\ndef get_ham_ising_tube(dtype, Ly, lam=-3.044):\n  \"\"\"Return the local term for the 2+1D Ising Hamiltonian on a narrow torus.\n\n  Defines the global Hamiltonian:\n  $H = -\\sum_{\\langle i, j \\rangle} X_i X_j + lam * \\sum_i Z_i ]$\n\n  Represents the Hamiltonian for the 2D torus as a 1-dimensional Hamiltonian,\n  where each \"site\" is a slice of the torus in the \"y\" direction. The site\n  dimension thus depends on the size of the system in the y direction.\n\n  Args:\n    dtype: The data type.\n    Ly: The size of the torus in the y direction (number of sites).\n    lam: The field strength.\n\n  Returns:\n    The Hamiltonian term, separated into a 1-site contribution and a 2-site\n    MPO.\n  \"\"\"\n  X = backend.np.array([[0.0, 1.0], [1.0, 0.0]])\n  Z = backend.np.array([[1.0, 0.0], [0.0, -1.0]])\n\n  Xcol = [\n      backend.np.kron(\n        backend.np.kron(backend.np.eye(2**i), X),\n        backend.np.eye(2**(Ly - i - 1)))\n      for i in range(Ly)\n  ]\n  Zcol = [\n      backend.np.kron(\n        backend.np.kron(backend.np.eye(2**i), Z),\n        backend.np.eye(2**(Ly - i - 1)))\n      for i in range(Ly)\n  ]\n\n  Xcol = [backend.convert_to_tensor(Xc, dtype=dtype) for Xc in Xcol]\n  Zcol = [backend.convert_to_tensor(Zc, dtype=dtype) for Zc in Zcol]\n\n  h1 = lam * sum(Zcol) - sum(Xcol[i] @ Xcol[(i + 1) % Ly] for i in range(Ly))\n\n  h_mpo_2site = ([-Xc for Xc in Xcol], Xcol)\n\n  return h1, h_mpo_2site\n\n\nclass TTNBackend():\n  \"\"\"Base backend class with method signatures.\"\"\"\n\n  np = None\n  name = None\n\n  def __init__(self):\n    self.ascend_uniform_op_local_graph = self.build(ascend_uniform_op_local)\n    self.ascend_uniform_op_local_top_graph = self.build(\n      ascend_uniform_op_local_top)\n    self.opt_energy_layer_once_graph = self.build(opt_energy_layer_once)\n    self._iso_from_envsq_decomp_graph = self.build(_iso_from_envsq_decomp)\n    self.opt_energy_env_graph = self.build(opt_energy_env)\n    self._iso_from_svd_graph = self.build(_iso_from_svd)\n    self._iso_from_svd_decomp_graph = self.build(_iso_from_svd_decomp)\n    self.all_states_1site_graph = self.build(all_states_1site)\n\n  def dtype_is_complex(self, dtype):\n    raise NotImplementedError()\n\n  def random_normal_mat(self, D1, D2, dtype):\n    raise NotImplementedError()\n\n  def conj(self, x):\n    raise NotImplementedError()\n  \n  def adjoint(self, x):\n    raise NotImplementedError()\n\n  def build(self, f):\n    return f\n\n  def trace(self, x):\n    raise NotImplementedError()\n\n  def transpose(self, x, axes):\n    raise NotImplementedError()\n\n  def reshape(self, x, shape):\n    raise NotImplementedError()\n\n  def convert_to_tensor(self, x, **kwargs):\n    raise NotImplementedError()\n\n  def device(self, dev):\n    raise NotImplementedError()\n\n  def cast(self, x, dtype):\n    raise NotImplementedError()\n\n  def zeros_like(self, x, **kwargs):\n    raise NotImplementedError()\n\n  def where(self, cond, x, y):\n    raise NotImplementedError()\n\n  def reduce_max(self, x):\n    raise NotImplementedError()\n\n  def to_real(self, x):\n    raise NotImplementedError()\n\n  def eye(self, dim, dtype=None):\n    raise NotImplementedError()\n\n  def diag(self, x):\n    raise NotImplementedError()\n\n  def diag_part(self, x):\n    raise NotImplementedError()\n\n  def sqrt(self, x):\n    raise NotImplementedError()\n\n  def abvals(self, x):\n    raise NotImplementedError()\n\n  def matmul(self, a, b, adjoint_b=False):\n    raise NotImplementedError()\n\n  def tensordot(self, a, b, axes):\n    raise NotImplementedError()\n\n  def norm(self, x, **kwargs):\n    raise NotImplementedError()\n\n  def svd(self, x):\n    raise NotImplementedError()\n\n  def svd_np(self, x, **kwargs):\n    raise NotImplementedError()\n\n  def eigh(self, x):\n    raise NotImplementedError()\n\n  def eigvalsh(self, x):\n    raise NotImplementedError()\n\n  def qr(self, x):\n    raise NotImplementedError()\n\n  def to_numpy(self, x):\n    raise NotImplementedError()\n\n  def executing_eagerly(self):\n    raise NotImplementedError()\n\n  def ncon(self, tensors, network_structure, *args, **kwargs):\n    return tensornetwork.ncon(\n      tensors, network_structure, *args, **kwargs, backend=self.name) \n\n\nclass TTNBackendTensorFlow(TTNBackend):\n  \"\"\"TensorFlow backend.\"\"\"\n\n  name = \"tensorflow\"\n\n  def __init__(self):\n    import numpy as np\n    import scipy.linalg as spla\n    import tensorflow as tf\n    self.np = np\n    self._spla = spla\n    self.tf = tf\n\n    super().__init__()\n\n  def dtype_is_complex(self, dtype):\n    return dtype.is_complex\n\n  def random_normal_mat(self, D1, D2, dtype):\n    if dtype.is_complex:\n      A = self.tf.complex(\n        self.tf.random.normal((D1, D2), dtype=dtype.real_dtype),\n        self.tf.random.normal((D1, D2), dtype=dtype.real_dtype)) / math.sqrt(2)\n    else:\n      A = self.tf.random.normal((D1, D2), dtype=dtype)\n    return A\n\n  def conj(self, x):\n    return self.tf.math.conj(x)\n  \n  def adjoint(self, x):\n    return self.tf.linalg.adjoint(x)\n\n  def build(self, f):\n    return self.tf.function(f, autograph=False)\n\n  def trace(self, x):\n    return self.tf.linalg.trace(x)\n\n  def transpose(self, x, axes):\n    return self.tf.transpose(x, axes)\n\n  def reshape(self, x, shape):\n    return self.tf.reshape(x, shape)\n\n  def convert_to_tensor(self, x, **kwargs):\n    return self.tf.convert_to_tensor(x, **kwargs)\n\n  def device(self, dev):\n    return self.tf.device(dev)\n\n  def cast(self, x, dtype):\n    return self.tf.cast(x, dtype)\n\n  def zeros_like(self, x, **kwargs):\n    return self.tf.zeros_like(x, **kwargs)\n\n  def where(self, cond, x, y):\n    return self.tf.where(cond, x, y)\n\n  def reduce_max(self, x):\n    return self.tf.reduce_max(x)\n\n  def to_real(self, x):\n    return self.tf.cast(x, x.dtype.real_dtype)\n\n  def eye(self, dim, dtype=None):\n    return self.tf.eye(dim, dtype=dtype)\n\n  def diag(self, x):\n    return self.tf.linalg.diag(x)\n\n  def diag_part(self, x):\n    return self.tf.linalg.diag_part(x)\n\n  def sqrt(self, x):\n    return self.tf.sqrt(x)\n\n  def abvals(self, x):\n    return self.tf.abs(x)\n\n  def matmul(self, a, b, adjoint_b=False):\n    return self.tf.matmul(a, b, adjoint_b=adjoint_b)\n\n  def tensordot(self, a, b, axes):\n    return self.tf.tensordot(a, b, axes)\n\n  def norm(self, x, **kwargs):\n    return self.tf.norm(x, **kwargs)\n\n  def svd(self, x):\n    return self.tf.linalg.svd(x)\n\n  def svd_np(self, x, **kwargs):\n    return self._spla.svd(x, **kwargs)\n\n  def eigh(self, x):\n    return self.tf.linalg.eigh(x)\n\n  def eigvalsh(self, x):\n    return self.tf.linalg.eigvalsh(x)\n\n  def qr(self, x):\n    return self.tf.linalg.qr(x)\n\n  def to_numpy(self, x):\n    return x.numpy()\n\n  def executing_eagerly(self):\n    return self.tf.executing_eagerly()\n\n\nclass TTNBackendNumpy(TTNBackend):\n  \"\"\"Numpy backend.\"\"\"\n\n  name = \"numpy\"\n\n  def __init__(self):\n    import numpy as np\n    import scipy.linalg as spla\n    self.np = np\n    self.really_np = np\n    self._spla = spla\n\n    super().__init__()\n\n  def dtype_is_complex(self, dtype):\n    return self.really_np.dtype(dtype).kind == 'c'\n\n  def random_normal_mat(self, D1, D2, dtype):\n    if self.dtype_is_complex(dtype):\n      A = (self.really_np.random.randn(D1,D2) +\n            1.j * self.really_np.random.randn(D1,D2)) / math.sqrt(2)\n      A = self.np.asarray(A, dtype)\n    else:\n      A = self.np.asarray(self.really_np.random.randn(D1,D2), dtype)\n    return A\n\n  def conj(self, x):\n    return self.np.conj(x)\n  \n  def adjoint(self, x):\n    return self.np.conj(self.np.transpose(x))\n\n  def build(self, f):\n    return f\n\n  def trace(self, x):\n    return self.np.trace(x)\n\n  def transpose(self, x, axes):\n    return self.np.transpose(x, axes)\n\n  def reshape(self, x, shape):\n    return self.np.reshape(x, shape)\n\n  def convert_to_tensor(self, x, **kwargs):\n    return self.np.array(x, **kwargs)\n\n  def device(self, dev):\n    return contextlib.suppress()\n\n  def cast(self, x, dtype):\n    return self.np.asarray(x, dtype)\n\n  def zeros_like(self, x, **kwargs):\n    return self.np.zeros_like(x, **kwargs)\n\n  def where(self, cond, x, y):\n    return self.np.where(cond, x, y)\n\n  def reduce_max(self, x):\n    return self.np.amax(x)\n\n  def to_real(self, x):\n    return self.np.real(x)\n\n  def eye(self, dim, dtype=None):\n    return self.np.eye(dim, dtype=dtype)\n\n  def diag(self, x):\n    return self.np.diag(x)\n\n  def diag_part(self, x):\n    return self.np.diagonal(x)\n\n  def sqrt(self, x):\n    return self.np.sqrt(x)\n\n  def abvals(self, x):\n    return self.np.abs(x)\n\n  def matmul(self, a, b, adjoint_b=False):\n    if adjoint_b:\n      return self.np.matmul(a, self.adjoint(b))\n    return self.np.matmul(a, b)\n\n  def tensordot(self, a, b, axes):\n    return self.np.tensordot(a, b, axes)\n\n  def norm(self, x, **kwargs):\n    return self.np.linalg.norm(x, **kwargs)\n\n  def svd(self, x):\n      u, s, vh = self.np.linalg.svd(x, full_matrices=False)\n      return s, u, self.adjoint(vh)\n\n  def svd_np(self, x, **kwargs):\n    return self._spla.svd(x, **kwargs)\n\n  def eigh(self, x):\n    return self.np.linalg.eigh(x)\n\n  def eigvalsh(self, x):\n    return self.np.linalg.eigvalsh(x)\n\n  def qr(self, x):\n    return self.np.linalg.qr(x)\n\n  def to_numpy(self, x):\n    return self.np.asarray(x)\n\n  def executing_eagerly(self):\n    return False\n\n\nclass TTNBackendJAX(TTNBackendNumpy):\n  \"\"\"JAX backend.\"\"\"\n\n  name = \"jax\"\n\n  def __init__(self):\n    import jax.numpy as np_jax\n    from jax import jit\n    self._jit = jit\n\n    super().__init__()\n\n    self.np = np_jax\n\n  def build(self, f):\n    return self._jit(f)\n"}
{"content": "#  Copyright (c) 2020, Apple Inc. All rights reserved.\n#\n#  Use of this source code is governed by a BSD-3-clause license that can be\n#  found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause\n\nimport logging as _logging\n\nimport math as _math\nimport numpy as _np\nfrom tqdm import tqdm as _tqdm\n\nfrom coremltools.converters.mil.mil import types\nfrom coremltools.converters.mil.mil import Builder as mb\nfrom coremltools.converters.mil.mil.var import Var, ListVar\nfrom coremltools.converters.mil.mil import Placeholder, Symbol\nfrom .internal_graph import *\nfrom .torch_op_registry import _TORCH_OPS_REGISTRY, register_torch_op\nfrom coremltools.converters.mil.mil.types.symbolic import any_symbolic, is_symbolic\n\n# The pytorch args for many of the below ops were sourced from\n# https://github.com/pytorch/pytorch/blob/d971007c291c0ead1003d12cd553d18ddb582207/torch/csrc/jit/mobile/register_mobile_ops.cpp#L216\n\n\n# This is a magic number in PyTorch. It's used as a default value in many\n# functions.\nPYTORCH_MAGIC_DEFAULT = 9223372036854775807\n\n\ndef _all_outputs_present(context, graph):\n    \"\"\" Returns true if all the symbols in the graph's output list are\n        present in context.\"\"\"\n    for outp in graph.outputs:\n        try:\n            context[outp]\n        except ValueError:\n            return False\n    return True\n\n\ndef convert_nodes(context, graph):\n    \"\"\"Iterate over the nodes of a graph or block and convert to MIL.\n\n        Arguments:\n            context: A TranscriptionContext object to pull node inputs and\n                assign node outputs.\n            graph: An InternalTorchIRGraph or InternalTorchIRBlock object.\n    \"\"\"\n    for node in _tqdm(graph.nodes, desc=\"Converting Frontend ==> MIL Ops\", unit=\" ops\"):\n        _add_op = _TORCH_OPS_REGISTRY.get(node.kind, None)\n        _logging.info(\"Converting op {} : {}\".format(node.name, node.kind))\n        if _add_op is None:\n            raise RuntimeError(\n                \"PyTorch convert function for op '{}' not implemented.\".format(node.kind)\n            )\n        else:\n            _add_op(context, node)\n\n        # We've generated all the outputs the graph needs, terminate conversion.\n        if _all_outputs_present(context, graph):\n            break\n\n\ndef convert_block(context, block, inputs):\n    \"\"\"Convert a block (sub-graph) to MIL. Conversion happens within a new\n        context frame.\n\n        Arguments:\n            context: A TranscriptionContext object to pull node inputs and\n                assign node outputs.\n            block: An InternalTorchIRBlock object.\n            inputs: List of Vars from the outer context that map to the block's\n                expected inputs. The number of inputs provided must match the\n                number expected by the block.\n    \"\"\"\n\n    if len(block.inputs) != len(inputs):\n        raise AssertionError\n\n    # Start a new context frame.\n    context.push((block.inputs, inputs))\n\n    # Add the block ops.\n    convert_nodes(context, block)\n\n    # Collect the block outputs.\n    outputs = [context[outp] for outp in block.outputs]\n\n    # Return to the previous context frame.\n    context.pop()\n    return outputs\n\n\n# Some ops will receive a dtype input as an integer\n# which maps to a torch dtype. The below mapping was found by\n# converting test models with different dtypes passed to ones.\nNUM_TO_TORCH_DTYPE = {\n    0: torch.uint8,\n    1: torch.int8,\n    2: torch.int16,\n    3: torch.int32,\n    4: torch.int64,\n    5: torch.float16,\n    6: torch.float32,\n    7: torch.float64,\n    11: torch.bool,\n}\n\nNUMPY_DTYPE_TO_TORCH_NUM = {\n    _np.uint8: 0,\n    _np.int8: 1,\n    _np.int16: 2,\n    _np.int32: 3,\n    _np.int64: 4,\n    _np.float16: 5,\n    _np.float32: 6,\n    _np.float64: 7,\n    _np.bool: 11,\n}\n\nNUM_TO_DTYPE_STRING = {\n    3: \"int32\",\n    4: \"int64\",\n    6: \"fp32\",\n    7: \"fp64\",\n    11: \"bool\",\n}\n\n\ndef decide_immediate_or_file(val):\n    if (\n        val is not None\n        and isinstance(val, (_np.ndarray, _np.generic))\n        and val.size >= 10\n    ):\n        return \"file_value\"\n    return \"immediate_value\"\n\n\ndef _get_inputs(context, node, expected=None):\n    \"\"\"\n    Look up a node's inputs in @context and return them as a list. If\n    @expected is not None, also verifies the number of inputs matches the\n    value of @expected.\n    \"\"\"\n    inputs = [context[name] for name in node.inputs]\n    if expected is not None:\n        expected = [expected] if not isinstance(expected, (list, tuple)) else expected\n\n        if len(inputs) not in expected:\n            raise ValueError(\n                \"node {} ({}) got {} input(s), expected {}\".format(\n                    node.name, node.kind, len(inputs), expected\n                )\n            )\n\n    return inputs\n\n\ndef _list_select(ls, index):\n    \"\"\" Sometimes we need to select a specific item from a list. If that item\n        is known at compile time, extract it as a const. Otherwise, if it's\n        symbolic, use gather.\n    \"\"\"\n    # TODO: gather doesn't work when the shape is known size.\n    if ls.sym_val is not None and not isinstance(ls.sym_val[index], Symbol):\n        res = mb.const(val=ls.sym_val[index])\n    else:\n        res = mb.gather(x=ls, indices=index)\n    return res\n\n\ndef _construct_constant(val, name):\n    # Converter cannot handle torch tensors.\n    if isinstance(val, torch.Tensor):\n        val = val.numpy()\n\n    # MIL casts ints to int32, which can't represent the 64 bit magic number.\n    # So we instead represent it with None, and any ops that might get the\n    # value will check for None instead.\n    if isinstance(val, int) and val == PYTORCH_MAGIC_DEFAULT:\n        val = None\n\n    mode = decide_immediate_or_file(val)\n    if val is None:\n        return None\n    else:\n        return mb.const(mode=mode, val=val, name=name)\n\n\n@register_torch_op\ndef constant(context, node):\n    if len(node.inputs) != 0:\n        raise AssertionError\n    if len(node.outputs) != 1:\n        raise AssertionError\n\n    name = node.name\n    val = node.attr[\"value\"]\n\n    const = _construct_constant(val, name)\n    context.add(const, torch_name=name)\n\n\ndef _array_construct(context, node, array_type):\n    if len(node.outputs) != 1:\n        raise AssertionError\n    inputs = _get_inputs(context, node)\n    scalar_inputs = [\n        inp\n        for inp in inputs\n        if isinstance(inp, Var) and inp.val is not None and len(inp.shape) == 0\n    ]\n\n    if len(scalar_inputs) == len(inputs):\n        # All the list items are compile-time scalar constants, so let's create\n        # a new const that concatenates them.\n        mode = \"immediate_value\"\n        val = array_type([inp.val for inp in inputs])\n        const = mb.const(mode=mode, val=val, name=node.name)\n        context.add(const)\n    else:\n        # If at least one input to the construct op is non-const, collect\n        # the inputs and add them directly to the context. Ops that use this\n        # node's output will take the list directly as input.\n        context.add(array_type(inputs), node.name)\n\n\n@register_torch_op\ndef tupleconstruct(context, node):\n    _array_construct(context, node, array_type=tuple)\n\n\n@register_torch_op\ndef listconstruct(context, node):\n    _array_construct(context, node, array_type=list)\n\n\n@register_torch_op\ndef eq(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    equal_to = mb.equal(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(equal_to)\n\n\n@register_torch_op\ndef ne(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    equal_to = mb.not_equal(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(equal_to)\n\n\n@register_torch_op\ndef le(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    less_equal = mb.less_equal(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(less_equal)\n\n\n@register_torch_op\ndef lt(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    less = mb.less(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(less)\n\n\n@register_torch_op\ndef ge(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    greater_equal = mb.greater_equal(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(greater_equal)\n\n\n@register_torch_op\ndef gt(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    greater = mb.greater(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(greater)\n\n\n@register_torch_op(torch_alias=[\"t\", \"transpose_\"])\ndef transpose(context, node):\n    if len(node.outputs) != 1:\n        raise AssertionError\n    inputs = _get_inputs(context, node)\n    x = inputs[0]\n\n    if len(node.inputs) == 1:\n        # PyTorch has several transpose ops that can be emitted. This one is only\n        # emitted when .t() is called on a tensor, which means it can only be\n        # called on a matrix.\n        if len(x.shape) > 2:\n            raise ValueError(\"transpose without dims for rank > 2 is unsupported\")\n        res = mb.transpose(x=x, perm=[1, 0], name=node.name)\n    else:\n        if len(inputs) != 3:\n            raise AssertionError\n        ax0 = inputs[1].val\n        ax1 = inputs[2].val\n\n        perm = list(range(len(x.shape)))\n        perm[ax0] = ax1\n        perm[ax1] = ax0\n\n        res = mb.transpose(x=x, perm=perm, name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef permute(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    perm = mb.transpose(x=inputs[0], perm=inputs[1], name=node.name)\n    context.add(perm)\n\n@register_torch_op\ndef pixel_shuffle(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    perm = mb.pixel_shuffle(x=inputs[0], upscale_factor=inputs[1], name=node.name)\n    context.add(perm)\n\n\n@register_torch_op(torch_alias=[\"bmm\"])\ndef matmul(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    if inputs[1].val is not None and \\\n            len(inputs[1].shape) == 2 and len(inputs[0].shape) <= 3:\n        res = mb.linear(x=inputs[0], weight=_np.transpose(inputs[1].val), name=node.name)\n    else:\n        res = mb.matmul(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(res)\n\n@register_torch_op\ndef add(context, node):\n    add_inputs = _get_inputs(context, node)\n    if len(node.outputs) != 1:\n        raise AssertionError\n\n    # TODO (sberardi): 3rd param to aten::add is a scale factor, need to handle that.\n    # out=input+alpha x other\n    # rdar://60175736\n    if len(add_inputs) > 2 and add_inputs[2].val != 1:\n        raise ValueError(\"ADD does not support scale factor param\")\n\n    add_node = mb.add(x=add_inputs[0], y=add_inputs[1], name=node.name)\n    context.add(add_node)\n\n@register_torch_op\ndef cumsum(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    res = mb.cumsum(x=inputs[0], axis=inputs[1], name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef addmm(context, node):\n    # addmm(Tensor input, Tensor mat1, Tensor mat2, Scalar beta=1, Scalar alpha=1)\n    # output = beta * input + alpha * mat1 * mat2\n\n    if len(node.inputs) != 5:\n        raise AssertionError\n    if len(node.outputs) != 1:\n        raise AssertionError\n\n    inputs = [context[name] for name in node.inputs]\n    bias = inputs[0]\n    mat1 = inputs[1]\n    mat2 = inputs[2]\n    beta = inputs[3]\n    alpha = inputs[4]\n\n    if beta.val != 1.0:\n        # Apply scaling factor beta to the bias.\n        bias = mb.mul(x=beta, y=bias, name=bias.name + \"_scaled\")\n        context.add(bias)\n\n    if alpha.val != 1.0:\n        # Apply scaling factor alpha to the input.\n        mat1 = mb.mul(x=alpha, y=mat1, name=mat1.name + \"_scaled\")\n        context.add(mat1)\n\n    # MIL linear will transpose mat2, but addmm expects that mat1 and mat2\n    # can multiply as is. So we add a tranpose.\n    mat2 = mb.transpose(x=mat2, perm=[1, 0], name=mat2.name + \"_transposed\")\n    context.add(mat2)\n\n    addmm_node = mb.linear(x=mat1, weight=mat2, bias=bias, name=node.name)\n    context.add(addmm_node)\n\n\n@register_torch_op(torch_alias=[\"conv2d\"])\ndef _convolution(context, node):\n    inputs = _get_inputs(context, node)\n\n    x = inputs[0]\n    weight = inputs[1]\n    bias = inputs[2]\n    strides = inputs[3]\n\n    # Expand padding. Torch accepts either an int (for all dimensions) or an n-tuple of ints (one per dimension), but\n    # we require a (2 * n)-tuple, where n is the number of spatial dimensions, start and end for each spatial dimension\n    pad = inputs[4].val\n\n    if weight.val.ndim in (3, 4):\n        # 1D and 2D: Need to explicitly state L-R, T-B pad\n        pad = _np.repeat(pad, 2)\n    elif weight.val.ndim == 5:\n        # 3D: Need to explicitly state F-Bk, L-R, T-B pad\n        if type(pad) == int:\n            pad = _np.repeat(pad, 6)\n        elif len(pad) == 3:\n            pad = _np.repeat(pad, 2)\n    else:\n        raise ValueError(\n            \"Invalid weight dimension. Must be 3, 4, or 5 for 1D, 2D, or 3D convolution, respectively.\"\n        )\n\n    dilations = inputs[5]\n    out_pad = None\n    if len(inputs) == 12:\n        transposed = inputs[6].val\n        out_pad = inputs[7].val\n        group = inputs[8]\n    elif len(inputs) == 7:\n        transposed = False\n        group = inputs[6]\n    else:\n        raise ValueError(\n            \"unexpected number of inputs for node {} ({}): {}\".format(\n                node.name, node.kind, len(inputs)\n            )\n        )\n\n    kwargs = {\n        \"x\": x,\n        \"strides\": strides,\n        \"pad_type\": \"custom\",\n        \"pad\": pad,\n        \"dilations\": dilations,\n        \"groups\": group,\n        \"name\": node.name,\n    }\n\n    # Bias is optional in PyTorch's convolution.\n    if bias is not None:\n        kwargs[\"bias\"] = bias\n\n    if transposed is True:\n        # Transposed convolution\n\n        # PyTorch weight ordering [Cin, Cout, *D]\n        # MIL expects [Cout, Cin, *D]\n        perm = _np.arange(len(weight.shape))\n        perm[[0, 1]] = perm[[1, 0]]\n        weight_transpose = mb.transpose(\n            x=weight, perm=perm, name=weight.name + \"_transpose\"\n        )\n\n        # Handle output_padding using pre-pad or post-crop\n        pre_pad = [0] * len(pad)\n        post_crop = [0] * len(pad)\n\n        if out_pad is not None and any(out_pad):\n            output_padding = [0] * len(pad)\n            # output padding adds additional padding on one of the side of dimension\n            # i.e. bottom from top-bottom,\n            #      right  from left-right\n            #      back   from front-back\n            # CoreML padding structure is similar [top, bottom, left, right]\n            # mapping output_padding to simplify further processing!\n            #\n            # For ConvTranspose2d: [bottom, right] -> [0, b, 0, r]\n            output_padding = [0 if i % 2 == 0 else out_pad[i//2] for i in range(len(pad))]\n            # TODO: rdar://65588783 ([PyTorch] Define and error out on unsupported configuration for output_padding)\n            # error out here with unsupported configuration along with output padding\n            if sum(pad) == 0 and any(output_padding):\n                raise ValueError(\"ConvTranspose configuration of padding=0 and output_padding > 0 not supported!\")\n            post_crop = pad.copy()\n            pad *= 0\n            for i in range(0, len(pad)):\n                if post_crop[i] >= output_padding[i]:\n                    post_crop[i] -= output_padding[i]\n                else:\n                    pre_pad[i] = output_padding[i] - post_crop[i]\n            kwargs[\"pad\"] = pre_pad\n            if any(pre_pad):\n                # Constant pad requires pad to be of length 2*input_rank\n                pre_pad = [0] * 2 * (len(x.shape) - 2) + pre_pad\n                x = mb.pad(x=x, pad=pre_pad)\n                kwargs[\"x\"] = x\n            if any(post_crop):\n                del kwargs[\"name\"]\n\n        kwargs[\"weight\"] = weight_transpose\n        conv = mb.conv_transpose(**kwargs)\n        if any(post_crop):\n            # TODO: rdar://65575826 (PyTorch converter: output_padding mapping to slice\n            # instead of crop layer for 1 and 3D ConvTranspose)\n            if len(post_crop) != 4:\n                raise ValueError(\"output_padding is supported only for ConvTranspose2D!\")\n            conv = mb.crop(x=conv, crop_height=post_crop[:2], crop_width=post_crop[2:4], name=node.name)\n    else:\n        # Normal convolution\n        kwargs[\"weight\"] = weight\n        conv = mb.conv(**kwargs)\n\n    context.add(conv)\n\n@register_torch_op\ndef softmax(context, node):\n    inputs = _get_inputs(context, node)\n\n    x = inputs[0]\n    axis = inputs[1]\n    res = mb.softmax(x=x, axis=axis, name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef flatten(context, node):\n    inputs = _get_inputs(context, node)\n\n    x = inputs[0]\n    dims = list(x.shape)\n    start_val = inputs[1].val\n    end_val = inputs[2].val\n\n    total = 1\n\n    start = len(dims) + start_val if start_val < 0 else start_val\n    end = len(dims) + end_val if end_val < 0 else end_val\n\n    if start > len(dims) or end > len(dims) or start < 0 or end < 0:\n        raise ValueError(\n            \"Invalid start and end. (start, end) == ({}, {})\".format(start, end_val)\n        )\n    if start > end:\n        raise ValueError(\n            \"Start must be before end. (start, end) == ({}, {})\".format(start, end_val)\n        )\n    for dim in dims[start : end + 1]:\n        total *= dim\n    dims = dims[:start] + [total] + dims[end + 1 :]\n\n    reshape = mb.reshape(x=x, shape=dims, name=node.name)\n    context.add(reshape)\n\n@register_torch_op\ndef softsign(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n\n    res = mb.softsign(x=inputs[0], name=node.name)\n    context.add(res)\n\n@register_torch_op(torch_alias=[\"relu_\"])\ndef relu(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n\n    res = mb.relu(x=inputs[0], name=node.name)\n    context.add(res)\n\n@register_torch_op\ndef prelu(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    x = inputs[0]\n    alpha = inputs[1]\n\n    res = mb.prelu(x=x, alpha=alpha, name=node.name)\n    context.add(res)\n\n@register_torch_op\ndef relu6(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n\n    res = mb.relu6(x=inputs[0], name=node.name)\n    context.add(res)\n\n@register_torch_op\ndef elu(context, node):\n    ## Torch port to ATen adds scale and input_scale which is set to 1\n    inputs = _get_inputs(context, node, expected=4)\n\n    res = mb.elu(x=inputs[0], alpha = inputs[1], name=node.name)\n    context.add(res)\n\n@register_torch_op(torch_alias=[\"leaky_relu_\"])\ndef leaky_relu(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n\n    res = mb.leaky_relu(x=inputs[0], alpha=inputs[1], name=node.name)\n    context.add(res)\n\n@register_torch_op\ndef softplus(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    x = inputs[0]\n    beta_ = inputs[1].val\n    C = x.shape[1]\n    alpha_br = _np.repeat(1.0 / beta_, C).astype('float32')\n    beta_br = _np.repeat(beta_, C).astype('float32')\n\n    res = mb.softplus_parametric(x=x, alpha = alpha_br, beta = beta_br, name=node.name)\n    context.add(res)\n\ndef _adjust_pad_for_ceil_mode(input_shape, kernel_sizes, stride_sizes, pad_sizes):\n    \"\"\" TODO Given an input tensor and pooling parameters, add the extra input\n        padding needed to replicate ceil_mode. If no padding is needed, returns\n        the original input. Otherwise, returns the Var returned by the new\n        padding op.\n    \"\"\"\n    new_pad = pad_sizes\n    for idx in range(len(input_shape)):\n        dim = input_shape[idx]\n        kernel = kernel_sizes[idx]\n        stride = stride_sizes[idx]\n        pad = pad_sizes[idx * 2 : idx * 2 + 2]\n        out_numerator = dim + pad[0] + pad[1] - kernel\n        remainder = out_numerator % stride\n        # Additional padding is added only on one side.\n        # https://stackoverflow.com/questions/59906456/in-pytorchs-maxpool2d-is-padding-added-depending-on-ceil-mode\n        if remainder > 0:\n            # MIL pooling does not support ceil_mode natively, but we can\n            # workaround by padding the input appropriately.\n            # rdar://60634390\n            _logging.warning(\"pooling padding adjusted to support ceil_mode=True\")\n            new_pad[2 * idx + 1] += stride - remainder\n\n    return new_pad\n\n\ndef _max_pool(context, node, inputs):\n    x = inputs[0]\n    kernel_sizes = inputs[1]\n    strides = inputs[2]\n    if strides.op.op_type == \"const\"  and (not list(strides.val)):\n        strides = mb.const(val=kernel_sizes.val, name=strides.name)\n\n    pad_type = \"custom\"\n    # Need to explicitly state L-R, T-B pad\n    pad = inputs[3]\n    pad = _np.repeat(pad.val, 2)\n    dilation = inputs[4].val\n    ceil_mode = inputs[5].val\n    if _np.any(dilation > 1):\n        # See: rdar://60633736 (Implement dilation for mil op max_pool)\n        raise ValueError(\"@max_pool does not support dilation > 1\")\n    if ceil_mode is True:\n        pad = _adjust_pad_for_ceil_mode(\n            x.shape[-2:], kernel_sizes.val, strides.val, pad\n        )\n\n    pool = mb.max_pool(\n        x=x,\n        kernel_sizes=kernel_sizes,\n        strides=strides,\n        pad_type=pad_type,\n        pad=pad,\n        name=node.name,\n    )\n    context.add(pool)\n\n\n@register_torch_op\ndef max_pool1d(context, node):\n    inputs = _get_inputs(context, node, expected=6)\n    _max_pool(context, node, inputs)\n\n\n@register_torch_op\ndef max_pool2d(context, node):\n    inputs = _get_inputs(context, node, expected=6)\n    _max_pool(context, node, inputs)\n\n\n@register_torch_op\ndef max_pool3d(context, node):\n    inputs = _get_inputs(context, node, expected=6)\n    _max_pool(context, node, inputs)\n\n\n@register_torch_op\ndef div(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n\n    res = mb.real_div(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(res)\n\n\n@register_torch_op(torch_alias=[\"floordiv\"])\ndef floor_divide(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    div_res = mb.floor_div(x=inputs[0], y=inputs[1])\n    # Pytorch's floor_divide always returns fp32, even if the inputs are int\n    res = mb.cast(x=div_res, dtype='fp32', name=node.name)\n    context.add(res)\n\n@register_torch_op\ndef true_divide(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    res = mb.real_div(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef mul(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n\n    res = mb.mul(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(res)\n\n\n@register_torch_op(torch_alias=[\"pow\"])\ndef pow_(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n\n    res = mb.pow(x=inputs[0], y=inputs[1], name=node.name)\n    context.add(res)\n\n\n@register_torch_op(torch_alias=[\"rsub\"])\ndef sub(context, node):\n    inputs = _get_inputs(context, node, expected=[2, 3])\n    if len(node.outputs) != 1:\n        raise AssertionError\n\n    if node.kind == \"rsub\":\n        # rsub reverses the order of arguments\n        y = inputs[0]\n        x = inputs[1]\n    else:\n        x = inputs[0]\n        y = inputs[1]\n\n    if len(inputs) > 2:\n        alpha = inputs[2].val\n\n        # TODO (sberardi): 3rd param to aten::sub is a scale factor, need to handle that.\n        # out=input-alpha x other\n        # rdar://60175736\n        if alpha != 1:\n            raise ValueError(\"SUB does not support scale factor param\")\n\n    res = mb.sub(x=x, y=y, name=node.name)\n    context.add(res)\n\n\n@register_torch_op(torch_alias=[\"sum\"])\ndef mean(context, node):\n    inputs = _get_inputs(context, node)\n\n    kwargs = {\"x\": inputs[0], \"name\": node.name}\n\n    # @axes is optional, so omit if None.\n    axes = inputs[1]\n    if axes is not None:\n        # @axes needs to be a list, but if only one axis was specified in the\n        # model, it will be constructed as an int. Construct a new constant as a\n        # list.\n        if not isinstance(axes.val, _np.ndarray):\n            axes = mb.const(val=[axes.val], name=axes.name + \"_list\")\n            context.add(axes)\n        kwargs[\"axes\"] = axes\n\n    # @keep_dims is optional.\n    if len(inputs) >= 3:\n        keep_dims = inputs[2]\n        kwargs[\"keep_dims\"] = keep_dims\n\n    # Last input to mean is an optional output tensor. We always expect this to\n    # be None or absent.\n    if not (len(inputs) <= 3 or inputs[3] is None):\n        raise AssertionError\n    func = mb.reduce_sum if node.kind == \"sum\" else mb.reduce_mean\n    res = func(**kwargs)\n    context.add(res)\n\n@register_torch_op\ndef squeeze(context, node):\n    inputs = _get_inputs(context, node)\n    if len(inputs) == 1:\n        res = mb.squeeze(x=inputs[0], name=node.name)\n    elif len(inputs) == 2:\n        squeeze_dim = inputs[1].val\n        res = mb.squeeze(x=inputs[0], axes=(squeeze_dim,), name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef unsqueeze(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    unsqueeze = mb.expand_dims(x=inputs[0], axes=[inputs[1].val], name=node.name)\n    context.add(unsqueeze)\n\n\n@register_torch_op\ndef size(context, node):\n    inputs = _get_inputs(context, node, expected=[1, 2])\n\n    # Get the shape of the tensor.\n    size_node = mb.shape(x=inputs[0], name=node.name + \"_shape\")\n    # Get the size of the tensor along the input dimension.\n    if len(node.inputs) == 2:\n        dim = inputs[1].val\n        size_node = _list_select(size_node, dim)\n    context.add(size_node, node.name)\n\n\n@register_torch_op(torch_alias=[\"reshape\"])\ndef view(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n    x = inputs[0]\n    shape = inputs[1]\n\n    if isinstance(shape, ListVar):\n        length = mb.list_length(ls=shape)\n        indices = mb.range_1d(start=0, end=length, step=1)\n        shape = mb.list_gather(ls=shape, indices=indices)\n\n    if isinstance(shape, list) and all([isinstance(dim, Var) and len(dim.shape) == 0 for dim in shape]) and any([dim.val is None for dim in shape]):\n        shape = mb.concat(values=shape, axis=0)\n\n    view = mb.reshape(x=x, shape=shape, name=node.name)\n    context.add(view)\n\n\n@register_torch_op\ndef adaptive_avg_pool2d(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n\n    _input = inputs[0]\n    output_size = inputs[1].val\n    if not isinstance(output_size, _np.ndarray):\n        raise AssertionError\n    output_size = tuple(output_size)\n\n    if output_size == (1, 1):\n        # Represent (1,1) output size via @reduce_mean\n        # Assume channel first ordering, reduce the last two (HW) dims.\n        axes = mb.const(val=[-2, -1], name=node.name + \"_axes\")\n        keep_dims = mb.const(val=True, name=node.name + \"_keep_dims\")\n\n        avg_pool = mb.reduce_mean(\n            x=_input, axes=axes, keep_dims=keep_dims, name=node.name\n        )\n    elif _input.shape is not None:\n        # TODO: The calculations to convert adaptive_pool to standard pool,\n        # given a known input size, come from\n        # https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work\n        # However, as indicated in that SO, this isn't quite how PyTorch\n        # computes adaptive pooling, leading to inaccuracies in model outputs.\n        # rdar://60900834\n        strides = [ind // outd for ind, outd in zip(_input.shape[-2:], output_size)]\n        pad_type = \"valid\"\n        # Need to explicity state L-R, T-B pad\n        pad = [0, 0, 0, 0]\n        dilation = [1, 1]\n        kernel_sizes = [\n            ind - s * (outd - 1)\n            for ind, outd, s in zip(_input.shape[-2:], output_size, strides)\n        ]\n        avg_pool = mb.avg_pool(\n            x=_input,\n            kernel_sizes=kernel_sizes,\n            strides=strides,\n            pad_type=pad_type,\n            pad=pad,\n            name=node.name,\n        )\n    else:\n        raise ValueError(\n            \"adaptive_avg_pool2d only supported when input tensor size is known or output size == (1,1). Recived: input size == {}, output size == {}\".format(\n                _input.shape_str(), output_size,\n            )\n        )\n\n    context.add(avg_pool)\n\n@register_torch_op\ndef adaptive_max_pool2d(context, node):\n    inputs = _get_inputs(context, node, expected=2)\n\n    _input = inputs[0]\n    output_size = inputs[1].val\n    if not isinstance(output_size, _np.ndarray):\n        raise AssertionError\n    output_size = tuple(output_size)\n\n    if output_size == (1, 1):\n        # Represent (1,1) output size via @reduce_max\n        # Assume channel first ordering, reduce the last two (HW) dims.\n        max_pool = mb.reduce_max(\n            x=_input, axes=[-2,-1], keep_dims=True, name=node.name\n        )\n    elif _input.shape is not None:\n        # TODO: The calculations to convert adaptive_pool to standard pool,\n        # given a known input size, come from\n        # https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work\n        # However, as indicated in that SO, this isn't quite how PyTorch\n        # computes adaptive pooling, leading to inaccuracies in model outputs.\n        # rdar://60900834\n        strides = [ind // outd for ind, outd in zip(_input.shape[-2:], output_size)]\n        pad_type = \"valid\"\n        # Need to explicity state L-R, T-B pad\n        pad = [0, 0, 0, 0]\n        dilation = [1, 1]\n        kernel_sizes = [\n            ind - s * (outd - 1)\n            for ind, outd, s in zip(_input.shape[-2:], output_size, strides)\n        ]\n        max_pool = mb.max_pool(\n            x=_input,\n            kernel_sizes=kernel_sizes,\n            strides=strides,\n            pad_type=pad_type,\n            pad=pad,\n            name=node.name,\n        )\n    else:\n        raise ValueError(\n            \"adaptive_max_pool2d only supported when input tensor size is known or output size == (1,1). Recived: input size == {}, output size == {}\".format(\n                _input.shape_str(), output_size,\n            )\n        )\n\n    context.add(max_pool)\n\n\n\n@register_torch_op\ndef batch_norm(context, node):\n    inputs = _get_inputs(context, node, expected=9)\n    # inputs skipped:\n    #   bool training (5)\n    #   float momentum (6)\n    #   bool cudnn_enabled (8)\n    _input = inputs[0]\n    weight = inputs[1]\n    bias = inputs[2]\n    running_mean = inputs[3]\n    running_var = inputs[4]\n    eps = inputs[7]\n    batch_norm = mb.batch_norm(\n        x=_input,\n        mean=running_mean,\n        variance=running_var,\n        gamma=weight,\n        beta=bias,\n        epsilon=eps,\n        name=node.name,\n    )\n    context.add(batch_norm)\n\n\n@register_torch_op\ndef instance_norm(context, node):\n    inputs = _get_inputs(context, node, expected=9)\n    x = inputs[0]\n    weight = inputs[1]\n    bias = inputs[2]\n    eps = inputs[7]\n    x = mb.instance_norm(\n        x=x,\n        gamma=weight,\n        beta=bias,\n        epsilon=eps,\n        name=node.name,\n    )\n    context.add(x)\n\n\n@register_torch_op\ndef embedding(context, node):\n    inputs = _get_inputs(context, node)\n    _input = inputs[0]\n    indices = inputs[1]\n\n    padding_idx = -1\n    scale_grad_by_freq = False\n    sparse = False\n    if len(inputs) >= 3:\n        padding_idx = inputs[2].val\n    if len(inputs) >= 4:\n        scale_grad_by_freq = inputs[3].val\n    if len(inputs) >= 5:\n        sparse = inputs[4].val\n\n    if padding_idx != -1 or scale_grad_by_freq or sparse:\n        _logging.warning(\n            \"CoreML embedding (gather) layer does not support any \"\n            \"inputs besides the weights and indices. Those given \"\n            \"will be ignored.\"\n        )\n\n    #  Changing the axis from 0 is not an option in torch, so we don't expose it\n    gather = mb.gather(x=_input, indices=indices, name=node.name)\n    context.add(gather)\n\n\n@register_torch_op(torch_alias=[\"hardtanh_\"])\ndef hardtanh(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    _input = inputs[0]\n    min_val = inputs[1].val\n    max_val = inputs[2].val\n\n    res = mb.clip(x=_input, alpha=min_val, beta=max_val, name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef cat(context, node):\n    inputs = _get_inputs(context, node)\n    axis = 0 if len(inputs) == 1 else inputs[1]\n    concat = mb.concat(values=inputs[0], axis=axis, name=node.name)\n    context.add(concat)\n\n\n@register_torch_op\ndef stack(context, node):\n    inputs = _get_inputs(context, node)\n\n    values = inputs[0]\n\n    if len(inputs) < 2:\n        axis = 0\n    else:\n        axis = inputs[1]\n\n    res = mb.stack(values=values, axis=axis, name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef item(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n\n    if inputs[0].shape == ():\n        # MIL ops that reduce already output a scalar, so no need to do\n        # anything.\n        res = inputs[0]\n    elif _np.all([d == 1 for d in inputs[0].shape]):\n        # Item only makes sense when called on a length 1 tensor. We use\n        # reduce_max as a workaround for not having a way to extract a scalar\n        # from a symbolic tensor.\n        res = mb.reduce_max(x=inputs[0], name=node.name)\n    else:\n        raise ValueError(\"expected input to be a scalar or a length 1 tensor\")\n    context.add(res, node.name)\n\n\ndef _cast(context, node, dtype, dtype_name):\n    inputs = _get_inputs(context, node, expected=1)\n    x = inputs[0]\n    # Input must either be a scalar or a (1 x 1 x ... x 1) tensor\n    if not (len(x.shape) == 0 or _np.all([d == 1 for d in x.shape])):\n        raise ValueError(\"input to cast must be either a scalar or a length 1 tensor\")\n\n    if x.val is not None:\n        # If x is a compile-time constant, directly cast it to @dtype if it's\n        # not one already.\n        if not isinstance(x.val, dtype):\n            res = mb.const(val=dtype(x.val), name=node.name)\n        else:\n            res = x\n    elif x.shape == (1,):\n        x = mb.squeeze(x=x, name=node.name + \"_item\")\n        res = mb.cast(x=x, dtype=dtype_name, name=node.name)\n    else:\n        if len(x.shape) > 0:\n            # TODO: There's no MIL op to extract a value from a symbolic tensor,\n            # so as a workaround we use reduce_max to convert it to a scalar.\n            x = mb.reduce_max(x=x, name=node.name + \"_item\")\n        res = mb.cast(x=x, dtype=dtype_name, name=node.name)\n    context.add(res, node.name)\n\n\n@register_torch_op(torch_alias=[\"bool\"])\ndef _bool(context, node):\n    _cast(context, node, bool, \"bool\")\n\n\n@register_torch_op(torch_alias=[\"int\"])\ndef _int(context, node):\n    _cast(context, node, int, \"int32\")\n\n\n@register_torch_op\ndef layer_norm(context, node):\n    inputs = _get_inputs(context, node, expected=6)\n    _input = inputs[0]\n    normalized_shape = inputs[1]\n    weight = inputs[2]\n    bias = inputs[3]\n    eps = inputs[4]\n    # cudnn_enable = inputs[5] unused\n\n    layer_norm = mb.layer_norm(\n        x=_input,\n        axes=list(range(-len(normalized_shape.val),0)),\n        gamma=weight,\n        beta=bias,\n        epsilon=eps,\n        name=node.name,\n    )\n    context.add(layer_norm)\n\n\n@register_torch_op\ndef numtotensor(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    x = inputs[0]\n    if x.shape != ():\n        raise ValueError(\n            \"numtotensor expected scalar input, got tensor with shape {}\".format(\n                x.shape\n            )\n        )\n    if isinstance(x.sym_val, Symbol):\n        context.add(x, node.name)\n    else:\n        res = mb.const(val=[x.sym_val], name=node.name)\n        context.add(res)\n\n\ndef _ifzo_to_ifoz(weights, name):\n    \"\"\"\n        i, f, z, o -> i, f, o, z\n        where weights_split[0] == i, etc.\n        Used to transform lstm weights from pytorch\n        to CoreML format\n    \"\"\"\n    split_size = weights.shape[0] // 4\n    weights_split = mb.split(x=weights, split_sizes=_np.array([split_size] * 4), axis=0)\n    weights_concat = mb.concat(\n        values=[weights_split[0], weights_split[1], weights_split[3], weights_split[2]],\n        axis=0,\n    )\n    # make transpose a noOP for 0/1d tensors\n    return mb.transpose(\n        x=weights_concat, perm=([1, 0] if len(weights.shape) > 1 else [0]), name=name\n    )\n\n\ndef _pytorch_hidden_to_coreml_milops(x, name):\n    \"\"\"\n        Used to transform lstm state values (hn, cn)\n        from pytorch to CoreML format.\n    \"\"\"\n    split_size = x.shape[0] // 2\n    x_split = mb.split(x=x, split_sizes=_np.array([split_size] * 2), axis=0)\n    x_concat = mb.concat(values=[x_split[0], x_split[1]], axis=2,)\n    # (4.) See docstring to @lstm\n    return mb.squeeze(x=x_concat, axes=_np.array([0]), name=name)\n\n\n@register_torch_op\ndef lstm(context, node):\n    inputs = _get_inputs(context, node, expected=9)\n\n    _input = inputs[0]\n    h0, c0 = inputs[1]\n    weights = inputs[2]\n    bias = inputs[3].val\n    num_layers = inputs[4].val\n    dropout = inputs[5]\n    bidirectional = inputs[7].val\n    batch_first = inputs[8].val\n\n    if num_layers != 1:\n        raise ValueError(\n            \"CoreML does not support stacked LSTM layers (LSTM \"\n            \"with num_layers > 1). Received {}. Redefine as \"\n            \" multiple layers if this is the desired \"\n            \"implementation.\".format(num_layers)\n        )\n\n    if batch_first:\n        _input = mb.transpose(x=_input, perm=[1, 0, 2], name=_input.name + \"_batch_first_transpose\")\n\n    expected_num_weights = 2 * num_layers * (int(bidirectional) + 1) * (int(bias) + 1)\n    if len(weights) != expected_num_weights:\n        raise ValueError(\n            \"Incorrect weights shape for lstm layer: Expected: {}. Recieved {}\".format(\n                expected_num_weights, len(weights)\n            )\n        )\n\n    # NOTE:\n    # Most of this code is to transform the tensors into\n    # a shape acceptable by the CoreML implementation of LSTM.\n    # Since this transforming is complicated and unintuitive we include\n    # a description of what is happening:\n\n    # For weights, biases and per direction, pytorch uses two tensors:\n    # (ii, if, ig, io) stacked on top of eachother for each layer (tensor 1)\n    # and (hi, hf, hg, ho) stacked on top of eachother for each layer (tensor 2)\n    # These weights are used in the calculation of the layers found in the torch.nn documentation:\n    # https://pytorch.org/docs/stable/nn.html\n\n    # The CoreML LSTM op expects two tensors, weight and bias. So\n    # the tensors for weight and bias are seperated from pytorch's @weights list (1.).\n    # For each individual weight and bias tensor, the CoreML LSTM op expects the form\n    # ii, if, io, ig and hi, hf, ho, hg, requiring the ifzo_to_ifoz function (2.).\n    # Each seperate weight and bias tensor is concatinated to\n    # form the two weight and bias tensors. (3.)\n    # In the bidirectional case, the forward and backward weights and biases\n    # are stacked on top of eachother instead of stored as seperate tensors in\n    # the @weights list. (4.)\n\n    # In the initial cell and hidden states, pytorch's tensor's 0th\n    # dimension stores each layer and direction.\n    # However, since CoreML's LSTM allows only one layer, the direction is squeezed out the state\n    # tensor. (4.)\n    # In the bidirectional case, the forward and backward state tensors are stacked on top of eachother.\n    # instead of stored in the layer and direction dimension\n    # using @_pytorch_hidden_to_coreml_milops (5.).\n\n    # For output: The CoreML LSTM op returns the final states with the first dimension: @num_layers\n    # squeezed out. To fit with the rest of the shapes expected down the line in\n    # the TorchIR graph- we unsqueeze that dimension in the final state output. (6.)\n    if bidirectional:\n        if bias:\n            # (1.)\n            biases = weights[2:4] + weights[6:8]\n            weights = weights[0:2] + weights[4:6]\n\n            # (2.)\n            if len(biases) != 4:\n                raise AssertionError\n            for index in range(len(biases)):\n                biases[index] = _ifzo_to_ifoz(\n                    biases[index],\n                    name=\"{}_lstm_bias_reshape_{}\".format(node.name, index),\n                )\n\n            # (4.)\n            f_stack = mb.stack(values=biases[0:2], axis=0,)\n            r_stack = mb.stack(values=biases[2:4], axis=0,)\n            # (3.)\n            final_biases = mb.concat(\n                values=(f_stack, r_stack),\n                axis=1,\n                name=node.name + \"_lstm_biases_concat\",\n            )\n\n        # (4.)\n        forward_concat = mb.concat(\n            values=[weights[0], weights[1]],\n            axis=1,\n            name=node.name + \"_lstm_weights_forward_concat\",\n        )\n        backward_concat = mb.concat(\n            values=[weights[2], weights[3]],\n            axis=1,\n            name=node.name + \"_lstm_weights_backward_concat\",\n        )\n        # (2.)\n        forward_transformed = _ifzo_to_ifoz(\n            forward_concat, name=node.name + \"_lstm_forward_weights_ifoz_to_ifzo\",\n        )\n        backward_transformed = _ifzo_to_ifoz(\n            backward_concat, name=node.name + \"_lstm_backward_weights_ifoz_to_ifzo\"\n        )\n        # (3.)\n        final_weights = mb.concat(\n            values=[forward_transformed, backward_transformed],\n            axis=1,\n            name=node.name + \"_lstm_weights_final_concat\",\n        )\n\n        # (5.)\n        h = _pytorch_hidden_to_coreml_milops(h0, name=\"_lstm_h0_reshaped\")\n        c = _pytorch_hidden_to_coreml_milops(c0, name=\"_lstm_c0_reshaped\")\n\n    else:\n        if bias:\n            # (1.)\n            biases = weights[len(weights) // 2 :]\n            weights = weights[: len(weights) // 2]\n            ih_b = biases[0]\n            hh_b = biases[1]\n\n            # (2.)\n            ih_b_transformed = _ifzo_to_ifoz(\n                ih_b, name=node.name + \"_lstm_ih_bias_transformed\",\n            )\n            hh_b_transformed = _ifzo_to_ifoz(\n                hh_b, name=node.name + \"_lstm_hh_bias_transformed\",\n            )\n\n            # (3.)\n            final_biases = mb.stack(\n                values=(ih_b_transformed, hh_b_transformed),\n                axis=0,\n                name=node.name + \"_lstm_bias_stacked\",\n            )\n\n        # (3.)\n        weights_concat = mb.concat(\n            values=weights, axis=1, name=node.name + \"_lstm_weights_concat\"\n        )\n        # (2.)\n        final_weights = _ifzo_to_ifoz(\n            weights_concat, name=node.name + \"_lstm_weights_ifoz_to_ifzo\",\n        )\n\n        # (4.)\n        h = mb.squeeze(x=h0, axes=_np.array([0]), name=node.name + \"_lstm_h0_squeeze\")\n        c = mb.squeeze(x=c0, axes=_np.array([0]), name=node.name + \"_lstm_c0_squeeze\")\n\n    lstm = mb.lstm(\n        x=_input,\n        initial_h=h,\n        initial_c=c,\n        weight=final_weights,\n        bias=(final_biases if bias else None),\n        direction=(\"bidirectional\" if bidirectional is True else \"forward\"),\n        output_sequence=True,\n        name=node.name if not batch_first else node.name + \"_batch_first\",\n    )\n\n    # (6.)\n    for index, (name, output) in enumerate(zip(node.outputs, lstm)):\n        if index > 0:\n            # Add in @num_layers in first dimension to hn, cn output\n            unsqueeze = mb.expand_dims(x=output, axes=[0], name=name)\n            context.add(unsqueeze)\n        else:\n            if batch_first:\n                output = mb.transpose(x=output, perm=[1, 0, 2], name=name)\n            context.add(output, name)\n\ndef _get_scales_from_output_size(output_size, input_shape):\n    scales = []\n    if output_size is not None:\n        # @output_size will be a list if scales was provided or a\n        # single var if output size was provided\n        if isinstance(output_size, list):\n            output_size = [output_size[0].val, output_size[1].val]\n        if isinstance(output_size, Var):\n            output_size = [output_size.val[0], output_size.val[1]]\n\n        # output size is computed using the formula\n        # floor (scale * input_size) in Core ML (and PyTorch)\n        # Thus, when computing the scales from the output size,\n        # add a small positive constant to the output size,\n        # to make sure that the floor formula results in the correct output\n        # size and not 1 unit smaller, due to float precision issues\n        # e.g. if output size = 34 and input size = 2, then scale will be\n        # 17, which can get represented as 16.9999, resulting in an output size of 33\n        # instead of 34, without this correction.\n        scales_h = (output_size[0] + 1e-4) / float(input_shape[-2])\n        scales_w = (output_size[1] + 1e-4) / float(input_shape[-1])\n        scales = [scales_h, scales_w]\n    return scales\n\n@register_torch_op\ndef upsample_bilinear2d(context, node):\n    inputs = _get_inputs(context, node)\n    _input = inputs[0]\n    output_size = inputs[1]\n    align_corners = bool(inputs[2].val)\n\n    if len(inputs) == 5:\n        # For torch==1.5.0, upsample_bilinear2d has 5 inputs.\n        scales_h = inputs[3]\n        scales_w = inputs[4]\n\n    scales = _get_scales_from_output_size(output_size, _input.shape)\n    if scales:\n        scales_h, scales_w = scales\n\n    upsample_bilinear = mb.upsample_bilinear(\n        x=_input,\n        scale_factor_height=scales_h,\n        scale_factor_width=scales_w,\n        align_corners=align_corners,\n        name=node.name,\n    )\n    context.add(upsample_bilinear)\n\n@register_torch_op\ndef upsample_nearest2d(context, node):\n    inputs = _get_inputs(context, node)\n    _input = inputs[0]\n    output_size = inputs[1]\n    if len(inputs) == 4:\n        scales_h = inputs[2]\n        scales_w = inputs[3]\n\n    scales = _get_scales_from_output_size(output_size, _input.shape)\n    if scales:\n        scales_h, scales_w = scales\n\n    if (\n        abs(scales_h - round(scales_h)) > 0.001\n        or abs(scales_w - round(scales_w)) > 0.001\n    ):\n        raise ValueError(\"Layer upsample_nearest2d only supports integral scales. Provided scales: {}. \"\n                         \"Please use upsample_bilinear2d for fractional scales\".format(scales))\n\n    upsample_nearest2d = mb.upsample_nearest_neighbor(\n        x=_input,\n        upscale_factor_height=int(round(scales_h)),\n        upscale_factor_width=int(round(scales_w)),\n        name=node.name,\n    )\n    context.add(upsample_nearest2d)\n\n@register_torch_op(torch_alias=[\"listunpack\"])\ndef tupleunpack(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    values = inputs[0]\n    # Node input could have been turned into constant array in @tupleconstruct\n    if not isinstance(values, tuple) and not isinstance(values, list):\n        values = values.val\n    if len(values) != len(node.outputs):\n        raise ValueError(\n            \"unpack node expected {} outputs, got {}\".format(\n                len(node.outputs), len(values)\n            )\n        )\n    if len(values) != len(node.outputs):\n        raise AssertionError\n    # @value is either a numpy primitive or a Var object\n    for value, output in zip(values, node.outputs):\n        if not isinstance(value, Var):\n            value = _construct_constant(value, name=output)\n        if not isinstance(value, Var):\n            raise AssertionError\n        context.add(value, output)\n\n\n@register_torch_op\ndef loop(context, node):\n    \"\"\" In TorchIR, a loop looks like:\n            %y_1, ..., %y_r = prim::Loop(%max_trip_count, %initial_condition, %x_1, ..., %x_r)\n            block0(%i, %a_1, ..., %a_r):\n                %b_1, ..., %b_m = some::node(%a_value_from_outer_block, %a_1)\n                %iter_condition = some::other_node(%a_2)\n                -> (%iter_condition, %b_1, ..., %b_r)\n\n        This translates to pseudo code as:\n            y_1, ..., y_r = x_1, ..., x_r\n            condition = initial_condition\n            i = 0\n            while condition and i < max_trip_count:\n                a_1, ..., a_r = y_1, ..., y_r\n\n                ############################################################\n                # Actual body of the loop\n                b_1, ..., b_m = some::node(a_value_from_outside_of_the_loop, a_1)\n                iter_condition = some::node(a_2)\n                ############################################################\n\n                y_1, ..., y_r = b_1, ..., b_r\n                condition = iter_condition\n                i += 1\n\n        Which further translates to MIL while_loop as:\n            loop_vars = (0, initial_condition, x_1, ..., x_r)\n            _cond = {\n                return (loop_vars[1] and loop_vars[0] < max_trip_count)\n            }\n            _body = {\n                a_1, ..., a_r = loop_vars[2], ..., loop_vars[-1]\n                b_1, ..., b_m = some::node(a_value_from_outside_of_the_loop, a_1)\n                iter_condition = some::node(a_2)\n                return (loop_vars[0] + 1, iter_condition, b_1, ..., b_r)\n            }\n\n        For loops pass True for %initial_condition and %iter_condition\n        While loops set %max_trip_count to INT_MAX and %i is unused\n    \"\"\"\n    name = node.name\n    # inputs[0]: max iter count\n    # inputs[1]: initial condition\n    # inputs[2]: block input 0\n    # ...\n    # inputs[N+2]: block input N\n    inputs = _get_inputs(context, node)\n    max_iter_count = inputs[0]\n\n    # Magic default signals this is a while-only loop, so no iteration count\n    # is needed.\n    has_iter_count = max_iter_count is not None\n\n    # Create an interation count. This will only be used if this is a for loop.\n    iter_count = mb.const(val=0, name=node.name + \"_iter\")\n    # @loop_vars is tuple(iter_count, cond, inputs...)\n    loop_vars = tuple([iter_count] + inputs[1:])\n\n    def _loop_cond(*loop_vars):\n        cond = loop_vars[1]\n\n        # Check the iteration count if we're keeping track.\n        if has_iter_count:\n            iter_count = loop_vars[0]\n            iter_cond = mb.less(\n                x=iter_count, y=max_iter_count, name=node.name + \"_cond\"\n            )\n            return mb.logical_and(x=cond, y=iter_cond)\n        else:\n            return mb.identity(x=cond)\n\n    def _shapes_are_equivalent(shape1, shape2):\n        \"\"\" Compares two sets of tensor shapes and returns True if they are\n            equivalent. That is, they are the same rank, and each dimension\n            is the same or symbolic.\n        \"\"\"\n        if len(shape1) != len(shape2):\n            return False\n\n        # Each dimension must have the same integer length, or else be\n        # symbolic.\n        all_equivalent = [\n            s1 == s2 or (isinstance(s1, Symbol) and isinstance(s2, Symbol))\n            for s1, s2 in zip(shape1, shape2)\n        ]\n        return all_equivalent\n\n    def _loop_body(*loop_vars):\n        block = node.blocks[0]\n        iter_var = loop_vars[0]\n        inputs = (iter_var,) + loop_vars[2:]\n        res = convert_block(context, block, inputs)\n\n        for input_var, output_var in zip(loop_vars[2:], res[1:]):\n            if not _shapes_are_equivalent(input_var.shape, output_var.shape):\n                _logging.warning(\n                    \"detected change in shape of loop variable. this could lead to incorrect inference results!\"\n                )\n                _logging.warning(\n                    \"{}:{} -> {}:{}\".format(\n                        input_var.name,\n                        input_var.shape,\n                        output_var.name,\n                        output_var.shape,\n                    )\n                )\n\n        # Update the iteration count if we're keeping track.\n        if has_iter_count:\n            iter_var = mb.add(x=iter_var, y=1, name=iter_var.name + \"_inc\")\n        else:\n            iter_var = mb.identity(x=iter_var)\n\n        # Must return tuple with same length and types as @loop_vars.\n        return tuple([iter_var,] + res)\n\n    loop = mb.while_loop(\n        _cond=_loop_cond, _body=_loop_body, loop_vars=loop_vars, name=name\n    )\n\n    # Make sure the loop returned the expected number of outputs. Note that the\n    # first two loop outputs are the iteration count and condition.\n    if len(loop) - 2 != len(node.outputs):\n        raise AssertionError\n    for output_name, output_var in zip(node.outputs, loop[2:]):\n        context.add(output_var, torch_name=output_name)\n\n\n@register_torch_op(torch_alias=[\"if\"])\ndef _if(context, node):\n    \"\"\" In TorchIR, a conditional looks like:\n            %y_1, ..., %y_r = prim::If(%condition)\n            block0():  # TRUE BRANCH, never takes arguments, has to return r outputs\n                %t_1, ..., %t_k = some::node(%a_value_from_outer_block)\n                -> (%t_1, ..., %t_r)\n            block1():  # FALSE BRANCH, never takes arguments, has to return r outputs\n                %f_1, ..., %f_m = some::node(%a_value_from_outer_block)\n                -> (%f_1, ..., %f_r)\n\n        This translates to pseudo code as:\n            if (condition):\n                t_1, ..., t_k = some::node(a_value_from_outer_block)\n                y_1, ..., y_r = t_1, ..., t_r\n            else:\n                f_1, ..., f_m = some::node(a_value_from_outer_block)\n                y_1, ..., y_r = f_1, ..., f_r\n\n        Which further translates to MIL cond as:\n            _true = {\n                t_1, ..., t_k = some::node(a_value_from_outer_block)\n                return (t_1, ..., t_r)\n            }\n            _false = {\n                f_1, ..., f_m = some::node(a_value_from_outer_block)\n                return (f_1, ..., f_m)\n            }\n    \"\"\"\n    name = node.name\n    # inputs[0]: condition\n    inputs = _get_inputs(context, node, expected=1)\n    condition = inputs[0]\n\n    if len(node.blocks) != 2:\n        raise AssertionError\n    true_block = node.blocks[0]\n    false_block = node.blocks[1]\n\n    def _true_path():\n        res = convert_block(context, true_block, [])\n        return tuple(res)\n\n    def _false_path():\n        res = convert_block(context, false_block, [])\n        return tuple(res)\n\n    cond = mb.cond(\n        pred=condition, _true_fn=_true_path, _false_fn=_false_path, name=name\n    )\n    # If the condition only returns one item, wrap it in a tuple.\n    if not isinstance(cond, tuple):\n        cond = (cond,)\n\n    # Make sure the condition returned the expected number of outputs.\n    if len(cond) != len(node.outputs):\n        raise AssertionError\n    for output_name, output_var in zip(node.outputs, cond):\n        context.add(output_var, torch_name=output_name)\n\n\n@register_torch_op\ndef select(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    _input = inputs[0]\n    dim = inputs[1].val\n    index = inputs[2].val\n\n    if dim.shape != ():\n        raise AssertionError\n    if index.shape != ():\n        raise AssertionError\n    if _input.val is not None:\n        raise AssertionError\n\n    # NOTE:\n    # Each index in @begin_array/@end_array corresponds to a dimension of @_input\n    # Each val of those arrays corresponds to the start/end index to slice in that dimension\n    begin_array = [0] * len(_input.shape)\n    begin_array[dim] = index\n    end_array = [s if isinstance(s, int) else 0 for s in _input.shape]\n    end_mask = [True] * len(_input.shape)\n    if index != -1:\n        end_array[dim] = index + 1\n        end_mask[dim] = False\n\n    slice_by_index = mb.slice_by_index(\n        x=_input,\n        begin=begin_array,\n        end=end_array,\n        end_mask=end_mask,\n        name=node.name + \"_slice_by_index\",\n    )\n    # Now we squeeze the dimension we have selected from to remove it\n    squeeze = mb.squeeze(\n        x=slice_by_index, axes=_np.array([dim]), name=node.name + \"_squeeze\"\n    )\n    context.add(squeeze, node.name)\n\n\n@register_torch_op\ndef ones(context, node):\n    inputs = _get_inputs(context, node, expected=[5, 6])\n    size = inputs[0]\n    # dtype = NUM_TO_TORCH_DTYPE[inputs[1].val] unused\n    # layout = inputs[2] unused\n    # device = inputs[3] unused\n    # requires_grad = inputs[4] unused\n    # out = inputs[5] unused\n    fill = mb.fill(shape=size, value=1.0, name=node.name)\n    context.add(fill)\n\n\n@register_torch_op\ndef ones_like(context, node):\n    inputs = _get_inputs(context, node, expected=6)\n    size = mb.shape(x=inputs[0])\n    # dtype = NUM_TO_TORCH_DTYPE[inputs[1].val] unused\n    # layout = inputs[2] unused\n    # device = inputs[3] unused\n    # requires_grad = inputs[4] unused\n    # out = inputs[5] unused\n    fill = mb.fill(shape=size, value=1.0, name=node.name)\n    context.add(fill)\n\n\ndef _avg_pool(context, node, inputs):\n    x = inputs[0]\n    kernel_sizes = inputs[1]\n    strides = inputs[2]\n    if strides.op.op_type == \"const\"  and (not list(strides.val)):\n        strides = mb.const(val=kernel_sizes.val, name=strides.name)\n    pad_type = \"custom\"\n    # Need to explicitly state L-R, T-B pad\n    pad = inputs[3]\n    pad = _np.repeat(pad.val, 2)\n    ceil_mode = inputs[4]\n    if ceil_mode.val is True:\n        rank = len(pad) // 2\n        pad = _adjust_pad_for_ceil_mode(\n            x.shape[-rank:], kernel_sizes.val, strides.val, pad\n        )\n    include_pad = inputs[5].val\n\n    pool = mb.avg_pool(\n        x=x,\n        kernel_sizes=kernel_sizes,\n        strides=strides,\n        pad_type=pad_type,\n        pad=pad,\n        name=node.name,\n        exclude_padding_from_average=not include_pad,\n    )\n    context.add(pool)\n\n\n@register_torch_op\ndef avg_pool1d(context, node):\n    inputs = _get_inputs(context, node, expected=6)\n    _avg_pool(context, node, inputs)\n\n\n@register_torch_op\ndef avg_pool2d(context, node):\n    inputs = _get_inputs(context, node, expected=7)\n    divisor_override = inputs[6]\n    if divisor_override is not None:\n        raise ValueError(\"divisor_override is not supported for avg_pool2d\")\n    _avg_pool(context, node, inputs)\n\n\n@register_torch_op\ndef avg_pool3d(context, node):\n    inputs = _get_inputs(context, node, expected=7)\n    divisor_override = inputs[6]\n    if divisor_override is not None:\n        raise ValueError(\"divisor_override is not supported for avg_pool3d\")\n    _avg_pool(context, node, inputs)\n\n\n@register_torch_op\ndef log_softmax(context, node):\n    inputs = _get_inputs(context, node)\n\n    x = inputs[0]\n    axis = inputs[1]\n    out = inputs[2]  # Ignored.\n    if out is not None:\n        raise AssertionError\n    res = mb.softmax(x=x, axis=axis, name=node.name + \"_softmax\")\n    res = mb.log(x=res, name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef sigmoid(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n\n    res = mb.sigmoid(x=inputs[0], name=node.name)\n    context.add(res)\n\n@register_torch_op\ndef hardsigmoid(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n\n    res = mb.sigmoid_hard(x=inputs[0], alpha=1.0/6, beta=0.5, name=node.name)\n    context.add(res)\n\n@register_torch_op\ndef gelu(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n\n    res = mb.gelu(x=inputs[0], name=node.name)\n    context.add(res)\n\n\n@register_torch_op(torch_alias=[\"slice\"])\ndef _slice(context, node):\n    inputs = _get_inputs(context, node, expected=5)\n    x = inputs[0]\n    dim = inputs[1].val\n\n    if inputs[2] and inputs[2].val is not None:\n        start = inputs[2].val\n    elif isinstance(inputs[2], Var):\n        start = inputs[2]\n    else:\n        start = 0\n\n    if inputs[3] and inputs[3].val is not None:\n        end = inputs[3].val\n    elif isinstance(inputs[3], Var):\n        end = inputs[3]\n    else:\n        end = None\n\n    step = inputs[4].val\n\n    if start == 0 and end is None and step == 1:\n        # Handling x[:], just pass through the tensor.\n        context.add(x, node.name)\n        return\n\n    begin_array = [0] * len(x.shape)\n    begin_array[dim] = start\n    end_array = [s if isinstance(s, int) else 0 for s in x.shape]\n    end_mask = [True] * len(x.shape)\n    if end is not None:\n        end_array[dim] = end\n        end_mask[dim] = False\n\n    if isinstance(start, Var):\n        begin_array = mb.concat(values=begin_array, axis=0)\n\n    if isinstance(end, Var):\n        end_array = mb.concat(values=end_array, axis=0)\n\n    kwargs = {\n        \"x\": x,\n        \"begin\": begin_array,\n        \"end\": end_array,\n        \"end_mask\": end_mask,\n        \"name\": node.name,\n    }\n\n    if step != 1:\n        stride_array = _np.array([1] * len(x.shape))\n        stride_array[dim] = step\n        kwargs[\"stride\"] = stride_array\n\n    res = mb.slice_by_index(**kwargs)\n    context.add(res)\n\n\n@register_torch_op(torch_alias=[\"split_with_sizes\"])\ndef split(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    x = inputs[0]\n    split_sizes = inputs[1]\n    dim = inputs[2].val\n\n    if not isinstance(split_sizes.val, _np.ndarray):\n        shape = mb.shape(x=x)\n        dim_size = _list_select(shape, dim)\n        # MIL split op needs the size of each split to be given explicitly.\n        num_whole_splits = mb.floor_div(x=dim_size, y=split_sizes)\n        remainder = mb.mod(x=dim_size, y=split_sizes)\n\n        # MIL doesn't have a way of turning a scalar into a tensor (list write\n        # only supports tensors). As a workaround, we create a constant [1]\n        # tensor and multiply it by the scalar value, thus creating a tensor\n        # with the scalar value in it.\n        tmp = mb.const(val=[1])\n        whole_sizes = mb.mul(x=tmp, y=split_sizes)\n        reps = mb.mul(x=tmp, y=num_whole_splits)\n        whole_sizes = mb.tile(x=whole_sizes, reps=reps)\n        if remainder.val == 0:\n            split_sizes = whole_sizes\n        else:\n            partial_size = mb.mul(x=tmp, y=remainder)\n            split_sizes = mb.concat(values=[whole_sizes, partial_size], axis=0)\n    res = mb.split(x=x, split_sizes=split_sizes, axis=dim, name=node.name)\n    context.add(res, torch_name=node.name)\n\n\n@register_torch_op\ndef to(context, node):\n    # @non_blocking and @copy are unused\n    inputs = _get_inputs(context, node)\n\n    if len(inputs) == 8:\n        _input = inputs[0]\n        dtype = inputs[1].val\n    elif len(inputs) == 7:\n        _input = inputs[0]\n        dtype = inputs[1].val\n    elif len(inputs) == 6:\n        _input = inputs[0]\n        device = inputs[1]\n        dtype = inputs[2].val\n        # non_blocking = inputs[3]\n        # copy = inputs[4]\n        # memory_format = inputs[5] # usually None\n    elif len(inputs) == 5:\n        _input = inputs[0]\n        dtype = NUMPY_DTYPE_TO_TORCH_NUM[inputs[1].val.dtype.type] if isinstance(inputs[1].val, _np.ndarray) else inputs[1].val\n        # non_blocking = inputs[2]\n        # copy = inputs[3]\n        # memory_format = inputs[4]\n    elif len(inputs) == 4:\n        _input = inputs[0]\n        dtype = inputs[1].val\n        # non_blocking = inputs[2]\n        # copy = inputs[3]\n    elif len(inputs) == 3:\n        # Since @non_blocking and @copy are unused, add back to context\n        _input = inputs[0]\n        # non_blocking = inputs[1]\n        # copy = inputs[2]\n        context.add(_input, torch_name=node.name)\n        return\n    else:\n        raise ValueError(\n            \"Received invalid arguments for PyTorch conversion of op {}\".format(node)\n        )\n\n    torch_dtype = NUM_TO_TORCH_DTYPE[dtype]\n    if isinstance(_input, Var) and _input.val is not None:\n        _input = _input.val\n        # numpy -> torch -> torch cast -> numpy\n        # This path is needed to use the mapping of passed in dtypes to torch dtypes.\n        casted_input = torch.tensor(_input).type(torch_dtype).numpy()\n        res = mb.const(mode=\"immediate_value\", val=casted_input, name=node.name)\n    else:\n        res = mb.cast(x=_input, dtype=NUM_TO_DTYPE_STRING[dtype], name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef erf(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    _input = inputs[0]\n    erf = mb.erf(x=_input, name=node.name)\n    context.add(erf)\n\n\n@register_torch_op(torch_alias=[\"scalarimplicit\"])\ndef implicittensortonum(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    _input = inputs[0]\n\n    if _input.shape == (): #already a scalar\n        context.add(_input, node.name)\n    else:\n        if _input.shape != (1,):\n            raise AssertionError\n        # shape: (1,) -> ()\n        squeeze = mb.squeeze(x=_input, name=node.name)\n        context.add(squeeze)\n\n\n@register_torch_op\ndef constantchunk(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    x = inputs[0]\n    # ConstantChunk gets its parameters as attributes of the node.\n    chunks = node.attr[\"chunks\"]\n    dim = node.attr[\"dim\"]\n\n    total = x.shape[dim]\n    size = int(_math.ceil(float(total) / float(chunks)))\n    split_sizes = [size] * int(_math.floor(total / size))\n    remainder = total - sum(split_sizes)\n    if remainder > 0:\n        split_sizes.append(remainder)\n\n    res = mb.split(x=x, split_sizes=split_sizes, axis=dim, name=node.name)\n    for val, name in zip(res, node.outputs):\n        context.add(val, name)\n\n\ndef _expand(context, name, tensor, shape):\n    reps = [ds if ds > 0 and ts == 1 else 1 for ts, ds in zip(tensor.shape, shape)]\n    res = mb.tile(x=tensor, reps=reps, name=name)\n    context.add(res)\n\n\n@register_torch_op\ndef expand(context, node):\n    # PyTorch 1.6+ has 3 inputs while older version has 2\n    inputs = _get_inputs(context, node, expected=[2, 3])\n\n    x = inputs[0]\n    shape = inputs[1].val\n\n    _expand(context, node.name, x, shape)\n\n\n@register_torch_op\ndef expand_as(context, node):\n    # PyTorch 1.6+ has 3 inputs while older version has 2\n    inputs = _get_inputs(context, node, expected=[2, 3])\n    x = inputs[0]\n    other = inputs[1]\n\n    _expand(context, node.name, x, other.shape)\n\n\n@register_torch_op\ndef arange(context, node):\n    inputs = _get_inputs(context, node)\n    # dtype = inputs[-4]\n    # layout = inputs[-3]\n    # device = inputs[-2]\n    # pin_memory = inputs[-1]\n    if len(inputs) == 5:\n        # inputs are [end, dtype, layout, device, pin_memory]\n        start = 0\n        end = inputs[0]\n        step = 1\n    elif len(inputs) == 6:\n        # inputs are [start, end, dtype, layout, device, pin_memory]\n        start = inputs[0]\n        end = inputs[1]\n        step = 1\n    elif len(inputs) == 7:\n        # inputs are [start, end, step, dtype, layout, device, pin_memory]\n        start = inputs[0]\n        end = inputs[1]\n        step = inputs[2]\n    else:\n        raise ValueError(\n            \"arange must have exactly 5, 6, or 7 inputs, got {}\".format(len(inputs))\n        )\n\n    res = mb.range_1d(start=start, end=end, step=step, name=node.name)\n    context.add(res)\n\n\n@register_torch_op(torch_alias=[\"masked_fill_\"])\ndef masked_fill(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    x = inputs[0]\n    mask = inputs[1]\n    value = inputs[2]\n    # @mb.select does not properly broadcast scalar input, so as a workaround\n    # we create a full sized tensor.\n    # rdar://61463562\n\n    if types.is_int(value.dtype):\n        # @mb.fill cannot handle value with dtype integer\n        # so we cast the value.\n        value = mb.cast(x=value, dtype=\"fp32\")\n    value = mb.fill(shape=x.shape, value=value, name=node.name + \"_value\")\n    res = mb.select(cond=mask, a=value, b=x, name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef meshgrid(context, node):\n    \"\"\"\n    For N input tensors, a meshgrid is constructed by viewing each tensor as an N-dimension tensor\n    with values in the dimension corresponding it its order in the args. (a.)\n    Then, it is expanded along dimensions corresponding to the dimensions of each\n    1d tensor in the order that they were passed in. (b.)\n\n    Each output tensor is put into a tuple that is returned. These tuples form\n    N, N-dimenional grids, where the ith grid is defined as expanding the ith input over\n    dimensions defined by the other inputs.\n    \"\"\"\n    inputs = _get_inputs(context, node)\n    if len(inputs) < 2:\n        raise ValueError(\"Requires > 2 tensor inputs.\")\n\n    # scalar inputs will be considered 1d tensors\n    tensor_inputs = []\n    for tensor_var in inputs:\n        if not isinstance(tensor_var.val, _np.ndarray):\n            tensor_inputs.append(_np.array(tensor_var.val))\n        else:\n            tensor_inputs.append(_np.array(tensor_var))\n\n    if any([len(tensor_var.shape) > 1 for tensor_var in inputs]):\n        raise ValueError(\"meshgrid recieved non-1d tensor.\")\n\n    dim_tuple = tuple(tensor_var.shape[0] for tensor_var in inputs)\n\n    grids = []\n    size = len(inputs)\n    for i in range(size):\n        view_shape = [1] * size\n        view_shape[i] = -1\n        view_shape = tuple(view_shape)\n        tensor = torch.tensor(inputs[i].val)\n        # (a.) in docstring\n        view = mb.reshape(\n            x=inputs[i], shape=view_shape, name=node.name + \"_view_\" + str(i)\n        )\n\n        # (b.) in docstring\n        reps = [\n            ds if ds > 0 and ts == 1 else 1 for ts, ds in zip(view.shape, dim_tuple)\n        ]\n        expand = mb.tile(x=view, reps=reps, name=node.name + \"_expand_\" + str(i))\n        grids.append(expand)\n\n    context.add(tuple(grids), node.name)\n\n\n# Defines all the nodes that are noOps\n@register_torch_op(\n    torch_alias=[\n        \"dropout\",\n        \"dropout_\",\n        \"feature_dropout\",\n        \"contiguous\",\n        \"device\",\n        \"detach\",\n        \"clone\",\n    ]\n)\ndef noop(context, node):\n    _logging.info(\"Setting pytorch op: {} to no-op.\".format(node))\n    inputs = _get_inputs(context, node)\n    _input = inputs[0]\n    context.add(_input, torch_name=node.name)\n\n\n@register_torch_op\ndef argmax(context, node):\n    inputs = _get_inputs(context, node)\n    x = inputs[0]\n    axis = inputs[1]\n    keep_dims = inputs[2]\n    res = mb.reduce_argmax(x=x, axis=axis, keep_dims=keep_dims, name=node.name)\n    context.add(res)\n\n\n@register_torch_op\ndef zeros(context, node):\n    inputs = _get_inputs(context, node, expected=5)\n    size = inputs[0].val\n    dtype = inputs[1].val\n    # layout = inputs[2] unused\n    # device = inputs[3] unused\n    # pin_memory = inputs[4] unused\n\n    torch_dtype = NUM_TO_TORCH_DTYPE[dtype]\n    zeros_array = torch.zeros(tuple(size)).type(torch_dtype).numpy()\n    const = mb.const(mode=\"immediate_value\", val=zeros_array, name=node.name)\n    context.add(const)\n\n\n@register_torch_op\ndef max(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    _input = inputs[0]\n    dim = inputs[1].val\n    keepdim = inputs[2].val\n\n    values = mb.reduce_max(x=_input, axes=[dim], keep_dims=keepdim)\n    indices = mb.reduce_argmax(x=_input, axis=dim, keep_dims=keepdim)\n    if len(node.outputs) != 2:\n        raise AssertionError\n    values_name = node.outputs[0]\n    indices_name = node.outputs[1]\n    context.add(values, torch_name=values_name)\n    context.add(indices, torch_name=indices_name)\n\n\n@register_torch_op\ndef argsort(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    ascending = mb.logical_not(x=inputs[2])\n    argsort = mb.argsort(x=inputs[0], axis=inputs[1], ascending=ascending, name=node.name)\n    context.add(argsort)\n\n\n@register_torch_op\ndef sort(context, node):\n    inputs = _get_inputs(context, node)\n    _input = inputs[0]\n    axis = inputs[1].val\n    descending = inputs[2].val\n    # NOTE: This is actually descending\n    # rdar://62901267 (argsort ascending is actually descending)\n    indices = mb.argsort(x=_input, axis=axis, ascending=descending)\n    values = mb.gather_along_axis(x=_input, indices=indices, axis=axis)\n\n    values_name = node.outputs[0]\n    indices_name = node.outputs[1]\n    context.add(values, torch_name=values_name)\n    context.add(indices, torch_name=indices_name)\n\n\n@register_torch_op\ndef append(context, node):\n    # Note: by applying torchir_passes.transform_inplace_ops the meaning of\n    # this op is changed from the original TorchIR. This op expects a python\n    # list or MIL List as its first input. If an MIL List, the second input\n    # must be a tensor of whatever shape the List expects. If not an MIL List,\n    # the second input can by anything. The result will be the second input\n    # joined to the first input, either by list_write if an MIL list, or\n    # append if a python list.\n    inputs = _get_inputs(context, node, expected=2)\n    ls = inputs[0]\n    value = inputs[1]\n\n    if isinstance(ls, list):\n        context.add(ls + [value], node.name)\n    elif isinstance(ls, ListVar):\n        index = mb.list_length(ls=ls, name=node.name + \"_index\")\n        res = mb.list_write(ls=ls, index=index, value=value, name=node.name)\n        context.add(res)\n    else:\n        raise ValueError(\"can only append to Python list or MIL ListVar, got {}.\".format(type(inputs[0])))\n\n\n@register_torch_op\ndef gather(context, node):\n    inputs = _get_inputs(context, node)\n    res = mb.gather_along_axis(x=inputs[0], indices=inputs[2], axis=inputs[1], name=node.name)\n    context.add(res)\n\n@register_torch_op(torch_alias=[\"abs\"])\ndef _abs(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.abs(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef repeat(context, node):\n    x = context[node.inputs[0]]\n    reps = context[node.inputs[1]]\n    context.add(mb.tile(x=x, reps=reps, name=node.name))\n\n@register_torch_op\ndef acos(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.acos(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef acosh(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.acosh(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef asin(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.asin(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef atan(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.atan(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef atanh(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.atanh(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef ceil(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.ceil(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef clamp(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    context.add(mb.clip(x=inputs[0], alpha=inputs[1], beta=inputs[2], name=node.name))\n\n@register_torch_op\ndef cos(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.cos(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef cosh(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.cosh(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef exp(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.exp(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef exp2(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.exp2(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef floor(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.floor(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef reciprocal(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.inverse(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef log(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.log(x=inputs[0], name=node.name))\n\n@register_torch_op(torch_alias=[\"round\"])\ndef _round(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.round(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef rsqrt(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.rsqrt(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef sin(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.sin(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef sinh(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.sinh(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef asinh(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.asinh(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef sqrt(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.sqrt(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef square(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    # mb.square is not supported in some backend\n    context.add(mb.mul(x=inputs[0], y=inputs[0], name=node.name))\n\n@register_torch_op\ndef tan(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.tan(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef tanh(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.tanh(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef threshold(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    x = inputs[0]\n    alpha = inputs[1]\n    threshold_val = inputs[2]\n\n    # Simple case (threshold_val == alpha)\n    if alpha.val == threshold_val.val:\n        threshold_node = mb.threshold(x=x, alpha=alpha, name=node.name)\n        context.add(threshold_node)\n        return\n\n    # Complex case (threshold_val != threshold)\n    threshold_node = mb.threshold(x=x, alpha=alpha, name=node.name + '_threshold')\n    context.add(threshold_node)\n\n    gt_node = mb.greater_equal(x=alpha, y=x, name=node.name + '_ge')\n    context.add(gt_node)\n    gt_node_32 = mb.cast(x=gt_node, dtype=\"fp32\", name=node.name + '_ge32')\n\n    mul_node = mb.linear_activation(x=gt_node_32, alpha=float(threshold_val.val - alpha.val),\n                                    name=node.name + '_mul')\n    context.add(mul_node)\n\n    final_node = mb.add(x=mul_node, y=threshold_node, name=node.name)\n    context.add(final_node)\n\n@register_torch_op\ndef sign(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.sign(x=inputs[0], name=node.name))\n\n@register_torch_op\ndef is_floating_point(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    is_float = types.is_float(inputs[0].dtype)\n    context.add(mb.const(val=is_float, name=node.name))\n\n@register_torch_op\ndef where(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    context.add(mb.select(cond=inputs[0], a=inputs[1], b=inputs[2], name=node.name))\n\n@register_torch_op\ndef neg(context, node):\n    inputs = _get_inputs(context, node, expected=1)\n    context.add(mb.mul(x=inputs[0], y=-1, name=node.name))\n\n@register_torch_op\ndef topk(context, node):\n    inputs = _get_inputs(context, node)\n    kwargs = {\"name\": node.name, \"x\": inputs[0], \"k\": inputs[1]}\n\n    if len(inputs) > 6:\n        raise Exception(\"Number of inputs to topk exceeds 6\")\n    # optional: @axis\n    if len(inputs) > 2:\n        if inputs[2] is not None:\n            kwargs[\"axis\"] = inputs[2].val\n\n    # optional: @ascending\n    if len(inputs) > 3:\n        largest = inputs[3].val\n        kwargs[\"ascending\"] = not largest\n\n    # last inputs to topk are optional - sorted and out.\n    if len(inputs) > 4:\n        if inputs[4].val is False:\n            raise Exception(\"Unsupported value for argument 'sorted' in topk. Supported values: True, but input \"\n                            \"is {}\".format(inputs[4].val))\n    if len(inputs) > 5:\n        if inputs[5] is not None:\n            raise Exception(\"Unsupported value for argument 'out' in topk. Supported values: None, but input \"\n                            \"is {}\".format(inputs[5].val))\n\n    res = mb.topk(**kwargs)\n\n    values_name = node.outputs[0]\n    indices_name = node.outputs[1]\n    context.add(res[0], torch_name=values_name)\n    context.add(res[1], torch_name=indices_name)\n\n@register_torch_op\ndef std(context, node):\n    inputs = _get_inputs(context, node)\n    x = inputs[0]\n    if not (len(inputs) == 2 or len(inputs) == 4):\n        raise ValueError(\"Number of inputs to the 'std' op must be\"\n                         \"2 or 4\")\n\n    keep_dim = False\n    axes = None\n    if len(inputs) == 2:\n        unbiased = inputs[1].val\n    if len(inputs) == 4:\n        axes = inputs[1].val\n        if isinstance(axes, int):\n            axes = [axes]\n        unbiased = inputs[2].val\n        keep_dim = inputs[3].val\n\n    need_rescale = False\n    if unbiased:\n        # If \"unbiased\" is True,\n        # then we need to divide by \"N-1\" (instead of \"N\") to compute the mean of (x-E[x])^2\n        # for an unbiased estimate of the variance /  standard deviation.\n        # In the sequence of MIL ops added below, we first compute the mean using \"N\", and only if its unbiased\n        # we rescale later, the final result.\n        # We ignore the \"unbiased\" flag, if any of the dimensions involved in this operation are dynamic\n        # (we could have still handled that case by using \"get_shape\" etc ops, but we don't do that here,\n        # trading performance for numerical accuracy)\n        if axes is None:\n            if not any_symbolic(x.shape) and _np.prod(x.shape) > 1:\n                N = _np.prod(x.shape)\n                need_rescale = True\n        else:\n            dims = []\n            # collect dimensions corresponding to \"axes\"\n            for axis in axes:\n                dims.append(x.shape[axis])\n            if all([not is_symbolic(s) for s in dims]):\n                N = _np.prod(dims)\n                if N > 1:\n                    need_rescale = True\n    if need_rescale:\n        rescale_factor = _np.sqrt(N/float(N-1))\n\n    x_mean = mb.reduce_mean(x=x, axes=axes, keep_dims=True)\n    x_demeaned = mb.sub(x=x, y=x_mean)\n    x_demeaned_square = mb.square(x=x_demeaned)\n    x_demeaned_square_mean = mb.reduce_mean(x=x_demeaned_square, axes=axes, keep_dims=keep_dim)\n    if need_rescale:\n        y_before_scale = mb.sqrt(x=x_demeaned_square_mean)\n        y = mb.mul(x=y_before_scale, y=rescale_factor, name=node.name)\n    else:\n        y = mb.sqrt(x=x_demeaned_square_mean, name=node.name)\n\n    context.add(y)\n\n@register_torch_op\ndef copy_(context, node):\n    inputs = _get_inputs(context, node, expected=3)\n    context.add(mb.identity(x=inputs[0], name=node.name))\n"}
{"content": "\nimport collections\nimport os\nimport random\n\nimport h5py\nimport PIL.Image as Image\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\n\n\nclass VisionLanguageDataset(Dataset):\n\n\tdef __init__(self, **kwargs):\n\t\tself.__dict__.update(kwargs)\n\t\tdata_path = kwargs[\"data_path\"]\n\t\tdata_name = kwargs[\"data_name\"]\n\t\tdata_type = kwargs[\"data_type\"]\n\t\tdata_info_path = os.path.join(data_path, \"{}_info.pt\".format(data_name))\n\t\tdata_file_path = os.path.join(data_path, \"{}_{}.h5\".format(data_name, data_type))\n\t\tself.data_info = torch.load(data_info_path)\n\t\tself.dataset = h5py.File(data_file_path, \"r\")\n\n\t\timage_path = kwargs[\"image_path\"]\n\t\timage_type = kwargs[\"image_type\"]\n\t\timage_split = kwargs[\"image_split\"]\n\t\tif image_type == \"rcnn\":\n\t\t\tself.imageset = RCNNDataset(image_path, image_split)\n\t\telif image_type == \"resnet\":\n\t\t\tself.imageset = ResnetDataset(image_path, image_split)\n\t\telse:\n\t\t\traise TypeError(\"image dataset type should be 'rcnn, resnet, or fused_resnet' \"\n\t\t\t\t\t\t\t\"detected {}\".format(image_type))\n\n\nclass RCNNDataset(Dataset):\n\n\tdef __init__(self, data_path, data_type):\n\t\tsuper(RCNNDataset, self).__init__()\n\t\timg_info_path = os.path.join(data_path, \"rcnn_{}.pt\".format(data_type))\n\t\tself.img2idx = torch.load(img_info_path)\n\n\t\tself.img_file_path = os.path.join(data_path, \"rcnn_{}.h5\".format(data_type))\n\t\tself.imageset = None\n\t\tself.features = None\n\t\tself.img_start_idx = None\n\t\tself.img_end_idx = None\n\n\tdef _initialize(self):\n\t\tif self.imageset is None or self.features is None or self.img_start_idx is None or self.img_end_idx is None:\n\t\t\tself.imageset = h5py.File(self.img_file_path, \"r\")\n\t\t\tself.features = self.imageset[\"features\"]\n\t\t\tself.img_start_idx = self.imageset[\"img_start_idx\"]\n\t\t\tself.img_end_idx = self.imageset[\"img_end_idx\"]\n\n\tdef __getitem__(self, index):\n\t\tself._initialize()\n\t\timg_idx = self.img2idx[index]\n\t\timg_start_idx = self.img_start_idx[img_idx]\n\t\timg_end_idx = self.img_end_idx[img_idx]\n\t\tnboxes = img_end_idx - img_start_idx + 1\n\n\t\timg = torch.zeros(1, 100, 2048, dtype=torch.float32)\n\t\t# img[0, :nboxes, :].copy_(torch.from_numpy(self.features[img_start_idx:img_end_idx+1,:]),\n\t\t#   non_blocking=True)\n\t\timg[0, :nboxes, :] = torch.from_numpy(self.features[img_start_idx:img_end_idx+1,:])\n\t\timg_mask = torch.zeros(1, 100, dtype=torch.float32)\n\t\timg_mask[0, :nboxes] = 1\n\n\t\treturn (img, img_mask)\n\n\tdef __len__(self):\n\t\treturn len(self.img2idx)\n\n\nclass ResnetDataset(Dataset):\n\n\tdef __init__(self, data_path, data_type):\n\t\tsuper(ResnetDataset, self).__init__()\n\t\timg_info_path = os.path.join(data_path, \"resnet_{}.pt\".format(data_type))\n\t\tself.img2idx = torch.load(img_info_path)\n\t\tself.transform = transforms.Compose([\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n\t\t\t])\n\n\t\tself.img_file_path = os.path.join(data_path, \"resnet_{}.h5\".format(data_type))\n\t\tself.imageset = None\n\t\tself.images = None\n\t\n\tdef _initialize(self):\n\t\tif self.imageset is None or self.images is None:\n\t\t\tself.imageset = h5py.File(self.img_file_path, \"r\")\n\t\t\tself.images = self.imageset[\"images\"]\n\n\tdef __getitem__(self, index):\n\t\tself._initialize()\n\t\timg_idx = self.img2idx[index]\n\t\timg = self.transform(Image.fromarray(self.images[img_idx])).unsqueeze(0)\n\n\t\treturn img\n\n\tdef __len__(self):\n\t\treturn len(self.img2idx)\n\n\nclass VQADataset(VisionLanguageDataset):\n\n\tdef __init__(self, data_path, data_name, data_type, image_path, image_type, image_split):\n\t\tsuper(VQADataset, self).__init__(data_path=data_path, data_name=data_name, \n\t\t\tdata_type=data_type, image_path=image_path, image_type=image_type, image_split=image_split)\n\n\t\tself.idx2word = self.data_info[\"idx2word\"]\n\t\tself.idx2ans = self.data_info[\"idx2ans\"]\n\t\tself.word2idx = self.data_info[\"word2idx\"]\n\t\tself.and2idx = self.data_info[\"ans2idx\"]\n\n\t\tself.img_idx = self.dataset[\"img_idx\"][:]\n\t\tself.questions = self.dataset[\"questions\"][:]\n\t\tself.ques_idx = self.dataset[\"ques_idx\"][:]\n\t\tself.ans_pool = self.dataset[\"ans_pool\"][:]\n\t\t\n\t\tif \"test\" in data_type:\n\t\t\tself.ans_idx = None\n\t\telse:\n\t\t\tself.ans_idx = self.dataset[\"ans_idx\"][:]\n\t\tprint(f\"Initializing {data_type} VQA dataset: {self.questions.shape[0]} questions, {len(self.imageset)} images\")\n\n\tdef __getitem__(self, index):\n\t\tques = torch.from_numpy(self.questions[[index]])\n\t\tques_idx = torch.from_numpy(self.ques_idx[[index]])\n\t\tans_idx = torch.from_numpy(self.ans_idx[[index]]) if self.ans_idx is not None else None\n\t\tques_mask = ques.ne(0).float()\n\t\timg_info = self.imageset[self.img_idx[index]]\n\n\t\treturn (img_info, ques, ques_mask, ans_idx, ques_idx)\n\n\tdef __len__(self):\n\t\treturn self.questions.shape[0]\n"}
{"content": "import numpy as np\nfrom neutralocean.eos.tools import make_eos, make_eos_s_t, vectorize_eos\nfrom neutralocean import potential_surf, anomaly_surf, omega_surf\nfrom neutralocean.synthocean import synthocean\nfrom neutralocean.lib import find_first_nan, val_bot\n\ngrav = 9.81\nrho_c = 1027.5\neos = make_eos(\"jmd95\", grav, rho_c)\neos_s_t = make_eos_s_t(\"jmd95\", grav, rho_c)\neos_ufunc = vectorize_eos(eos)\n\n\ndef make_simple_stp(shape):\n    S, T, Z, _ = synthocean(shape)\n    # Raise the sea-floor in some casts\n    # Make one profile be land, and three profiles have a shallower bottom\n    # (the last having just one valid bottle)\n    S[1, 1, 45:] = T[1, 1, 45:] = np.nan  # deep ocean\n    S[2, 1, 5:] = T[2, 1, 5:] = np.nan  # shallow ocean\n    S[3, 1, 1:] = T[3, 1, 1:] = np.nan  # coastal ocean (1 valid bottle)\n    return S, T, Z\n\n\ndef test_potential_surf():\n    # Test sigma_surf using a prescribed reference depth and isovalue\n    S, T, Z = make_simple_stp((16, 32, 50))\n    z_ref = 0.0\n    isoval = 1027.0\n    s, t, z, _ = potential_surf(\n        S, T, Z, ref=z_ref, isoval=isoval, eos=eos, TOL_P_SOLVER=1e-8, diags=False\n    )\n\n    \u03c3 = np.ma.masked_invalid(eos_ufunc(s, t, z_ref))\n    assert np.ma.allclose(\u03c3, isoval)\n\n    # Calculate surface potential density\n    \u03c3_sfc = eos_ufunc(S[:, :, 0], T[:, :, 0], z_ref)\n\n    # Calculate seafloor potential density\n    n_good = find_first_nan(S)\n    S_bot, T_bot = (val_bot(x, n_good) for x in (S, T))\n    \u03c3_bot = eos_ufunc(S_bot, T_bot, z_ref)\n\n    \u03c3 = eos_ufunc(s, t, z_ref)\n\n    # check for each cast that\n    # potential density on surface nearly matches isovalue,\n    # or the surface does not intersect this cast (\u03c3 is nan) because of one of\n    # three conditions: the cast was land, the surface outcropped, or the surface incropped.\n    assert np.all(\n        (np.abs(\u03c3 - isoval) < 1e-8)\n        | (np.isnan(\u03c3) & ((n_good == 0) | (isoval < \u03c3_sfc) | (\u03c3_bot < isoval)))\n    )\n\n\ndef test_anomaly_surf():\n    # Test delta_surf using prescribed reference values and a given cast and depth\n    # that the surface will intersect.\n    S, T, Z = make_simple_stp((16, 32, 50))\n    s_ref, t_ref = 34.5, 4.0\n    i0, j0 = (int(x / 2) for x in S.shape[:-1])  # ref cast in middle of domain\n    pin_z = 1000.0  # find surface through this depth at ref cast\n    s, t, z, d = anomaly_surf(\n        S,\n        T,\n        Z,\n        ref=(s_ref, t_ref),\n        pin_cast=(i0, j0),\n        pin_p=pin_z,\n        eos=eos,\n        TOL_P_SOLVER=1e-8,\n        diags=True,  # True to get isoval output\n        wrap=(False, False),  # Only needed since diags=True\n    )\n\n    isoval = d[\"isoval\"]\n    \u03b4 = np.ma.masked_invalid(eos_ufunc(s, t, z) - eos_ufunc(s_ref, t_ref, z))\n    assert np.ma.allclose(\u03b4, isoval)\n\n    # Calculate surface potential density\n    \u03b4_sfc = eos_ufunc(S[:, :, 0], T[:, :, 0], Z[0]) - eos_ufunc(s_ref, t_ref, Z[0])\n\n    # Calculate seafloor potential density\n    n_good = find_first_nan(S)\n    S_bot, T_bot, Z_bot = (val_bot(x, n_good) for x in (S, T, Z))\n    \u03b4_bot = eos_ufunc(S_bot, T_bot, Z_bot) - eos_ufunc(s_ref, t_ref, Z_bot)\n\n    \u03b4 = eos_ufunc(s, t, z) - eos_ufunc(s_ref, t_ref, z)\n\n    # check for each cast that\n    # in-situ density anomaly on surface nearly matches isovalue,\n    # or the surface does not intersect this cast (\u03c3 is nan) because of one of\n    # three conditions: the cast was land, the surface outcropped, or the surface incropped.\n    assert np.all(\n        (np.abs(\u03b4 - isoval) < 1e-8)\n        | (np.isnan(\u03b4) & ((n_good == 0) | (isoval < \u03b4_sfc) | (\u03b4_bot < isoval)))\n    )\n\n\n\"\"\"\nomega test not done yet\ndef test_omega_surf():\n    # Test omega_surf, initialized from a potential density surface\n    S, T, Z = make_simple_stp((16, 32, 50))\n    z_ref = 0.0\n    isoval = 1027.0\n    i0, j0 = (int(x / 2) for x in S.shape[:-1])  # ref cast in middle of domain\n    s, t, z, d = omega_surf(\n        S,\n        T,\n        Z,\n        ref=z_ref,\n        isoval=isoval,\n        pin_cast=(i0, j0),\n        eos=eos,\n        eos_s_t=eos_s_t,\n        TOL_P_SOLVER=1e-8,\n        diags=True,\n        wrap=(False, False),\n    )\n\n    \u03c3 = np.ma.masked_invalid(eos_ufunc(s, t, z_ref))\n    assert np.ma.allclose(\u03c3, isoval)\n\n    # Calculate surface potential density\n    \u03c3_sfc = eos_ufunc(S[:, :, 0], T[:, :, 0], z_ref)\n\n    # Calculate seafloor potential density\n    n_good = find_first_nan(S)\n    S_bot, T_bot = (val_bot(x, n_good) for x in (S, T))\n    \u03c3_bot = eos_ufunc(S_bot, T_bot, z_ref)\n\n    \u03c3 = eos_ufunc(s, t, z_ref)\n\n    # check for each cast that\n    # potential density on surface nearly matches isovalue,\n    # or the surface does not intersect this cast (\u03c3 is nan) because of one of\n    # three conditions: the cast was land, the surface outcropped, or the surface incropped.\n    assert np.all(\n        (np.abs(\u03c3 - isoval) < 1e-8)\n        | (np.isnan(\u03c3) & ((n_good == 0) | (isoval < \u03c3_sfc) | (\u03c3_bot < isoval)))\n    )\n\"\"\"\n"}
{"content": "import os\nfrom typing import Dict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\nimport math\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nimport pickle\nimport wandb\nimport re\ntry:\n    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel\nexcept:\n    from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel\nimport matplotlib.pyplot as plt\nfrom dkt.utils import duplicate_name_changer\n\n\nclass LSTM(nn.Module):\n\n    def __init__(self, args):\n        super(LSTM, self).__init__()\n        self.args = args\n        self.device = args.device\n\n        self.hidden_dim = self.args.hidden_dim\n        self.n_layers = self.args.n_layers\n\n        cate_col_num = len(self.args.cate_cols.keys())\n        cont_col_num = len(self.args.cont_cols)\n        divider = (bool(cate_col_num) + bool(cont_col_num))\n        if divider == 0:\n            raise RuntimeError(\"no feature found.\")\n        self.embedding_interaction = nn.Embedding(\n            3, self.hidden_dim//3)\n        # Embedding\n        # interaction\uc740 \ud604\uc7ac correct\ub85c \uad6c\uc131\ub418\uc5b4\uc788\ub2e4. correct(1, 2) + padding(0)\n        if self.args.cate_cols:\n            self.embedding_category = {}\n            for col_name, col_len in self.args.cate_cols.items():\n                self.embedding_category[col_name] = nn.Embedding(\n                    col_len + 1, self.hidden_dim//3)\n                setattr(self, f'emb_{col_name}',\n                        self.embedding_category[col_name])\n\n        # embedding combination projection\n        self.cate_proj = nn.Sequential(\n            nn.Linear((self.hidden_dim//3)*(cate_col_num+1),\n                      self.hidden_dim),\n            nn.LayerNorm(self.hidden_dim)\n        )\n\n        if self.args.cont_cols:\n            self.bn_cont = nn.BatchNorm1d(cont_col_num)\n            self.embedding_cont = nn.Sequential(\n                nn.Linear(cont_col_num,\n                          self.hidden_dim),\n                nn.LayerNorm(\n                    self.hidden_dim\n                )\n            )\n        self.comb_proj = nn.Sequential(\n            nn.ReLU(),\n            nn.Linear(self.args.hidden_dim *\n                      divider, self.args.hidden_dim),\n            nn.LayerNorm(self.args.hidden_dim))\n\n        self.lstm = nn.LSTM(self.hidden_dim,\n                            self.hidden_dim,\n                            self.n_layers,\n                            batch_first=True)\n\n        # Fully connected layer\n        self.fc = nn.Linear(self.hidden_dim, 1)\n\n        self.activation = nn.Sigmoid()\n\n    def init_hidden(self, batch_size):\n        h = torch.zeros(\n            self.n_layers,\n            batch_size,\n            self.hidden_dim)\n        h = h.to(self.device)\n\n        c = torch.zeros(\n            self.n_layers,\n            batch_size,\n            self.hidden_dim)\n        c = c.to(self.device)\n\n        return (h, c)\n\n    def forward(self, input: Dict):\n\n        batch_size = input['oth']['interaction'].size(0)\n        # Embedding\n\n        embed_interaction = self.embedding_interaction(\n            input['oth']['interaction'])\n\n        feature_linear = []\n\n        cate_embed_list = []\n        if self.args.cate_cols:\n            for k in self.embedding_category.keys():\n                cate_embed_list.append(\n                    self.embedding_category[k](input['cate'][k]))\n\n        embed_list = [embed_interaction] + cate_embed_list\n        cate_embed = torch.cat(embed_list, 2)\n\n        feature_linear.append(self.cate_proj(cate_embed))\n\n        if self.args.cont_cols:\n            cont_col_list = []\n            for cont_col in self.args.cont_cols:\n                cont_col_list.append(input['cont'][cont_col].unsqueeze(2))\n            cont_all = torch.cat(cont_col_list, 2)\n            cont = self.bn_cont(\n                cont_all.view(-1, cont_all.size(-1))).view(batch_size, -1, cont_all.size(-1))\n\n            feature_linear.append(self.embedding_cont(cont))\n\n        X = self.comb_proj(torch.cat(feature_linear, 2))\n\n        hidden = self.init_hidden(batch_size)\n        out, hidden = self.lstm(X, hidden)\n        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n\n        out = self.fc(out)\n        preds = self.activation(out).view(batch_size, -1)\n\n        return preds\n\n\nclass Feed_Forward_block(nn.Module):\n    \"\"\"\n    out =  Relu( M_out*w1 + b1) *w2 + b2\n    \"\"\"\n\n    def __init__(self, dim_ff):\n        super().__init__()\n        self.layer1 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n        self.layer2 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n\n    def forward(self, ffn_in):\n        return self.layer2(F.relu(self.layer1(ffn_in)))\n\n\nclass LastQuery(nn.Module):\n    def __init__(self, args):\n        super(LastQuery, self).__init__()\n        self.args = args\n        self.device = args.device\n\n        self.hidden_dim = self.args.hidden_dim\n\n        cate_col_num = len(self.args.cate_cols.keys())\n        cont_col_num = len(self.args.cont_cols)\n        divider = (bool(cate_col_num) + bool(cont_col_num))\n        if divider == 0:\n            raise RuntimeError(\"no feature found.\")\n        # Embedding\n        # interaction\uc740 \ud604\uc7ac correct\uc73c\ub85c \uad6c\uc131\ub418\uc5b4\uc788\ub2e4. correct(1, 2) + padding(0)\n        self.embedding_interaction = nn.Embedding(\n            3, self.hidden_dim//3)\n\n        if self.args.cate_cols:\n            self.embedding_category = {}\n            for col_name, col_len in self.args.cate_cols.items():\n                self.embedding_category[col_name] = nn.Embedding(\n                    col_len + 1, self.hidden_dim//3)\n                setattr(self, f'emb_{col_name}',\n                        self.embedding_category[col_name])\n\n        # embedding combination projection\n        self.cate_proj = nn.Sequential(\n            nn.Linear((self.hidden_dim//3)*(cate_col_num+1),\n                      self.hidden_dim),\n            nn.LayerNorm(self.hidden_dim)\n        )\n\n        if self.args.cont_cols:\n            self.bn_cont = nn.BatchNorm1d(cont_col_num)\n            self.embedding_cont = nn.Sequential(\n                nn.Linear(cont_col_num, self.hidden_dim),\n                nn.LayerNorm(self.hidden_dim))\n\n        # embedding combination projection\n        self.comb_proj = nn.Sequential(\n            nn.Dropout(self.args.drop_out),\n            nn.Linear(self.hidden_dim*divider, self.hidden_dim),\n            nn.LayerNorm(self.hidden_dim))\n\n        # \uae30\uc874 keetar\ub2d8 \uc194\ub8e8\uc158\uc5d0\uc11c\ub294 Positional Embedding\uc740 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4\n        # \ud558\uc9c0\ub9cc \uc0ac\uc6a9 \uc5ec\ubd80\ub294 \uc790\uc720\ub86d\uac8c \uacb0\uc815\ud574\uc8fc\uc138\uc694 :)\n\n        # Encoder\n        self.query = nn.Linear(\n            in_features=self.hidden_dim, out_features=self.hidden_dim\n        )\n        self.key = nn.Linear(\n            in_features=self.hidden_dim, out_features=self.hidden_dim\n        )\n        self.value = nn.Linear(\n            in_features=self.hidden_dim, out_features=self.hidden_dim\n        )\n\n        self.attn = nn.MultiheadAttention(\n            embed_dim=self.hidden_dim, num_heads=self.args.n_heads\n        )\n        self.mask = None  # last query\uc5d0\uc11c\ub294 \ud544\uc694\uac00 \uc5c6\uc9c0\ub9cc \uc218\uc815\uc744 \uace0\ub824\ud558\uc5ec\uc11c \ub123\uc5b4\ub460\n        self.ffn = Feed_Forward_block(self.hidden_dim)\n\n        self.ln1 = nn.LayerNorm(self.hidden_dim)\n        self.ln2 = nn.LayerNorm(self.hidden_dim)\n\n        # LSTM\n        self.lstm = nn.LSTM(\n            self.hidden_dim, self.hidden_dim, self.args.n_layers, batch_first=True\n        )\n\n        # Fully connected layer\n        self.fc = nn.Linear(self.hidden_dim, 1)\n\n        self.activation = nn.Sigmoid()\n\n    def get_pos(self, seq_len):\n        # use sine positional embeddinds\n        return torch.arange(seq_len).unsqueeze(0)\n\n    def init_hidden(self, batch_size):\n        h = torch.zeros(self.args.n_layers, batch_size, self.args.hidden_dim)\n        h = h.to(self.device)\n\n        c = torch.zeros(self.args.n_layers, batch_size, self.args.hidden_dim)\n        c = c.to(self.device)\n\n        return (h, c)\n\n    def forward(self, input):\n        # Categorical Variable Embedding\n\n        batch_size = input['oth']['interaction'].size(0)\n        # Embedding\n\n        embed_interaction = self.embedding_interaction(\n            input['oth']['interaction'])\n\n        feature_linear = []\n\n        cate_embed_list = []\n        if self.args.cate_cols:\n            for k in self.embedding_category.keys():\n                cate_embed_list.append(\n                    self.embedding_category[k](input['cate'][k]))\n\n        embed_list = [embed_interaction] + cate_embed_list\n        cate_embed = torch.cat(embed_list, 2)\n\n        feature_linear.append(self.cate_proj(cate_embed))\n\n        # continuous variable embedding\n        # batch normalization\n        if self.args.cont_cols:\n            cont_col_list = []\n            for cont_col in self.args.cont_cols:\n                cont_col_list.append(input['cont'][cont_col].unsqueeze(2))\n            cont_all = torch.cat(cont_col_list, 2)\n            cont = self.bn_cont(\n                cont_all.view(-1, cont_all.size(-1))).view(batch_size, -1, cont_all.size(-1))\n            embed_cont = self.embedding_cont(cont)\n\n            feature_linear.append(embed_cont)\n\n        # Running LSTM\n        embed = self.comb_proj(torch.cat(feature_linear, 2))\n\n        # Positional Embedding\n        # last query\uc5d0\uc11c\ub294 positional embedding\uc744 \ud558\uc9c0 \uc54a\uc74c\n        # position = self.get_pos(seq_len).to('cuda')\n        # embed_pos = self.embedding_position(position)\n        # embed = embed + embed_pos\n\n        ####################### ENCODER #####################\n        q = self.query(embed)[:, -1:, :].permute(1, 0, 2)\n        k = self.key(embed).permute(1, 0, 2)\n        v = self.value(embed).permute(1, 0, 2)\n\n        # attention\n        # last query only\n        out, _ = self.attn(q, k, v)\n\n        # residual + layer norm\n        out = out.permute(1, 0, 2)\n        out = embed + out\n        out = self.ln1(out)\n\n        # feed forward network\n        out = self.ffn(out)\n\n        # residual + layer norm\n        out = embed + out\n        out = self.ln2(out)\n\n        ###################### LSTM #####################\n        hidden = self.init_hidden(batch_size)\n        out, hidden = self.lstm(out, hidden)\n\n        ###################### DNN #####################\n        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n        out = self.fc(out)\n\n        preds = self.activation(out).view(batch_size, -1)\n\n        return preds\n\n\nclass LSTMATTN(nn.Module):\n\n    def __init__(self, args):\n        super(LSTMATTN, self).__init__()\n        self.args = args\n        self.device = args.device\n\n        self.hidden_dim = self.args.hidden_dim\n        self.n_layers = self.args.n_layers\n        self.n_heads = self.args.n_heads\n        self.drop_out = self.args.drop_out\n\n        cate_col_num = len(self.args.cate_cols.keys())\n        cont_col_num = len(self.args.cont_cols)\n        divider = (bool(cate_col_num) + bool(cont_col_num))\n        if divider == 0:\n            raise RuntimeError(\"no feature found.\")\n        # Embedding\n        # interaction\uc740 \ud604\uc7ac correct\uc73c\ub85c \uad6c\uc131\ub418\uc5b4\uc788\ub2e4. correct(1, 2) + padding(0)\n        self.embedding_interaction = nn.Embedding(\n            3, self.hidden_dim//3)\n\n        if self.args.cate_cols:\n            self.embedding_category = {}\n            for col_name, col_len in self.args.cate_cols.items():\n                self.embedding_category[col_name] = nn.Embedding(\n                    col_len + 1, self.hidden_dim//3)\n                setattr(self, f'emb_{col_name}',\n                        self.embedding_category[col_name])\n\n        # embedding combination projection\n        self.cate_proj = nn.Sequential(\n            nn.Linear((self.hidden_dim//3)*(cate_col_num+1),\n                      self.hidden_dim),\n            nn.LayerNorm(self.hidden_dim)\n        )\n\n        if self.args.cont_cols:\n            self.bn_cont = nn.BatchNorm1d(cont_col_num)\n            self.embedding_cont = nn.Sequential(\n                nn.Linear(cont_col_num, self.hidden_dim),\n                nn.LayerNorm(self.hidden_dim))\n\n        # embedding combination projection\n        self.comb_proj = nn.Sequential(\n            nn.Dropout(self.args.drop_out),\n            nn.Linear(self.hidden_dim*divider, self.hidden_dim),\n            nn.LayerNorm(self.hidden_dim))\n\n        self.lstm = nn.LSTM(self.hidden_dim,\n                            self.hidden_dim,\n                            self.n_layers,\n                            batch_first=True)\n\n        self.config = BertConfig(\n            3,  # not used\n            hidden_size=self.hidden_dim,\n            num_hidden_layers=1,\n            num_attention_heads=self.n_heads,\n            intermediate_size=self.hidden_dim,\n            hidden_dropout_prob=self.drop_out,\n            attention_probs_dropout_prob=self.drop_out,\n        )\n        self.attn = BertEncoder(self.config)\n\n        # Fully connected layer\n        self.fc = nn.Linear(self.hidden_dim, 1)\n\n        self.activation = nn.Sigmoid()\n\n    def init_hidden(self, batch_size):\n        h = torch.zeros(\n            self.n_layers,\n            batch_size,\n            self.hidden_dim)\n        h = h.to(self.device)\n\n        c = torch.zeros(\n            self.n_layers,\n            batch_size,\n            self.hidden_dim)\n        c = c.to(self.device)\n\n        return (h, c)\n\n    def forward(self, input):\n\n        # Categorical Variable Embedding\n        batch_size = input['oth']['interaction'].size(0)\n        # Embedding\n\n        embed_interaction = self.embedding_interaction(\n            input['oth']['interaction'])\n\n        feature_linear = []\n\n        cate_embed_list = []\n        if self.args.cate_cols:\n            for k in self.embedding_category.keys():\n                cate_embed_list.append(\n                    self.embedding_category[k](input['cate'][k]))\n\n        embed_list = [embed_interaction] + cate_embed_list\n        cate_embed = torch.cat(embed_list, 2)\n\n        feature_linear.append(self.cate_proj(cate_embed))\n\n        # continuous variable embedding\n        # batch normalization\n        if self.args.cont_cols:\n            cont_col_list = []\n            for cont_col in self.args.cont_cols:\n                cont_col_list.append(input['cont'][cont_col].unsqueeze(2))\n            cont_all = torch.cat(cont_col_list, 2)\n            cont = self.bn_cont(\n                cont_all.view(-1, cont_all.size(-1))).view(batch_size, -1, cont_all.size(-1))\n            embed_cont = self.embedding_cont(cont)\n\n            feature_linear.append(embed_cont)\n\n        # Running LSTM\n        embed = self.comb_proj(torch.cat(feature_linear, 2))\n        hidden = self.init_hidden(batch_size)\n        out, hidden = self.lstm(embed, hidden)\n        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n\n        extended_attention_mask = input['oth'][\"mask\"].unsqueeze(\n            1).unsqueeze(2)\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=torch.float32)\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        head_mask = [None] * self.n_layers\n\n        encoded_layers = self.attn(\n            out, extended_attention_mask, head_mask=head_mask)\n        sequence_output = encoded_layers[-1]\n\n        # if self.args.loss == 'arcface':\n        #     sequence_output = sequence_output[:, -1,\n        #                                       :].contiguous().view(batch_size, -1)\n        #     return sequence_output\n\n        out = self.fc(sequence_output)\n\n        preds = self.activation(out).view(batch_size, -1)\n\n        return preds\n\n\nclass LGBM:\n    def __init__(self, args):\n        self.args = args\n\n    def train(self, train, valid, test, args):\n        result = {\"epoch\": 0, \"train_loss\": 0, \"train_auc\": 0, \"train_acc\": 0,\n                  \"valid_auc\": 0.7, \"valid_acc\": 0.7}\n        wandb.log(result)\n        # X, y \uac12 \ubd84\ub9ac\n        y_train = train['answerCode'].values.ravel()\n        train = train.drop(['answerCode'], axis=1)\n\n        y_valid = valid['answerCode'].values.ravel()\n        valid = valid.drop(['answerCode'], axis=1)\n        lgb_train = lgb.Dataset(train[train.columns], y_train)\n        lgb_valid = lgb.Dataset(valid[valid.columns], y_valid)\n        os.makedirs(os.path.join('./models/', args.model_alias), exist_ok=True)\n        folder= os.path.join('./models/', args.model_alias)\n        output = os.path.join(folder, \"model.txt\")\n        \n        args.seed = 74\n        args.extra_trees = False\n        args.lr = 0.041015616417097805\n        args.xgb_dart = True\n        args.num_leaves = 62\n        args.drop_out = 0.05177613641421279\n        param =  {\n                'tree_learner': args.tl,\n                'seed': args.seed,\n                'drop_seed': args.seed,\n                'objective': 'binary',\n                'metric': 'auc',\n                'boosting': args.boosting,\n                'num_threads': args.num_workers,\n                'extra_trees': args.extra_trees,\n                'drop_rate': args.drop_out,\n                'xgboost_dart_mode': args.xgb_dart,\n                'output_model': output,\n                'learning_rate': args.lr,\n                'device_type': \"cpu\",\n                # 'max_bin': args.max_bin,\n                'num_leaves': args.num_leaves\n            }\n        print(param)\n        model = lgb.train(\n            {\n                'tree_learner': args.tl,\n                'seed': args.seed,\n                'drop_seed': args.seed,\n                'objective': 'binary',\n                'metric': 'auc',\n                'boosting': args.boosting,\n                'num_threads': args.num_workers,\n                # 'extra_trees': args.extra_trees,\n                'drop_rate': args.drop_out,\n                'xgboost_dart_mode': args.xgb_dart,\n                'output_model': output,\n                'learning_rate': args.lr,\n                'device_type': \"cpu\",\n                # 'max_bin': args.max_bin,\n                'num_leaves': args.num_leaves\n            },\n            lgb_train,\n            valid_sets=[lgb_train, lgb_valid],\n            verbose_eval=10,\n            num_boost_round=20,\n            early_stopping_rounds=200,\n        )\n        model.save_model(output)\n        preds = model.predict(valid[valid.columns])\n        acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n        auc = roc_auc_score(y_valid, preds)\n        print(f'VALID AUC : {auc} ACC : {acc}\\n')\n        result = {\"epoch\": self.args.n_epochs-1, \"train_loss\": 0, \"train_auc\": 0, \"train_acc\": 0,\n                  \"valid_auc\": auc, \"valid_acc\": acc}\n        wandb.log(result)\n\n        # ax = lgb.plot_importance(model)\n        # fig = ax.figure\n        # fig.set_size_inches(30, 40)\n        # output_name = duplicate_name_changer(\n        #     f'./output/', f'{self.args.model}{self.args.save_suffix}')\n        # os.makedirs(os.path.join('./output/', output_name), exist_ok=True)\n        # plt.savefig(os.path.join(os.path.join(\n        #     './output/', output_name), 'impor.png'))\n        # LEAVE LAST INTERACTION ONLY\n        # test = test[test['userID'] != test['userID'].shift(-1)]\n        # # DROP ANSWERCODE\n        # test = test.drop(['answerCode'], axis=1)\n\n        total_preds = model.predict(test[test.columns])\n        # SAVE OUTPUT\n        output_name = duplicate_name_changer(\n        self.args.output_dir, f\"final.csv\")\n        write_path = os.path.join(os.path.join('./output/', args.model_alias), output_name)\n        os.makedirs(self.args.output_dir, exist_ok=True)\n        with open('./final.csv', 'w', encoding='utf8') as w:\n            print(\"writing prediction : {}\".format(write_path))\n            w.write(\"id,prediction\\n\")\n            for id, p in enumerate(total_preds):\n                w.write('{},{}\\n'.format(id,p))\n\n\nclass TfixupSaint(nn.Module):\n    def __init__(self, args):\n        super(TfixupSaint, self).__init__()\n        self.args = args\n        self.device = args.device\n\n        # Defining some parameters\n        self.hidden_dim = self.args.hidden_dim\n        self.n_layers = self.args.n_layers\n        self.dropout = self.args.drop_out\n\n        cate_col_num = len(self.args.cate_cols.keys())\n        cont_col_num = len(self.args.cont_cols)\n        divider = (bool(cate_col_num) + bool(cont_col_num))\n        if divider == 0:\n            raise RuntimeError(\"no feature found.\")\n\n        # encoder\n        if self.args.cate_cols:\n            self.embedding_category = {}\n            for col_name, col_len in self.args.cate_cols.items():\n                self.embedding_category[col_name] = nn.Embedding(\n                    col_len + 1, self.hidden_dim//3)\n                setattr(self, f'embedding_{col_name}',\n                        self.embedding_category[col_name])\n        self.enc_comb_proj = nn.Linear(\n            (self.hidden_dim // 3) * cate_col_num, self.hidden_dim)\n\n        # Embedding\n        # interaction\uc740 \ud604\uc7ac correct\uc73c\ub85c \uad6c\uc131\ub418\uc5b4\uc788\ub2e4. correct(1, 2) + padding(0)\n        self.embedding_interaction = nn.Embedding(3, self.hidden_dim // 3)\n\n        if \"tfixup\" in self.args.model.lower():\n            self.cate_proj = nn.Linear((self.hidden_dim//3)*1,\n                                       self.hidden_dim)\n        else:\n            self.cate_proj = nn.Sequential(\n                nn.Linear((self.hidden_dim//3)*1,\n                          self.hidden_dim),\n                nn.LayerNorm(self.hidden_dim)\n            )\n\n        # Decoder embed\n\n        if self.args.cont_cols:\n            self.cont_bn = nn.BatchNorm1d(cont_col_num)\n            if \"tfixup\" in self.args.model.lower():\n                self.cont_proj = nn.Linear(cont_col_num, self.hidden_dim)\n            else:\n                self.cont_proj = nn.Sequential(\n                    nn.Linear(cont_col_num, self.hidden_dim),\n                    nn.LayerNorm(self.hidden_dim)\n                )\n\n        self.comb_proj = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n\n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(\n            self.hidden_dim, self.dropout, self.args.max_seq_len\n        )\n        self.pos_decoder = PositionalEncoding(\n            self.hidden_dim, self.dropout, self.args.max_seq_len\n        )\n        self.transformer = nn.Transformer(\n            d_model=self.hidden_dim,\n            nhead=self.args.n_heads,\n            num_encoder_layers=self.args.n_layers,\n            num_decoder_layers=self.args.n_layers,\n            dim_feedforward=self.hidden_dim,\n            dropout=self.dropout,\n            activation=\"relu\",\n        )\n\n        self.fc = nn.Linear(self.hidden_dim, 1)\n        self.activation = nn.Sigmoid()\n\n        self.enc_mask = None\n        self.dec_mask = None\n        self.enc_dec_mask = None\n\n        # T-Fixup\n        if \"tfixup\" in self.args.model.lower():\n            print(\"#######tfixup start!######\")\n            # \ucd08\uae30\ud654 (Initialization)\n            self.tfixup_initialization()\n            print(\"T-Fixupbb Initialization Done\")\n\n            # \uc2a4\ucf00\uc77c\ub9c1 (Scaling)\n            self.tfixup_scaling()\n            print(f\"T-Fixup Scaling Done\")\n\n    def tfixup_initialization(self):\n        # \uc6b0\ub9ac\ub294 padding idx\uc758 \uacbd\uc6b0 \ubaa8\ub450 0\uc73c\ub85c \ud1b5\uc77c\ud55c\ub2e4\n        padding_idx = 0\n\n        for name, param in self.named_parameters():\n            if re.match(r\"^embedding*\", name):\n                nn.init.normal_(param, mean=0, std=param.shape[1] ** -0.5)\n                nn.init.constant_(param[padding_idx], 0)\n            elif re.match(r\".*ln.*|.*bn.*\", name):\n                continue\n            elif re.match(r\".*norm.*\", name):\n                continue\n            elif re.match(r\".*weight*\", name):\n                # nn.init.xavier_uniform_(param)\n                nn.init.xavier_normal_(param)\n\n    def tfixup_scaling(self):\n        temp_state_dict = {}\n\n        # \ud2b9\uc815 layer\ub4e4\uc758 \uac12\uc744 \uc2a4\ucf00\uc77c\ub9c1\ud55c\ub2e4\n        for name, param in self.named_parameters():\n\n            # TODO: \ubaa8\ub378 \ub0b4\ubd80\uc758 module \uc774\ub984\uc774 \ub2ec\ub77c\uc9c0\uba74 \uc9c1\uc811 \uc218\uc815\ud574\uc11c\n            #       module\uc774 scaling \ub420 \uc218 \uc788\ub3c4\ub85d \ubcc0\uacbd\ud574\uc8fc\uc790\n            # print(name)\n\n            if re.match(r\"^embedding*\", name):\n                temp_state_dict[name] = (\n                    9 * self.args.n_layers) ** (-1 / 4) * param\n            elif re.match(r\".*norm.*\", name):\n                continue\n            elif re.match(r\".*ln.*|.*bn.*\", name):\n                continue\n            elif re.match(r\"encoder.*linear.*weight|encoder.*attn.*out.*weight\", name):\n                temp_state_dict[name] = (\n                    0.67 * (self.args.n_layers) ** (-1 / 4)\n                ) * param\n            elif re.match(r\"encoder.*attn.*in.*weight\", name):\n                temp_state_dict[name] = (0.67 * (self.args.n_layers) ** (-1 / 4)) * (\n                    param * (2 ** 0.5)\n                )\n            elif re.match(r\"decoder.*linear.*weight|decoder.*attn.*out.*weight\", name):\n                temp_state_dict[name] = (\n                    9 * (self.args.n_layers) ** (-1 / 4)) * param\n            elif re.match(r\"decoder.*attn.*in.*weight\", name):\n                temp_state_dict[name] = (9 * (self.args.n_layers) ** (-1 / 4)) * (\n                    param * (2 ** 0.5)\n                )\n\n        # \ub098\uba38\uc9c0 layer\ub294 \uc6d0\ub798 \uac12 \uadf8\ub300\ub85c \ub123\ub294\ub2e4\n        for name in self.state_dict():\n            if name not in temp_state_dict:\n                temp_state_dict[name] = self.state_dict()[name]\n\n        self.load_state_dict(temp_state_dict)\n\n    def get_mask(self, seq_len):\n        mask = torch.from_numpy(np.triu(np.ones((seq_len, seq_len)), k=1))\n\n        return mask.masked_fill(mask == 1, float(\"-inf\"))\n\n    def forward(self, input):\n        batch_size = input['oth'][\"interaction\"].size(0)\n        seq_len = input['oth'][\"interaction\"].size(1)\n        embed_interaction = self.embedding_interaction(\n            input['oth'][\"interaction\"])\n        feature_linear = []\n\n        cate_embed_list = []\n        if self.args.cate_cols:\n            for k in self.embedding_category.keys():\n                cate_embed_list.append(\n                    self.embedding_category[k](input['cate'][k]))\n\n        embed_enc = torch.cat(cate_embed_list, 2)\n\n        embed_enc = self.enc_comb_proj(embed_enc)\n\n        # DECODER\n        # Categorical\n        cate_embed = torch.cat([embed_interaction], 2)\n        feature_linear.append(self.cate_proj(cate_embed))\n        # Continuous\n        if self.args.cont_cols:\n            cont_col_list = []\n            for cont_col in self.args.cont_cols:\n                cont_col_list.append(input['cont'][cont_col].unsqueeze(-1))\n            cont_all = torch.cat(cont_col_list, -1)\n            cont = self.cont_bn(\n                cont_all.view(-1, cont_all.size(-1))).view(batch_size, -1, cont_all.size(-1))\n            embed_cont = self.cont_proj(cont)\n\n            feature_linear.append(embed_cont)\n\n        embed_dec = torch.cat(feature_linear, 2)\n        embed_dec = self.comb_proj(embed_dec)\n\n        # ATTENTION MASK \uc0dd\uc131\n        # encoder\ud558\uace0 decoder\uc758 mask\ub294 \uac00\ub85c \uc138\ub85c \uae38\uc774\uac00 \ubaa8\ub450 \ub3d9\uc77c\ud558\uc5ec\n        # \uc0ac\uc2e4 \uc774\ub807\uac8c 3\uac1c\ub85c \ub098\ub20c \ud544\uc694\uac00 \uc5c6\ub2e4\n        if self.enc_mask is None or self.enc_mask.size(0) != seq_len:\n            self.enc_mask = self.get_mask(seq_len).to(self.device)\n\n        if self.dec_mask is None or self.dec_mask.size(0) != seq_len:\n            self.dec_mask = self.get_mask(seq_len).to(self.device)\n\n        if self.enc_dec_mask is None or self.enc_dec_mask.size(0) != seq_len:\n            self.enc_dec_mask = self.get_mask(seq_len).to(self.device)\n\n        embed_enc = embed_enc.permute(1, 0, 2)\n        embed_dec = embed_dec.permute(1, 0, 2)\n\n        # Positional encoding\n        embed_enc = self.pos_encoder(embed_enc)\n        embed_dec = self.pos_decoder(embed_dec)\n\n        out = self.transformer(\n            embed_enc,\n            embed_dec,\n            src_mask=self.enc_mask,\n            tgt_mask=self.dec_mask,\n            memory_mask=self.enc_dec_mask,\n        )\n\n        out = out.permute(1, 0, 2)\n        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n        out = self.fc(out)\n        out = out.view(batch_size, -1)\n\n        preds = self.activation(out)\n\n        return preds\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=1000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.scale = nn.Parameter(torch.ones(1))\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(\n            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.scale * self.pe[:x.size(0), :]\n        return self.dropout(x)\n"}
{"content": "\"\"\"\nA module containing a class that produces Python code for simulating a PySB\nmodel without requiring PySB itself (note that NumPy and SciPy are still\nrequired). This offers a way of distributing a model to those who do not have\nPySB.\n\nFor information on how to use the model exporters, see the documentation\nfor :py:mod:`pysb.export`.\n\nStructure of the standalone Python code\n=======================================\n\nThe standalone Python code defines a class, ``Model``, with a method\n``simulate`` that can be used to simulate the model.\n\nAs shown in the code for the Robertson model below, the ``Model`` class defines\nthe fields ``parameters``, ``observables``, and ``initial_conditions`` as lists\nof ``collections.namedtuple`` objects that allow access to the features of the\nmodel.\n\nThe ``simulate`` method has the following signature::\n\n    def simulate(self, tspan, param_values=None, view=False):\n\nwith arguments as follows:\n\n* ``tspan`` specifies the array of timepoints\n* ``param_values`` is an optional vector of parameter values that can be used\n  to override the nominal values defined in the PySB model\n* ``view`` is an optional boolean argument that specifies if the simulation\n  output arrays are returned as copies (views) of the original. If True,\n  returns copies of the arrays, allowing changes to be made to values in the\n  arrays without affecting the originals.\n\n``simulate`` returns a tuple of two arrays. The first array is a matrix\nwith timecourses for each species in the model as the columns. The\nsecond array is a numpy record array for the model's observables, which can\nbe indexed by name.\n\nOutput for the Robertson example model\n======================================\n\nExample code generated for the Robertson model, ``pysb.examples.robertson``:\n\n.. literalinclude:: ../../examples/robertson_standalone.py\n\nUsing the standalone Python model\n=================================\n\nAn example usage pattern for the standalone Robertson model, once generated::\n\n    # Import the standalone model file\n    import robertson_standalone\n    import numpy\n    from matplotlib import pyplot as plt\n\n    # Instantiate the model object (the constructor takes no arguments)\n    model = robertson_standalone.Model()\n\n    # Simulate the model\n    tspan = numpy.linspace(0, 100)\n    (species_output, observables_output) = model.simulate(tspan)\n\n    # Plot the results\n    plt.figure()\n    plt.plot(tspan, observables_output['A_total'])\n    plt.show()\n\"\"\"\n\nimport pysb\nimport pysb.bng\nimport sympy\nfrom pysb.export import Exporter, pad, ExpressionsNotSupported, \\\n    CompartmentsNotSupported\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from io import StringIO\nimport re\n\nclass PythonExporter(Exporter):\n    \"\"\"A class for returning the standalone Python code for a given PySB model.\n\n    Inherits from :py:class:`pysb.export.Exporter`, which implements\n    basic functionality for all exporters.\n    \"\"\"\n    def export(self):\n        \"\"\"Export Python code for simulation of a model without PySB.\n\n        Returns\n        -------\n        string\n            String containing the standalone Python code.\n        \"\"\"\n        if self.model.expressions:\n            raise ExpressionsNotSupported()\n        if self.model.compartments:\n            raise CompartmentsNotSupported()\n\n        output = StringIO()\n        pysb.bng.generate_equations(self.model)\n\n        # Note: This has a lot of duplication from pysb.integrate.\n        # Can that be helped?\n\n        code_eqs = '\\n'.join(['ydot[%d] = %s;' %\n                                 (i, sympy.ccode(self.model.odes[i]))\n                              for i in range(len(self.model.odes))])\n        code_eqs = re.sub(r'__s(\\d+)',\n                          lambda m: 'y[%s]' % (int(m.group(1))), code_eqs)\n        for i, p in enumerate(self.model.parameters):\n            code_eqs = re.sub(r'\\b(%s)\\b' % p.name, 'p[%d]' % i, code_eqs)\n\n        if self.docstring:\n            output.write('\"\"\"')\n            output.write(self.docstring)\n            output.write('\"\"\"\\n\\n')\n        output.write(\"# exported from PySB model '%s'\\n\" % self.model.name)\n        output.write(pad(r\"\"\"\n            import numpy\n            import scipy.integrate\n            import collections\n            import itertools\n            import distutils.errors\n            \"\"\"))\n        output.write(pad(r\"\"\"\n            _use_inline = False\n            # try to inline a C statement to see if inline is functional\n            try:\n                import weave\n                weave.inline('int i;', force=1)\n                _use_inline = True\n            except ImportError:\n                pass\n            except distutils.errors.CompileError:\n                pass\n\n            Parameter = collections.namedtuple('Parameter', 'name value')\n            Observable = collections.namedtuple('Observable', 'name species coefficients')\n            Initial = collections.namedtuple('Initial', 'param_index species_index')\n            \"\"\"))\n        output.write(\"\\n\")\n\n        output.write(\"class Model(object):\\n\")\n        init_data = {\n            'num_species': len(self.model.species),\n            'num_params': len(self.model.parameters),\n            'num_observables': len(self.model.observables),\n            'num_ics': len(self.model.initial_conditions),\n            }\n        output.write(pad(r\"\"\"\n            def __init__(self):\n                self.y = None\n                self.yobs = None\n                self.integrator = scipy.integrate.ode(self.ode_rhs)\n                self.integrator.set_integrator('vode', method='bdf',\n                                               with_jacobian=True)\n                self.y0 = numpy.empty(%(num_species)d)\n                self.ydot = numpy.empty(%(num_species)d)\n                self.sim_param_values = numpy.empty(%(num_params)d)\n                self.parameters = [None] * %(num_params)d\n                self.observables = [None] * %(num_observables)d\n                self.initial_conditions = [None] * %(num_ics)d\n            \"\"\", 4) % init_data)\n        for i, p in enumerate(self.model.parameters):\n            p_data = (i, repr(p.name), p.value)\n            output.write(\" \" * 8)\n            output.write(\"self.parameters[%d] = Parameter(%s, %.17g)\\n\" % p_data)\n        output.write(\"\\n\")\n        for i, obs in enumerate(self.model.observables):\n            obs_data = (i, repr(obs.name), repr(obs.species),\n                        repr(obs.coefficients))\n            output.write(\" \" * 8)\n            output.write(\"self.observables[%d] = Observable(%s, %s, %s)\\n\" %\n                         obs_data)\n        output.write(\"\\n\")\n        for i, (cp, param) in enumerate(self.model.initial_conditions):\n            ic_data = (i, self.model.parameters.index(param),\n                       self.model.get_species_index(cp))\n            output.write(\" \" * 8)\n            output.write(\"self.initial_conditions[%d] = Initial(%d, %d)\\n\" %\n                         ic_data)\n        output.write(\"\\n\")\n\n        output.write(\"    if _use_inline:\\n\")\n        output.write(pad(r\"\"\"\n            def ode_rhs(self, t, y, p):\n                ydot = self.ydot\n                weave.inline(r'''%s''', ['ydot', 't', 'y', 'p'])\n                return ydot\n            \"\"\", 8) % (pad('\\n' + code_eqs, 16) + ' ' * 16))\n        output.write(\"    else:\\n\")\n        output.write(pad(r\"\"\"\n            def ode_rhs(self, t, y, p):\n                ydot = self.ydot\n                %s\n                return ydot\n            \"\"\", 8) % pad('\\n' + code_eqs, 12).replace(';','').strip())\n\n        # note the simulate method is fixed, i.e. it doesn't require any templating\n        output.write(pad(r\"\"\"\n            def simulate(self, tspan, param_values=None, view=False):\n                if param_values is not None:\n                    # accept vector of parameter values as an argument\n                    if len(param_values) != len(self.parameters):\n                        raise Exception(\"param_values must have length %d\" %\n                                        len(self.parameters))\n                    self.sim_param_values[:] = param_values\n                else:\n                    # create parameter vector from the values in the model\n                    self.sim_param_values[:] = [p.value for p in self.parameters]\n                self.y0.fill(0)\n                for ic in self.initial_conditions:\n                    self.y0[ic.species_index] = self.sim_param_values[ic.param_index]\n                if self.y is None or len(tspan) != len(self.y):\n                    self.y = numpy.empty((len(tspan), len(self.y0)))\n                    if len(self.observables):\n                        self.yobs = numpy.ndarray(len(tspan),\n                                        list(zip((obs.name for obs in self.observables),\n                                            itertools.repeat(float))))\n                    else:\n                        self.yobs = numpy.ndarray((len(tspan), 0))\n                    self.yobs_view = self.yobs.view(float).reshape(len(self.yobs),\n                                                                   -1)\n                # perform the actual integration\n                self.integrator.set_initial_value(self.y0, tspan[0])\n                self.integrator.set_f_params(self.sim_param_values)\n                self.y[0] = self.y0\n                t = 1\n                while self.integrator.successful() and self.integrator.t < tspan[-1]:\n                    self.y[t] = self.integrator.integrate(tspan[t])\n                    t += 1\n                for i, obs in enumerate(self.observables):\n                    self.yobs_view[:, i] = \\\n                        (self.y[:, obs.species] * obs.coefficients).sum(1)\n                if view:\n                    y_out = self.y.view()\n                    yobs_out = self.yobs.view()\n                    for a in y_out, yobs_out:\n                        a.flags.writeable = False\n                else:\n                    y_out = self.y.copy()\n                    yobs_out = self.yobs.copy()\n                return (y_out, yobs_out)\n            \"\"\", 4))\n\n        return output.getvalue()\n\n\n"}
{"content": "import torch as th\nimport torch.nn as nn\nimport time\n\n#Define a basic MLP Pytorch Model.\nclass PyTorchMlp(nn.Module):  \n\n  def __init__(self, n_inputs=4, n_actions=2):\n        nn.Module.__init__(self)\n        \n        # Apply linear transformation y=xA^T + b\n        self.fc1 = nn.Linear(n_inputs, 512) #size of input sample. Size of output sample\n        self.fc2 = nn.Linear(512, 256)      \n        self.fc3 = nn.Linear(256, n_actions)      \n        # self.activ_fn = nn.Tanh()\n        self.activ_fn = nn.ReLU()\n        self.out_activ = nn.Softmax(dim=0)\n\n  def forward(self, x):\n        #Should shape x? [120]\n        x = self.activ_fn(self.fc1(x)) #8  Relu-> 512\n        x = self.activ_fn(self.fc2(x)) #512 Relu-> 256\n        x = self.out_activ(self.fc3(x)) #256 Softmax -> 8 \n        return x\n\n# Convert weights from \ndef copy_mlp_weights(baselines_model):\n    torch_mlp = PyTorchMlp(n_inputs=120, n_actions=8)\n    model_params = baselines_model.get_parameters()\n\n    policy_keys = [key for key in model_params.keys() if \"pi\" in key]\n    policy_params = [model_params[key] for key in policy_keys]\n        \n    for (th_key, pytorch_param), key, policy_param in zip(torch_mlp.named_parameters(), policy_keys, policy_params):\n        # copy parameters from stable baselines model\n        param = policy_param.copy()\n\n        # weight of fully connected layer\n        if len(param.shape) == 2:\n            # transpose parameter\n            param = param.T\n\n        #bias\n        if 'b' in key:\n            # remove all dimensions with size 1\n            param = param.squeeze()\n\n        param = th.from_numpy(param)\n        pytorch_param.data.copy_(param.data.clone())\n        \n        \n        #param = th.from_numpy(policy_param)    \n        #Copies parameters from baselines model to pytorch model\n        # print(th_key, key)\n        # print(pytorch_param.shape, param.shape, policy_param.shape)\n        # pytorch_param.data.copy_(param.data.clone().t())\n        \n    return torch_mlp\n\n\n"}
{"content": "import os\nfrom opts import parse_opts\nfrom data.train_data_loader import DataSet as train_DataSet\nfrom data.test_data_loader import DataSet as test_DataSet\nfrom lib.spatial_transforms import *\nimport torch\n\nopt = parse_opts()\n\nopt.scales = [opt.initial_scale]\nfor i in range(1, opt.n_scales):\n    opt.scales.append(opt.scales[-1] * opt.scale_step)\n\nopt.mean = get_mean(opt.norm_value)\nprint(opt)\n\ntorch.manual_seed(opt.manual_seed)\n\ncheck_train_dataloader = False\ncheck_test_dataloader = False\n\nif check_train_dataloader:\n    spatial_transform = get_train_spatial_transform(opt)\n    temporal_transform = None\n    target_transform = None\n    list_root_path = list()\n    list_root_path.append(os.path.join(opt.root_dir, opt.train_subdir))\n    list_root_path.append(os.path.join(opt.root_dir, opt.only_gradual_subdir))\n    print(list_root_path, flush=True)\n    print(\"[INFO] reading : \", opt.video_list_path, flush=True)\n    training_data = train_DataSet(list_root_path, opt.video_list_path, opt,\n                            spatial_transform=spatial_transform,\n                            temporal_transform=temporal_transform,\n                            target_transform=target_transform, sample_duration=opt.sample_duration)\n    weights = torch.DoubleTensor(training_data.weights)\n    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n    training_data_loader = torch.utils.data.DataLoader(training_data, batch_size=opt.batch_size, sampler=sampler)\n\n    if opt.iter_per_epoch == 0:\n        if opt.is_full_data:\n            opt.iter_per_epoch = 500000\n        else:\n            opt.iter_per_epoch = 70000\n\n        opt.iter_per_epoch = int(opt.iter_per_epoch / opt.batch_size)\n    print(\"iter_per_epoch : {}, batch_size : {}\".format(opt.iter_per_epoch, opt.batch_size))\n\n    iter_count = list([0, 0, 0])\n    i = 0\n    print(\"start_check\")\n    for _, (data, labels) in enumerate(training_data_loader):\n        if i % 1000 == 0:\n            print(i)\n        if i == opt.iter_per_epoch or i==1000:\n            break\n        for label in labels:\n            iter_count[int(label)] += 1\n            i += 1\n\n    print(iter_count)\n\nif check_test_dataloader:\n    spatial_transform = get_test_spatial_transform(opt)\n    temporal_transform = None\n    target_transform = None\n    # list_root_path : train path, only_gradual path\n    # `19.3.7 : add only_gradual path\n    root_dir = os.path.join(opt.root_dir, opt.test_subdir)\n    # print(root_dir, flush=True)\n    # print(opt.test_list_path, flush=True)\n    with open(opt.test_list_path, 'r') as f:\n        video_name_list = [line.strip('\\n') for line in f.readlines()]\n\n    res = {}\n    # print('\\n====> Testing Start', flush=True)\n    for idx, video_name in enumerate(video_name_list[:5]):\n        print(\"Process {}\".format(idx+1), end=' ', flush=True)\n        test_data = test_DataSet(root_dir, video_name,\n                                 spatial_transform=spatial_transform,\n                                 temporal_transform=temporal_transform,\n                                 target_transform=target_transform,\n                                 sample_duration=opt.sample_duration,\n                                 candidate=opt.candidate)\n        test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=opt.batch_size,\n                                                       num_workers=opt.n_threads, pin_memory=True)\n\n        frame_pos = []\n        for i, (clip, boundary) in enumerate(test_data_loader):\n            boundary = boundary.data.numpy()\n            for _ in boundary:\n                frame_pos.append(int(_+1))\n        print('{} : {}'.format(video_name, frame_pos))\n"}
{"content": "\"\"\"\nMerges many .WAV files into one great big one and keeps track of the sample\noffset for each one.\n\"\"\"\n\nfrom . import wave_to_numpy\nfrom .. import constants\nfrom ..data import READ\nfrom ..util.elapsed_bar import elapsed_iterator\nfrom .corpus import Corpus\nimport numpy as np\n\nTOTAL_FRAMES = 76522480090\nEND = '(END)'\n\n\ndef merge(mmap, nframes, files, index):\n    # TODO: this should use data. resources now\n    writer = wave_to_numpy.memmap(mmap, nframes, 'w+')\n    frames = 0\n\n    for i, f in enumerate(elapsed_iterator(files)):\n        try:\n            reader = wave_to_numpy.reader(f)\n            nsamples, nchannels = reader.shape\n            if nchannels == 1:\n                reader = np.repeat(reader, 2, axis=1)\n            elif nchannels != 2:\n                raise ValueError\n            writer[frames : frames + nsamples] = reader\n            frames += nsamples\n            if frames != index[i]:\n                raise ValueError(f'bad frame count {frames}, {f}, {index[i]}')\n        except Exception:\n            print('In file', f)\n            raise\n    return writer\n\n\ndef find_bad(files):\n    for f in elapsed_iterator(files):\n        try:\n            wave_to_numpy.reader(f)\n        except Exception:\n            print(f)\n\n\nif __name__ == '__main__':\n    if False:\n        find_bad(Corpus.filenames)\n    else:\n        merge(constants.CORPUS, TOTAL_FRAMES, Corpus.filenames, READ.index())\n"}
{"content": "#!/usr/bin/env python\nimport os\nimport time\nimport datetime\nimport json\nimport multiprocessing as mp\nimport random\nimport numpy as np\nfrom collections import deque\nfrom collections import namedtuple\nimport cv2\nimport torch\nimport psycopg2\nimport matplotlib.pyplot as plt\n\nimport db_initializer\nimport tables\nimport trade_environment\nfrom trade_environment import TradeEnvironment\nimport model\nimport action_suggester\n\n\nclass Actor:\n\n\tdef __init__(self, params, param_set_id, actor_id, status_dict, shared_state, remote_mem):\n\t\tself.params = params\n\t\tself.param_set_id = param_set_id\n\t\tself.actor_id = actor_id\n\t\tself.status_dict = status_dict\n\t\tself.shared_state = shared_state\n\t\tself.remote_mem = remote_mem\n\n\t\tep = params['env']\n\t\tap = params['actor']\n\t\tlp = params['learner']\n\t\tmodel_formula = f'model.{lp[\"model\"]}(self.state_shape, self.action_dim, \"Actor# {actor_id}\", hidden_size={lp[\"hidden_size\"]})'\n\t\tmodel_formula_target = f'model.{lp[\"model\"]}(self.state_shape, self.action_dim, hidden_size={lp[\"hidden_size\"]})'\n\n\t\tself.window_size = ep['window_size']\n\t\tself.state_shape = tuple(ep['frames_height_width'])\n\t\tself.frame_num = self.state_shape[0]\n\t\tself.action_dim = trade_environment.action_num\n\t\tself.num_steps = ap[\"num_steps\"] # N\u30b9\u30c6\u30c3\u30d7\u6570\n\t\tself.n_step_transition_batch_size = ap['n_step_transition_batch_size']\n\t\tself.env = TradeEnvironment('test.dat', self.window_size, self.state_shape[1:])\n\t\tself.epsilon = ap['epsilon']**(1 + ap['alpha'] * self.actor_id / (ap['num_actors'] - 1))\n\t\tself.last_Q_state_dict_id = 0\n\n\t\tself.Q = eval(model_formula)\n\t\tself.Q_target = eval(model_formula_target) # Target Q network which is slow moving replica of self.Q\n\t\tQ_state_dict = self.shared_state[\"Q_state_dict\"]\n\t\tself.Q.load_state_dict(Q_state_dict[0])\n\t\tself.Q_target.load_state_dict(Q_state_dict[1])\n\t\tself.sum_los = 0\n\n\tdef make_state(self, state, frame):\n\t\treturn np.concatenate((frame, (state if state.shape[0] < self.frame_num else state[:state.shape[0] - 1])), axis=0)\n\n\tdef run(self):\n\t\ttorch.set_num_threads(1)\n\n\t\tap = self.params['actor']\n\t\tlp = self.params['learner']\n\n\t\tconn = psycopg2.connect(self.params[\"db\"][\"connection_string\"])\n\t\tconn.autocommit = True\n\n\t\tstatus_dict = self.status_dict\n\n\t\tt = tables.ActorData()\n\t\trecord_type = t.get_record_type()\n\t\trecord_insert = t.get_insert()\n\n\t\tt = tables.RewardAdjData()\n\t\treward_adj_record_type = t.get_record_type()\n\t\treward_adj_insert = t.get_insert()\n\n\t\tparam_set_id = self.param_set_id\n\t\tactor_id = self.actor_id\n\t\tnow = datetime.datetime.now\n\t\tep_count = 0\n\t\tep_len = 0\n\t\tep_reward = 0.0\n\t\tsum_reward = 0.0\n\n\t\ttransitions = deque()\n\t\tn_step_transitions = []\n\t\tn_step_transition_batch_size = self.n_step_transition_batch_size\n\t\tnum_steps = self.num_steps\n\t\tgamma = ap['gamma']\n\t\tgamma_n = gamma**num_steps\n\n\t\tQ = self.Q\n\t\tQ_target = self.Q_target\n\t\ttake_offsets = torch.arange(n_step_transition_batch_size) * self.action_dim\n\n\t\twait_shared_memory_clear = ap['wait_shared_memory_clear']\n\n\t\tself.env.spread = ap['spread']\n\t\tself.env.loss_cut = ap['loss_cut']\n\n\t\t# Actor\u306e\u6700\u5f8c\u306e\u30b9\u30c6\u30fc\u30bf\u30b9\u3092\u8aad\u307f\u8fbc\u3080\n\t\tactor_state_file = f'{db_initializer.get_state_dict_name(self.params)}.{self.actor_id}.json'\n\t\tactor_state = {}\n\t\tif os.path.isfile(actor_state_file):\n\t\t\twith open(actor_state_file, 'r') as f:\n\t\t\t\tactor_state = json.load(f)\n\t\t\t\tep_count = actor_state['ep_count']\n\t\t\t\tsum_reward = actor_state['sum_reward']\n\n\t\t# \u304a\u52e7\u3081\u30a2\u30af\u30b7\u30e7\u30f3\u751f\u6210\u3001\u5831\u916c\u8abf\u6574\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u53d6\u5f97\u3059\u308b\n\t\tsuggester = eval(f'action_suggester.{ap[\"action_suggester\"].format(\"self.env\")}')\n\t\treward_adjuster = eval(f'action_suggester.{ap[\"reward_adjuster\"].format(\"suggester\")}')\n\n\t\tdef plc_random(q):\n\t\t\t\"\"\"\u8a08\u7b97\u3055\u308c\u305f\u30a2\u30af\u30b7\u30e7\u30f3\u307e\u305f\u306f\u4e71\u6570\u3092\u53d6\u5f97\u3059\u308b.\n\t\t\t\"\"\"\n\t\t\tq_action = torch.argmax(q, 0).item()\n\t\t\tif random.random() < self.epsilon:\n\t\t\t\treturn random.randrange(0, len(q)), q_action\n\t\t\telse:\n\t\t\t\treturn q_action, q_action\n\n\t\tdef plc_suggested(q):\n\t\t\t\"\"\"\u8a08\u7b97\u3055\u308c\u305f\u30a2\u30af\u30b7\u30e7\u30f3\u307e\u305f\u306f\u304a\u52e7\u3081\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u53d6\u5f97\u3059\u308b.\n\t\t\t\"\"\"\n\t\t\tq_action = torch.argmax(q, 0).item()\n\t\t\tif random.random() < self.epsilon:\n\t\t\t\treturn suggester.get_suggested_action(), q_action\n\t\t\telse:\n\t\t\t\treturn q_action, q_action\n\n\t\t# \u65b9\u7b56\u3092\u53d6\u5f97\u3059\u308b\n\t\tpolicy = eval(ap['policy'])\n\n\t\twhile not status_dict['request_quit']:\n\t\t\tep_len = 0\n\t\t\tep_count += 1\n\t\t\tep_reward = 0.0\n\t\t\tterminal = False\n\n\t\t\t# \u521d\u671f\u72b6\u614b\u306e\u53d6\u5f97\n\t\t\tstate = self.env.reset()\n\t\t\tsuggester.start_episode()\n\t\t\tfor _ in range(self.frame_num - 1):\n\t\t\t\tframe, reward_info, terminal, _ = self.env.step(0, 0)\n\t\t\t\tif terminal:\n\t\t\t\t\tbreak\n\t\t\t\tstate = self.make_state(state, frame)\n\n\t\t\twhile not terminal:\n\t\t\t\t# \u72b6\u614b\u3068\u30dd\u30ea\u30b7\u30fc\u3092\u57fa\u306b\u53d6\u308b\u3079\u304d\u884c\u52d5\u3092\u9078\u629e\u3059\u308b\n\t\t\t\twith torch.no_grad():\n\t\t\t\t\tif self.actor_id == 7:\n\t\t\t\t\t\tmodel.show_plot = True\n\t\t\t\t\tq = Q(torch.tensor(state.reshape((1,) + state.shape))).squeeze()\n\t\t\t\t\tif self.actor_id == 7:\n\t\t\t\t\t\tmodel.plt_pause(0.001)\n\t\t\t\t\t\tmodel.show_plot = False\n\t\t\t\taction = policy(q)\n\n\t\t\t\t# \u6307\u5b9a\u30a2\u30af\u30b7\u30e7\u30f3\u304b\u3089\u5831\u916c\u8abf\u6574\u91cf\u3092\u53d6\u5f97\u3059\u308b\n\t\t\t\treward_adj = reward_adjuster.adjust_reward(action[0])\n\n\t\t\t\t# \u74b0\u5883\u306b\u884c\u52d5\u3092\u9069\u7528\u3057\u6b21\u306e\u72b6\u614b\u3092\u53d6\u5f97\u3059\u308b\n\t\t\t\tindex_in_episode = self.env.index_in_episode\n\t\t\t\tframe, reward_info, terminal, _ = self.env.step(action[0], action[1])\n\t\t\t\tnext_state = self.make_state(state, frame)\n\t\t\t\treward_org = reward_info[0]\n\t\t\t\treward = reward_org + reward_adj\n\t\t\t\timg = next_state.sum(axis=0)\n\t\t\t\timg *= 1.0 / img.max()\n\t\t\t\tcv2.imshow(f'Actor# {self.actor_id}', img)\n\t\t\t\t# self.env.render()\n\n\t\t\t\t# N-StepTransition \u306e\u305f\u3081\u306b\u72b6\u614b\u9077\u79fb\u60c5\u5831\u3092\u8ffd\u52a0\u3059\u308b\n\t\t\t\ttransitions.append((state, action[0], reward, next_state, terminal))\n\n\t\t\t\t# N-StepTransition \u306e\u51e6\u7406\n\t\t\t\tlen_transitions = len(transitions)\n\t\t\t\tif num_steps <= len_transitions or terminal:\n\t\t\t\t\t# N-StepTransition \u3092\u751f\u6210\u3057\u3066\u8ffd\u52a0\n\t\t\t\t\tfirst = transitions[0]\n\t\t\t\t\tlatest = transitions[len_transitions - 1]\n\t\t\t\t\tr = first[2]\n\t\t\t\t\tg = gamma\n\t\t\t\t\tfor i in range(1, len_transitions):\n\t\t\t\t\t\tr += transitions[i][2] * g\n\t\t\t\t\t\tg *= gamma\n\t\t\t\t\ttransitions.popleft()\n\t\t\t\t\tn_step_transitions.append((first[0], first[1], r, latest[1], latest[3], latest[4]))\n\n\t\t\t\t\t# N-StepTransition \u304c\u3042\u308b\u7a0b\u5ea6\u6e9c\u307e\u3063\u305f\u3089\u512a\u5148\u5ea6\u3068\u306a\u308bTD\u8aa4\u5dee\u3092\u8a08\u7b97\u3057\u3066\u30ea\u30e2\u30fc\u30c8\u30e1\u30e2\u30ea\u306b\u8ffd\u52a0\n\t\t\t\t\tif n_step_transition_batch_size <= len(n_step_transitions):\n\t\t\t\t\t\t# \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u512a\u5148\u5ea6\u3068\u306a\u308bTD\u8aa4\u5dee\u3092\u8a08\u7b97\u3059\u308b\n\t\t\t\t\t\ts = torch.tensor([t[0] for t in n_step_transitions], dtype=torch.float32)\n\t\t\t\t\t\ta = torch.tensor([t[1] for t in n_step_transitions], dtype=torch.int64)\n\t\t\t\t\t\tr = torch.tensor([t[2] for t in n_step_transitions], dtype=torch.float32)\n\t\t\t\t\t\ta_latest = torch.tensor([t[3] for t in n_step_transitions], dtype=torch.int64)\n\t\t\t\t\t\ts_latest = torch.tensor([t[4] for t in n_step_transitions], dtype=torch.float32)\n\t\t\t\t\t\tterm = torch.tensor([t[5] for t in n_step_transitions], dtype=torch.float32)\n\t\t\t\t\t\tn_step_transitions.clear()\n\n\t\t\t\t\t\twith torch.no_grad():\n\t\t\t\t\t\t\tQ.eval()\n\t\t\t\t\t\t\tQ_target.eval()\n\t\t\t\t\t\t\tGt = r + (1.0 - term) * gamma_n * Q_target(s_latest).take(take_offsets + a_latest).squeeze()\n\t\t\t\t\t\t\tpriorities = (Gt - Q(s).take(take_offsets + a).squeeze()).abs()\n\t\t\t\t\t\t\tdel Gt\n\n\t\t\t\t\t\t# Learner \u5074\u304c\u51e6\u7406\u3059\u308b\u306e\u3092\u5f85\u3063\u3066\u304b\u3089\u30ea\u30e2\u30fc\u30c8\u30e1\u30e2\u30ea\u306b\u8ffd\u52a0\n\t\t\t\t\t\t# \u203btorch.tensor \u306e\u307e\u307e\u9001\u308b\u3068Learner\u5074\u306e\u90fd\u5408\u3067\u554f\u984c\u304c\u3042\u308b\u306e\u3067 numpy \u306b\u3057\u3066\u3044\u308b\n\t\t\t\t\t\tif wait_shared_memory_clear:\n\t\t\t\t\t\t\twhile n_step_transition_batch_size <= self.remote_mem.qsize():\n\t\t\t\t\t\t\t\ttime.sleep(0.001)\n\t\t\t\t\t\ts = s.numpy()\n\t\t\t\t\t\ta = a.numpy().astype(np.int8)\n\t\t\t\t\t\tr = r.numpy()\n\t\t\t\t\t\ta_latest = a_latest.numpy().astype(np.int8)\n\t\t\t\t\t\ts_latest = s_latest.numpy()\n\t\t\t\t\t\tterm = term.numpy().astype(np.int8)\n\t\t\t\t\t\tbatch = [v for v in zip(s, a, r, a_latest, s_latest, term)]\n\t\t\t\t\t\tself.remote_mem.put((priorities, batch))\n\n\t\t\t\t# Learner \u304b\u3089\u306e\u5171\u6709\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u66f4\u65b0\u3055\u308c\u3066\u3044\u305f\u3089\u30ed\u30fc\u30c9\u3059\u308b\n\t\t\t\tid = status_dict['Q_state_dict_id']\n\t\t\t\tif 1 <= id - self.last_Q_state_dict_id:\n\t\t\t\t\tprint(f'Actor#: {self.actor_id} state loaded.')\n\t\t\t\t\tQ_state_dict = self.shared_state[\"Q_state_dict\"]\n\t\t\t\t\tself.Q.load_state_dict(Q_state_dict[0])\n\t\t\t\t\tself.Q_target.load_state_dict(Q_state_dict[1])\n\t\t\t\t\tself.last_Q_state_dict_id = id\n\n\t\t\t\t# \u7d42\u4e86\u8981\u6c42\u304c\u3042\u3063\u305f\u3089\u30eb\u30fc\u30d7\u3092\u629c\u3051\u308b\n\t\t\t\tif cv2.waitKey(1) == 27 or status_dict['request_quit']:\n\t\t\t\t\tstatus_dict['request_quit'] = True\n\t\t\t\t\tbreak\n\n\t\t\t\t# \u6b21\u306e\u30eb\u30fc\u30d7\u306b\u5099\u3048\u308b\n\t\t\t\tstate = next_state\n\t\t\t\tep_reward += reward_org\n\t\t\t\tep_len += 1\n\t\t\t\tsum_reward += reward_org\n\n\t\t\t\t# \u30a8\u30d4\u30bd\u30fc\u30c9\u7d42\u4e86\u304b\u307e\u305f\u306f\u5831\u916c\u304c\u5165\u3063\u305f\u969b\u306bDB\u3078\u30c7\u30fc\u30bf\u767b\u9332\n\t\t\t\tif terminal or reward_org:\n\t\t\t\t\tif action[0] == action[1]:\n\t\t\t\t\t\tprint(\n\t\t\t\t\t\t    f'Actor#: {self.actor_id} t: {t} rew: {reward_org} {reward} act: {action[0]} ep_len: {ep_len} ep_rew: {ep_reward} sum_rew: {sum_reward}'\n\t\t\t\t\t\t)\n\t\t\t\t\ttrain_num = status_dict['train_num']\n\t\t\t\t\trecord = record_type(param_set_id, actor_id, now(), train_num, ep_count, self.env.cur_episode,\n\t\t\t\t\t                     index_in_episode, action[0], action[1], reward_info[0], reward_info[1], reward_info[2],\n\t\t\t\t\t                     reward_info[3], sum_reward)\n\t\t\t\t\twith conn.cursor() as cur:\n\t\t\t\t\t\trecord_insert(cur, record)\n\n\t\t\t\t# \u5831\u916c\u8abf\u6574\u91cf\u3092DB\u3078\u767b\u9332\n\t\t\t\twith conn.cursor() as cur:\n\t\t\t\t\treward_adj_insert(cur, reward_adj_record_type(param_set_id, actor_id, ep_count, index_in_episode,\n\t\t\t\t\t                                              reward_adj))\n\n\t\t# Actor \u306e\u73fe\u5728\u306e\u30b9\u30c6\u30fc\u30bf\u30b9\u3092\u4fdd\u5b58\u3057\u3066\u304a\u304f\n\t\tactor_state['ep_count'] = ep_count\n\t\tactor_state['sum_reward'] = sum_reward\n\t\twith open(actor_state_file, 'w') as f:\n\t\t\tjson.dump(actor_state, f)\n\n\t\tprint(f'Actor#: {self.actor_id} end')\n\n\nif __name__ == \"__main__\":\n\t\"\"\" \n\tSimple standalone test routine for Actor class\n\t\"\"\"\n\timport learner\n\n\twith open('parameters.json', 'r') as f:\n\t\tparams = json.load(f)\n\n\tparam_set_id = db_initializer.initialize(params)\n\n\tmp_manager = mp.Manager()\n\tstatus_dict = mp_manager.dict()\n\tshared_state = mp_manager.dict()\n\tshared_mem = mp_manager.Queue()\n\n\tparams['actor']['wait_shared_memory_clear'] = False\n\n\tstatus_dict['quit'] = False\n\tstatus_dict['Q_state_dict_stored'] = False\n\tstatus_dict['request_quit'] = False\n\n\tl = learner.Learner(params, param_set_id, status_dict, shared_state, shared_mem)\n\n\tactor = Actor(params, param_set_id, 0, status_dict, shared_state, shared_mem)\n\tactor.run()\n"}
{"content": "import numpy as np\nimport pandas as pd\nfrom collections import deque\nimport Coreset\n\n\nclass MergeAndReduceTree(object):\n    def __init__(self, file_path, leaf_size):\n        self.file_path = file_path\n        self.leaf_size = leaf_size\n        self.tree = deque()\n\n    def attainRoot(self):\n        for data_chunk in pd.read_csv(self.file_path, chunksize=self.leaf_size):\n            data_chunk = data_chunk.to_numpy()\n            temp = Coreset.Coreset()\n\n            self.tree.append(Coreset.Coreset())\n\n"}
{"content": "\nimport torch\nimport time\nimport matplotlib.pyplot as plt\nplt.rcParams[\"legend.loc\"] = \"upper right\"\nplt.rcParams['axes.titlesize'] = 'xx-large'\nplt.rcParams['axes.labelsize'] = 'x-large'\nplt.rcParams['legend.fontsize'] = 'xx-large'\nplt.rcParams['xtick.labelsize'] = 'x-large'\nplt.rcParams['ytick.labelsize'] = 'x-large'\n#from matplotlib import rc\n#rc('text', usetex=True)\n#plt.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath}\"]\nfrom termcolor import cprint\nimport numpy as np\nimport os\nimport shutil\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom src.utils import pload, pdump, yload, ydump, mkdir, bmv\nfrom src.utils import bmtm, bmtv, bmmt, axat\nfrom datetime import datetime\nfrom src.lie_algebra import SO3\nfrom src.iekf import RecorderIEKF as IEKF\nfrom sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n\n\nclass BaseProcessing:\n    def __init__(self, res_dir, tb_dir, net_class, net_params, address, dt):\n        self.res_dir = res_dir\n        self.tb_dir = tb_dir\n        self.net_class = net_class\n        self.net_params = net_params\n        self._ready = False\n        self.train_params = {}\n        self.figsize = (20, 12)\n        self.dt = dt # (s)\n        self.address, self.tb_address = self.find_address(address)\n        if address is None:  # create new address\n            pdump(self.net_params, self.address, 'net_params.p')\n            ydump(self.net_params, self.address, 'net_params.yaml')\n        else:  # pick the network parameters\n            self.net_params = pload(self.address, 'net_params.p')\n            self.train_params = pload(self.address, 'train_params.p')\n            self._ready = True\n        self.path_weights = os.path.join(self.address, 'weights.pt')\n        self.net = self.net_class(**self.net_params)\n        if self._ready:  # fill network parameters\n            self.load_weights()\n        self.seq = None\n\n    def find_address(self, address):\n        \"\"\"return path where net and training info are saved\"\"\"\n        if address == 'last':\n            addresses = sorted(os.listdir(self.res_dir))\n            tb_address = os.path.join(self.tb_dir, str(len(addresses)))\n            address = os.path.join(self.res_dir, addresses[-1])\n        elif address is None:\n            now = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n            address = os.path.join(self.res_dir, now)\n            mkdir(address)\n            tb_address = os.path.join(self.tb_dir, now)\n        else:\n            tb_address = None\n        return address, tb_address\n\n    def load_weights(self):\n        weights = torch.load(self.path_weights)\n        self.net.load_state_dict(weights)\n        self.net.cuda()\n\n    def train(self, dataset_class, dataset_params, train_params):\n        \"\"\"train the neural network. GPU is assumed\"\"\"\n        self.train_params = train_params\n        pdump(self.train_params, self.address, 'train_params.p')\n        ydump(self.train_params, self.address, 'train_params.yaml')\n\n        hparams = self.get_hparams(dataset_class, dataset_params, train_params)\n        ydump(hparams, self.address, 'hparams.yaml')\n\n        # define datasets\n        dataset_train = dataset_class(**dataset_params, mode='train')\n        dataset_train.init_train()\n        dataset_val = dataset_class(**dataset_params, mode='val')\n        dataset_val.init_val()\n\n        # get class\n        Optimizer = train_params['optimizer_class']\n        Scheduler = train_params['scheduler_class']\n        Loss = train_params['loss_class']\n\n        # get parameters\n        dataloader_params = train_params['dataloader']\n        optimizer_params = train_params['optimizer']\n        scheduler_params = train_params['scheduler']\n        loss_params = train_params['loss']\n\n        # define optimizer, scheduler and loss\n        dataloader = DataLoader(dataset_train, **dataloader_params)\n        optimizer = Optimizer(self.net.parameters(), **optimizer_params)\n        scheduler = Scheduler(optimizer, **scheduler_params)\n        criterion = Loss(**loss_params)\n\n        # remaining training parameters\n        freq_val = train_params['freq_val']\n        n_epochs = train_params['n_epochs']\n\n        # init net w.r.t dataset\n        self.net = self.net.cuda()\n        mean_u, std_u = dataset_train.mean_u, dataset_train.std_u\n        self.net.set_normalized_factors(mean_u, std_u)\n\n        # start tensorboard writer\n        writer = SummaryWriter(self.tb_address)\n        start_time = time.time()\n        best_loss = torch.Tensor([float('Inf')])\n\n        #\u00a0define some function for seeing evolution of training\n        def write(epoch, loss_epoch):\n            writer.add_scalar('loss/train', loss_epoch.item(), epoch)\n            writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n            print('Train Epoch: {:2d} \\tLoss: {:.4f}'.format(\n                epoch, loss_epoch.item()))\n            scheduler.step(epoch)\n\n        def write_time(epoch, start_time):\n            delta_t = time.time() - start_time\n            print(\"Amount of time spent for epochs \" +\n                \"{}-{}: {:.1f}s\\n\".format(epoch - freq_val, epoch, delta_t))\n            writer.add_scalar('time_spend', delta_t, epoch)\n\n        def write_val(loss, best_loss):\n            if loss <= best_loss:\n                msg = 'validation loss decreases! :) '\n                msg += '(curr/prev loss {:.4f}/{:.4f})'.format(loss.item(),\n                    best_loss.item())\n                cprint(msg, 'green')\n                best_loss = loss\n                self.save_net()\n            else:\n                msg = 'validation loss increases! :( '\n                msg += '(curr/prev loss {:.4f}/{:.4f})'.format(loss.item(),\n                    best_loss.item())\n                cprint(msg, 'yellow')\n            writer.add_scalar('loss/val', loss.item(), epoch)\n            return best_loss\n\n        # training loop !\n        for epoch in range(1, n_epochs + 1):\n            loss_epoch = self.loop_train(dataloader, optimizer, criterion)\n            write(epoch, loss_epoch)\n            scheduler.step(epoch)\n            if epoch % freq_val == 0:\n                loss = self.loop_val(dataset_val, criterion)\n                write_time(epoch, start_time)\n                best_loss = write_val(loss, best_loss)\n                start_time = time.time()\n        # training is over !\n\n        # test on new data\n        dataset_test = dataset_class(**dataset_params, mode='test')\n        self.load_weights()\n        test_loss = self.loop_val(dataset_test, criterion)\n        dict_loss = {\n            'final_loss/val': best_loss.item(),\n            'final_loss/test': test_loss.item()\n            }\n        writer.add_hparams(hparams, dict_loss)\n        ydump(dict_loss, self.address, 'final_loss.yaml')\n        writer.close()\n\n    def loop_train(self, dataloader, optimizer, criterion):\n        \"\"\"Forward-backward loop over training data\"\"\"\n        loss_epoch = 0\n        optimizer.zero_grad()\n        for us, xs in dataloader:\n            us = dataloader.dataset.add_noise(us.cuda())\n            hat_xs = self.net(us)\n            loss = criterion(xs.cuda(), hat_xs)/len(dataloader)\n            loss.backward()\n            loss_epoch += loss.detach().cpu()\n        optimizer.step()\n        return loss_epoch\n\n    def loop_val(self, dataset, criterion):\n        \"\"\"Forward loop over validation data\"\"\"\n        loss_epoch = 0\n        self.net.eval()\n        with torch.no_grad():\n            for i in range(len(dataset)):\n                us, xs = dataset[i]\n                hat_xs = self.net(us.cuda().unsqueeze(0))\n                loss = criterion(xs.cuda().unsqueeze(0), hat_xs)/len(dataset)\n                loss_epoch += loss.cpu()\n        self.net.train()\n        return loss_epoch\n\n    def save_net(self):\n        \"\"\"save the weights on the net in CPU\"\"\"\n        self.net.eval().cpu()\n        torch.save(self.net.state_dict(), self.path_weights)\n        self.net.train().cuda()\n\n    def get_hparams(self, dataset_class, dataset_params, train_params):\n        \"\"\"return all training hyperparameters in a dict\"\"\"\n        Optimizer = train_params['optimizer_class']\n        Scheduler = train_params['scheduler_class']\n        Loss = train_params['loss_class']\n\n        # get training class parameters\n        dataloader_params = train_params['dataloader']\n        optimizer_params = train_params['optimizer']\n        scheduler_params = train_params['scheduler']\n        loss_params = train_params['loss']\n\n        # remaining training parameters\n        freq_val = train_params['freq_val']\n        n_epochs = train_params['n_epochs']\n\n        dict_class = {\n            'Optimizer': str(Optimizer),\n            'Scheduler': str(Scheduler),\n            'Loss': str(Loss)\n        }\n\n        return {**dict_class, **dataloader_params, **optimizer_params,\n                **loss_params, **scheduler_params,\n                'n_epochs': n_epochs, 'freq_val': freq_val}\n\n    def test(self, dataset_class, dataset_params, modes):\n        \"\"\"test a network once training is over\"\"\"\n\n        # get loss function\n        Loss = self.train_params['loss_class']\n        loss_params = self.train_params['loss']\n        criterion = Loss(**loss_params)\n\n        # test on each type of sequence\n        for mode in modes:\n            dataset = dataset_class(**dataset_params, mode=mode)\n            self.loop_test(dataset, criterion)\n\n    def loop_test(self, dataset, criterion):\n        \"\"\"Forward loop over test data\"\"\"\n        self.net.eval()\n        for i in range(len(dataset)):\n            seq = dataset.sequences[i]\n            us, xs = dataset[i]\n            with torch.no_grad():\n                hat_xs = self.net(us.cuda().unsqueeze(0))\n            loss = criterion(xs.cuda().unsqueeze(0), hat_xs)\n            mkdir(self.address, seq)\n            mondict = {\n                'hat_xs': hat_xs[0].cpu(),\n                'loss': loss.cpu().item(),\n            }\n            pdump(mondict, self.address, seq, 'results.p')\n\n    def display_test(self, dataset_class, dataset_params, mode):\n        raise NotImplementedError\n    \n    def get_results(self, seq):\n        return pload(self.address, seq, 'results.p')['hat_xs']\n    \n\n\n    @property\n    def end_title(self):\n        return \" for sequence \" + self.seq.replace(\"_\", \" \")\n\n    def savefig(self, axs, fig, name):\n        if isinstance(axs, np.ndarray):\n            for i in range(len(axs)):\n                axs[i].grid()\n        else:\n            axs.grid()\n        fig.tight_layout()\n        fig.savefig(os.path.join(self.address, self.seq, name + '.png'))\n        plt.close('all')\n\n\nclass ZUPTProcessing(BaseProcessing):\n    def __init__(self, res_dir, tb_dir, net_class, net_params, address, dt):\n        super().__init__(res_dir, tb_dir, net_class, net_params, address, dt)\n\n    def display_test(self, dataset_class, dataset_params, mode):\n        dataset = dataset_class(**dataset_params, mode=mode)\n        zupts = torch.zeros(0)\n        hat_zupts = torch.zeros(0)\n\n        for i, seq in enumerate(dataset.sequences):\n            print('\\n', 'Compute result for sequence ' + seq)\n            self.seq = seq\n            # get ground truth pose\n            self.gt = dataset.load_gt(i)\n\n            # get data and estimate\n            self.us, self.zupt = dataset[i]\n            self.N = self.us.shape[0]\n            self.hat_zupt = torch.sigmoid(self.get_results(seq))\n            self.ts = torch.linspace(0, self.N*self.dt, self.N)\n\n            self.convert()\n            self.zupt_plot()\n            zupts = torch.cat((zupts, self.zupt))\n            hat_zupts = torch.cat((hat_zupts, self.hat_zupt))\n            \n        zupts = zupts.numpy()\n        hat_zupts = hat_zupts.numpy()\n        fpr, tpr, ths = roc_curve(zupts, hat_zupts)\n        precision, recall, ths2 = precision_recall_curve(zupts, hat_zupts)\n        auc = roc_auc_score(zupts, hat_zupts)\n\n        self.print_and_save_auc(auc)\n        self.roc_plot(fpr, tpr)\n        self.pr_plot(precision, recall)\n\n    \n\n    def zupt_plot(self):\n        title = \"ROC curve \" + self.end_title\n        vs = self.gt['vs'].norm(dim=1)\n        vs /= vs.max()\n        zupt = 1 - self.zupt\n        hat_zupt = 1 - self.hat_zupt\n        fig, ax = plt.subplots(figsize=self.figsize)\n        ax.set(xlabel='$t(s)$', ylabel='ZUPT',\n            title=title)\n        plt.plot(self.ts, vs, color=\"red\", label=r'true speed')\n        plt.plot(self.ts, zupt, color=\"black\", label=r'true')\n        plt.plot(self.ts, hat_zupt, color=\"blue\", label=r'net')\n        \n        self.savefig(ax, fig, self.seq + \"_zupt\")\n    \n    def convert(self):\n        # s -> min\n        l = 1/60\n        self.ts *= l\n        \n        # m/s -> km/h\n        l = 3.6\n        self.gt['vs'] *= l\n\n    def roc_plot(self, fpr, tpr):\n        title = \"ROC curve\"\n        fig, ax = plt.subplots(figsize=self.figsize)\n        ax.set(xlabel='false positive rate', ylabel='true positive rate',\n            title=title)\n        plt.plot(fpr, tpr, color=\"blue\", label=r'net')\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        self.savefig(ax, fig, 'roc')\n\n    def pr_plot(self, precision, recall):\n        title = \"precision recall curve\"\n        fig, ax = plt.subplots(figsize=self.figsize)\n        ax.set(xlabel='precision', ylabel='recall', title=title)\n        plt.plot(recall, precision, color=\"blue\", label=r'net')\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        self.savefig(ax, fig, 'precisionrecall')\n        \n    def print_and_save_auc(self, auc):\n        print('')\n        print('Area Under Curve (AUC): {:.5f}'.format(auc))\n\n        mondict = {\n            \"auc\": auc.item(),\n        }\n        ydump(mondict, self.address, \"net_result.yaml\")\n\n\n\nclass KalmanProcessing(BaseProcessing):\n    def __init__(self, res_dir, tb_dir, net_class, bbb_net_params, address, dt, iekf_params, train_params):\n        super().__init__(res_dir, tb_dir, net_class, bbb_net_params, None, dt)\n        # delete and replace address\n        #shutil.rmtree(self.address)\n        self.address = address\n        self.train_params = train_params\n        self.iekf_params = iekf_params\n\n    def loop_test(self, dataset, criterion):\n        for i in range(len(dataset)):\n            seq = dataset.sequences[i]\n            print('Testing sequence ' + seq + ' (mode is ' + dataset.mode + ')')\n            ts, us, Nshift = dataset.get_test(i)\n            if seq == 'urban16':\n                Nshift += 3000\n            kf = IEKF(**self.iekf_params)\n            zupts = torch.sigmoid(self.get_results(seq))\n            us, zupts, covs = kf.nets2iekf(self.net, us, Nshift, zupts)\n            # run filter !\n            kf.forward(ts, us, zupts, covs)\n            kf.dump(self.address, seq, zupts, covs)\n\n    def display_test(self, dataset_class, dataset_params, mode):\n        dataset = dataset_class(**dataset_params, mode=mode)\n        for i, seq in enumerate(dataset.sequences):\n            print('\\n', 'compute result for sequence ' + seq)\n            self.seq = seq\n            # get ground truth pose\n            self.gt = dataset.load_gt(i)\n            self.gt['Rots'] = SO3.from_quaternion(self.gt['qs'].cuda()).cpu()\n\n            # get data and estimate\n            self.us, self.zupt = dataset[i]\n            self.iekf = self.get_iekf_results(seq)\n            self.N = self.iekf['ps'].shape[0]\n            N0 = self.us.shape[0]-self.N\n            self.us = self.us[N0:]\n            self.zupt = self.zupt[N0:]\n            for key, val in self.gt.items():\n                self.gt[key] = val[N0:]\n            self.ts = torch.linspace(0, self.N*self.dt, self.N)\n            \n            self.align_traj()\n            self.convert()\n            self.plot_orientation()\n            self.plot_velocity()\n            self.plot_velocity_in_body_frame()\n            self.plot_position()\n            self.plot_horizontal_position()\n            self.plot_bias_gyro()\n            self.plot_bias_acc()\n            self.plot_gyro()\n            self.plot_acc()\n            self.plot_zupt()\n            self.plot_orientation_err()\n            self.plot_velocity_err()\n            self.plot_body_velocity_err()\n            self.plot_position_err()\n\n    def get_iekf_results(self, seq):\n        return pload(self.address, seq, 'iekf.p')\n    \n    def align_traj(self):\n        \"\"\"yaw only and position alignment at initial time\"\"\"\n        self.gt['rpys'] = SO3.to_rpy(self.gt['Rots'].cuda()).cpu()\n        self.iekf['rpys'] = SO3.to_rpy(self.iekf['Rots'].cuda()).cpu()\n        \n        self.gt['ps'] -= self.gt['ps'][0].clone()\n        self.iekf['ps'] -= self.iekf['ps'][0].clone()\n        rpys = self.gt['rpys'][:2] - self.iekf['rpys'][:2]\n        Rot = SO3.from_rpy(rpys[:, 0], rpys[:, 1], rpys[:, 2])\n        Rot = Rot[0].repeat(self.iekf['ps'].shape[0], 1, 1)\n        \n        self.iekf['Rots'] = Rot.bmm(self.iekf['Rots'])\n        self.iekf['vs'] = bmv(Rot, self.iekf['vs'])\n        self.iekf['ps'] = bmv(Rot, self.iekf['ps'])\n        self.iekf['rpys'] = SO3.to_rpy(self.iekf['Rots'].cuda()).cpu()\n\n    def convert(self):\n        # s -> min\n        l = 1/60\n        self.ts *= l\n        \n        # m/s -> km/h\n        l = 3.6\n        self.gt['vs'] *= l\n        self.iekf['vs'] *= l\n        self.iekf['Ps'][:, 3:6] *= l**2\n\n        # rad/s -> deg/s\n        l = 180/np.pi\n        self.iekf['b_omegas'] *= l\n        self.us[:, :3] *= l\n        self.iekf['Ps'][:, 9:12] *= l**2\n        \n        # rad -> deg\n        l = 180/np.pi\n        self.gt['rpys'] *= l\n        self.iekf['rpys'] *= l\n\n    def plot_orientation(self):\n        title = \"Orientation as function of time \" + self.end_title\n        true = self.gt['rpys']\n        mean = self.iekf['rpys']\n        std = 180/np.pi*3*self.iekf['Ps'][:, :3].sqrt()\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='roll (deg)', title=title)\n        axs[1].set(ylabel='pitch (deg)')\n        axs[2].set(xlabel='$t$ (min)', ylabel='yaw (deg)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, true[:, i], color=\"black\")\n            axs[i].plot(self.ts, mean[:, i], color=\"green\")\n            axs[i].plot(self.ts, (mean+std)[:, i], color='green', alpha=0.5)\n            axs[i].plot(self.ts, (mean-std)[:, i], color='green', alpha=0.5)\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'ground truth', r'IEKF', r'$3\\sigma$'], ncol=3)\n        self.savefig(axs, fig, 'orientation_time')\n    \n        \n    def plot_velocity(self):\n        title = \"Velocity as function of time \" + self.end_title\n        true = self.gt['vs']\n        mean = self.iekf['vs']\n        std = 3*self.iekf['Ps'][:, 3:6].sqrt()\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='$\\mathbf{v}_n^x$ (km/h)', title=title)\n        axs[1].set(ylabel='$\\mathbf{v}_n^y$ (km/h)')\n        axs[2].set(xlabel='$t$ (min)', ylabel='$\\mathbf{v}_n^z$ (km/h)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, true[:, i], color=\"black\")\n            axs[i].plot(self.ts, mean[:, i], color=\"green\")\n            axs[i].plot(self.ts, (mean+std)[:, i], color='green', alpha=0.5)\n            axs[i].plot(self.ts, (mean-std)[:, i], color='green', alpha=0.5)\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'ground truth', r'IEKF', r'$3\\sigma$'], ncol=3)\n        self.savefig(axs, fig, 'velocity')\n    \n    def plot_velocity_in_body_frame(self):\n        title = \"Body velocity as function of time \" + self.end_title\n        true = bmv(self.gt['Rots'].transpose(1, 2), self.gt['vs'])\n        mean = bmv(self.iekf['Rots'].transpose(1, 2), self.iekf['vs'])\n        # get 3 sigma uncertainty\n        P = torch.diag_embed(self.iekf['Ps'][:, :6], offset=0, dim1=-2, dim2=-1)\n        J = P.new_zeros(P.shape[0], 3, 6)\n        J[:, :, :3] = SO3.wedge(mean)\n        J[:, :, 3:6] = self.iekf['Rots'].transpose(1, 2)\n        std = J.bmm(P).bmm(J.transpose(1, 2)).diagonal(dim1=1, dim2=2).sqrt()\n\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='$(\\mathbf{R}_n^T\\mathbf{v}_n)^x$ (km/h)', \n            title=title)\n        axs[1].set(ylabel='$(\\mathbf{R}_n^T\\mathbf{v}_n)^y$ (km/h)')\n        axs[2].set(xlabel='$t$ (min)',\n            ylabel='$(\\mathbf{R}_n^T\\mathbf{v}_n)^z$ (km/h)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, true[:, i], color=\"black\")\n            axs[i].plot(self.ts, mean[:, i], color=\"green\")\n            axs[i].plot(self.ts, (mean+std)[:, i], color='green', alpha=0.5)\n            axs[i].plot(self.ts, (mean-std)[:, i], color='green', alpha=0.5)\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'ground truth', r'IEKF', r'$3\\sigma$'], ncol=3)\n        self.savefig(axs, fig, 'body_velocity')\n    \n    def plot_position(self):\n        title = \"Position as function of time \" + self.end_title\n        true = self.gt['ps']\n        mean = self.iekf['ps']\n        std = 3*self.iekf['Ps'][:, 6:9].sqrt()\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='$\\mathbf{p}_n^x$ (km)', title=title)\n        axs[1].set(ylabel='$\\mathbf{p}_n^y$ (km)')\n        axs[2].set(xlabel='$t$ (min)', ylabel='$\\mathbf{p}_n^z$ (km)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, true[:, i], color=\"black\")\n            axs[i].plot(self.ts, mean[:, i], color=\"green\")\n            axs[i].plot(self.ts, (mean+std)[:, i], color='green', alpha=0.5)\n            axs[i].plot(self.ts, (mean-std)[:, i], color='green', alpha=0.5)\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'ground truth', r'IEKF', r'$3\\sigma$'], ncol=3)\n        self.savefig(axs, fig, 'position_time')\n    \n    def plot_horizontal_position(self):\n        title = \"Horizontal position \" + self.end_title\n        true = self.gt['ps']\n        mean = self.iekf['ps']\n        fig, ax = plt.subplots(1, 1, sharex=True, figsize=self.figsize)\n        ax.set(ylabel='$\\mathbf{p}_n^x$ (km)', label='$\\mathbf{p}_n^y$ (km)', title=title)\n        ax.plot(true[:, 0], true[:, 1], color=\"black\")\n        ax.plot(mean[:, 0], mean[:, 1], color=\"green\")\n        fig.legend([r'ground truth', r'IEKF'], ncol=2)\n        self.savefig(ax, fig, 'horizontal_position')\n    \n    def plot_bias_gyro(self):\n        title = \"Gyro biases as function of time \" + self.end_title\n        mean = self.iekf['b_omegas']\n        std = 3*self.iekf['Ps'][:, 9:12].sqrt()\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='$(\\mathbf{b}_n^\\omega)^x$ (deg/s)', title=title)\n        axs[1].set(ylabel='$(\\mathbf{b}_n^\\omega)^y$ (deg/s)')\n        axs[2].set(xlabel='$t$ (min)', \n                   ylabel='$(\\mathbf{b}_n^\\omega)^z$ (deg/s)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, mean[:, i], color=\"green\")\n            axs[i].plot(self.ts, (mean+std)[:, i], color='green', alpha=0.5)\n            axs[i].plot(self.ts, (mean-std)[:, i], color='green', alpha=0.5)\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'IEKF'])\n        self.savefig(axs, fig, 'bias_gyro')\n\n    def plot_bias_acc(self):\n        title = \"Accelerometer biases as function of time \" + self.end_title\n        mean = self.iekf['b_accs']\n        std = 3*self.iekf['Ps'][:, 9:12].sqrt()\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='$(\\mathbf{b}_n^a)^x$ ($m/s^2$)', title=title)\n        axs[1].set(ylabel='$(\\mathbf{b}_n^a)^y$ ($m/s^2$)')\n        axs[2].set(xlabel='$t$ (min)', ylabel='$(\\mathbf{b}_n^a)^z$ ($m/s^2$)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, mean[:, i], color=\"green\")\n            axs[i].plot(self.ts, (mean+std)[:, i], color='green', alpha=0.5)\n            axs[i].plot(self.ts, (mean-std)[:, i], color='green', alpha=0.5)\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'IEKF'])\n        self.savefig(axs, fig, 'bias_acc')\n\n    def plot_gyro(self):\n        title = \"Gyro as function of time \" + self.end_title\n        mean = self.us[:, :3]\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        #axs[0].set(ylabel=r'$\\boldsymbol{\\omega}_n^x$ ($deg/s$)', title=title)\n        #axs[1].set(ylabel=r'$\\boldsymbol{\\omega}_n^y$ ($deg/s$)')\n        #axs[2].set(xlabel='$t$ (min)', ylabel=r'$\\boldsymbol{\\omega}_n^z$ ($deg/s$)')\n        axs[0].set(ylabel=r'omega_nx (deg/s)', title=title)\n        axs[1].set(ylabel=r'omega_ny (deg/s)')\n        axs[2].set(xlabel='t (min)', ylabel=r'omega_nz (deg/s)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, mean[:, i], color=\"blue\")\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'IMU'])\n        self.savefig(axs, fig, 'gyro')\n\n    def plot_acc(self):\n        title = \"Accelerometer as function of time \" + self.end_title\n        mean = self.us[:, 3:6]\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel=r'$\\mathbf{a}_n^x$ ($m/s^2$)', title=title)\n        axs[1].set(ylabel=r'$\\mathbf{a}_n^y$ ($m/s^2$)')\n        axs[2].set(xlabel='$t$ (min)', ylabel=r'$\\mathbf{a}_n^z$ ($m/s^2$)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, mean[:, i], color=\"blue\", label=r'IMU')\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        self.savefig(axs, fig, 'acc')\n\n    def plot_zupt(self):\n        pass\n\n    def plot_covs(self):\n        title = \"Standard deviation measurement as function of time \" + self.end_title\n        std = self.iekf['covs'].sqrt().log()\n        fig, axs = plt.subplots(5, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='log scale', title=title)\n        axs[1].set(ylabel='log scale')\n        axs[2].set(xlabel='$t$ (min)', ylabel='log scale')\n        \n        for i in range(5):\n            axs[i].plot(self.ts, std[:, i])\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        self.savefig(axs, fig, 'position_error')\n\n    def plot_orientation_err(self):\n        title = \"Position error as function of time \" + self.end_title\n        err = SO3.log(bmtm(self.gt['Rots'].cuda(), \n                        self.iekf['Rots'].cuda())).cpu()\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='roll (deg)', title=title)\n        axs[1].set(ylabel='pitch (deg)')\n        axs[2].set(xlabel='$t$ (min)', ylabel='yaw (deg)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, err[:, i], color=\"blue\")\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        self.savefig(axs, fig, 'orientation_error')\n\n    def plot_velocity_err(self):\n        title = \"Velocity error as function of time \" + self.end_title\n        err = self.gt['vs'] - self.iekf['vs']\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='$\\mathbf{v}_n^x$ (km/h)', title=title)\n        axs[1].set(ylabel='$\\mathbf{v}_n^y$ (km/h)')\n        axs[2].set(xlabel='$t$ (min)', ylabel='$\\mathbf{v}_n^z$ (km/h)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, err[:, i], color=\"blue\")\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'IEKF'])\n        self.savefig(axs, fig, 'velocity_error')\n\n\n    def plot_body_velocity_err(self):\n        title = \"Body velocity error as function of time \" + self.end_title\n        vs = bmv(self.gt['Rots'], self.gt['vs'])\n        hat_vs = bmv(self.iekf['Rots'], self.iekf['vs'])\n        err = vs - hat_vs\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='$(\\mathbf{R}_n^T\\mathbf{v}_n)^x$ (km/h)', title=title)\n        axs[1].set(ylabel='$\\mathbf{v}_n^y$ (km/h)')\n        axs[2].set(xlabel='$t$ (min)', ylabel='$\\mathbf{v}_n^z$ (km/h)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, err[:, i], color=\"blue\")\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'IEKF'])\n        self.savefig(axs, fig, 'body_velocity_error')\n\n    def plot_position_err(self):\n        title = \"Position error as function of time \" + self.end_title\n        err = self.gt['ps'] - self.iekf['ps']\n        fig, axs = plt.subplots(3, 1, sharex=True, figsize=self.figsize)\n        axs[0].set(ylabel='$\\mathbf{p}_n^x$ (m)', title=title)\n        axs[1].set(ylabel='$\\mathbf{p}_n^y$ (m)')\n        axs[2].set(xlabel='$t$ (min)', ylabel='$\\mathbf{p}_n^z$ (m)')\n        \n        for i in range(3):\n            axs[i].plot(self.ts, err[:, i], color=\"blue\")\n            axs[i].set_xlim(self.ts[0], self.ts[-1])\n        fig.legend([r'IEKF'])\n        self.savefig(axs, fig, 'position_error')\n"}
{"content": "import unittest\nfrom igraph import *\nfrom igraph.test.foreign import temporary_file\n\nclass CliqueTests(unittest.TestCase):\n    def setUp(self):\n        self.g=Graph.Full(6)\n        self.g.delete_edges([(0, 1), (0, 2), (3, 5)])\n\n    def testCliques(self):\n        tests = {(4, -1): [[1, 2, 3, 4], [1, 2, 4, 5]],\n                 (2, 2): [[0, 3], [0, 4], [0, 5],\n                          [1, 2], [1, 3], [1, 4], [1, 5],\n                          [2, 3], [2, 4], [2, 5], [3, 4], [4, 5]],\n                 (-1, -1): [[0], [1], [2], [3], [4], [5],\n                            [0, 3], [0, 4], [0, 5],\n                            [1, 2], [1, 3], [1, 4], [1, 5],\n                            [2, 3], [2, 4], [2, 5], [3, 4], [4, 5],\n                            [0, 3, 4], [0, 4, 5],\n                            [1, 2, 3], [1, 2, 4], [1, 2, 5],\n                            [1, 3, 4], [1, 4, 5], [2, 3, 4], [2, 4, 5],\n                            [1, 2, 3, 4], [1, 2, 4, 5]]}\n        for (lo, hi), exp in tests.items():\n            self.assertEqual(sorted(exp), sorted(map(sorted, self.g.cliques(lo, hi))))\n\n    def testLargestCliques(self):\n        self.assertEqual(sorted(map(sorted, self.g.largest_cliques())),\n                         [[1, 2, 3, 4], [1, 2, 4, 5]])\n\n    def testMaximalCliques(self):\n        self.assertEqual(sorted(map(sorted, self.g.maximal_cliques())),\n                         [[0, 3, 4], [0, 4, 5],\n                          [1, 2, 3, 4], [1, 2, 4, 5]])\n        self.assertEqual(sorted(map(sorted, self.g.maximal_cliques(min=4))),\n                         [[1, 2, 3, 4], [1, 2, 4, 5]])\n        self.assertEqual(sorted(map(sorted, self.g.maximal_cliques(max=3))),\n                         [[0, 3, 4], [0, 4, 5]])\n\n    def testMaximalCliquesFile(self):\n        def read_cliques(fname):\n            with open(fname) as fp:\n                return sorted(sorted(int(item) for item in line.split())\n                                for line in fp)\n\n        with temporary_file() as fname:\n            self.g.maximal_cliques(file=fname)\n            self.assertEqual([[0, 3, 4], [0, 4, 5], [1, 2, 3, 4], [1, 2, 4, 5]],\n                             read_cliques(fname))\n\n        with temporary_file() as fname:\n            self.g.maximal_cliques(min=4, file=fname)\n            self.assertEqual([[1, 2, 3, 4], [1, 2, 4, 5]], read_cliques(fname))\n\n        with temporary_file() as fname:\n            self.g.maximal_cliques(max=3, file=fname)\n            self.assertEqual([[0, 3, 4], [0, 4, 5]], read_cliques(fname))\n\n    def testCliqueNumber(self):\n        self.assertEqual(self.g.clique_number(), 4)\n        self.assertEqual(self.g.omega(), 4)\n\nclass IndependentVertexSetTests(unittest.TestCase):\n    def setUp(self):\n        self.g1=Graph.Tree(5, 2, TREE_UNDIRECTED)\n        self.g2=Graph.Tree(10, 2, TREE_UNDIRECTED)\n\n    def testIndependentVertexSets(self):\n        tests = {(4, -1): [],\n                 (2, 2): [(0, 3), (0, 4), (1, 2), (2, 3), (2, 4), (3, 4)],\n                 (-1, -1): [(0,), (1,), (2,), (3,), (4,),\n                            (0, 3), (0, 4), (1, 2), (2, 3), (2, 4),\n                            (3, 4), (0, 3, 4), (2, 3, 4)]}\n        for (lo, hi), exp in tests.items():\n            self.assertEqual(exp, self.g1.independent_vertex_sets(lo, hi))\n\n    def testLargestIndependentVertexSets(self):\n        self.assertEqual(self.g1.largest_independent_vertex_sets(),\n                         [(0, 3, 4), (2, 3, 4)])\n\n    def testMaximalIndependentVertexSets(self):\n        self.assertEqual(self.g2.maximal_independent_vertex_sets(),\n                         [(0, 3, 4, 5, 6), (0, 3, 5, 6, 9),\n                          (0, 4, 5, 6, 7, 8), (0, 5, 6, 7, 8, 9),\n                          (1, 2, 7, 8, 9), (1, 5, 6, 7, 8, 9),\n                          (2, 3, 4), (2, 3, 9), (2, 4, 7, 8)])\n\n    def testIndependenceNumber(self):\n        self.assertEqual(self.g2.independence_number(), 6)\n        self.assertEqual(self.g2.alpha(), 6)\n\n\nclass MotifTests(unittest.TestCase):\n    def setUp(self):\n        self.g = Graph.Erdos_Renyi(100, 0.2, directed=True)\n\n    def testDyads(self):\n        \"\"\"\n        @note: this test is not exhaustive, it only checks whether the\n          L{DyadCensus} objects \"understand\" attribute and item accessors\n        \"\"\"\n        dc = self.g.dyad_census()\n        accessors = [\"mut\", \"mutual\", \"asym\", \"asymm\", \"asymmetric\", \"null\"]\n        for a in accessors:\n            self.assertTrue(isinstance(getattr(dc, a), int))\n            self.assertTrue(isinstance(dc[a], int))\n        self.assertTrue(isinstance(list(dc), list))\n        self.assertTrue(isinstance(tuple(dc), tuple))\n        self.assertTrue(len(list(dc)) == 3)\n        self.assertTrue(len(tuple(dc)) == 3)\n\n    def testTriads(self):\n        \"\"\"\n        @note: this test is not exhaustive, it only checks whether the\n          L{TriadCensus} objects \"understand\" attribute and item accessors\n        \"\"\"\n        tc = self.g.triad_census()\n        accessors = [\"003\", \"012\", \"021d\", \"030C\"]\n        for a in accessors:\n            self.assertTrue(isinstance(getattr(tc, \"t\"+a), int))\n            self.assertTrue(isinstance(tc[a], int))\n        self.assertTrue(isinstance(list(tc), list))\n        self.assertTrue(isinstance(tuple(tc), tuple))\n        self.assertTrue(len(list(tc)) == 16)\n        self.assertTrue(len(tuple(tc)) == 16)\n\nclass CliqueBenchmark(object):\n    \"\"\"This is a benchmark, not a real test case. You can run it\n    using:\n\n    >>> from igraph.test.cliques import CliqueBenchmark\n    >>> CliqueBenchmark().run()\n    \"\"\"\n\n    def __init__(self):\n        from time import time\n        import gc\n        self.time = time\n        self.gc_collect = gc.collect\n\n    def run(self):\n        self.printIntro()\n        self.testRandom()\n        self.testMoonMoser()\n        self.testGRG()\n\n    def printIntro(self):\n        print(\"n = number of vertices\")\n        print(\"#cliques = number of maximal cliques found\")\n        print(\"t1 = time required to determine the clique number\")\n        print(\"t2 = time required to determine and save all maximal cliques\")\n        print()\n\n    def timeit(self, g):\n        start = self.time()\n        omega = g.clique_number()\n        mid = self.time()\n        cl = g.maximal_cliques()\n        end = self.time()\n        self.gc_collect()\n        return len(cl), mid-start, end-mid\n\n    def testRandom(self):\n        np = {100: [0.6, 0.7],\n              300: [0.1, 0.2, 0.3, 0.4],\n              500: [0.1, 0.2, 0.3],\n              700: [0.1, 0.2],\n              1000:[0.1, 0.2],\n              10000: [0.001, 0.003, 0.005, 0.01, 0.02]}\n\n        print()\n        print(\"Erdos-Renyi random graphs\")\n        print(\"       n        p #cliques        t1        t2\")\n        for n in sorted(np.keys()):\n            for p in np[n]:\n                g = Graph.Erdos_Renyi(n, p)\n                result = self.timeit(g)\n                print(\"%8d %8.3f %8d %8.4fs %8.4fs\" % \\\n                    tuple([n, p] + list(result)))\n\n    def testMoonMoser(self):\n        ns = [15, 27, 33]\n\n        print()\n        print(\"Moon-Moser graphs\")\n        print(\"       n exp_clqs #cliques        t1        t2\")\n        for n in ns:\n            n3 = n/3\n            types = list(range(n3)) * 3\n            el = [(i, j) for i in range(n) for j in range(i+1,n) if types[i] != types[j]]\n            g = Graph(n, el)\n            result = self.timeit(g)\n            print(\"%8d %8d %8d %8.4fs %8.4fs\" % \\\n                tuple([n, (3**(n/3))] + list(result)))\n\n    def testGRG(self):\n        ns = [100, 1000, 5000, 10000, 25000, 50000]\n\n        print()\n        print(\"Geometric random graphs\")\n        print(\"       n        d #cliques        t1        t2\")\n        for n in ns:\n            d = 2. / (n ** 0.5)\n            g = Graph.GRG(n, d)\n            result = self.timeit(g)\n            print(\"%8d %8.3f %8d %8.4fs %8.4fs\" % \\\n                tuple([n, d] + list(result)))\n\n\ndef suite():\n    clique_suite = unittest.makeSuite(CliqueTests)\n    indvset_suite = unittest.makeSuite(IndependentVertexSetTests)\n    motif_suite = unittest.makeSuite(MotifTests)\n    return unittest.TestSuite([clique_suite, indvset_suite, motif_suite])\n\ndef test():\n    runner = unittest.TextTestRunner()\n    runner.run(suite())\n\nif __name__ == \"__main__\":\n    test()\n\n"}
{"content": "#!/usr/bin/python\n\nimport numpy.f2py\nr=numpy.f2py.f2py2e.main()\n"}
{"content": "import time\nfrom math import sqrt, tan, sin, cos, pi, ceil, floor, acos, atan, asin, degrees, radians, log, atan2, acos, asin\nfrom random import *\nimport numpy\nfrom pymclevel import alphaMaterials, MCSchematic, MCLevel, BoundingBox\nfrom mcplatform import *\n\nimport utilityFunctions\nfrom helper import *\n\n\nclass ChunkAnalysis:\n\tMOST_BUMPY_STDDEV = 10.\n\n\tdef __init__(self, level, chunk, slices):\n\t\tself.box = chunk.bounds\n\t\tself.histogram = self.calculateHistogram(level, chunk, slices)\n\t\tself.resourceTypeHistogram = self.calculateResourceTypeHistogram() # A dict of floats from 0 to 1 denoting the fraction of all\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   # blocks in the chunk that are of a particular resource type\n\t\tself.flatness = self.calculateFlatness(level) # A float from 0 to 1, 1 being the most flat and 0 being the most bumpy\n\t\t# self.numBlocks = 0 # A total count of all non-air blocks within the chunk\n\t\t# self.groundPercentage = 0 # A float from 0 to 1 denoting the fraction of the chunk's ground that is not liquid water\n\t\t# self.resourceProximity = {}\n\t\t# self.fitness = 0\n\n\tdef calculateHistogram(self, level, chunk, slices):\n\t\ttypes = numpy.zeros(65536, dtype='uint32')\n\t\tblocks = numpy.array(chunk.Blocks[slices], dtype='uint16')\n\t\tb = numpy.bincount(blocks.ravel())\n\t\ttypes[:b.shape[0]] = types[:b.shape[0]].astype(int) + b\n\t\tpresentTypes = types.nonzero()\n\t\thistogram = { level.materials[t & 0xfff, t >> 12].ID : types[t] for t in presentTypes[0] }\n\t\tself.numBlocks = 65536. - histogram[0]\n\t\tdel histogram[0] # Removing air from the histogram\n\t\treturn histogram\n\n\tdef calculateResourceTypeHistogram(self):\n\t\thistogram = {\"B\": 0, \"W\": 0, \"V\": 0, \"N\": 0}\n\t\tfor id in self.histogram:\n\t\t\tif id in NATUAL_BLOCKS:\n\t\t\t\thistogram[NATUAL_BLOCKS[id]['Type']] += self.histogram[id]\n\t\tfor key in histogram:\n\t\t\thistogram[key] /= self.numBlocks\n\t\treturn histogram\n\n\tdef calculateFlatness(self, level):\n\t\tyPositions = []\n\t\tfor x in xrange(self.box.minx, self.box.maxx):\n\t\t\tfor z in xrange(self.box.minz, self.box.maxz):\n\t\t\t\ty = getGroundYPos(x, z)\n\t\t\t\t# Skip areas where the surface is liquid water\n\t\t\t\tif not isLiquidWater(level.blockAt(x, y, z)):\n\t\t\t\t\tyPositions.append(y)\n\t\t#self.numGroundBlocks = len(yPositions)\n\t\tself.groundPercentage = len(yPositions) / 256.\n\t\tif len(yPositions) < 1:\n\t\t\theightStdDev = ChunkAnalysis.MOST_BUMPY_STDDEV\n\t\telse:\n\t\t\theightStdDev = numpy.std(yPositions)\n\t\t# Determining a flatness score from 0 to 1 based on the standard deviation in the height\n\t\treturn max(1 - (heightStdDev / ChunkAnalysis.MOST_BUMPY_STDDEV), 0)\n\n\tdef calculateResourceProximity(self, allChunks, maxScores):\n\t\tself.resourceProximity = {\"B\": 0, \"W\": 0, \"V\": 0}\n\t\tfor chunk in allChunks:\n\t\t\tsqrDist = self.sqrDistance(chunk)\n\t\t\tsqrDist += 1 # To prevent divide by 0 errors\n\t\t\tself.resourceProximity[\"B\"] += chunk.resourceTypeHistogram[\"B\"] / sqrDist\n\t\t\tself.resourceProximity[\"W\"] += chunk.resourceTypeHistogram[\"W\"] / sqrDist\n\t\t\tself.resourceProximity[\"V\"] += chunk.resourceTypeHistogram[\"V\"] / sqrDist\n\t\tif self.resourceProximity[\"B\"] > maxScores[\"B\"]:\n\t\t\tmaxScores[\"B\"] = self.resourceProximity[\"B\"]\n\t\tif self.resourceProximity[\"W\"] > maxScores[\"W\"]:\n\t\t\tmaxScores[\"W\"] = self.resourceProximity[\"W\"]\n\t\tif self.resourceProximity[\"V\"] > maxScores[\"V\"]:\n\t\t\tmaxScores[\"V\"] = self.resourceProximity[\"V\"]\n\n\t# Returns the squared distance between the 2 chunks (the distance is in terms of chunks; a return value of 1 = 16 blocks)\n\tdef sqrDistance(self, other):\n\t\t# Delta x and z between chunk origins is divided by 16 so 1 chunk over corresponds to a distance of 1\n\t\treturn (((self.box.minx - other.box.minx)/16) ** 2.) + (((self.box.minz - other.box.minz)/16) ** 2.)\n\n\t# Returns the distance between the 2 chunks (the distance is in terms of chunks; a return value of 1 = 16 blocks)\n\tdef distance(self, other):\n\t\treturn sqrt(self.sqrDistance(other))\n\n\tdef calculateCommercialFitness(self, maxScores):\n\t\t# Identifying fitness parameters\n\t\tidealFlatness = 1\n\t\tidealWater = 1\n\t\tidealGround = 0.8\n\t\tminGround = 0.15\n\t\t# Calculating each parameter's score\n\t\tif self.groundPercentage < minGround:\n\t\t\tself.fitness = -999999\n\t\t\treturn\n\t\tflatnessFitness = abs(self.flatness - idealFlatness) * -1\n\t\twaterFitness = abs((self.resourceProximity[\"W\"] / maxScores[\"W\"]) - idealWater) * -1\n\t\tgroundFitness = abs(self.groundPercentage - idealGround) * -1\n\t\t# Summing the fitness parameters\n\t\tself.fitness = flatnessFitness + waterFitness + groundFitness\n\n\tdef calculateIndustrialFitness(self, maxScores, commercialCenter):\n\t\t# Identifying fitness parameters\n\t\tidealValuable = 1\n\t\tidealBuildingMaterial = 1\n\t\tidealDistFromCommercial = 5\n\t\tminGround = 0.15\n\t\t# Calculating each parameter's score\n\t\tif self.groundPercentage < minGround or self in commercialCenter:\n\t\t\tself.fitness = -999999\n\t\t\treturn\n\t\tvaluableFitness = abs((self.resourceProximity[\"V\"] / maxScores[\"V\"]) - idealValuable) * -1.25\n\t\tbuildingMaterialFitness = abs((self.resourceProximity[\"B\"] / maxScores[\"B\"]) - idealBuildingMaterial) * -1\n\t\tavgDist1 = 0\n\t\tfor i in commercialCenter:\n\t\t\tavgDist1 += self.distance(i)\n\t\tif len(commercialCenter) > 0:\n\t\t\tavgDist1 /= float(len(commercialCenter))\n\t\tdistFitness = abs(avgDist1 - idealDistFromCommercial) * -0.0625#-0.25\n\t\t# Summing the fitness parameters\n\t\tself.fitness = valuableFitness + buildingMaterialFitness + distFitness\n\n\tdef calculateAgriculturalFitness(self, maxScores, commercialCenter, industrialCenter):\n\t\t# Identifying fitness parameters\n\t\tidealFlatness = 1\n\t\tidealWater = 0.25\n\t\tidealGround = 1\n\t\tidealDistFromCommercial = 3.5\n\t\tminGround = 0.5\n\t\t# Calculating each parameter's score\n\t\tif self.groundPercentage < minGround or self in commercialCenter or self in industrialCenter:\n\t\t\tself.fitness = -999999\n\t\t\treturn\n\t\tflatnessFitness = abs(self.flatness - idealFlatness) * -1.5\n\t\twaterFitness = abs((self.resourceProximity[\"W\"] / maxScores[\"W\"]) - idealWater) * -1\n\t\tgroundFitness = abs(self.groundPercentage - idealGround) * -1.25\n\t\tavgDist1 = 0\n\t\tfor i in commercialCenter:\n\t\t\tavgDist1 += self.distance(i)\n\t\tif len(commercialCenter) > 0:\n\t\t\tavgDist1 /= float(len(commercialCenter))\n\t\tdistFitness = abs(avgDist1 - idealDistFromCommercial) * -0.0625#-0.25\n\t\t# Summing the fitness parameters\n\t\tself.fitness = flatnessFitness + waterFitness + groundFitness + distFitness\n\n\tdef calculateHighClassResidentialFitness(self, maxScores, commercialCenter, industrialCenter, agriculturalCenter):\n\t\t# Identifying fitness parameters\n\t\tidealFlatness = 0.9\n\t\tidealWater = 0.25\n\t\tidealGround = 1\n\t\tidealDistFromCommercial = 0\n\t\tidealDistFromIndustrial = 6\n\t\tidealDistFromAgricultural = 2.5\n\t\tminGround = 0.5\n\t\t# Calculating each parameter's score\n\t\tif self.groundPercentage < minGround or self in [commercialCenter, industrialCenter, agriculturalCenter]:\n\t\t\tself.fitness = -999999\n\t\t\treturn\n\t\tflatnessFitness = abs(self.flatness - idealFlatness) * -1\n\t\twaterFitness = abs((self.resourceProximity[\"W\"] / maxScores[\"W\"]) - idealWater) * -1\n\t\tgroundFitness = abs(self.groundPercentage - idealGround) * -0.5\n\t\tavgDist1 = 0\n\t\tfor i in commercialCenter:\n\t\t\tavgDist1 += self.distance(i)\n\t\tif len(commercialCenter) > 0:\n\t\t\tavgDist1 /= float(len(commercialCenter))\n\t\tavgDist2 = 0\n\t\tfor i in industrialCenter:\n\t\t\tavgDist2 += self.distance(i)\n\t\tif len(industrialCenter) > 0:\n\t\t\tavgDist2 /= float(len(industrialCenter))\n\t\tavgDist3 = 0\n\t\tfor i in agriculturalCenter:\n\t\t\tavgDist3 += self.distance(i)\n\t\tif len(agriculturalCenter) > 0:\n\t\t\tavgDist3 /= float(len(agriculturalCenter))\n\t\tdistFitness1 = abs(avgDist1 - idealDistFromCommercial) * -0.1875#-0.75\n\t\tdistFitness2 = abs(avgDist2 - idealDistFromIndustrial) * -0.125#-0.5\n\t\tdistFitness3 = abs(avgDist3 - idealDistFromAgricultural) * -0.0625#-0.25\n\t\t# Summing the fitness parameters\n\t\tself.fitness = flatnessFitness + waterFitness + groundFitness + distFitness1 + distFitness2 + distFitness3\n\n\tdef calculateLowClassResidentialFitness(self, maxScores, commercialCenter, industrialCenter, agriculturalCenter, highClassResidentialCenter):\n\t\t# Identifying fitness parameters\n\t\tidealFlatness = 1\n\t\tidealGround = 1\n\t\tidealDistFromCommercial = 4\n\t\tidealDistFromIndustrial = 0\n\t\tminGround = 0.5\n\t\t# Calculating each parameter's score\n\t\tif self.groundPercentage < minGround or self in [commercialCenter, industrialCenter, agriculturalCenter, highClassResidentialCenter]:\n\t\t\tself.fitness = -999999\n\t\t\treturn\n\t\tflatnessFitness = abs(self.flatness - idealFlatness) * -1\n\t\tgroundFitness = abs(self.groundPercentage - idealGround) * -1\n\t\tavgDist1 = 0\n\t\tfor i in commercialCenter:\n\t\t\tavgDist1 += self.distance(i)\n\t\tif len(commercialCenter) > 0:\n\t\t\tavgDist1 /= float(len(commercialCenter))\n\t\tavgDist2 = 0\n\t\tfor i in industrialCenter:\n\t\t\tavgDist2 += self.distance(i)\n\t\tif len(industrialCenter) > 0:\n\t\t\tavgDist2 /= float(len(industrialCenter))\n\t\tdistFitness1 = abs(avgDist1 - idealDistFromCommercial) * -0.125#-0.5\n\t\tdistFitness2 = abs(avgDist2 - idealDistFromIndustrial) * -0.1875#-0.75\n\t\t# Summing the fitness parameters\n\t\tself.fitness = flatnessFitness + groundFitness + distFitness1 + distFitness2\n\n\n\tdef analyzeGround(self, level, distBelowGround, distAboveGround):\n\t\tbHist = {}\n\t\tvHist = {}\n\t\tfor x in xrange(self.box.minx, self.box.maxx):\n\t\t\tfor z in xrange(self.box.minz, self.box.maxz):\n\t\t\t\tgroundY = getGroundYPos(x, z)\n\t\t\t\tfor y in xrange(groundY - distBelowGround, groundY + distAboveGround + 1):\n\t\t\t\t\tid = level.blockAt(x, y, z)\n\t\t\t\t\tif id in NATUAL_BLOCKS:\n\t\t\t\t\t\tif NATUAL_BLOCKS[id][\"Type\"] == \"B\":\n\t\t\t\t\t\t\tif id in bHist:\n\t\t\t\t\t\t\t\tbHist[id] += 1\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tbHist[id] = 1\n\t\t\t\t\t\telif NATUAL_BLOCKS[id][\"Type\"] == \"V\":\n\t\t\t\t\t\t\tif id in vHist:\n\t\t\t\t\t\t\t\tvHist[id] += 1\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tvHist[id] = 1\n\t\tsortedBHist = [(bHist[key], key) for key in bHist]\n\t\tsortedBHist.sort()\n\t\tsortedBHist.reverse()\n\t\tsortedBHist = [(i[1], i[0]) for i in sortedBHist]\n\t\tsortedVHist = [(vHist[key], key) for key in vHist]\n\t\tsortedVHist.sort()\n\t\tsortedVHist.reverse()\n\t\tsortedVHist = [(i[1], i[0]) for i in sortedVHist]\n\t\treturn sortedBHist, sortedVHist\n\n\n\t# Necessary for sorting\n\tdef __lt__(self, other):\n\t\treturn self.fitness < other.fitness\n\n\n\n"}
{"content": "#!/usr/bin/env python3\n\n\"\"\"\n    This file contains many helper methods that are being re-used in multiple classes and do not specifically belong to\n    a class.\n\"\"\"\n\n__author__ = \"<NAME> (<EMAIL>)\"\n__copyright__ = \"BSD 3-Clause License\"\n\nimport ast\nimport collections\nimport functools\nimport json\nimport operator\nimport os.path\nimport re\nimport warnings\n\nfrom functools import reduce\nfrom typing import List, Dict\n\nimport dace\nimport numpy as np\n\n\nITERATORS = [\"i\", \"j\", \"k\"]\n\n\ndef deprecated(func):\n    \"\"\"\n    This is a decorator which can be used to mark functions as deprecated. It will result in a warning being emitted\n    when the function is used.\n    \"\"\"\n    @functools.wraps(func)\n    def new_func(*args, **kwargs):\n        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n                      category=DeprecationWarning,\n                      stacklevel=2)\n        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n        return func(*args, **kwargs)\n\n    return new_func\n\n\ndef str_to_dtype(dtype_str: str) -> dace.dtypes.typeclass:\n    \"\"\"\n    Conversion from the data type name (string) to its type defined in dace.\n    :param dtype_str: string data type\n    :return: dace data type\n    \"\"\"\n    if not isinstance(dtype_str, str):  # type check\n        raise TypeError(\"Expected string, got: \" + type(dtype_str).__name__)\n    try:\n        return getattr(dace.dtypes, dtype_str)  # match type\n    except AttributeError:\n        pass\n    raise AttributeError(\"Unsupported data type: \" + dtype_str)  # missmatch\n\n\ndef parse_json(config_path: str) -> Dict:\n    \"\"\"\n    Read input file from disk and parse it.\n    :param config_path: path to the file\n    :return: parsed file\n    \"\"\"\n    # check file exists\n    if not os.path.isfile(config_path):\n        relative = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                config_path)\n        if not os.path.isfile(relative):\n            raise RuntimeError(\"file {} does not exists.\".format(config_path))\n        config_path = relative\n    # open file in with-clause, to ensure proper file closing even in the event of an exception\n    with open(config_path, \"r\") as file_handle:\n        # try to parse it\n        config = json.loads(file_handle.read())  # type: dict\n    # Save the path to the config\n    config[\"path\"] = os.path.dirname(os.path.abspath(config_path))\n\n    def walk(d):  # replace string data type in config\n        for key, val in d.items():\n            if isinstance(val, dict):\n                walk(val)\n            else:\n                if key == \"data_type\":\n                    d[key] = str_to_dtype(val)\n\n    walk(config)\n    # return dict\n    return config\n\n\ndef max_dict_entry_key(dict1: Dict[str, List[int]]) -> str:\n    \"\"\"\n    Get key of largest value entry out of the input dictionary.\n    :param dict1: a dictionary with keys as names and values as buffer sizes\n    :return: key of buffer entry with maximum size\n    \"\"\"\n    # check type\n    if not isinstance(dict1, dict):\n        raise Exception(\"dict1 should be of type {}, but is of type {}\".format(\n            type(dict), type(dict1)))\n    # extract max value entry\n    return max(dict1, key=dict1.get)\n\n\ndef list_add_cwise(list1: List, list2: List) -> List:\n    \"\"\"\n    Merge two lists by component-wise addition.\n    :param list1: input list: summand\n    :param list2: input list: summand\n    :return: merged list\n    \"\"\"\n    # check type\n    if not isinstance(list1, list):\n        raise Exception(\"list1 should be of type {}, but is of type {}\".format(\n            type(list), type(list1)))\n    if not isinstance(list2, list):\n        raise Exception(\"list2 should be of type {}, but is of type {}\".format(\n            type(list), type(list2)))\n    # do map lambda operation over both lists\n    return list(map(lambda x, y: x + y, list1, list2))\n\n\ndef list_subtract_cwise(list1: List, list2: List) -> List:\n    \"\"\"\n    Merge two lists by component-wise subtraction.\n    :param list1: input list: minuend\n    :param list2: input list: subtrahend\n    :return: merged list\n    \"\"\"\n    # check type\n    if not isinstance(list1, list):\n        raise Exception(\"list1 should be of type {}, but is of type {}\".format(\n            type(list), type(list1)))\n    if not isinstance(list2, list):\n        raise Exception(\"list2 should be of type {}, but is of type {}\".format(\n            type(list), type(list2)))\n    # do map lambda operation over both lists\n    return list(\n        map(lambda x, y: x - y\n            if x is not None and y is not None else None, list1, list2))\n\n\ndef dim_to_abs_val(input: List[int], dimensions: List[int]) -> int:\n    \"\"\"\n    Computes scalar number out of independent dimension unit.\n    :param input: vector to evaluate\n    :param dimensions: vector of global array dimensions\n    :return: scalar value\n    \"\"\"\n    # dim [X, Y, Z], size [a, b, c] -> 1*c + X*(b + Y*a) = [a, b, c] * transpose([Z*Y, Z, 1])\n    vec = [\n        reduce(operator.mul, dimensions[i + 1:], 1)\n        for i in range(len(dimensions))\n    ]\n    return reduce(operator.add, map(operator.mul, input, vec))  # inner product\n\n\ndef load_array(input_config: Dict, prefix=None, shape=None):\n    \"\"\"\n    Load array from file or list into numpy array.\n    :param input_config: External data input file config.\n    :param prefix: Additional path to search for input file.\n    :return: Data stored in a numpy array.\n    \"\"\"\n    # get path to either source file or direct to the embedded array\n    data = input_config[\"data\"]\n    dtype = input_config[\"data_type\"].type\n    if isinstance(data, str):  # source file or autogenerated\n        m = re.match(r\"([^:]+):(.+)\", data)\n        if m:\n            is_scalar = (\"input_dims\" in input_config\n                         and input_config[\"input_dims\"] is not None\n                         and len(input_config[\"input_dims\"]) <= 0)\n            if shape is None and not is_scalar:\n                raise ValueError(\n                    \"Must provide shape when using generated inputs\")\n            if m.group(1) == \"constant\":\n                val = float(m.group(2))\n                if is_scalar:\n                    return val\n                else:\n                    arr = np.empty(shape, dtype=dtype)\n                    arr[:] = val\n                    return arr\n            elif m.group(1) == \"random\":\n                m1 = re.match(r\"([0-9\\.]+).+([0-9\\.]+)\", m.group(2))\n                rand_min = float(m1.group(1))\n                rand_max = float(m2.group(1))\n                if is_scalar:\n                    return (rand_min + (rand_max - rand_min) * np.rand(1))[0]\n                else:\n                    return rand_min + (rand_max - rand_min) * np.rand(*shape)\n            else:\n                raise ValueError(\"Unknown generation: \" + m.group(1))\n        path = data\n        if not os.path.isfile(path):\n            if prefix is not None:\n                path = os.path.join(prefix, path)\n            if not os.path.isfile(path):\n                raise FileNotFoundError(\"File {} does not exists.\".format(data))\n        if path.endswith(\".csv\"):\n            return np.genfromtxt(path, dtype, delimiter=',')\n        elif path.endswith(\".dat\"):\n            return np.fromfile(path, dtype)\n        else:\n            raise ValueError(\"Invalid file type: \" + path)\n    elif shape is not None and len(shape) == 0:\n        return dtype(data)\n    elif isinstance(data, np.ndarray):  # embedded array: already numpy array\n        return data\n    else:\n        # embedded array: collection item -> convert to np array\n        return np.array(data, dtype=input_config[\"data_type\"].type)\n\n\ndef load_input_arrays(input_configs: Dict, prefix=None, shape=None) -> Dict:\n    \"\"\"\n    Loads input arrays for the passed program into memory.\n    :param input_configs: Input configuration from program tree as generated\n                          by parse_json.\n    :return: Dictionary of input names to input data as numpy arrays.\n    \"\"\"\n    # add all input arrays to the dict\n    input_arrays = dict()\n    for arr_name, source in input_configs.items():\n        arr = load_array(source, prefix, shape)\n        try:\n            if len(arr.shape) > 0:  # Don't call for scalars\n                arr = aligned(arr, 64)\n        except AttributeError:\n            pass  # Don't call for scalars\n        input_arrays[arr_name] = arr\n    return input_arrays\n\n\ndef save_array(array, path):\n    \"\"\"\n    Saves array to a binary file.\n    :param array: numpy array to save.\n    :param path: Path to binary file where data will be saved.\n    \"\"\"\n    array.tofile(path)\n\n\ndef save_output_arrays(outputs: Dict, output_dir=str()):\n    \"\"\"\n    Saves output arrays to individual files.\n    :param outputs: Dictionary of array names to numpy arrays.\n    :param output_dir: Folder to store files in.\n    \"\"\"\n    # store all arrays in the output directory path\n    for arr_name, arr_data in outputs.items():\n        path = os.path.join(output_dir, arr_name + \".dat\")\n        save_array(arr_data, path)\n\n\ndef arrays_are_equal(reference, result, tolerance=1e-5):\n    \"\"\"\n    Check if two arrays are equal within a tolerance.\n    :param reference: numpy array or path to file.\n    :param result: numpy array or path to file.\n    :param tolerance: Maximum relative (fractional) difference between arrays.\n    \"\"\"\n    if not isinstance(reference, np.ndarray):\n        reference = load_array(reference)\n    if not isinstance(result, np.ndarray):\n        result = load_array(result)\n    # tolerate zeroes by adding epsilon to the divisor\n    relative_diff = (np.abs(reference - result) /\n                     (np.maximum.reduce([reference, result]) +\n                      np.finfo(reference.dtype).eps))\n    return np.all(relative_diff <= tolerance)\n\n\ndef unique(iterable):\n    \"\"\"\n    Removes duplicates in the passed iterable.\n    :param iterable: iterable with potential duplicates\n    :return iterable without duplicates\n    \"\"\"\n    try:\n        return type(iterable)(\n            [i for i in sorted(set(iterable), key=lambda x: iterable.index(x))])\n    except TypeError:\n        return type(iterable)(collections.OrderedDict(\n            zip(map(str, iterable), iterable)).values())\n\n\ndef convert_3d_to_1d(dimensions: List[int], index: List[int]) -> int:\n    \"\"\"\n    Convert the size of form [a, b, c] to absolute values according to the global dimensions of the problem.\n    :param dimensions: problem size (e.g. size of the input arrays)\n    :param index: 3d value of the size\n    :return the 1d value of the size\n    \"\"\"\n    # convert [i, j, k] to flat 1D array index using the given dimensions [dimX, dimY, dimZ]\n    # index = i*dimY*dimZ + j*dimZ + k = (i*dimY + j)*dimZ + k\n    if not index:\n        return 0  # empty list\n    elif num_dims(index) == 3:\n        return dim_to_abs_val(index, dimensions)\n    elif num_dims(index) == 2:\n        if index[0] is None:\n            return index[1] * dimensions[2] + index[2]\n        elif index[1] is None:\n            return index[0] * dimensions[2] + index[2]\n        elif index[2] is None:\n            return index[0] * dimensions[1] + index[1]\n    elif num_dims(index) == 1:\n        return [x for x in index if x is not None][0]\n\n\ndef num_dims(index: List[int]):\n    \"\"\"\n\n    :param index:\n    :return:\n    \"\"\"\n    return functools.reduce(lambda x, y: x + 1\n                            if y is not None else x, index, 0)\n\n\n# credits: https://stackoverflow.com/questions/9895787/memory-alignment-for-fast-fft-in-python-using-shared-arrays\ndef aligned(a, alignment=16):\n    if (a.ctypes.data % alignment) == 0:\n        return a\n\n    extra = alignment / a.itemsize\n    buf = np.empty(a.size + int(extra), dtype=a.dtype)\n    ofs = int((-buf.ctypes.data % alignment) / a.itemsize)\n    aa = buf[ofs:ofs + a.size].reshape(a.shape)\n    np.copyto(aa, a)\n    assert (aa.ctypes.data % alignment) == 0\n    return aa\n\n\nclass OpCounter(ast.NodeVisitor):\n    def __init__(self):\n        self._operation_count = {}\n\n    @property\n    def operation_count(self):\n        return self._operation_count\n\n    def visit_BinOp(self, node: ast.BinOp):\n        if isinstance(node.left, ast.Subscript) or isinstance(\n                node.left, ast.BinOp) or isinstance(\n                    node.right, ast.Subscript) or isinstance(\n                        node.right, ast.BinOp):\n            op_name = type(node.op).__name__\n            if op_name not in self._operation_count:\n                self._operation_count[op_name] = 0\n            self._operation_count[op_name] += 1\n        self.generic_visit(node)\n\n    def visit_Call(self, node: ast.Call):\n        op_name = node.func.id\n        if op_name not in self._operation_count:\n            self._operation_count[op_name] = 0\n        self._operation_count[op_name] += 1\n        self.generic_visit(node)\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Basic helper function test.\n    \"\"\"\n\n    test = num_dims([1, None, 1])\n\n    example_list = [[1, 2, 2], [1, 2, 3], [3, 2, 1], [2, 3, 1]]\n    print(\"properties of list {}:\\nmin: {}\\nmax: {}\\n\".format(\n        example_list, min(example_list), max(example_list)))\n\n    example_dict = {\n        \"small\": [0, 10, 10],\n        \"very small\": [0, 1, 0],\n        \"extra large\": [12, 1, 2],\n        \"large\": [10, 10, 10]\n    }\n    print(\"max value entry key of dict {} is:\\n\\'{}\\'\".format(\n        example_dict, max_dict_entry_key(example_dict)))\n\n    array = aligned(np.array([1.0, 2.0, 3.0]), 64)\n    if array.ctypes.data % 64 == 0:  # check if address is aligned\n        print(\"correct alignment\")\n    else:\n        print(\"wrong alignment\")\n"}
{"content": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function)\n\nfrom functools import reduce\nfrom operator import mul, add\n\ntry:\n    import numpy as np\n    from numpy import any as _any\n\n    def prodpow(bases, exponents):\n        \"\"\"\n        Examples\n        --------\n        >>> prodpow([2, 3], np.array([[0, 1], [1, 2]]))\n        array([ 3, 18])\n\n        \"\"\"\n        exponents = np.asarray(exponents)\n        return np.multiply.reduce(bases**exponents, axis=-1)\n\nexcept ImportError:  # no NumPy available\n    def _any(arg):\n        if arg is True:\n            return True\n        if arg is False:\n            return False\n        return any(arg)\n\n    def prodpow(bases, exponents):\n        \"\"\"\n        Examples\n        --------\n        >>> prodpow([2, 3], [[0, 1], [1, 2]])\n        [3, 18]\n\n        \"\"\"\n        result = []\n        for row in exponents:\n            res = 1\n            for b, e in zip(bases, row):\n                res *= b**e\n            result.append(res)\n        return result\n\n\ndef get_backend(backend):\n    if isinstance(backend, str):\n        backend = __import__(backend)\n    if backend is None:\n        try:\n            import numpy as backend\n        except ImportError:\n            import math as backend\n    return backend\n\n\ndef intdiv(p, q):\n    \"\"\" Integer divsions which rounds toward zero\n\n    Examples\n    --------\n    >>> intdiv(3, 2)\n    1\n    >>> intdiv(-3, 2)\n    -1\n    >>> -3 // 2\n    -2\n\n    \"\"\"\n    r = p // q\n    if r < 0 and q*r != p:\n        r += 1\n    return r\n\n\nclass NameSpace:\n    def __init__(self, default):\n        self._NameSpace_default = default\n        self._NameSpace_attr_store = {}\n\n    def __getattr__(self, attr):\n        if attr.startswith('_NameSpace_'):\n            return self.__dict__[attr]\n        else:\n            try:\n                return self._NameSpace_attr_store[attr]\n            except KeyError:\n                return getattr(self._NameSpace_default, attr)\n\n    def __setattr__(self, attr, val):\n        if attr.startswith('_NameSpace_'):\n            self.__dict__[attr] = val\n        else:\n            self._NameSpace_attr_store[attr] = val\n\n    def as_dict(self):\n        items = self._NameSpace_default.__dict__.items()\n        result = {k: v for k, v in items if not k.startswith('_')}\n        result.update(self._NameSpace_attr_store)\n        return result\n\n\nclass AttributeDict(object):\n    def __init__(self, d):\n        self.__dict__.update(d)\n\n\ndef reducemap(args, reduce_op, map_op):\n    return reduce(reduce_op, map(map_op, *args))\n\n\ndef vec_dot_vec(vec1, vec2):\n    # return np.dot(vec1, vec2)\n    # return np.add.reduce(np.multiply(vec1, vec2))\n    return reducemap((vec1, vec2), add, mul)\n\n\ndef mat_dot_vec(iter_mat, iter_vec, iter_term=None):  # pure python (slow)\n    if iter_term is None:\n        return [vec_dot_vec(row, iter_vec) for row in iter_mat]\n    else:\n        # daxpy\n        return [vec_dot_vec(row, iter_vec) + term for row, term\n                in zip(iter_mat, iter_term)]\n\n\n# def composition_balance(substances, concs, composition_number):\n#     if not hasattr(concs, 'ndim') or concs.ndim == 1:\n#         res = 0\n#     elif concs.ndim == 2:\n#         res = np.zeros(concs.shape[0])\n#         concs = concs.T\n#     else:\n#         raise NotImplementedError\n#     for s, c in zip(substances, concs):\n#         res += s.composition.get(composition_number, 0)*c\n#     return res\n"}
{"content": "from __future__ import print_function\nimport sys\nfrom os import path, makedirs, system, remove\n\nsys.path.append(\".\")\nsys.path.append(\"..\")\n\nimport time\nimport argparse\nimport uuid\nimport json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim import Adam, SGD\nfrom utils.io_ import seeds, Writer, get_logger, Index2Instance, prepare_data, write_extra_labels\nfrom utils.models.sequence_tagger import Sequence_Tagger\nfrom utils import load_word_embeddings\nfrom utils.tasks.seqeval import accuracy_score, f1_score, precision_score, recall_score\n\nuid = uuid.uuid4().hex[:6]\n\nlogger = get_logger('SequenceTagger')\n\ndef read_arguments():\n    args_ = argparse.ArgumentParser(description='Sovling SequenceTagger')\n    args_.add_argument('--dataset', choices=['ontonotes', 'ud'], help='Dataset', required=True)\n    args_.add_argument('--domain', help='domain', required=True)\n    args_.add_argument('--rnn_mode', choices=['RNN', 'LSTM', 'GRU'], help='architecture of rnn',\n                       required=True)\n    args_.add_argument('--task', default='distance_from_the_root', choices=['distance_from_the_root', 'number_of_children', 'relative_pos_based', 'language_model'], help='sequence_tagger task')\n    args_.add_argument('--num_epochs', type=int, default=200, help='Number of training epochs')\n    args_.add_argument('--batch_size', type=int, default=64, help='Number of sentences in each batch')\n    args_.add_argument('--hidden_size', type=int, default=256, help='Number of hidden units in RNN')\n    args_.add_argument('--tag_space', type=int, default=128, help='Dimension of tag space')\n    args_.add_argument('--num_layers', type=int, default=1, help='Number of layers of RNN')\n    args_.add_argument('--num_filters', type=int, default=50, help='Number of filters in CNN')\n    args_.add_argument('--kernel_size', type=int, default=3, help='Size of Kernel for CNN')\n    args_.add_argument('--use_pos', action='store_true', help='use part-of-speech embedding.')\n    args_.add_argument('--use_char', action='store_true', help='use character embedding and CNN.')\n    args_.add_argument('--word_dim', type=int, default=300, help='Dimension of word embeddings')\n    args_.add_argument('--pos_dim', type=int, default=50, help='Dimension of POS embeddings')\n    args_.add_argument('--char_dim', type=int, default=50, help='Dimension of Character embeddings')\n    args_.add_argument('--initializer', choices=['xavier'], help='initialize model parameters')\n    args_.add_argument('--opt', choices=['adam', 'sgd'], help='optimization algorithm')\n    args_.add_argument('--momentum', type=float, default=0.9, help='momentum of optimizer')\n    args_.add_argument('--betas', nargs=2, type=float, default=[0.9, 0.9], help='betas of optimizer')\n    args_.add_argument('--learning_rate', type=float, default=0.01, help='Learning rate')\n    args_.add_argument('--decay_rate', type=float, default=0.05, help='Decay rate of learning rate')\n    args_.add_argument('--schedule', type=int, help='schedule for learning rate decay')\n    args_.add_argument('--clip', type=float, default=5.0, help='gradient clipping')\n    args_.add_argument('--gamma', type=float, default=0.0, help='weight for regularization')\n    args_.add_argument('--epsilon', type=float, default=1e-8, help='epsilon for adam')\n    args_.add_argument('--p_rnn', nargs=2, type=float, required=True, help='dropout rate for RNN')\n    args_.add_argument('--p_in', type=float, default=0.33, help='dropout rate for input embeddings')\n    args_.add_argument('--p_out', type=float, default=0.33, help='dropout rate for output layer')\n    args_.add_argument('--unk_replace', type=float, default=0.,\n                       help='The rate to replace a singleton word with UNK')\n    args_.add_argument('--punct_set', nargs='+', type=str, help='List of punctuations')\n    args_.add_argument('--word_embedding', choices=['random', 'glove', 'fasttext', 'word2vec'],\n                       help='Embedding for words')\n    args_.add_argument('--word_path', help='path for word embedding dict - in case word_embedding is not random')\n    args_.add_argument('--freeze_word_embeddings', action='store_true', help='frozen the word embedding (disable fine-tuning).')\n    args_.add_argument('--char_embedding', choices=['random'], help='Embedding for characters',\n                       required=True)\n    args_.add_argument('--char_path', help='path for character embedding dict')\n    args_.add_argument('--wiki_path', help='WIKPEDIA DATA PATH [LTI RESEARCH]') # [rram]\n    args_.add_argument('--use_unlabeled_data', action='store_true', help='flag to use unlabeled data.')\n    args_.add_argument('--use_labeled_data', action='store_true', help='flag to use labeled data.')\n    args_.add_argument('--model_path', help='path for saving model file.', required=True)\n    args_.add_argument('--parser_path', help='path for loading parser files.', default=None)\n    args_.add_argument('--load_path', help='path for loading saved source model file.', default=None)\n    args_.add_argument('--strict',action='store_true', help='if True loaded model state should contain '\n                                                            'exactly the same keys as current model')\n    args_.add_argument('--eval_mode', action='store_true', help='evaluating model without training it')\n    args = args_.parse_args()\n    args_dict = {}\n    args_dict['wiki_path'] = args.wiki_path \n    args_dict['dataset'] = args.dataset\n    args_dict['domain'] = args.domain\n    args_dict['task'] = args.task\n    args_dict['rnn_mode'] = args.rnn_mode\n    args_dict['load_path'] = args.load_path\n    args_dict['strict'] = args.strict\n    args_dict['model_path'] = args.model_path\n    if not path.exists(args_dict['model_path']):\n        makedirs(args_dict['model_path'])\n    args_dict['parser_path'] = args.parser_path\n    args_dict['model_name'] = 'domain_' + args_dict['domain']\n    args_dict['full_model_name'] = path.join(args_dict['model_path'],args_dict['model_name'])\n    args_dict['use_unlabeled_data'] = args.use_unlabeled_data\n    args_dict['use_labeled_data'] = args.use_labeled_data\n    if args_dict['task'] == 'number_of_children':\n        args_dict['data_paths'] = write_extra_labels.add_number_of_children(args_dict['model_path'], args_dict['parser_path'], args_dict['domain'], args_dict['domain'],\n                                                                            use_unlabeled_data=args_dict['use_unlabeled_data'],\n                                                                            use_labeled_data=args_dict['use_labeled_data'])\n    elif args_dict['task'] == 'distance_from_the_root':\n        args_dict['data_paths'] = write_extra_labels.add_distance_from_the_root(args_dict['model_path'], args_dict['parser_path'], args_dict['domain'], args_dict['domain'],\n                                                                                use_unlabeled_data=args_dict['use_unlabeled_data'],\n                                                                                use_labeled_data=args_dict['use_labeled_data'])\n    elif args_dict['task'] == 'relative_pos_based':\n        args_dict['data_paths'] = write_extra_labels.add_relative_pos_based(args_dict['model_path'], args_dict['parser_path'], args_dict['domain'], args_dict['domain'],\n                                                                                 use_unlabeled_data=args_dict['use_unlabeled_data'],\n                                                                                 use_labeled_data=args_dict['use_labeled_data'])\n    else: #args_dict['task'] == 'language_model':\n        args_dict['data_paths'] = write_extra_labels.add_language_model(args_dict['model_path'], args_dict['parser_path'], args_dict['domain'], args_dict['domain'],\n                                                            use_unlabeled_data=args_dict['use_unlabeled_data'],\n                                                            use_labeled_data=args_dict['use_labeled_data'])\n    args_dict['splits'] = args_dict['data_paths'].keys()\n    alphabet_data_paths = deepcopy(args_dict['data_paths'])\n    if args_dict['dataset'] == 'ontonotes':\n        data_path = 'data/onto_pos_ner_dp'\n    else:\n        data_path = 'data/ud_pos_ner_dp'\n    # Adding more resources to make sure equal alphabet size for all domains\n    for split in args_dict['splits']:\n        if args_dict['dataset'] == 'ontonotes':\n            alphabet_data_paths['additional_' + split] = data_path + '_' + split + '_' + 'all'\n        else:\n            if '_' in args_dict['domain']:\n                alphabet_data_paths[split] = data_path + '_' + split + '_' + args_dict['domain'].split('_')[0]\n            else:\n                alphabet_data_paths[split] = args_dict['data_paths'][split]\n\n    # rram\n    alphabet_data_paths['wiki_path'] = args_dict['wiki_path']\n    # rram\n\n    args_dict['alphabet_data_paths'] = alphabet_data_paths\n    args_dict['num_epochs'] = args.num_epochs\n    args_dict['batch_size'] = args.batch_size\n    args_dict['hidden_size'] = args.hidden_size\n    args_dict['tag_space'] = args.tag_space\n    args_dict['num_layers'] = args.num_layers\n    args_dict['num_filters'] = args.num_filters\n    args_dict['kernel_size'] = args.kernel_size\n    args_dict['learning_rate'] = args.learning_rate\n    args_dict['initializer'] = nn.init.xavier_uniform_ if args.initializer == 'xavier' else None\n    args_dict['opt'] = args.opt\n    args_dict['momentum'] = args.momentum\n    args_dict['betas'] = tuple(args.betas)\n    args_dict['epsilon'] = args.epsilon\n    args_dict['decay_rate'] = args.decay_rate\n    args_dict['clip'] = args.clip\n    args_dict['gamma'] = args.gamma\n    args_dict['schedule'] = args.schedule\n    args_dict['p_rnn'] = tuple(args.p_rnn)\n    args_dict['p_in'] = args.p_in\n    args_dict['p_out'] = args.p_out\n    args_dict['unk_replace'] = args.unk_replace\n    args_dict['punct_set'] = None\n    if args.punct_set is not None:\n        args_dict['punct_set'] = set(args.punct_set)\n        logger.info(\"punctuations(%d): %s\" % (len(args_dict['punct_set']), ' '.join(args_dict['punct_set'])))\n    args_dict['freeze_word_embeddings'] = args.freeze_word_embeddings\n    args_dict['word_embedding'] = args.word_embedding\n    args_dict['word_path'] = args.word_path\n    args_dict['use_char'] = args.use_char\n    args_dict['char_embedding'] = args.char_embedding\n    args_dict['char_path'] = args.char_path\n    args_dict['use_pos'] = args.use_pos\n    args_dict['pos_dim'] = args.pos_dim\n    args_dict['word_dict'] = None\n    args_dict['word_dim'] = args.word_dim\n    if args_dict['word_embedding'] != 'random' and args_dict['word_path']:\n        args_dict['word_dict'], args_dict['word_dim'] = load_word_embeddings.load_embedding_dict(args_dict['word_embedding'],\n                                                                                                 args_dict['word_path'])\n    args_dict['char_dict'] = None\n    args_dict['char_dim'] = args.char_dim\n    if args_dict['char_embedding'] != 'random':\n        args_dict['char_dict'], args_dict['char_dim'] = load_word_embeddings.load_embedding_dict(args_dict['char_embedding'],\n                                                                                                 args_dict['char_path'])\n    args_dict['pos_dict'] = None\n    args_dict['alphabet_path'] = path.join(args_dict['model_path'], 'alphabets' + '_src_domain_' + args_dict['domain'] + '/')\n    args_dict['alphabet_parser_path'] = path.join(args_dict['parser_path'], 'alphabets' + '_src_domain_' + args_dict['domain'] + '/')\n    args_dict['model_name'] = path.join(args_dict['model_path'], args_dict['model_name'])\n    args_dict['eval_mode'] = args.eval_mode\n    args_dict['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    args_dict['word_status'] = 'frozen' if args.freeze_word_embeddings else 'fine tune'\n    args_dict['char_status'] = 'enabled' if args.use_char else 'disabled'\n    args_dict['pos_status'] = 'enabled' if args.use_pos else 'disabled'\n    logger.info(\"Saving arguments to file\")\n    save_args(args, args_dict['full_model_name'])\n    logger.info(\"Creating Alphabets\")\n    alphabet_dict = creating_alphabets(args_dict['alphabet_path'], args_dict['alphabet_parser_path'], args_dict['alphabet_data_paths'])\n    args_dict = {**args_dict, **alphabet_dict}\n    ARGS = namedtuple('ARGS', args_dict.keys())\n    my_args = ARGS(**args_dict)\n    return my_args\n\n\ndef creating_alphabets(alphabet_path, alphabet_parser_path, alphabet_data_paths):\n    data_paths_list = alphabet_data_paths.values()\n    alphabet_dict = {}\n    alphabet_dict['alphabets'] = prepare_data.create_alphabets_for_sequence_tagger(alphabet_path, alphabet_parser_path, data_paths_list)\n    for k, v in alphabet_dict['alphabets'].items():\n        num_key = 'num_' + k.split('_alphabet')[0]\n        alphabet_dict[num_key] = v.size()\n        logger.info(\"%s : %d\" % (num_key, alphabet_dict[num_key]))\n    return alphabet_dict\n\ndef construct_embedding_table(alphabet, tokens_dict, dim, token_type='word'):\n    if tokens_dict is None:\n        return None\n    scale = np.sqrt(3.0 / dim)\n    table = np.empty([alphabet.size(), dim], dtype=np.float32)\n    table[prepare_data.UNK_ID, :] = np.random.uniform(-scale, scale, [1, dim]).astype(np.float32)\n    oov_tokens = 0\n    for token, index in alphabet.items():\n        if token in tokens_dict:\n            embedding = tokens_dict[token]\n        elif token.lower() in tokens_dict:\n            embedding = tokens_dict[token.lower()]\n        else:\n            embedding = np.random.uniform(-scale, scale, [1, dim]).astype(np.float32)\n            oov_tokens += 1\n        table[index, :] = embedding\n    print('token type : %s, number of oov: %d' % (token_type, oov_tokens))\n    table = torch.from_numpy(table)\n    return table\n\ndef get_free_gpu():\n    system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free > tmp.txt')\n    memory_available = [int(x.split()[2]) for x in open('tmp.txt', 'r').readlines()]\n    remove(\"tmp.txt\")\n    free_device = 'cuda:' + str(np.argmax(memory_available))\n    return free_device\n\ndef save_args(args, full_model_name):\n    arg_path = full_model_name + '.arg.json'\n    argparse_dict = vars(args)\n    with open(arg_path, 'w') as f:\n        json.dump(argparse_dict, f)\n\ndef generate_optimizer(args, lr, params):\n    params = filter(lambda param: param.requires_grad, params)\n    if args.opt == 'adam':\n        return Adam(params, lr=lr, betas=args.betas, weight_decay=args.gamma, eps=args.epsilon)\n    elif args.opt == 'sgd':\n        return SGD(params, lr=lr, momentum=args.momentum, weight_decay=args.gamma, nesterov=True)\n    else:\n        raise ValueError('Unknown optimization algorithm: %s' % args.opt)\n\n\ndef save_checkpoint(model, optimizer, opt, dev_eval_dict, test_eval_dict, full_model_name):\n    path_name = full_model_name + '.pt'\n    print('Saving model to: %s' % path_name)\n    state = {'model_state_dict': model.state_dict(),\n             'optimizer_state_dict': optimizer.state_dict(),\n             'opt': opt, 'dev_eval_dict': dev_eval_dict, 'test_eval_dict': test_eval_dict}\n    torch.save(state, path_name)\n\n\ndef load_checkpoint(args, model, optimizer, dev_eval_dict, test_eval_dict, start_epoch, load_path, strict=True):\n    print('Loading saved model from: %s' % load_path)\n    checkpoint = torch.load(load_path, map_location=args.device)\n    if checkpoint['opt'] != args.opt:\n        raise ValueError('loaded optimizer type is: %s instead of: %s' % (checkpoint['opt'], args.opt))\n    model.load_state_dict(checkpoint['model_state_dict'], strict=strict)\n    if strict:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        dev_eval_dict = checkpoint['dev_eval_dict']\n        test_eval_dict = checkpoint['test_eval_dict']\n        start_epoch = dev_eval_dict['in_domain']['epoch']\n    return model, optimizer, dev_eval_dict, test_eval_dict, start_epoch\n\n\ndef build_model_and_optimizer(args):\n    word_table = construct_embedding_table(args.alphabets['word_alphabet'], args.word_dict, args.word_dim, token_type='word')\n    char_table = construct_embedding_table(args.alphabets['char_alphabet'], args.char_dict, args.char_dim, token_type='char')\n    pos_table = construct_embedding_table(args.alphabets['pos_alphabet'], args.pos_dict, args.pos_dim, token_type='pos')\n    model = Sequence_Tagger(args.word_dim, args.num_word, args.char_dim, args.num_char,\n                            args.use_pos, args.use_char, args.pos_dim, args.num_pos,\n                            args.num_filters, args.kernel_size, args.rnn_mode,\n                            args.hidden_size, args.num_layers, args.tag_space, args.num_auto_label,\n                            embedd_word=word_table, embedd_char=char_table, embedd_pos=pos_table,\n                            p_in=args.p_in, p_out=args.p_out, p_rnn=args.p_rnn,\n                            initializer=args.initializer)\n    optimizer = generate_optimizer(args, args.learning_rate, model.parameters())\n    start_epoch = 0\n    dev_eval_dict = {'in_domain': initialize_eval_dict()}\n    test_eval_dict = {'in_domain': initialize_eval_dict()}\n    if args.load_path:\n        model, optimizer, dev_eval_dict, test_eval_dict, start_epoch = \\\n            load_checkpoint(args, model, optimizer,\n                            dev_eval_dict, test_eval_dict,\n                            start_epoch, args.load_path, strict=args.strict)\n    if args.freeze_word_embeddings:\n        model.rnn_encoder.word_embedd.weight.requires_grad = False\n        # model.rnn_encoder.char_embedd.weight.requires_grad = False\n        # model.rnn_encoder.pos_embedd.weight.requires_grad = False\n    device = args.device\n    model.to(device)\n    return model, optimizer, dev_eval_dict, test_eval_dict, start_epoch\n\n\ndef initialize_eval_dict():\n    eval_dict = {}\n    eval_dict['auto_label_accuracy'] = 0.0\n    eval_dict['auto_label_precision'] = 0.0\n    eval_dict['auto_label_recall'] = 0.0\n    eval_dict['auto_label_f1'] = 0.0\n    return eval_dict\n\ndef in_domain_evaluation(args, datasets, model, optimizer, dev_eval_dict, test_eval_dict, epoch,\n                         best_model, best_optimizer, patient):\n    # In-domain evaluation\n    curr_dev_eval_dict = evaluation(args, datasets['dev'], 'dev', model, args.domain, epoch, 'current_results')\n    is_best_in_domain = dev_eval_dict['in_domain']['auto_label_f1'] <= curr_dev_eval_dict['auto_label_f1']\n\n    if is_best_in_domain:\n        for key, value in curr_dev_eval_dict.items():\n            dev_eval_dict['in_domain'][key] = value\n        curr_test_eval_dict = evaluation(args, datasets['test'], 'test', model, args.domain, epoch, 'current_results')\n        for key, value in curr_test_eval_dict.items():\n            test_eval_dict['in_domain'][key] = value\n        best_model = deepcopy(model)\n        best_optimizer = deepcopy(optimizer)\n        patient = 0\n    else:\n        patient += 1\n    if epoch == args.num_epochs:\n        # rram\n        wiki_tmp = prepare_data.read_data_to_variable(args.wiki_path, args.alphabets, args.device, # rram\n                                                symbolic_root=True) # rram\n        write_results(args, wiki_tmp, args.domain, 'wiki_path', best_model, args.domain, None) # rram \n        # rram\n\n        # save in-domain checkpoint\n        for split in ['dev', 'test']:\n            eval_dict = dev_eval_dict['in_domain'] if split == 'dev' else test_eval_dict['in_domain']\n            write_results(args, datasets[split], args.domain, split, best_model, args.domain, eval_dict)\n        save_checkpoint(best_model, best_optimizer, args.opt, dev_eval_dict, test_eval_dict, args.full_model_name)\n\n    print('\\n')\n    return dev_eval_dict, test_eval_dict, best_model, best_optimizer, patient\n\n\ndef evaluation(args, data, split, model, domain, epoch, str_res='results'):\n    # evaluate performance on data\n    model.eval()\n    auto_label_idx2inst = Index2Instance(args.alphabets['auto_label_alphabet'])\n    eval_dict = initialize_eval_dict()\n    eval_dict['epoch'] = epoch\n    pred_labels = []\n    gold_labels = []\n    for batch in prepare_data.iterate_batch(data, args.batch_size, args.device): # add debug statements here [rram] \n        word, char, pos, ner, heads, arc_tags, auto_label, masks, lengths = batch\n        output, masks, lengths = model.forward(word, char, pos, mask=masks, length=lengths)\n        auto_label_preds = model.decode(output, mask=masks, length=lengths, leading_symbolic=prepare_data.NUM_SYMBOLIC_TAGS)\n        lengths = lengths.cpu().numpy()\n        word = word.data.cpu().numpy()\n        pos = pos.data.cpu().numpy()\n        ner = ner.data.cpu().numpy()\n        heads = heads.data.cpu().numpy()\n        arc_tags = arc_tags.data.cpu().numpy()\n        auto_label = auto_label.data.cpu().numpy()\n        auto_label_preds = auto_label_preds.data.cpu().numpy()\n        gold_labels += auto_label_idx2inst.index2instance(auto_label, lengths, symbolic_root=True)\n        pred_labels += auto_label_idx2inst.index2instance(auto_label_preds, lengths, symbolic_root=True)\n\n    eval_dict['auto_label_accuracy'] = accuracy_score(gold_labels, pred_labels) * 100\n    eval_dict['auto_label_precision'] = precision_score(gold_labels, pred_labels) * 100\n    eval_dict['auto_label_recall'] = recall_score(gold_labels, pred_labels) * 100\n    eval_dict['auto_label_f1'] = f1_score(gold_labels, pred_labels) * 100\n    print_results(eval_dict, split, domain, str_res)\n    return eval_dict\n\n\ndef print_results(eval_dict, split, domain, str_res='results'):\n    print('----------------------------------------------------------------------------------------------------------------------------')\n    print('Testing model on domain %s' % domain)\n    print('--------------- sequence_tagger - %s ---------------' % split)\n    print(\n        str_res + ' on ' + split + ' accuracy: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%% (epoch: %d)'\n        % (eval_dict['auto_label_accuracy'], eval_dict['auto_label_precision'], eval_dict['auto_label_recall'], eval_dict['auto_label_f1'],\n           eval_dict['epoch']))\n\n\ndef write_results(args, data, data_domain, split, model, model_domain, eval_dict):\n    str_file = args.full_model_name + '_' + split + '_model_domain_' + model_domain + '_data_domain_' + data_domain\n    res_filename = str_file + '_res.txt'\n    pred_filename = str_file + '_pred.txt'\n    gold_filename = str_file + '_gold.txt'\n    print(f'writing {split} result to: {str_file}_[res,pred,gold].txt') # rram\n    \n    # save results dictionary into a file\n    with open(res_filename, 'w') as f:\n        json.dump(eval_dict, f)\n\n    # save predictions and gold labels into files\n    pred_writer = Writer(args.alphabets)\n    gold_writer = Writer(args.alphabets)\n    pred_writer.start(pred_filename)\n    gold_writer.start(gold_filename)\n    for batch in prepare_data.iterate_batch(data, args.batch_size, args.device):\n        word, char, pos, ner, heads, arc_tags, auto_label, masks, lengths = batch\n        output, masks, lengths = model.forward(word, char, pos, mask=masks, length=lengths)\n        auto_label_preds = model.decode(output, mask=masks, length=lengths,\n                                        leading_symbolic=prepare_data.NUM_SYMBOLIC_TAGS)\n        lengths = lengths.cpu().numpy()\n        word = word.data.cpu().numpy()\n        pos = pos.data.cpu().numpy()\n        ner = ner.data.cpu().numpy()\n        heads = heads.data.cpu().numpy()\n        arc_tags = arc_tags.data.cpu().numpy()\n        auto_label_preds = auto_label_preds.data.cpu().numpy()\n        # writing predictions\n        pred_writer.write(word, pos, ner, heads, arc_tags, lengths, auto_label=auto_label_preds, symbolic_root=True)\n        # writing gold labels\n        gold_writer.write(word, pos, ner, heads, arc_tags, lengths, auto_label=auto_label, symbolic_root=True)\n\n    pred_writer.close()\n    gold_writer.close()\n\ndef main():\n    logger.info(\"Reading and creating arguments\")\n    args = read_arguments()\n    logger.info(\"Reading Data\")\n    datasets = {}\n    for split in args.splits:\n        dataset = prepare_data.read_data_to_variable(args.data_paths[split], args.alphabets, args.device,\n                                                     symbolic_root=True)\n        datasets[split] = dataset\n\n    # rram\n    dataset = prepare_data.read_data_to_variable(args.wiki_path, args.alphabets, args.device,\n                                                symbolic_root=True)\n    datasets['wiki_path'] = dataset \n    #rram\n\n    logger.info(\"Creating Networks\")\n    num_data = sum(datasets['train'][1])\n    model, optimizer, dev_eval_dict, test_eval_dict, start_epoch = build_model_and_optimizer(args)\n    best_model = deepcopy(model)\n    best_optimizer = deepcopy(optimizer)\n    logger.info('Training INFO of in domain %s' % args.domain)\n    logger.info('Training on Dependecy Parsing')\n    print(model)\n    logger.info(\"train: gamma: %f, batch: %d, clip: %.2f, unk replace: %.2f\" % (args.gamma, args.batch_size, args.clip, args.unk_replace))\n    logger.info('number of training samples for %s is: %d' % (args.domain, num_data))\n    logger.info(\"dropout(in, out, rnn): (%.2f, %.2f, %s)\" % (args.p_in, args.p_out, args.p_rnn))\n    logger.info(\"num_epochs: %d\" % (args.num_epochs))\n    print('\\n')\n\n    if not args.eval_mode:\n        logger.info(\"Training\")\n        num_batches = prepare_data.calc_num_batches(datasets['train'], args.batch_size)\n        lr = args.learning_rate\n        patient = 0\n        terminal_patient = 0\n        decay = 0\n        for epoch in range(start_epoch + 1, args.num_epochs + 1):\n            print('Epoch %d (Training: rnn mode: %s, optimizer: %s, learning rate=%.6f, eps=%.1e, decay rate=%.2f (schedule=%d, decay=%d)): ' % (\n                epoch, args.rnn_mode, args.opt, lr, args.epsilon, args.decay_rate, args.schedule, decay))\n            model.train()\n            total_loss = 0.0\n            total_train_inst = 0.0\n\n            iter = prepare_data.iterate_batch_rand_bucket_choosing(\n                    datasets['train'], args.batch_size, args.device, unk_replace=args.unk_replace)\n            start_time = time.time()\n            batch_num = 0\n            for batch_num, batch in enumerate(iter):\n                batch_num = batch_num + 1\n                optimizer.zero_grad()\n                # compute loss of main task\n                word, char, pos, ner_tags, heads, arc_tags, auto_label, masks, lengths = batch\n                output, masks, lengths = model.forward(word, char, pos, mask=masks, length=lengths)\n                loss = model.loss(output, auto_label, mask=masks, length=lengths)\n\n                # update losses\n                num_insts = masks.data.sum() - word.size(0)\n                total_loss += loss.item() * num_insts\n                total_train_inst += num_insts\n                # optimize parameters\n                loss.backward()\n                clip_grad_norm_(model.parameters(), args.clip)\n                optimizer.step()\n\n                time_ave = (time.time() - start_time) / batch_num\n                time_left = (num_batches - batch_num) * time_ave\n\n                # update log\n                if batch_num % 50 == 0:\n                    log_info = 'train: %d/%d, domain: %s, total loss: %.2f, time left: %.2fs' % \\\n                               (batch_num, num_batches, args.domain, total_loss / total_train_inst, time_left)\n                    sys.stdout.write(log_info)\n                    sys.stdout.write('\\n')\n                    sys.stdout.flush()\n            print('\\n')\n            print('train: %d/%d, domain: %s, total_loss: %.2f, time: %.2fs' %\n                  (batch_num, num_batches, args.domain, total_loss / total_train_inst, time.time() - start_time))\n\n            dev_eval_dict, test_eval_dict, best_model, best_optimizer, patient = in_domain_evaluation(args, datasets, model, optimizer, dev_eval_dict, test_eval_dict, epoch, best_model, best_optimizer, patient)\n            if patient == 0:\n                terminal_patient = 0\n            else:\n                terminal_patient += 1\n            if terminal_patient >= 4 * args.schedule:\n                # Save best model and terminate learning\n                cur_epoch = epoch\n                epoch = args.num_epochs\n                in_domain_evaluation(args, datasets, model, optimizer, dev_eval_dict, test_eval_dict, epoch, best_model,\n                                     best_optimizer, patient)\n                log_info = 'Terminating training in epoch %d' % (cur_epoch)\n                sys.stdout.write(log_info)\n                sys.stdout.write('\\n')\n                sys.stdout.flush()\n                return\n            if patient >= args.schedule:\n                lr = args.learning_rate / (1.0 + epoch * args.decay_rate)\n                optimizer = generate_optimizer(args, lr, model.parameters())\n                print('updated learning rate to %.6f' % lr)\n                patient = 0\n            print_results(test_eval_dict['in_domain'], 'test', args.domain, 'best_results')\n            print('\\n')\n\n    else:\n        logger.info(\"Evaluating\")\n        epoch = start_epoch\n        for split in ['train', 'dev', 'test']:\n            evaluation(args, datasets[split], split, model, args.domain, epoch, 'best_results')\n        # evaluation(args, datasets['wiki_path'], 'wiki_path', model, args.domain, epoch, 'best_results') # rram\n\n\nif __name__ == '__main__':\n    main()\n"}
{"content": "import tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef _bytes_feature(value):\r\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n    if isinstance(value, type(tf.constant(0))):\r\n        # BytesList won't unpack a string from an EagerTensor.\r\n        value = value.numpy()\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\n\r\ndef _float_feature(value):\r\n    \"\"\"Returns a float_list from a float / double.\"\"\"\r\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n\r\n\r\ndef _int64_feature(value):\r\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n\r\ndef to_array_feature(value):\r\n    \"\"\"\r\n    value_type: np.float32\r\n    value: [1.0, 2.0]\r\n    \"\"\"\r\n    # isinstance(value, list)\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\n\r\ndef ToFloat32ArrayFeature(value):\r\n    return _bytes_feature(value.astype(np.float32).tostring())\r\n\r\n\r\ndef ToInt32ArrayFeature(value):\r\n    return _bytes_feature(value.astype(np.int32).tostring())\r\n\r\n\r\ndef _parse_ndarray_feature(value):\r\n    '''\r\n    def parse_fn(example_proto):\r\n        features_desc = {\r\n            \"audio\": tf.FixedLenFeature((), tf.string)\r\n        }\r\n        parsed_features = tf.parse_single_example(example_proto, features_desc)\r\n        float32Array = tf.decode_raw(parsed_features['audio'], tf.float32)\r\n    '''\r\n    return tf.decode_raw(value, tf.float32)\r\n"}
{"content": "# Copyright 2020 kurosawa. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport onnx\nfrom onnx_tf.backend import prepare\nimport torch\nimport tensorflow as tf\nimport os\nimport shutil\nimport sys\nimport subprocess\nfrom .common import *\nfrom .onnx import cv2onnx\nimport numpy as np\n\ndef onnx2tflite(onnx_file, tflite_path):\n    print(onnx_file, tflite_path)\n    onnx_model = onnx.load(onnx_file)\n    tf_rep = prepare(onnx_model)\n    tmp_pb_file = \"tmp.pb\"\n    tf_rep.export_graph(tmp_pb_file)\n    converter = tf.lite.TFLiteConverter.from_saved_model(tmp_pb_file)\n    tflite_model = converter.convert()\n    shutil.rmtree(tmp_pb_file)\n    with open(tflite_path, \"wb\") as f:\n        f.write(tflite_model)\n\ndef cv2tflite(model, input_shape, tflite_path, edgetpu=False, quantization=False):\n    \"\"\"\n    convert torch model to tflite model using onnx\n    \"\"\"\n    onnx_input_flag = False\n    if type(model) == str:\n        ext = os.path.splitext(model)[1]\n        if ext == \".onnx\":\n            onnx_input_flag = True\n    tmp_pb_file = \"tmp.pb\"\n    if not onnx_input_flag:\n        onnx_file = \"tmp.onnx\"\n        cv2onnx(model, input_shape, onnx_file)\n    else:\n        onnx_file = model\n    onnx_model = onnx.load(onnx_file)\n    onnx_input_names = [input.name for input in onnx_model.graph.input]\n    onnx_output_names = [output.name for output in onnx_model.graph.output]\n    tf_rep = prepare(onnx_model)\n    tf_rep.export_graph(tmp_pb_file)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(tmp_pb_file)\n\n    if quantization:\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n    if edgetpu:\n        if type(input_shape[0]) == tuple:\n            if check_model_is_cuda(model):\n                dummy_input = tuple([np.randn(ishape) for ishape in input_shape])\n            else:\n                dummy_input = tuple([np.randn(ishape) for ishape in input_shape])\n        elif type(input_shape) == tuple:\n            if check_model_is_cuda(model):\n                dummy_input = np.randn(input_shape)\n            else:\n                dummy_input = np.randn(input_shape)\n        else:\n            raise Exception(\"input_shape must be tuple\")\n        train = tf.convert_to_tensor(input_data)\n        my_ds = tf.data.Dataset.from_tensor_slices((train)).batch(10)\n        def representative_dataset_gen():\n            for input_value in my_ds.take(10):\n                yield [input_value]\n        converter.representative_dataset = representative_dataset_gen\n        converter.allow_custom_ops = True\n        converter.experimental_new_converter = True\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        converter.inference_input_type = tf.int8\n        converter.inference_output_type = tf.int8\n\n    # convert tensorflow to tflite model\n    tflite_model = converter.convert()\n\n    with open(tflite_path, \"wb\") as f:\n        f.write(tflite_model)\n    if not onnx_input_flag:\n        os.remove(onnx_file)\n    shutil.rmtree(tmp_pb_file)\n\n    if edgetpu:\n        subprocess.check_call(f\"edgetpu_compiler {tflite_path}\", shell=True)\n\n\ndef load_tflite(tflitepath):\n    interpreter = tf.lite.Interpreter(model_path=tflitepath)\n    # allocate memory\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef infer_tflite(interpreter, input_data, bm=None):\n    # get model input and output propaty\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    ## get input shape\n    #input_shape = input_details[0][\"shape\"]\n    def execute():\n        # set tensor pointer to index\n        if type(input_data) == tuple:\n            for i, data in enumerate(input_data):\n                interpreter.set_tensor(input_details[i][\"index\"], data)\n        else:\n            interpreter.set_tensor(input_details[0][\"index\"], input_data)\n        # execute infer\n        interpreter.invoke()\n        # get result from index of output_details\n        output_data = interpreter.get_tensor(output_details[0][\"index\"])\n        return output_data\n    if bm:\n        output_data = bm.measure(execute, name=\"tflite\")()\n    else:\n        output_data = execute()\n    return output_data\n"}
{"content": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# <NAME> 07/29/21\n\nimport pickle\nimport numpy as np\nimport argparse\n\n# Only argument is file to compare to expected_output.\nparser = argparse.ArgumentParser(\"Verify output of sample DNAS extension.\")\nparser.add_argument(\"--output_to_verify\",\n                    type=str,\n                    default=None,\n                    help=\"Path to all configs no_checkpoints file for sampled architectures.\")\n\nargs = parser.parse_args()\n\n# Load both files.\nexpected_output = pickle.load(open(\"expected_output\", \"rb\"))\nto_verify_output = pickle.load(open(args.output_to_verify, \"rb\"))\n\n# Compare training losses.\nexpected_output_dictionary = {curr_output[\"save_file_name\"] : curr_output[\"evaluation_information\"][\"training_loss\"] for curr_output in expected_output}\nto_verify_output_dictionary = {curr_output[\"save_file_name\"] : curr_output[\"evaluation_information\"][\"training_loss\"] for curr_output in to_verify_output}\n\nresults_verified = True\nfor save_file_name, expected_loss in expected_output_dictionary.items():\n    # Get the loss to veriy.\n    loss_to_verify = to_verify_output_dictionary[save_file_name]\n\n    # Convert both loss arrays to numpy 1D arrays.\n    expected_loss_np = np.ndarray.flatten(np.array(expected_loss))\n    to_verify_np = np.ndarray.flatten(np.array(loss_to_verify))\n\n    # Compute relative error.\n    relative_error = np.abs(to_verify_np - expected_loss_np)/expected_loss_np\n\n    # Make sure all errors are low enough.\n    for i, curr_error in enumerate(list(relative_error)):\n        if curr_error > 1e-4:\n            print(f\"INCONSISTENCY FOUND WITH RELATIVE ERROR {curr_error} FOR FILE {save_file_name} WITH EXPECTED LOSS {expected_loss_np[i]} AND LOSS TO VERIFY {to_verify_np[i]}.\")\n            results_verified = False\n\nif results_verified:\n    print(f\"RESULTS VERIFIED!\")\nelse:\n    print(f\"DUE TO THE INCONSISTENCIES LISTED ABOVE, RESULTS NOT VERIFIED.\")\n"}
{"content": "# Copyright 2021 Zilliz. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom towhee import Build, Inject\nfrom towhee.utils.yaml_utils import load_yaml, dump_yaml\n\nrender_result = \"\"\"\nname: test_pipeline\nvariables:\n    test_op: 'filip-halt/timm-image-embedding'\n    model_name: 'efficientnet_b3'\noperators:\n- name: embedding_model\n  function: 'towhee/test_op'\n  tag: main\n  init_args:\n    model_name: efficientnet_b3\n  inputs:\n  - df: image\n    name: image\n    col: 0\n  outputs:\n  - df: embedding\n  iter_info:\n    type: map\ndataframes:\n- name: embedding\n  columns:\n  - name: feature_vector\n    vtype: numpy.ndarray\n\"\"\".strip()\n\n\nclass TestTemplateBuild(unittest.TestCase):\n    \"\"\"\n    tests for template build\n    \"\"\"\n    def test_template_build(self):\n        pipe = Build(test_op='towhee/test_op').pipeline('builtin/template_test')\n        self.assertEqual(repr(pipe), render_result)\n\n\nclass TestTemplateInject(unittest.TestCase):\n    \"\"\"\n    tests for template inject\n    \"\"\"\n    def test_template_inject(self):\n        pipe = Inject(embedding_model=dict(function='towhee/test_op')).pipeline('builtin/template_test')\n        self.assertEqual(dump_yaml(load_yaml(repr(pipe))), dump_yaml(load_yaml(render_result)))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
